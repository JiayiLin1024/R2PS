{"nl": "python check if a variable is iterable", "code": ["def _is_iterable(item): \"\"\" Checks if an item is iterable (list, tuple, generator), but not string \"\"\" return isinstance(item, collections.Iterable) and not isinstance(item, six.string_types)", "def is_iterable(obj): \"\"\" Are we being asked to look up a list of things, instead of a single thing? We check for the `__iter__` attribute so that this can cover types that don't have to be known by this module, such as NumPy arrays. Strings, however, should be considered as atomic values to look up, not iterables. The same goes for tuples, since they are immutable and therefore valid entries. We don't need to check for the Python 2 `unicode` type, because it doesn't have an `__iter__` attribute anyway. \"\"\" return ( hasattr(obj, \"__iter__\") and not isinstance(obj, str) and not isinstance(obj, tuple) )", "def is_seq(obj): \"\"\" Returns True if object is not a string but is iterable \"\"\" if not hasattr(obj, '__iter__'): return False if isinstance(obj, basestring): return False return True", "def is_iterable_but_not_string(obj): \"\"\" Determine whether or not obj is iterable but not a string (eg, a list, set, tuple etc). \"\"\" return hasattr(obj, '__iter__') and not isinstance(obj, str) and not isinstance(obj, bytes)", "def is_iterable(value): \"\"\"must be an iterable (list, array, tuple)\"\"\" return isinstance(value, np.ndarray) or isinstance(value, list) or isinstance(value, tuple), value"], "rank": 4}
{"nl": "how to prevent a file from modifying python", "code": ["def example_write_file_to_disk_if_changed(): \"\"\" Try to remove all comments from a file, and save it if changes were made. \"\"\" my_file = FileAsObj('/tmp/example_file.txt') my_file.rm(my_file.egrep('^#')) if my_file.changed: my_file.save()", "def _replace_file(path, content): \"\"\"Writes a file if it doesn't already exist with the same content. This is useful because cargo uses timestamps to decide whether to compile things.\"\"\" if os.path.exists(path): with open(path, 'r') as f: if content == f.read(): print(\"Not overwriting {} because it is unchanged\".format(path), file=sys.stderr) return with open(path, 'w') as f: f.write(content)", "def make_file_read_only(file_path): \"\"\" Removes the write permissions for the given file for owner, groups and others. :param file_path: The file whose privileges are revoked. :raise FileNotFoundError: If the given file does not exist. \"\"\" old_permissions = os.stat(file_path).st_mode os.chmod(file_path, old_permissions & ~WRITE_PERMISSIONS)", "def flipwritable(fn, mode=None): \"\"\" Flip the writability of a file and return the old mode. Returns None if the file is already writable. \"\"\" if os.access(fn, os.W_OK): return None old_mode = os.stat(fn).st_mode os.chmod(fn, stat.S_IWRITE | old_mode) return old_mode", "def make_writeable(filename): \"\"\" Make sure that the file is writeable. Useful if our source is read-only. \"\"\" if not os.access(filename, os.W_OK): st = os.stat(filename) new_permissions = stat.S_IMODE(st.st_mode) | stat.S_IWUSR os.chmod(filename, new_permissions)"], "rank": 9}
{"nl": "how to make functions that returns a function python", "code": ["def map_wrap(f): \"\"\"Wrap standard function to easily pass into 'map' processing. \"\"\" @functools.wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) return wrapper", "def apply(filter): \"\"\"Manufacture decorator that filters return value with given function. ``filter``: Callable that takes a single parameter. \"\"\" def decorator(callable): return lambda *args, **kwargs: filter(callable(*args, **kwargs)) return decorator", "def get_Callable_args_res(clb): \"\"\"Python version independent function to obtain the parameters of a typing.Callable object. Returns as tuple: args, result. Tested with CPython 2.7, 3.5, 3.6 and Jython 2.7.1. \"\"\" try: return clb.__args__, clb.__result__ except AttributeError: # Python 3.6 return clb.__args__[:-1], clb.__args__[-1]", "def return_type(type_name, formatter=None): \"\"\"Specify that this function returns a typed value. Args: type_name (str): A type name known to the global typedargs type system formatter (str): An optional name of a formatting function specified for the type given in type_name. \"\"\" def _returns(func): annotated(func) func.metadata.typed_returnvalue(type_name, formatter) return func return _returns", "def _iterable_to_varargs_method(func): \"\"\"decorator to convert a method taking a iterable to a *args one\"\"\" def wrapped(self, *args, **kwargs): return func(self, args, **kwargs) return wrapped"], "rank": 2}
{"nl": "how to print without moving cursor python", "code": ["def ensure_newline(self): \"\"\" use before any custom printing when using the progress iter to ensure your print statement starts on a new line instead of at the end of a progress line \"\"\" DECTCEM_SHOW = '\\033[?25h' # show cursor AT_END = DECTCEM_SHOW + '\\n' if not self._cursor_at_newline: self.write(AT_END) self._cursor_at_newline = True", "def _cursorLeft(self): \"\"\" Handles \"cursor left\" events \"\"\" if self.cursorPos > 0: self.cursorPos -= 1 sys.stdout.write(console.CURSOR_LEFT) sys.stdout.flush()", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def get_cursor(self): \"\"\"Return the virtual cursor position. The cursor can be moved with the :any:`move` method. Returns: Tuple[int, int]: The (x, y) coordinate of where :any:`print_str` will continue from. .. seealso:: :any:move` \"\"\" x, y = self._cursor width, height = self.parent.get_size() while x >= width: x -= width y += 1 if y >= height and self.scrollMode == 'scroll': y = height - 1 return x, y", "def erase_lines(n=1): \"\"\" Erases n lines from the screen and moves the cursor up to follow \"\"\" for _ in range(n): print(codes.cursor[\"up\"], end=\"\") print(codes.cursor[\"eol\"], end=\"\")"], "rank": 1}
{"nl": "calculate an average in python using the count of an array", "code": ["def average(arr): \"\"\"average of the values, must have more than 0 entries. :param arr: list of numbers :type arr: number[] a number array :return: average :rtype: float \"\"\" if len(arr) == 0: sys.stderr.write(\"ERROR: no content in array to take average\\n\") sys.exit() if len(arr) == 1: return arr[0] return float(sum(arr))/float(len(arr))", "def variance(arr): \"\"\"variance of the values, must have 2 or more entries. :param arr: list of numbers :type arr: number[] a number array :return: variance :rtype: float \"\"\" avg = average(arr) return sum([(float(x)-avg)**2 for x in arr])/float(len(arr)-1)", "def average(iterator): \"\"\"Iterative mean.\"\"\" count = 0 total = 0 for num in iterator: count += 1 total += num return float(total)/count", "def moving_average(a, n): \"\"\"Moving average over one-dimensional array. Parameters ---------- a : np.ndarray One-dimensional array. n : int Number of entries to average over. n=2 means averaging over the currrent the previous entry. Returns ------- An array view storing the moving average. \"\"\" ret = np.cumsum(a, dtype=float) ret[n:] = ret[n:] - ret[:-n] return ret[n - 1:] / n", "def average_arrays(arrays: List[mx.nd.NDArray]) -> mx.nd.NDArray: \"\"\" Take a list of arrays of the same shape and take the element wise average. :param arrays: A list of NDArrays with the same shape that will be averaged. :return: The average of the NDArrays in the same context as arrays[0]. \"\"\" if not arrays: raise ValueError(\"arrays is empty.\") if len(arrays) == 1: return arrays[0] check_condition(all(arrays[0].shape == a.shape for a in arrays), \"nd array shapes do not match\") return mx.nd.add_n(*arrays) / len(arrays)"], "rank": 2}
{"nl": "how to make a 2d array with booleans in python", "code": ["def is_bool_matrix(l): r\"\"\"Checks if l is a 2D numpy array of bools \"\"\" if isinstance(l, np.ndarray): if l.ndim == 2 and (l.dtype == bool): return True return False", "def unpackbools(integers, dtype='L'): \"\"\"Yield booleans unpacking integers of dtype bit-length. >>> list(unpackbools([42], 'B')) [False, True, False, True, False, True, False, False] \"\"\" atoms = ATOMS[dtype] for chunk in integers: for a in atoms: yield not not chunk & a", "def maskIndex(self): \"\"\" Returns a boolean index with True if the value is masked. Always has the same shape as the maksedArray.data, event if the mask is a single boolan. \"\"\" if isinstance(self.mask, bool): return np.full(self.data.shape, self.mask, dtype=np.bool) else: return self.mask", "def abs_img(img): \"\"\" Return an image with the binarised version of the data of `img`.\"\"\" bool_img = np.abs(read_img(img).get_data()) return bool_img.astype(int)", "def _check_and_convert_bools(self): \"\"\"Replace boolean variables by the characters 'F'/'T' \"\"\" replacements = { True: 'T', False: 'F', } for key in self.bools: if isinstance(self[key], bool): self[key] = replacements[self[key]]"], "rank": 1}
{"nl": "check if a variable is an array python", "code": ["def is_integer_array(val): \"\"\" Checks whether a variable is a numpy integer array. Parameters ---------- val The variable to check. Returns ------- bool True if the variable is a numpy integer array. Otherwise False. \"\"\" return is_np_array(val) and issubclass(val.dtype.type, np.integer)", "def is_int_vector(l): r\"\"\"Checks if l is a numpy array of integers \"\"\" if isinstance(l, np.ndarray): if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'): return True return False", "def is_array(self, key): \"\"\"Return True if variable is a numpy array\"\"\" data = self.model.get_data() return isinstance(data[key], (ndarray, MaskedArray))", "def is_iterable(value): \"\"\"must be an iterable (list, array, tuple)\"\"\" return isinstance(value, np.ndarray) or isinstance(value, list) or isinstance(value, tuple), value", "def is_vector(inp): \"\"\" Returns true if the input can be interpreted as a 'true' vector Note ---- Does only check dimensions, not if type is numeric Parameters ---------- inp : numpy.ndarray or something that can be converted into ndarray Returns ------- Boolean True for vectors: ndim = 1 or ndim = 2 and shape of one axis = 1 False for all other arrays \"\"\" inp = np.asarray(inp) nr_dim = np.ndim(inp) if nr_dim == 1: return True elif (nr_dim == 2) and (1 in inp.shape): return True else: return False"], "rank": 1}
{"nl": "python 3 remove directory recursively", "code": ["def rrmdir(directory): \"\"\" Recursivly delete a directory :param directory: directory to remove \"\"\" for root, dirs, files in os.walk(directory, topdown=False): for name in files: os.remove(os.path.join(root, name)) for name in dirs: os.rmdir(os.path.join(root, name)) os.rmdir(directory)", "def clean_out_dir(directory): \"\"\" Delete all the files and subdirectories in a directory. \"\"\" if not isinstance(directory, path): directory = path(directory) for file_path in directory.files(): file_path.remove() for dir_path in directory.dirs(): dir_path.rmtree()", "def _cleanup(path: str) -> None: \"\"\"Cleanup temporary directory.\"\"\" if os.path.isdir(path): shutil.rmtree(path)", "def safe_rmtree(directory): \"\"\"Delete a directory if it's present. If it's not present, no-op.\"\"\" if os.path.exists(directory): shutil.rmtree(directory, True)", "def _clear_dir(dirName): \"\"\" Remove a directory and it contents. Ignore any failures. \"\"\" # If we got here, clear dir for fname in os.listdir(dirName): try: os.remove( os.path.join(dirName, fname) ) except Exception: pass try: os.rmdir(dirName) except Exception: pass"], "rank": 1}
{"nl": "x limit and y limit in python", "code": ["def _get_xy_scaling_parameters(self): \"\"\"Get the X/Y coordinate limits for the full resulting image\"\"\" return self.mx, self.bx, self.my, self.by", "def set_xlimits(self, min=None, max=None): \"\"\"Set limits for the x-axis. :param min: minimum value to be displayed. If None, it will be calculated. :param max: maximum value to be displayed. If None, it will be calculated. \"\"\" self.limits['xmin'] = min self.limits['xmax'] = max", "def set_limits(self, min_=None, max_=None): \"\"\" Sets limits for this config value If the resulting integer is outside those limits, an exception will be raised :param min_: minima :param max_: maxima \"\"\" self._min, self._max = min_, max_", "def values(self): \"\"\"Gets the user enter max and min values of where the raster points should appear on the y-axis :returns: (float, float) -- (min, max) y-values to bound the raster plot by \"\"\" lower = float(self.lowerSpnbx.value()) upper = float(self.upperSpnbx.value()) return (lower, upper)", "def set_xlimits_widgets(self, set_min=True, set_max=True): \"\"\"Populate axis limits GUI with current plot values.\"\"\" xmin, xmax = self.tab_plot.ax.get_xlim() if set_min: self.w.x_lo.set_text('{0}'.format(xmin)) if set_max: self.w.x_hi.set_text('{0}'.format(xmax))"], "rank": 4}
{"nl": "python string formatting in list", "code": ["def list_formatter(handler, item, value): \"\"\"Format list.\"\"\" return u', '.join(str(v) for v in value)", "def format_line(data, linestyle): \"\"\"Formats a list of elements using the given line style\"\"\" return linestyle.begin + linestyle.sep.join(data) + linestyle.end", "def list_to_str(list, separator=','): \"\"\" >>> list = [0, 0, 7] >>> list_to_str(list) '0,0,7' \"\"\" list = [str(x) for x in list] return separator.join(list)", "def _make_cmd_list(cmd_list): \"\"\" Helper function to easily create the proper json formated string from a list of strs :param cmd_list: list of strings :return: str json formatted \"\"\" cmd = '' for i in cmd_list: cmd = cmd + '\"' + i + '\",' cmd = cmd[:-1] return cmd", "def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res"], "rank": 1}
{"nl": "get distinct in list python", "code": ["def distinct(xs): \"\"\"Get the list of distinct values with preserving order.\"\"\" # don't use collections.OrderedDict because we do support Python 2.6 seen = set() return [x for x in xs if x not in seen and not seen.add(x)]", "def distinct(l): \"\"\" Return a list where the duplicates have been removed. Args: l (list): the list to filter. Returns: list: the same list without duplicates. \"\"\" seen = set() seen_add = seen.add return (_ for _ in l if not (_ in seen or seen_add(_)))", "def unique(input_list): \"\"\" Return a list of unique items (similar to set functionality). Parameters ---------- input_list : list A list containg some items that can occur more than once. Returns ------- list A list with only unique occurances of an item. \"\"\" output = [] for item in input_list: if item not in output: output.append(item) return output", "def fast_distinct(self): \"\"\" Because standard distinct used on the all fields are very slow and works only with PostgreSQL database this method provides alternative to the standard distinct method. :return: qs with unique objects \"\"\" return self.model.objects.filter(pk__in=self.values_list('pk', flat=True))", "def uniq(seq): \"\"\" Return a copy of seq without duplicates. \"\"\" seen = set() return [x for x in seq if str(x) not in seen and not seen.add(str(x))]"], "rank": 1}
{"nl": "python read dot file", "code": ["def graph_from_dot_file(path): \"\"\"Load graph as defined by a DOT file. The file is assumed to be in DOT format. It will be loaded, parsed and a Dot class will be returned, representing the graph. \"\"\" fd = file(path, 'rb') data = fd.read() fd.close() return graph_from_dot_data(data)", "def depgraph_to_dotsrc(dep_graph, show_cycles, nodot, reverse): \"\"\"Convert the dependency graph (DepGraph class) to dot source code. \"\"\" if show_cycles: dotsrc = cycles2dot(dep_graph, reverse=reverse) elif not nodot: dotsrc = dep2dot(dep_graph, reverse=reverse) else: dotsrc = None return dotsrc", "def read_dict_from_file(file_path): \"\"\" Read a dictionary of strings from a file \"\"\" with open(file_path) as file: lines = file.read().splitlines() obj = {} for line in lines: key, value = line.split(':', maxsplit=1) obj[key] = eval(value) return obj", "def export_to_dot(self, filename: str = 'output') -> None: \"\"\" Export the graph to the dot file \"filename.dot\". \"\"\" with open(filename + '.dot', 'w') as output: output.write(self.as_dot())", "def cmd_dot(conf: Config): \"\"\"Print out a neat targets dependency tree based on requested targets. Use graphviz to render the dot file, e.g.: > ybt dot :foo :bar | dot -Tpng -o graph.png \"\"\" build_context = BuildContext(conf) populate_targets_graph(build_context, conf) if conf.output_dot_file is None: write_dot(build_context, conf, sys.stdout) else: with open(conf.output_dot_file, 'w') as out_file: write_dot(build_context, conf, out_file)"], "rank": 1}
{"nl": "traverse all but the last item in a list in python", "code": ["def butlast(iterable): \"\"\"Yield all items from ``iterable`` except the last one. >>> list(butlast(['spam', 'eggs', 'ham'])) ['spam', 'eggs'] >>> list(butlast(['spam'])) [] >>> list(butlast([])) [] \"\"\" iterable = iter(iterable) try: first = next(iterable) except StopIteration: return for second in iterable: yield first first = second", "def walk_tree(root): \"\"\"Pre-order depth-first\"\"\" yield root for child in root.children: for el in walk_tree(child): yield el", "def __iter__(self): \"\"\" Iterate through tree, leaves first following http://stackoverflow.com/questions/6914803/python-iterator-through-tree-with-list-of-children \"\"\" for node in chain(*imap(iter, self.children)): yield node yield self", "def empty_tree(input_list): \"\"\"Recursively iterate through values in nested lists.\"\"\" for item in input_list: if not isinstance(item, list) or not empty_tree(item): return False return True", "def do_last(environment, seq): \"\"\"Return the last item of a sequence.\"\"\" try: return next(iter(reversed(seq))) except StopIteration: return environment.undefined('No last item, sequence was empty.')"], "rank": 1}
{"nl": "python buffer is smaller than requested size", "code": ["def _read_stream_for_size(stream, buf_size=65536): \"\"\"Reads a stream discarding the data read and returns its size.\"\"\" size = 0 while True: buf = stream.read(buf_size) size += len(buf) if not buf: break return size", "def isFull(self): \"\"\" Returns True if the response buffer is full, and False otherwise. The buffer is full if either (1) the number of items in the value list is >= pageSize or (2) the total length of the serialised elements in the page is >= maxBufferSize. If page_size or max_response_length were not set in the request then they're not checked. \"\"\" return ( (self._pageSize > 0 and self._numElements >= self._pageSize) or (self._bufferSize >= self._maxBufferSize) )", "def _nbytes(buf): \"\"\"Return byte-size of a memoryview or buffer.\"\"\" if isinstance(buf, memoryview): if PY3: # py3 introduces nbytes attribute return buf.nbytes else: # compute nbytes on py2 size = buf.itemsize for dim in buf.shape: size *= dim return size else: # not a memoryview, raw bytes/ py2 buffer return len(buf)", "def full(self): \"\"\"Return ``True`` if the queue is full, ``False`` otherwise (not reliable!). Only applicable if :attr:`maxsize` is set. \"\"\" return self.maxsize and len(self.list) >= self.maxsize or False", "def _check_limit(self): \"\"\"Intenal method: check if current cache size exceeds maximum cache size and pop the oldest item in this case\"\"\" # First compress self._compress() # Then check the max size if len(self._store) >= self._max_size: self._store.popitem(last=False)"], "rank": 1}
{"nl": "how to read file from aws s3 using python s3fs", "code": ["def s3_get(url: str, temp_file: IO) -> None: \"\"\"Pull a file directly from S3.\"\"\" s3_resource = boto3.resource(\"s3\") bucket_name, s3_path = split_s3_path(url) s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)", "def download_file_from_bucket(self, bucket, file_path, key): \"\"\" Download file from S3 Bucket \"\"\" with open(file_path, 'wb') as data: self.__s3.download_fileobj(bucket, key, data) return file_path", "def download_file(bucket_name, path, target, sagemaker_session): \"\"\"Download a Single File from S3 into a local path Args: bucket_name (str): S3 bucket name path (str): file path within the bucket target (str): destination directory for the downloaded file. sagemaker_session (:class:`sagemaker.session.Session`): a sagemaker session to interact with S3. \"\"\" path = path.lstrip('/') boto_session = sagemaker_session.boto_session s3 = boto_session.resource('s3') bucket = s3.Bucket(bucket_name) bucket.download_file(path, target)", "def get_as_bytes(self, s3_path): \"\"\" Get the contents of an object stored in S3 as bytes :param s3_path: URL for target S3 location :return: File contents as pure bytes \"\"\" (bucket, key) = self._path_to_bucket_and_key(s3_path) obj = self.s3.Object(bucket, key) contents = obj.get()['Body'].read() return contents", "def read_key(self, key, bucket_name=None): \"\"\" Reads a key from S3 :param key: S3 key that will point to the file :type key: str :param bucket_name: Name of the bucket in which the file is stored :type bucket_name: str \"\"\" obj = self.get_key(key, bucket_name) return obj.get()['Body'].read().decode('utf-8')"], "rank": 1}
{"nl": "python wrapper function for a method", "code": ["def Proxy(f): \"\"\"A helper to create a proxy method in a class.\"\"\" def Wrapped(self, *args): return getattr(self, f)(*args) return Wrapped", "def method(func): \"\"\"Wrap a function as a method.\"\"\" attr = abc.abstractmethod(func) attr.__imethod__ = True return attr", "def __getattr__(self, name): \"\"\"Return wrapper to named api method.\"\"\" return functools.partial(self._obj.request, self._api_prefix + name)", "def __getattr__(self, item: str) -> Callable: \"\"\"Get a callable that sends the actual API request internally.\"\"\" return functools.partial(self.call_action, item)", "def command(name, mode): \"\"\" Label a method as a command with name. \"\"\" def decorator(fn): commands[name] = fn.__name__ _Client._addMethod(fn.__name__, name, mode) return fn return decorator"], "rank": 1}
{"nl": "how to check value is int or float python", "code": ["def is_integer(value: Any) -> bool: \"\"\"Return true if a value is an integer number.\"\"\" return (isinstance(value, int) and not isinstance(value, bool)) or ( isinstance(value, float) and isfinite(value) and int(value) == value )", "def is_float(value): \"\"\"must be a float\"\"\" return isinstance(value, float) or isinstance(value, int) or isinstance(value, np.float64), float(value)", "def is_numeric(value): \"\"\"Test if a value is numeric. \"\"\" return type(value) in [ int, float, np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64, np.float128 ]", "def test_value(self, value): \"\"\"Test if value is an instance of float.\"\"\" if not isinstance(value, float): raise ValueError('expected float value: ' + str(type(value)))", "def test_value(self, value): \"\"\"Test if value is an instance of int.\"\"\" if not isinstance(value, int): raise ValueError('expected int value: ' + str(type(value)))"], "rank": 2}
{"nl": "how to join two data frames python", "code": ["def cross_join(df1, df2): \"\"\" Return a dataframe that is a cross between dataframes df1 and df2 ref: https://github.com/pydata/pandas/issues/5401 \"\"\" if len(df1) == 0: return df2 if len(df2) == 0: return df1 # Add as lists so that the new index keeps the items in # the order that they are added together all_columns = pd.Index(list(df1.columns) + list(df2.columns)) df1['key'] = 1 df2['key'] = 1 return pd.merge(df1, df2, on='key').loc[:, all_columns]", "def _join(verb): \"\"\" Join helper \"\"\" data = pd.merge(verb.x, verb.y, **verb.kwargs) # Preserve x groups if isinstance(verb.x, GroupedDataFrame): data.plydata_groups = list(verb.x.plydata_groups) return data", "def intersect(self, other): \"\"\" Return a new :class:`DataFrame` containing rows only in both this frame and another frame. This is equivalent to `INTERSECT` in SQL. \"\"\" return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)", "def merge(left, right, how='inner', key=None, left_key=None, right_key=None, left_as='left', right_as='right'): \"\"\" Performs a join using the union join function. \"\"\" return join(left, right, how, key, left_key, right_key, join_fn=make_union_join(left_as, right_as))", "def get_join_cols(by_entry): \"\"\" helper function used for joins builds left and right join list for join function \"\"\" left_cols = [] right_cols = [] for col in by_entry: if isinstance(col, str): left_cols.append(col) right_cols.append(col) else: left_cols.append(col[0]) right_cols.append(col[1]) return left_cols, right_cols"], "rank": 1}
{"nl": "how to execute a parser script in python", "code": ["def main(pargs): \"\"\"This should only be used for testing. The primary mode of operation is as an imported library. \"\"\" input_file = sys.argv[1] fp = ParseFileLineByLine(input_file) for i in fp: print(i)", "def cli_run(): \"\"\"docstring for argparse\"\"\" parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow') parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+') parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda') args = parser.parse_args() main(args)", "def main(argv=None): \"\"\"Main command line interface.\"\"\" if argv is None: argv = sys.argv[1:] cli = CommandLineTool() return cli.run(argv)", "def parsed_args(): parser = argparse.ArgumentParser(description=\"\"\"python runtime functions\"\"\", epilog=\"\") parser.add_argument('command',nargs='*', help=\"Name of the function to run with arguments\") args = parser.parse_args() return (args, parser)", "def doc_parser(): \"\"\"Utility function to allow getting the arguments for a single command, for Sphinx documentation\"\"\" parser = argparse.ArgumentParser( prog='ambry', description='Ambry {}. Management interface for ambry, libraries ' 'and repositories. '.format(ambry._meta.__version__)) return parser"], "rank": 2}
{"nl": "python add default value", "code": ["def set_default(self_,param_name,value): \"\"\" Set the default value of param_name. Equivalent to setting param_name on the class. \"\"\" cls = self_.cls setattr(cls,param_name,value)", "def setdefault(obj, field, default): \"\"\"Set an object's field to default if it doesn't have a value\"\"\" setattr(obj, field, getattr(obj, field, default))", "def setdefault(self, name: str, default: Any=None) -> Any: \"\"\"Set an attribute with a default value.\"\"\" return self.__dict__.setdefault(name, default)", "def set_default(self, key, value): \"\"\"Set the default value for this key. Default only used when no value is provided by the user via arg, config or env. \"\"\" k = self._real_key(key.lower()) self._defaults[k] = value", "def set_default(self, section, option, default): \"\"\"If the option did not exist, create a default value.\"\"\" if not self.parser.has_option(section, option): self.parser.set(section, option, default)"], "rank": 2}
{"nl": "how to remove empty elements in a list python", "code": ["def remove_empty_text(utterances: List[Utterance]) -> List[Utterance]: \"\"\"Remove empty utterances from a list of utterances Args: utterances: The list of utterance we are processing \"\"\" return [utter for utter in utterances if utter.text.strip() != \"\"]", "def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def de_blank(val): \"\"\"Remove blank elements in `val` and return `ret`\"\"\" ret = list(val) if type(val) == list: for idx, item in enumerate(val): if item.strip() == '': ret.remove(item) else: ret[idx] = item.strip() return ret", "def clear_list_value(self, value): \"\"\" Clean the argument value to eliminate None or Falsy values if needed. \"\"\" # Don't go any further: this value is empty. if not value: return self.empty_value # Clean empty items if wanted if self.clean_empty: value = [v for v in value if v] return value or self.empty_value", "def unique(seq): \"\"\"Return the unique elements of a collection even if those elements are unhashable and unsortable, like dicts and sets\"\"\" cleaned = [] for each in seq: if each not in cleaned: cleaned.append(each) return cleaned"], "rank": 5}
{"nl": "validating an input is an integer python", "code": ["def validate_int(value): \"\"\" Integer validator \"\"\" if value and not isinstance(value, int): try: int(str(value)) except (TypeError, ValueError): raise ValidationError('not a valid number') return value", "def test_value(self, value): \"\"\"Test if value is an instance of int.\"\"\" if not isinstance(value, int): raise ValueError('expected int value: ' + str(type(value)))", "def clean_int(x) -> int: \"\"\" Returns its parameter as an integer, or raises ``django.forms.ValidationError``. \"\"\" try: return int(x) except ValueError: raise forms.ValidationError( \"Cannot convert to integer: {}\".format(repr(x)))", "def is_int_type(val): \"\"\"Return True if `val` is of integer type.\"\"\" try: # Python 2 return isinstance(val, (int, long)) except NameError: # Python 3 return isinstance(val, int)", "def check_int(integer): \"\"\" Check if number is integer or not. :param integer: Number as str :return: Boolean \"\"\" if not isinstance(integer, str): return False if integer[0] in ('-', '+'): return integer[1:].isdigit() return integer.isdigit()"], "rank": 3}
{"nl": "how to detect if list in python has no elements", "code": ["def contains_empty(features): \"\"\"Check features data are not empty :param features: The features data to check. :type features: list of numpy arrays. :return: True if one of the array is empty, False else. \"\"\" if not features: return True for feature in features: if feature.shape[0] == 0: return True return False", "def is_empty(self): \"\"\"Returns True if this node has no children, or if all of its children are ParseNode instances and are empty. \"\"\" return all(isinstance(c, ParseNode) and c.is_empty for c in self.children)", "def _get_non_empty_list(cls, iter): \"\"\"Return a list of the input, excluding all ``None`` values.\"\"\" res = [] for value in iter: if hasattr(value, 'items'): value = cls._get_non_empty_dict(value) or None if value is not None: res.append(value) return res", "def match_empty(self, el): \"\"\"Check if element is empty (if requested).\"\"\" is_empty = True for child in self.get_children(el, tags=False): if self.is_tag(child): is_empty = False break elif self.is_content_string(child) and RE_NOT_EMPTY.search(child): is_empty = False break return is_empty", "def is_empty_shape(sh: ShExJ.Shape) -> bool: \"\"\" Determine whether sh has any value \"\"\" return sh.closed is None and sh.expression is None and sh.extra is None and \\ sh.semActs is None"], "rank": 1}
{"nl": "how to move cursor to the next line in python", "code": ["def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def cursor_up(self, count=1): \"\"\" (for multiline edit). Move cursor to the previous line. \"\"\" original_column = self.preferred_column or self.document.cursor_position_col self.cursor_position += self.document.get_cursor_up_position( count=count, preferred_column=original_column) # Remember the original column for the next up/down movement. self.preferred_column = original_column", "def go_to_line(self, line): \"\"\" Moves the text cursor to given line. :param line: Line to go to. :type line: int :return: Method success. :rtype: bool \"\"\" cursor = self.textCursor() cursor.setPosition(self.document().findBlockByNumber(line - 1).position()) self.setTextCursor(cursor) return True", "def move_up(lines=1, file=sys.stdout): \"\"\" Move the cursor up a number of lines. Esc[ValueA: Moves the cursor up by the specified number of lines without changing columns. If the cursor is already on the top line, ANSI.SYS ignores this sequence. \"\"\" move.up(lines).write(file=file)", "def _go_to_line(editor, line): \"\"\" Move cursor to this line in the current buffer. \"\"\" b = editor.application.current_buffer b.cursor_position = b.document.translate_row_col_to_index(max(0, int(line) - 1), 0)"], "rank": 5}
{"nl": "definition python nonetype to int", "code": ["def is_int_type(val): \"\"\"Return True if `val` is of integer type.\"\"\" try: # Python 2 return isinstance(val, (int, long)) except NameError: # Python 3 return isinstance(val, int)", "def is_integer(obj): \"\"\"Is this an integer. :param object obj: :return: \"\"\" if PYTHON3: return isinstance(obj, int) return isinstance(obj, (int, long))", "def _to_numeric(val): \"\"\" Helper function for conversion of various data types into numeric representation. \"\"\" if isinstance(val, (int, float, datetime.datetime, datetime.timedelta)): return val return float(val)", "def cast_int(x): \"\"\" Cast unknown type into integer :param any x: :return int: \"\"\" try: x = int(x) except ValueError: try: x = x.strip() except AttributeError as e: logger_misc.warn(\"parse_str: AttributeError: String not number or word, {}, {}\".format(x, e)) return x", "def _isint(string): \"\"\" >>> _isint(\"123\") True >>> _isint(\"123.45\") False \"\"\" return type(string) is int or \\ (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \\ _isconvertible(int, string)"], "rank": 7}
{"nl": "how to truncate to two decimals in python", "code": ["def truncate(value: Decimal, n_digits: int) -> Decimal: \"\"\"Truncates a value to a number of decimals places\"\"\" return Decimal(math.trunc(value * (10 ** n_digits))) / (10 ** n_digits)", "def price_rounding(price, decimals=2): \"\"\"Takes a decimal price and rounds to a number of decimal places\"\"\" try: exponent = D('.' + decimals * '0') except InvalidOperation: # Currencies with no decimal places, ex. JPY, HUF exponent = D() return price.quantize(exponent, rounding=ROUND_UP)", "def round_to_x_digits(number, digits): \"\"\" Returns 'number' rounded to 'digits' digits. \"\"\" return round(number * math.pow(10, digits)) / math.pow(10, digits)", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0", "def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)"], "rank": 1}
{"nl": "python enable executable permisions on file", "code": ["def set_executable(filename): \"\"\"Set the exectuable bit on the given filename\"\"\" st = os.stat(filename) os.chmod(filename, st.st_mode | stat.S_IEXEC)", "def _MakeExecutable(self, metadata_script): \"\"\"Add executable permissions to a file. Args: metadata_script: string, the path to the executable file. \"\"\" mode = os.stat(metadata_script).st_mode os.chmod(metadata_script, mode | stat.S_IEXEC)", "def add_exec_permission_to(target_file): \"\"\"Add executable permissions to the file :param target_file: the target file whose permission to be changed \"\"\" mode = os.stat(target_file).st_mode os.chmod(target_file, mode | stat.S_IXUSR)", "def chmod_add_excute(filename): \"\"\" Adds execute permission to file. :param filename: :return: \"\"\" st = os.stat(filename) os.chmod(filename, st.st_mode | stat.S_IEXEC)", "def make_executable(script_path): \"\"\"Make `script_path` executable. :param script_path: The file to change \"\"\" status = os.stat(script_path) os.chmod(script_path, status.st_mode | stat.S_IEXEC)"], "rank": 3}
{"nl": "how to calculate manhattan distance in python", "code": ["def _manhattan_distance(vec_a, vec_b): \"\"\"Return manhattan distance between two lists of numbers.\"\"\" if len(vec_a) != len(vec_b): raise ValueError('len(vec_a) must equal len(vec_b)') return sum(map(lambda a, b: abs(a - b), vec_a, vec_b))", "def manhattan_distance_numpy(object1, object2): \"\"\"! @brief Calculate Manhattan distance between two objects using numpy. @param[in] object1 (array_like): The first array_like object. @param[in] object2 (array_like): The second array_like object. @return (double) Manhattan distance between two objects. \"\"\" return numpy.sum(numpy.absolute(object1 - object2), axis=1).T", "def manhattan(h1, h2): # # 7 us @array, 31 us @list \\w 100 bins r\"\"\" Equal to Minowski distance with :math:`p=1`. See also -------- minowski \"\"\" h1, h2 = __prepare_histogram(h1, h2) return scipy.sum(scipy.absolute(h1 - h2))", "def distL1(x1,y1,x2,y2): \"\"\"Compute the L1-norm (Manhattan) distance between two points. The distance is rounded to the closest integer, for compatibility with the TSPLIB convention. The two points are located on coordinates (x1,y1) and (x2,y2), sent as parameters\"\"\" return int(abs(x2-x1) + abs(y2-y1)+.5)", "def levenshtein_distance_metric(a, b): \"\"\" 1 - farthest apart (same number of words, all diff). 0 - same\"\"\" return (levenshtein_distance(a, b) / (2.0 * max(len(a), len(b), 1)))"], "rank": 1}
{"nl": "python longest directed path", "code": ["def dag_longest_path(graph, source, target): \"\"\" Finds the longest path in a dag between two nodes \"\"\" if source == target: return [source] allpaths = nx.all_simple_paths(graph, source, target) longest_path = [] for l in allpaths: if len(l) > len(longest_path): longest_path = l return longest_path", "def path_distance(points): \"\"\" Compute the path distance from given set of points \"\"\" vecs = np.diff(points, axis=0)[:, :3] d2 = [np.dot(p, p) for p in vecs] return np.sum(np.sqrt(d2))", "def maxDepth(self, currentDepth=0): \"\"\"Compute the depth of the longest branch of the tree\"\"\" if not any((self.left, self.right)): return currentDepth result = 0 for child in (self.left, self.right): if child: result = max(result, child.maxDepth(currentDepth + 1)) return result", "def _dfs_cycle_detect(graph, node, path, visited_nodes): \"\"\" search graph for cycle using DFS continuing from node path contains the list of visited nodes currently on the stack visited_nodes is the set of already visited nodes :param graph: :param node: :param path: :param visited_nodes: :return: \"\"\" visited_nodes.add(node) for target in graph[node]: if target in path: # cycle found => return current path return path + [target] else: return _dfs_cycle_detect(graph, target, path + [target], visited_nodes) return None", "def get_longest_orf(orfs): \"\"\"Find longest ORF from the given list of ORFs.\"\"\" sorted_orf = sorted(orfs, key=lambda x: len(x['sequence']), reverse=True)[0] return sorted_orf"], "rank": 1}
{"nl": "how to tell what you python path is", "code": ["def path_for_import(name): \"\"\" Returns the directory path for the given package or module. \"\"\" return os.path.dirname(os.path.abspath(import_module(name).__file__))", "def get_module_path(modname): \"\"\"Return module *modname* base path\"\"\" return osp.abspath(osp.dirname(sys.modules[modname].__file__))", "def getScriptLocation(): \"\"\"Helper function to get the location of a Python file.\"\"\" location = os.path.abspath(\"./\") if __file__.rfind(\"/\") != -1: location = __file__[:__file__.rfind(\"/\")] return location", "def getpackagepath(): \"\"\" *Get the root path for this python package - used in unit testing code* \"\"\" moduleDirectory = os.path.dirname(__file__) packagePath = os.path.dirname(__file__) + \"/../\" return packagePath", "def get_base_dir(): \"\"\" Return the base directory \"\"\" return os.path.split(os.path.abspath(os.path.dirname(__file__)))[0]"], "rank": 4}
{"nl": "bin data into integers python", "code": ["def intToBin(i): \"\"\" Integer to two bytes \"\"\" # divide in two parts (bytes) i1 = i % 256 i2 = int(i / 256) # make string (little endian) return i.to_bytes(2, byteorder='little')", "def intToBin(i): \"\"\" Integer to two bytes \"\"\" # devide in two parts (bytes) i1 = i % 256 i2 = int(i / 256) # make string (little endian) return chr(i1) + chr(i2)", "def to_bin(data, width): \"\"\" Convert an unsigned integer to a numpy binary array with the first element the MSB and the last element the LSB. \"\"\" data_str = bin(data & (2**width-1))[2:].zfill(width) return [int(x) for x in tuple(data_str)]", "def convert_bytes_to_ints(in_bytes, num): \"\"\"Convert a byte array into an integer array. The number of bytes forming an integer is defined by num :param in_bytes: the input bytes :param num: the number of bytes per int :return the integer array\"\"\" dt = numpy.dtype('>i' + str(num)) return numpy.frombuffer(in_bytes, dt)", "def bytes_to_bits(bytes_): \"\"\"Convert bytes to a list of bits \"\"\" res = [] for x in bytes_: if not isinstance(x, int): x = ord(x) res += byte_to_bits(x) return res"], "rank": 3}
{"nl": "how do i cut off leading zeroes in python", "code": ["def __remove_trailing_zeros(self, collection): \"\"\"Removes trailing zeroes from indexable collection of numbers\"\"\" index = len(collection) - 1 while index >= 0 and collection[index] == 0: index -= 1 return collection[:index + 1]", "def remove_leading_zeros(num: str) -> str: \"\"\" Strips zeros while handling -, M, and empty strings \"\"\" if not num: return num if num.startswith('M'): ret = 'M' + num[1:].lstrip('0') elif num.startswith('-'): ret = '-' + num[1:].lstrip('0') else: ret = num.lstrip('0') return '0' if ret in ('', 'M', '-') else ret", "def _remove_blank(l): \"\"\" Removes trailing zeros in the list of integers and returns a new list of integers\"\"\" ret = [] for i, _ in enumerate(l): if l[i] == 0: break ret.append(l[i]) return ret", "def drop_trailing_zeros_decimal(num): \"\"\" Drops the trailinz zeros from decimal value. Returns a string \"\"\" out = str(num) return out.rstrip('0').rstrip('.') if '.' in out else out", "def drop_trailing_zeros(num): \"\"\" Drops the trailing zeros in a float that is printed. \"\"\" txt = '%f' %(num) txt = txt.rstrip('0') if txt.endswith('.'): txt = txt[:-1] return txt"], "rank": 1}
{"nl": "accesing elements in a heap in python", "code": ["def fix(h, i): \"\"\"Rearrange the heap after the item at position i got updated.\"\"\" down(h, i, h.size()) up(h, i)", "def push(h, x): \"\"\"Push a new value into heap.\"\"\" h.push(x) up(h, h.size()-1)", "def _heapify_max(x): \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\" n = len(x) for i in reversed(range(n//2)): _siftup_max(x, i)", "def _heappush_max(heap, item): \"\"\" why is this not in heapq \"\"\" heap.append(item) heapq._siftdown_max(heap, 0, len(heap) - 1)", "def heappush_max(heap, item): \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\" heap.append(item) _siftdown_max(heap, 0, len(heap) - 1)"], "rank": 1}
{"nl": "auto crop particular object from image in python", "code": ["def crop_box(im, box=False, **kwargs): \"\"\"Uses box coordinates to crop an image without resizing it first.\"\"\" if box: im = im.crop(box) return im", "def resize(self): \"\"\" Get target size for a cropped image and do the resizing if we got anything usable. \"\"\" resized_size = self.get_resized_size() if not resized_size: return self.image = self.image.resize(resized_size, Image.ANTIALIAS)", "def resize(self, size): \"\"\"Return a new Image instance with the given size.\"\"\" return Image(self.pil_image.resize(size, PIL.Image.ANTIALIAS))", "def scale_min(im, targ, interpolation=cv2.INTER_AREA): \"\"\" Scale the image so that the smallest axis is of size targ. Arguments: im (array): image targ (int): target size \"\"\" r,c,*_ = im.shape ratio = targ/min(r,c) sz = (scale_to(c, ratio, targ), scale_to(r, ratio, targ)) return cv2.resize(im, sz, interpolation=interpolation)", "def resize_image(self, data, size): \"\"\" Resizes the given image to fit inside a box of the given size. \"\"\" from machina.core.compat import PILImage as Image image = Image.open(BytesIO(data)) # Resize! image.thumbnail(size, Image.ANTIALIAS) string = BytesIO() image.save(string, format='PNG') return string.getvalue()"], "rank": 1}
{"nl": "bi linear interpolation in python", "code": ["def _linear_interpolation(x, X, Y): \"\"\"Given two data points [X,Y], linearly interpolate those at x. \"\"\" return (Y[1] * (x - X[0]) + Y[0] * (X[1] - x)) / (X[1] - X[0])", "def lin_interp(x, rangeX, rangeY): \"\"\" Interpolate linearly variable x in rangeX onto rangeY. \"\"\" s = (x - rangeX[0]) / mag(rangeX[1] - rangeX[0]) y = rangeY[0] * (1 - s) + rangeY[1] * s return y", "def _linearInterpolationTransformMatrix(matrix1, matrix2, value): \"\"\" Linear, 'oldstyle' interpolation of the transform matrix.\"\"\" return tuple(_interpolateValue(matrix1[i], matrix2[i], value) for i in range(len(matrix1)))", "def value(self, progress_indicator): \"\"\" Interpolate linearly between start and end \"\"\" return interpolate.interpolate_linear_single(self.initial_value, self.final_value, progress_indicator)", "def interpolate(f1: float, f2: float, factor: float) -> float: \"\"\" Linearly interpolate between two float values. \"\"\" return f1 + (f2 - f1) * factor"], "rank": 1}
{"nl": "python check for all numeric types", "code": ["def is_numeric_dtype(dtype): \"\"\"Return ``True`` if ``dtype`` is a numeric type.\"\"\" dtype = np.dtype(dtype) return np.issubsctype(getattr(dtype, 'base', None), np.number)", "def _is_numeric(self, values): \"\"\"Check to be sure values are numbers before doing numerical operations.\"\"\" if len(values) > 0: assert isinstance(values[0], (float, int)), \\ \"values must be numbers to perform math operations. Got {}\".format( type(values[0])) return True", "def is_numeric(value): \"\"\"Test if a value is numeric. \"\"\" return type(value) in [ int, float, np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64, np.float128 ]", "def is_strict_numeric(n: Node) -> bool: \"\"\" numeric denotes typed literals with datatypes xsd:integer, xsd:decimal, xsd:float, and xsd:double. \"\"\" return is_typed_literal(n) and cast(Literal, n).datatype in [XSD.integer, XSD.decimal, XSD.float, XSD.double]", "def is_number(obj): \"\"\"Check if obj is number.\"\"\" return isinstance(obj, (int, float, np.int_, np.float_))"], "rank": 1}
{"nl": "join list of strings with character python", "code": ["def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res", "def commajoin_as_strings(iterable): \"\"\" Join the given iterable with ',' \"\"\" return _(u',').join((six.text_type(i) for i in iterable))", "def delimited(items, character='|'): \"\"\"Returns a character delimited version of the provided list as a Python string\"\"\" return '|'.join(items) if type(items) in (list, tuple, set) else items", "def join(mapping, bind, values): \"\"\" Merge all the strings. Put space between them. \"\"\" return [' '.join([six.text_type(v) for v in values if v is not None])]", "def _grammatical_join_filter(l, arg=None): \"\"\" :param l: List of strings to join :param arg: A pipe-separated list of final_join (\" and \") and initial_join (\", \") strings. For example :return: A string that grammatically concatenates the items in the list. \"\"\" if not arg: arg = \" and |, \" try: final_join, initial_joins = arg.split(\"|\") except ValueError: final_join = arg initial_joins = \", \" return grammatical_join(l, initial_joins, final_join)"], "rank": 3}
{"nl": "how to use chain with a json file python", "code": ["def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def load(cls, fp, **kwargs): \"\"\"wrapper for :py:func:`json.load`\"\"\" json_obj = json.load(fp, **kwargs) return parse(cls, json_obj)", "def load(cls, fname): \"\"\" Loads the dictionary from json file :param fname: file to load from :return: loaded dictionary \"\"\" with open(fname) as f: return Config(**json.load(f))", "def open_json(file_name): \"\"\" returns json contents as string \"\"\" with open(file_name, \"r\") as json_data: data = json.load(json_data) return data", "def load(cls, fname): \"\"\" Loads the flow from a JSON file. :param fname: the file to load :type fname: str :return: the flow :rtype: Flow \"\"\" with open(fname) as f: content = f.readlines() return Flow.from_json(''.join(content))"], "rank": 14}
{"nl": "validate credit card number in python using string input", "code": ["def is_valid(number): \"\"\"determines whether the card number is valid.\"\"\" n = str(number) if not n.isdigit(): return False return int(n[-1]) == get_check_digit(n[:-1])", "def check_valid(number, input_base=10): \"\"\" Checks if there is an invalid digit in the input number. Args: number: An number in the following form: (int, int, int, ... , '.' , int, int, int) (iterable container) containing positive integers of the input base input_base(int): The base of the input number. Returns: bool, True if all digits valid, else False. Examples: >>> check_valid((1,9,6,'.',5,1,6), 12) True >>> check_valid((8,1,15,9), 15) False \"\"\" for n in number: if n in (\".\", \"[\", \"]\"): continue elif n >= input_base: if n == 1 and input_base == 1: continue else: return False return True", "def is_rfc2822(instance: str): \"\"\"Validates RFC2822 format\"\"\" if not isinstance(instance, str): return True return email.utils.parsedate(instance) is not None", "def is_valid(email): \"\"\"Email address validation method. :param email: Email address to be saved. :type email: basestring :returns: True if email address is correct, False otherwise. :rtype: bool \"\"\" if isinstance(email, basestring) and EMAIL_RE.match(email): return True return False", "def __validate_email(self, email): \"\"\"Checks if a string looks like an email address\"\"\" e = re.match(self.EMAIL_ADDRESS_REGEX, email, re.UNICODE) if e: return email else: error = \"Invalid email address: \" + str(email) msg = self.GRIMOIRELAB_INVALID_FORMAT % {'error': error} raise InvalidFormatError(cause=msg)"], "rank": 1}
{"nl": "cython python2 bool function", "code": ["def convertToBool(): \"\"\" Convert a byte value to boolean (0 or 1) if the global flag strictBool is True \"\"\" if not OPTIONS.strictBool.value: return [] REQUIRES.add('strictbool.asm') result = [] result.append('pop af') result.append('call __NORMALIZE_BOOLEAN') result.append('push af') return result", "def boolean(value): \"\"\" Configuration-friendly boolean type converter. Supports both boolean-valued and string-valued inputs (e.g. from env vars). \"\"\" if isinstance(value, bool): return value if value == \"\": return False return strtobool(value)", "def to_bool(value): # type: (Any) -> bool \"\"\" Convert a value into a bool but handle \"truthy\" strings eg, yes, true, ok, y \"\"\" if isinstance(value, _compat.string_types): return value.upper() in ('Y', 'YES', 'T', 'TRUE', '1', 'OK') return bool(value)", "def process_bool_arg(arg): \"\"\" Determine True/False from argument \"\"\" if isinstance(arg, bool): return arg elif isinstance(arg, basestring): if arg.lower() in [\"true\", \"1\"]: return True elif arg.lower() in [\"false\", \"0\"]: return False", "def _isbool(string): \"\"\" >>> _isbool(True) True >>> _isbool(\"False\") True >>> _isbool(1) False \"\"\" return isinstance(string, _bool_type) or\\ (isinstance(string, (_binary_type, _text_type)) and string in (\"True\", \"False\"))"], "rank": 17}
{"nl": "python list remove list indices must be", "code": ["def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def remove_elements(target, indices): \"\"\"Remove multiple elements from a list and return result. This implementation is faster than the alternative below. Also note the creation of a new list to avoid altering the original. We don't have any current use for the original intact list, but may in the future...\"\"\" copied = list(target) for index in reversed(indices): del copied[index] return copied", "def filter_list_by_indices(lst, indices): \"\"\"Return a modified list containing only the indices indicated. Args: lst: Original list of values indices: List of indices to keep from the original list Returns: list: Filtered list of values \"\"\" return [x for i, x in enumerate(lst) if i in indices]", "def unique(list): \"\"\" Returns a copy of the list without duplicates. \"\"\" unique = []; [unique.append(x) for x in list if x not in unique] return unique", "def filter_none(list_of_points): \"\"\" :param list_of_points: :return: list_of_points with None's removed \"\"\" remove_elementnone = filter(lambda p: p is not None, list_of_points) remove_sublistnone = filter(lambda p: not contains_none(p), remove_elementnone) return list(remove_sublistnone)"], "rank": 1}
{"nl": "how to eliminate instances in python", "code": ["def unique(transactions): \"\"\" Remove any duplicate entries. \"\"\" seen = set() # TODO: Handle comments return [x for x in transactions if not (x in seen or seen.add(x))]", "def cleanup(self): \"\"\"Forcefully delete objects from memory In an ideal world, this shouldn't be necessary. Garbage collection guarantees that anything without reference is automatically removed. However, because this application is designed to be run multiple times from the same interpreter process, extra case must be taken to ensure there are no memory leaks. Explicitly deleting objects shines a light on where objects may still be referenced in the form of an error. No errors means this was uneccesary, but that's ok. \"\"\" for instance in self.context: del(instance) for plugin in self.plugins: del(plugin)", "def clear_instance(cls): \"\"\"unset _instance for this class and singleton parents. \"\"\" if not cls.initialized(): return for subclass in cls._walk_mro(): if isinstance(subclass._instance, cls): # only clear instances that are instances # of the calling class subclass._instance = None", "def detach_all(self): \"\"\" Detach from all tracked classes and objects. Restore the original constructors and cleanse the tracking lists. \"\"\" self.detach_all_classes() self.objects.clear() self.index.clear() self._keepalive[:] = []", "def pop(): \"\"\"Remove instance from instance list\"\"\" pid = os.getpid() thread = threading.current_thread() Wdb._instances.pop((pid, thread))"], "rank": 3}
{"nl": "calculate standard deviation using python", "code": ["def stddev(values, meanval=None): #from AI: A Modern Appproach \"\"\"The standard deviation of a set of values. Pass in the mean if you already know it.\"\"\" if meanval == None: meanval = mean(values) return math.sqrt( sum([(x - meanval)**2 for x in values]) / (len(values)-1) )", "def _std(self,x): \"\"\" Compute standard deviation with ddof degrees of freedom \"\"\" return np.nanstd(x.values,ddof=self._ddof)", "def weighted_std(values, weights): \"\"\" Calculate standard deviation weighted by errors \"\"\" average = np.average(values, weights=weights) variance = np.average((values-average)**2, weights=weights) return np.sqrt(variance)", "def standard_deviation(numbers): \"\"\"Return standard deviation.\"\"\" numbers = list(numbers) if not numbers: return 0 mean = sum(numbers) / len(numbers) return (sum((n - mean) ** 2 for n in numbers) / len(numbers)) ** .5", "def stderr(a): \"\"\" Calculate the standard error of a. \"\"\" return np.nanstd(a) / np.sqrt(sum(np.isfinite(a)))"], "rank": 5}
{"nl": "intialize a list with size and 0 python", "code": ["def __init__(self, capacity=10): \"\"\" Initialize python List with capacity of 10 or user given input. Python List type is a dynamic array, so we have to restrict its dynamic nature to make it work like a static array. \"\"\" super().__init__() self._array = [None] * capacity self._front = 0 self._rear = 0", "def _crop_list_to_size(l, size): \"\"\"Make a list a certain size\"\"\" for x in range(size - len(l)): l.append(False) for x in range(len(l) - size): l.pop() return l", "def __init__(self, ba=None): \"\"\"Constructor.\"\"\" self.bytearray = ba or (bytearray(b'\\0') * self.SIZEOF)", "def batch(items, size): \"\"\"Batches a list into a list of lists, with sub-lists sized by a specified batch size.\"\"\" return [items[x:x + size] for x in xrange(0, len(items), size)]", "def chunk_list(l, n): \"\"\"Return `n` size lists from a given list `l`\"\"\" return [l[i:i + n] for i in range(0, len(l), n)]"], "rank": 1}
{"nl": "instagram login python requests", "code": ["def login(self, user: str, passwd: str) -> None: \"\"\"Log in to instagram with given username and password and internally store session object. :raises InvalidArgumentException: If the provided username does not exist. :raises BadCredentialsException: If the provided password is wrong. :raises ConnectionException: If connection to Instagram failed. :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call :meth:`Instaloader.two_factor_login`.\"\"\" self.context.login(user, passwd)", "async def login( username: str, password: str, brand: str, websession: ClientSession = None) -> API: \"\"\"Log in to the API.\"\"\" api = API(brand, websession) await api.authenticate(username, password) return api", "def auth_request(self, url, headers, body): \"\"\"Perform auth request for token.\"\"\" return self.req.post(url, headers, body=body)", "def login(self, username, password=None, token=None): \"\"\"Login user for protected API calls.\"\"\" self.session.basic_auth(username, password)", "def fetch_token(self, **kwargs): \"\"\"Exchange a code (and 'state' token) for a bearer token\"\"\" return super(AsanaOAuth2Session, self).fetch_token(self.token_url, client_secret=self.client_secret, **kwargs)"], "rank": 1}
{"nl": "python image detect shape", "code": ["def get_shape(img): \"\"\"Return the shape of img. Paramerers ----------- img: Returns ------- shape: tuple \"\"\" if hasattr(img, 'shape'): shape = img.shape else: shape = img.get_data().shape return shape", "def calculate_dimensions(image, long_side, short_side): \"\"\"Returns the thumbnail dimensions depending on the images format.\"\"\" if image.width >= image.height: return '{0}x{1}'.format(long_side, short_side) return '{0}x{1}'.format(short_side, long_side)", "def _validate_image_rank(self, img_array): \"\"\" Images must be either 2D or 3D. \"\"\" if img_array.ndim == 1 or img_array.ndim > 3: msg = \"{0}D imagery is not allowed.\".format(img_array.ndim) raise IOError(msg)", "def get_image_dimension(self, url): \"\"\" Return a tuple that contains (width, height) Pass in a url to an image and find out its size without loading the whole file If the image wxh could not be found, the tuple will contain `None` values \"\"\" w_h = (None, None) try: if url.startswith('//'): url = 'http:' + url data = requests.get(url).content im = Image.open(BytesIO(data)) w_h = im.size except Exception: logger.warning(\"Error getting image size {}\".format(url), exc_info=True) return w_h", "def get_combined_size(tiles): \"\"\"Calculate combined size of tiles.\"\"\" # TODO: Refactor calculating layout to avoid repetition. columns, rows = calc_columns_rows(len(tiles)) tile_size = tiles[0].image.size return (tile_size[0] * columns, tile_size[1] * rows)"], "rank": 1}
{"nl": "how to rotate an array 90 degrees in python", "code": ["def rotateImage(image, angle): \"\"\" rotates a 2d array to a multiple of 90 deg. 0 = default 1 = 90 deg. cw 2 = 180 deg. 3 = 90 deg. ccw \"\"\" image = [list(row) for row in image] for n in range(angle % 4): image = list(zip(*image[::-1])) return image", "def create_rot2d(angle): \"\"\"Create 2D rotation matrix\"\"\" ca = math.cos(angle) sa = math.sin(angle) return np.array([[ca, -sa], [sa, ca]])", "def rotateImage(img, angle): \"\"\" querries scipy.ndimage.rotate routine :param img: image to be rotated :param angle: angle to be rotated (radian) :return: rotated image \"\"\" imgR = scipy.ndimage.rotate(img, angle, reshape=False) return imgR", "def from_rotation_vector(rot): \"\"\"Convert input 3-vector in axis-angle representation to unit quaternion Parameters ---------- rot: (Nx3) float array Each vector represents the axis of the rotation, with norm proportional to the angle of the rotation in radians. Returns ------- q: array of quaternions Unit quaternions resulting in rotations corresponding to input rotations. Output shape is rot.shape[:-1]. \"\"\" rot = np.array(rot, copy=False) quats = np.zeros(rot.shape[:-1]+(4,)) quats[..., 1:] = rot[...]/2 quats = as_quat_array(quats) return np.exp(quats)", "def _rotate(n, x, y, rx, ry): \"\"\"Rotate and flip a quadrant appropriately Based on the implementation here: https://en.wikipedia.org/w/index.php?title=Hilbert_curve&oldid=797332503 \"\"\" if ry == 0: if rx == 1: x = n - 1 - x y = n - 1 - y return y, x return x, y"], "rank": 1}
{"nl": "python flask deployment static path", "code": ["def glr_path_static(): \"\"\"Returns path to packaged static files\"\"\" return os.path.abspath(os.path.join(os.path.dirname(__file__), '_static'))", "def static_url(path, absolute=False): \"\"\" Shorthand for returning a URL for the requested static file. Arguments: path -- the path to the file (relative to the static files directory) absolute -- whether the link should be absolute or relative \"\"\" if os.sep != '/': path = '/'.join(path.split(os.sep)) return flask.url_for('static', filename=path, _external=absolute)", "def default_static_path(): \"\"\" Return the path to the javascript bundle \"\"\" fdir = os.path.dirname(__file__) return os.path.abspath(os.path.join(fdir, '../assets/'))", "def get_static_url(): \"\"\"Return a base static url, always ending with a /\"\"\" path = getattr(settings, 'STATIC_URL', None) if not path: path = getattr(settings, 'MEDIA_URL', None) if not path: path = '/' return path", "def staticdir(): \"\"\"Return the location of the static data directory.\"\"\" root = os.path.abspath(os.path.dirname(__file__)) return os.path.join(root, \"static\")"], "rank": 5}
{"nl": "python 'worksheet' object is not callable", "code": ["def __init__(self, filename, formatting_info=False, handle_ambiguous_date=None): \"\"\"Initialize the ExcelWorkbook instance.\"\"\" super().__init__(filename) self.workbook = xlrd.open_workbook(self.filename, formatting_info=formatting_info) self.handle_ambiguous_date = handle_ambiguous_date", "def set_cell_value(cell, value): \"\"\" Convenience method for setting the value of an openpyxl cell This is necessary since the value property changed from internal_value to value between version 1.* and 2.*. \"\"\" if OPENPYXL_MAJOR_VERSION > 1: cell.value = value else: cell.internal_value = value", "def add_chart(self, chart, row, col): \"\"\" Adds a chart to the worksheet at (row, col). :param xltable.Chart Chart: chart to add to the workbook. :param int row: Row to add the chart at. \"\"\" self.__charts.append((chart, (row, col)))", "def _openpyxl_read_xl(xl_path: str): \"\"\" Use openpyxl to read an Excel file. \"\"\" try: wb = load_workbook(filename=xl_path, read_only=True) except: raise else: return wb", "def wr_row_mergeall(self, worksheet, txtstr, fmt, row_idx): \"\"\"Merge all columns and place text string in widened cell.\"\"\" hdridxval = len(self.hdrs) - 1 worksheet.merge_range(row_idx, 0, row_idx, hdridxval, txtstr, fmt) return row_idx + 1"], "rank": 3}
{"nl": "how to covert a data set to series in python", "code": ["def from_series(cls, series): \"\"\"Convert a pandas.Series into an xarray.DataArray. If the series's index is a MultiIndex, it will be expanded into a tensor product of one-dimensional coordinates (filling in missing values with NaN). Thus this operation should be the inverse of the `to_series` method. \"\"\" # TODO: add a 'name' parameter name = series.name df = pd.DataFrame({name: series}) ds = Dataset.from_dataframe(df) return ds[name]", "def _possibly_convert_objects(values): \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into datetime64 and timedelta64, according to the pandas convention. \"\"\" return np.asarray(pd.Series(values.ravel())).reshape(values.shape)", "def to_dataframe(products): \"\"\"Return the products from a query response as a Pandas DataFrame with the values in their appropriate Python types. \"\"\" try: import pandas as pd except ImportError: raise ImportError(\"to_dataframe requires the optional dependency Pandas.\") return pd.DataFrame.from_dict(products, orient='index')", "def reverse_transform(self, col): \"\"\"Converts data back into original format. Args: col(pandas.DataFrame): Data to transform. Returns: pandas.DataFrame \"\"\" output = pd.DataFrame() output[self.col_name] = self.get_category(col[self.col_name]) return output", "def _maybe_to_categorical(array): \"\"\" Coerce to a categorical if a series is given. Internal use ONLY. \"\"\" if isinstance(array, (ABCSeries, ABCCategoricalIndex)): return array._values elif isinstance(array, np.ndarray): return Categorical(array) return array"], "rank": 1}
{"nl": "python dataset get first 50 rows", "code": ["def genfirstvalues(cursor: Cursor, arraysize: int = 1000) \\ -> Generator[Any, None, None]: \"\"\" Generate the first value in each row. Args: cursor: the cursor arraysize: split fetches into chunks of this many records Yields: the first value of each row \"\"\" return (row[0] for row in genrows(cursor, arraysize))", "def tail(self, n=10): \"\"\" Get an SArray that contains the last n elements in the SArray. Parameters ---------- n : int The number of elements to fetch Returns ------- out : SArray A new SArray which contains the last n rows of the current SArray. \"\"\" with cython_context(): return SArray(_proxy=self.__proxy__.tail(n))", "def fetchallfirstvalues(self, sql: str, *args) -> List[Any]: \"\"\"Executes SQL; returns list of first values of each row.\"\"\" rows = self.fetchall(sql, *args) return [row[0] for row in rows]", "def head_and_tail_print(self, n=5): \"\"\"Display the first and last n elements of a DataFrame.\"\"\" from IPython import display display.display(display.HTML(self._head_and_tail_table(n)))", "def get_last_row(dbconn, tablename, n=1, uuid=None): \"\"\" Returns the last `n` rows in the table \"\"\" return fetch(dbconn, tablename, n, uuid, end=True)"], "rank": 1}
{"nl": "how to check in python if token is person or not", "code": ["def match_paren(self, tokens, item): \"\"\"Matches a paren.\"\"\" match, = tokens return self.match(match, item)", "def identify_request(request: RequestType) -> bool: \"\"\" Try to identify whether this is an ActivityPub request. \"\"\" # noinspection PyBroadException try: data = json.loads(decode_if_bytes(request.body)) if \"@context\" in data: return True except Exception: pass return False", "def is_identifier(string): \"\"\"Check if string could be a valid python identifier :param string: string to be tested :returns: True if string can be a python identifier, False otherwise :rtype: bool \"\"\" matched = PYTHON_IDENTIFIER_RE.match(string) return bool(matched) and not keyword.iskeyword(string)", "def check_auth(email, password): \"\"\"Check if a username/password combination is valid. \"\"\" try: user = User.get(User.email == email) except User.DoesNotExist: return False return password == user.password", "def _begins_with_one_of(sentence, parts_of_speech): \"\"\"Return True if the sentence or fragment begins with one of the parts of speech in the list, else False\"\"\" doc = nlp(sentence) if doc[0].tag_ in parts_of_speech: return True return False"], "rank": 46}
{"nl": "number of standard deviations python from a fit", "code": ["def standard_deviation(numbers): \"\"\"Return standard deviation.\"\"\" numbers = list(numbers) if not numbers: return 0 mean = sum(numbers) / len(numbers) return (sum((n - mean) ** 2 for n in numbers) / len(numbers)) ** .5", "def _std(self,x): \"\"\" Compute standard deviation with ddof degrees of freedom \"\"\" return np.nanstd(x.values,ddof=self._ddof)", "def sem(inlist): \"\"\" Returns the estimated standard error of the mean (sx-bar) of the values in the passed list. sem = stdev / sqrt(n) Usage: lsem(inlist) \"\"\" sd = stdev(inlist) n = len(inlist) return sd / math.sqrt(n)", "def lsem (inlist): \"\"\" Returns the estimated standard error of the mean (sx-bar) of the values in the passed list. sem = stdev / sqrt(n) Usage: lsem(inlist) \"\"\" sd = stdev(inlist) n = len(inlist) return sd/math.sqrt(n)", "def stddev(values, meanval=None): #from AI: A Modern Appproach \"\"\"The standard deviation of a set of values. Pass in the mean if you already know it.\"\"\" if meanval == None: meanval = mean(values) return math.sqrt( sum([(x - meanval)**2 for x in values]) / (len(values)-1) )"], "rank": 3}
{"nl": "python cgi form get value", "code": ["def parse_form(self, req, name, field): \"\"\"Pull a form value from the request.\"\"\" return core.get_value(req.POST, name, field)", "def fieldstorage(self): \"\"\" `cgi.FieldStorage` from `wsgi.input`. \"\"\" if self._fieldstorage is None: if self._body is not None: raise ReadBodyTwiceError() self._fieldstorage = cgi.FieldStorage( environ=self._environ, fp=self._environ['wsgi.input'] ) return self._fieldstorage", "def parse_form(self, req, name, field): \"\"\"Pull a form value from the request.\"\"\" return get_value(req.body_arguments, name, field)", "def getfield(f): \"\"\"convert values from cgi.Field objects to plain values.\"\"\" if isinstance(f, list): return [getfield(x) for x in f] else: return f.value", "def parse_querystring(self, req, name, field): \"\"\"Pull a querystring value from the request.\"\"\" return core.get_value(req.args, name, field)"], "rank": 6}
{"nl": "python receive **kwargs pass on **kwargs", "code": ["def apply_kwargs(func, **kwargs): \"\"\"Call *func* with kwargs, but only those kwargs that it accepts. \"\"\" new_kwargs = {} params = signature(func).parameters for param_name in params.keys(): if param_name in kwargs: new_kwargs[param_name] = kwargs[param_name] return func(**new_kwargs)", "def updateFromKwargs(self, properties, kwargs, collector, **unused): \"\"\"Primary entry point to turn 'kwargs' into 'properties'\"\"\" properties[self.name] = self.getFromKwargs(kwargs)", "def update(self, dictionary=None, **kwargs): \"\"\" Adds/overwrites all the keys and values from the dictionary. \"\"\" if not dictionary == None: kwargs.update(dictionary) for k in list(kwargs.keys()): self[k] = kwargs[k]", "def _sourced_dict(self, source=None, **kwargs): \"\"\"Like ``dict(**kwargs)``, but where the ``source`` key is special. \"\"\" if source: kwargs['source'] = source elif self.source: kwargs['source'] = self.source return kwargs", "def _merge_args_with_kwargs(args_dict, kwargs_dict): \"\"\"Merge args with kwargs.\"\"\" ret = args_dict.copy() ret.update(kwargs_dict) return ret"], "rank": 11}
{"nl": "python white space check", "code": ["def _check_whitespace(string): \"\"\" Make sure thre is no whitespace in the given string. Will raise a ValueError if whitespace is detected \"\"\" if string.count(' ') + string.count('\\t') + string.count('\\n') > 0: raise ValueError(INSTRUCTION_HAS_WHITESPACE)", "def is_blankspace(self, char): \"\"\" Test if a character is a blankspace. Parameters ---------- char : str The character to test. Returns ------- ret : bool True if character is a blankspace, False otherwise. \"\"\" if len(char) > 1: raise TypeError(\"Expected a char.\") if char in self.blankspaces: return True return False", "def clean_whitespace(statement): \"\"\" Remove any consecutive whitespace characters from the statement text. \"\"\" import re # Replace linebreaks and tabs with spaces statement.text = statement.text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ') # Remove any leeding or trailing whitespace statement.text = statement.text.strip() # Remove consecutive spaces statement.text = re.sub(' +', ' ', statement.text) return statement", "def strip_spaces(s): \"\"\" Strip excess spaces from a string \"\"\" return u\" \".join([c for c in s.split(u' ') if c])", "def _check_surrounded_by_space(self, tokens, i): \"\"\"Check that a binary operator is surrounded by exactly one space.\"\"\" self._check_space(tokens, i, (_MUST, _MUST))"], "rank": 1}
{"nl": "how to check for eof in python", "code": ["def eof(fd): \"\"\"Determine if end-of-file is reached for file fd.\"\"\" b = fd.read(1) end = len(b) == 0 if not end: curpos = fd.tell() fd.seek(curpos - 1) return end", "def do_EOF(self, args): \"\"\"Exit on system end of file character\"\"\" if _debug: ConsoleCmd._debug(\"do_EOF %r\", args) return self.do_exit(args)", "def _readuntil(f, end=_TYPE_END): \"\"\"Helper function to read bytes until a certain end byte is hit\"\"\" buf = bytearray() byte = f.read(1) while byte != end: if byte == b'': raise ValueError('File ended unexpectedly. Expected end byte {}.'.format(end)) buf += byte byte = f.read(1) return buf", "def feed_eof(self): \"\"\"Send a potentially \"ragged\" EOF. This method will raise an SSL_ERROR_EOF exception if the EOF is unexpected. \"\"\" self._incoming.write_eof() ssldata, appdata = self.feed_ssldata(b'') assert appdata == [] or appdata == [b'']", "def next (self): # File-like object. \"\"\"This is to support iterators over a file-like object. \"\"\" result = self.readline() if result == self._empty_buffer: raise StopIteration return result"], "rank": 1}
{"nl": "how to clear python variables at begining of code", "code": ["def clear_globals_reload_modules(self): \"\"\"Clears globals and reloads modules\"\"\" self.code_array.clear_globals() self.code_array.reload_modules() # Clear result cache self.code_array.result_cache.clear()", "def _clear(self): \"\"\"Resets all assigned data for the current message.\"\"\" self._finished = False self._measurement = None self._message = None self._message_body = None", "def _release(self): \"\"\"Destroy self since closures cannot be called again.\"\"\" del self.funcs del self.variables del self.variable_values del self.satisfied", "def clear_global(self): \"\"\"Clear only any cached global data. \"\"\" vname = self.varname logger.debug(f'global clearning {vname}') if vname in globals(): logger.debug('removing global instance var: {}'.format(vname)) del globals()[vname]", "def forget_coords(self): \"\"\"Forget all loaded coordinates.\"\"\" self.w.ntotal.set_text('0') self.coords_dict.clear() self.redo()"], "rank": 1}
{"nl": "how to check if sql connection is open in python", "code": ["def raw_connection_from(engine_or_conn): \"\"\"Extract a raw_connection and determine if it should be automatically closed. Only connections opened by this package will be closed automatically. \"\"\" if hasattr(engine_or_conn, 'cursor'): return engine_or_conn, False if hasattr(engine_or_conn, 'connection'): return engine_or_conn.connection, False return engine_or_conn.raw_connection(), True", "def in_transaction(self): \"\"\"Check if this database is in a transactional context.\"\"\" if not hasattr(self.local, 'tx'): return False return len(self.local.tx) > 0", "def reopen(self): \"\"\"Reopen the tough connection. It will not complain if the connection cannot be reopened. \"\"\" try: self._con.reopen() except Exception: if self._transcation: self._transaction = False try: self._con.query('rollback') except Exception: pass else: self._transaction = False self._closed = False self._setsession() self._usage = 0", "def db_exists(): \"\"\"Test if DATABASES['default'] exists\"\"\" logger.info(\"Checking to see if %s already exists\", repr(DB[\"NAME\"])) try: # Hide stderr since it is confusing here psql(\"\", stderr=subprocess.STDOUT) except subprocess.CalledProcessError: return False return True", "def exists(self): \"\"\" Performs an existence check on the remote database. :returns: Boolean True if the database exists, False otherwise \"\"\" resp = self.r_session.head(self.database_url) if resp.status_code not in [200, 404]: resp.raise_for_status() return resp.status_code == 200"], "rank": 1}
{"nl": "select rows if a field is null in python", "code": ["def selectnone(table, field, complement=False): \"\"\"Select rows where the given field is `None`.\"\"\" return select(table, field, lambda v: v is None, complement=complement)", "def selectnotnone(table, field, complement=False): \"\"\"Select rows where the given field is not `None`.\"\"\" return select(table, field, lambda v: v is not None, complement=complement)", "def selectnotin(table, field, value, complement=False): \"\"\"Select rows where the given field is not a member of the given value.\"\"\" return select(table, field, lambda v: v not in value, complement=complement)", "def _none_value(self): \"\"\"Get an appropriate \"null\" value for this field's type. This is used internally when setting the field to None. \"\"\" if self.out_type == int: return 0 elif self.out_type == float: return 0.0 elif self.out_type == bool: return False elif self.out_type == six.text_type: return u''", "def is_all_field_none(self): \"\"\" :rtype: bool \"\"\" if self._type_ is not None: return False if self._value is not None: return False if self._name is not None: return False return True"], "rank": 1}
{"nl": "plot remove axis python", "code": ["def axes_off(ax): \"\"\"Get rid of all axis ticks, lines, etc. \"\"\" ax.set_frame_on(False) ax.axes.get_yaxis().set_visible(False) ax.axes.get_xaxis().set_visible(False)", "def clean_axis(axis): \"\"\"Remove ticks, tick labels, and frame from axis\"\"\" axis.get_xaxis().set_ticks([]) axis.get_yaxis().set_ticks([]) for spine in list(axis.spines.values()): spine.set_visible(False)", "def _hide_tick_lines_and_labels(axis): \"\"\" Set visible property of ticklines and ticklabels of an axis to False \"\"\" for item in axis.get_ticklines() + axis.get_ticklabels(): item.set_visible(False)", "def clear(self): \"\"\" clear plot \"\"\" self.axes.cla() self.conf.ntrace = 0 self.conf.xlabel = '' self.conf.ylabel = '' self.conf.title = ''", "def clear_matplotlib_ticks(self, axis=\"both\"): \"\"\"Clears the default matplotlib ticks.\"\"\" ax = self.get_axes() plotting.clear_matplotlib_ticks(ax=ax, axis=axis)"], "rank": 1}
{"nl": "python check is object is defined", "code": ["def is_defined(self, objtxt, force_import=False): \"\"\"Return True if object is defined\"\"\" return self.interpreter.is_defined(objtxt, force_import)", "def is_builtin_object(node: astroid.node_classes.NodeNG) -> bool: \"\"\"Returns True if the given node is an object from the __builtin__ module.\"\"\" return node and node.root().name == BUILTINS_NAME", "def isroutine(object): \"\"\"Return true if the object is any kind of function or method.\"\"\" return (isbuiltin(object) or isfunction(object) or ismethod(object) or ismethoddescriptor(object))", "def is_defined(self, obj, force_import=False): \"\"\"Return True if object is defined in current namespace\"\"\" from spyder_kernels.utils.dochelpers import isdefined ns = self._get_current_namespace(with_magics=True) return isdefined(obj, force_import=force_import, namespace=ns)", "def is_descriptor_class(desc, include_abstract=False): r\"\"\"Check calculatable descriptor class or not. Returns: bool \"\"\" return ( isinstance(desc, type) and issubclass(desc, Descriptor) and (True if include_abstract else not inspect.isabstract(desc)) )"], "rank": 1}
{"nl": "python remove characters from query", "code": ["def filter_query_string(query): \"\"\" Return a version of the query string with the _e, _k and _s values removed. \"\"\" return '&'.join([q for q in query.split('&') if not (q.startswith('_k=') or q.startswith('_e=') or q.startswith('_s'))])", "def filter_query(s): \"\"\" Filters given query with the below regex and returns lists of quoted and unquoted strings \"\"\" matches = re.findall(r'(?:\"([^\"]*)\")|([^\"]*)', s) result_quoted = [t[0].strip() for t in matches if t[0]] result_unquoted = [t[1].strip() for t in matches if t[1]] return result_quoted, result_unquoted", "def escape(s): \"\"\"Escape a URL including any /.\"\"\" if not isinstance(s, bytes): s = s.encode('utf-8') return quote(s, safe='~')", "def url_encode(url): \"\"\" Convert special characters using %xx escape. :param url: str :return: str - encoded url \"\"\" if isinstance(url, text_type): url = url.encode('utf8') return quote(url, ':/%?&=')", "def _escape(s): \"\"\" Helper method that escapes parameters to a SQL query. \"\"\" e = s e = e.replace('\\\\', '\\\\\\\\') e = e.replace('\\n', '\\\\n') e = e.replace('\\r', '\\\\r') e = e.replace(\"'\", \"\\\\'\") e = e.replace('\"', '\\\\\"') return e"], "rank": 1}
{"nl": "python, get timezone info in python", "code": ["def get_timezone() -> Tuple[datetime.tzinfo, str]: \"\"\"Discover the current time zone and it's standard string representation (for source{d}).\"\"\" dt = get_datetime_now().astimezone() tzstr = dt.strftime(\"%z\") tzstr = tzstr[:-2] + \":\" + tzstr[-2:] return dt.tzinfo, tzstr", "def timestamp(format=DATEFMT, timezone='Africa/Johannesburg'): \"\"\" Return current datetime with timezone applied [all timezones] print sorted(pytz.all_timezones) \"\"\" return formatdate(datetime.now(tz=pytz.timezone(timezone)))", "def with_tz(request): \"\"\" Get the time with TZ enabled \"\"\" dt = datetime.now() t = Template('{% load tz %}{% localtime on %}{% get_current_timezone as TIME_ZONE %}{{ TIME_ZONE }}{% endlocaltime %}') c = RequestContext(request) response = t.render(c) return HttpResponse(response)", "def reload_localzone(): \"\"\"Reload the cached localzone. You need to call this if the timezone has changed.\"\"\" global _cache_tz _cache_tz = pytz.timezone(get_localzone_name()) utils.assert_tz_offset(_cache_tz) return _cache_tz", "def now(timezone=None): \"\"\" Return a naive datetime object for the given ``timezone``. A ``timezone`` is any pytz- like or datetime.tzinfo-like timezone object. If no timezone is given, then UTC is assumed. This method is best used with pytz installed:: pip install pytz \"\"\" d = datetime.datetime.utcnow() if not timezone: return d return to_timezone(d, timezone).replace(tzinfo=None)"], "rank": 1}
{"nl": "python how to replace a string with underscores", "code": ["def normalise_string(string): \"\"\" Strips trailing whitespace from string, lowercases it and replaces spaces with underscores \"\"\" string = (string.strip()).lower() return re.sub(r'\\W+', '_', string)", "def camelcase_underscore(name): \"\"\" Convert camelcase names to underscore \"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def convert(name): \"\"\"Convert CamelCase to underscore Parameters ---------- name : str Camelcase string Returns ------- name : str Converted name \"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def camel_to_underscore(string): \"\"\"Convert camelcase to lowercase and underscore. Recipe from http://stackoverflow.com/a/1176023 Args: string (str): The string to convert. Returns: str: The converted string. \"\"\" string = FIRST_CAP_RE.sub(r'\\1_\\2', string) return ALL_CAP_RE.sub(r'\\1_\\2', string).lower()", "def us2mc(string): \"\"\"Transform an underscore_case string to a mixedCase string\"\"\" return re.sub(r'_([a-z])', lambda m: (m.group(1).upper()), string)"], "rank": 1}
{"nl": "code to take the transpose of a matrix in python", "code": ["def transpose(table): \"\"\" transpose matrix \"\"\" t = [] for i in range(0, len(table[0])): t.append([row[i] for row in table]) return t", "def __rmatmul__(self, other): \"\"\" Matrix multiplication using binary `@` operator in Python>=3.5. \"\"\" return self.T.dot(np.transpose(other)).T", "def transform_from_rot_trans(R, t): \"\"\"Transforation matrix from rotation matrix and translation vector.\"\"\" R = R.reshape(3, 3) t = t.reshape(3, 1) return np.vstack((np.hstack([R, t]), [0, 0, 0, 1]))", "def matrixTimesVector(MM, aa): \"\"\" :param MM: A matrix of size 3x3 :param aa: A vector of size 3 :return: A vector of size 3 which is the product of the matrix by the vector \"\"\" bb = np.zeros(3, np.float) for ii in range(3): bb[ii] = np.sum(MM[ii, :] * aa) return bb", "def trans_from_matrix(matrix): \"\"\" Convert a vtk matrix to a numpy.ndarray \"\"\" t = np.zeros((4, 4)) for i in range(4): for j in range(4): t[i, j] = matrix.GetElement(i, j) return t"], "rank": 1}
{"nl": "python check if two images are the same", "code": ["def is_same_shape(self, other_im, check_channels=False): \"\"\" Checks if two images have the same height and width (and optionally channels). Parameters ---------- other_im : :obj:`Image` image to compare check_channels : bool whether or not to check equality of the channels Returns ------- bool True if the images are the same shape, False otherwise \"\"\" if self.height == other_im.height and self.width == other_im.width: if check_channels and self.channels != other_im.channels: return False return True return False", "def _sim_fill(r1, r2, imsize): \"\"\" calculate the fill similarity over the image \"\"\" bbsize = ( (max(r1[\"max_x\"], r2[\"max_x\"]) - min(r1[\"min_x\"], r2[\"min_x\"])) * (max(r1[\"max_y\"], r2[\"max_y\"]) - min(r1[\"min_y\"], r2[\"min_y\"])) ) return 1.0 - (bbsize - r1[\"size\"] - r2[\"size\"]) / imsize", "def is_empty(self): \"\"\"Checks for an empty image. \"\"\" if(((self.channels == []) and (not self.shape == (0, 0))) or ((not self.channels == []) and (self.shape == (0, 0)))): raise RuntimeError(\"Channels-shape mismatch.\") return self.channels == [] and self.shape == (0, 0)", "def _is_image_sequenced(image): \"\"\"Determine if the image is a sequenced image.\"\"\" try: image.seek(1) image.seek(0) result = True except EOFError: result = False return result", "def is_progressive(image): \"\"\" Check to see if an image is progressive. \"\"\" if not isinstance(image, Image.Image): # Can only check PIL images for progressive encoding. return False return ('progressive' in image.info) or ('progression' in image.info)"], "rank": 1}
{"nl": "is string python conditional", "code": ["def is_identifier(string): \"\"\"Check if string could be a valid python identifier :param string: string to be tested :returns: True if string can be a python identifier, False otherwise :rtype: bool \"\"\" matched = PYTHON_IDENTIFIER_RE.match(string) return bool(matched) and not keyword.iskeyword(string)", "def _isbool(string): \"\"\" >>> _isbool(True) True >>> _isbool(\"False\") True >>> _isbool(1) False \"\"\" return isinstance(string, _bool_type) or\\ (isinstance(string, (_binary_type, _text_type)) and string in (\"True\", \"False\"))", "def __is__(cls, s): \"\"\"Test if string matches this argument's format.\"\"\" return s.startswith(cls.delims()[0]) and s.endswith(cls.delims()[1])", "def find_if_expression_as_statement(node): \"\"\"Finds an \"if\" expression as a statement\"\"\" return ( isinstance(node, ast.Expr) and isinstance(node.value, ast.IfExp) )", "def is_symbol(string): \"\"\" Return true if the string is a mathematical symbol. \"\"\" return ( is_int(string) or is_float(string) or is_constant(string) or is_unary(string) or is_binary(string) or (string == '(') or (string == ')') )"], "rank": 1}
{"nl": "python function to remove headers", "code": ["def remove_hop_by_hop_headers(headers): \"\"\"Remove all HTTP/1.1 \"Hop-by-Hop\" headers from a list or :class:`Headers` object. This operation works in-place. .. versionadded:: 0.5 :param headers: a list or :class:`Headers` object. \"\"\" headers[:] = [ (key, value) for key, value in headers if not is_hop_by_hop_header(key) ]", "def set_header(self, name, value): \"\"\" Create a new response header, replacing any previously defined headers with the same name. \"\"\" self._headers[_hkey(name)] = [_hval(value)]", "def with_headers(self, headers): \"\"\"Sets multiple headers on the request and returns the request itself. Keyword arguments: headers -- a dict-like object which contains the headers to set. \"\"\" for key, value in headers.items(): self.with_header(key, value) return self", "def add_header(self, name, value): \"\"\" Add an additional response header, not removing duplicates. \"\"\" self._headers.setdefault(_hkey(name), []).append(_hval(value))", "def clear(self) -> None: \"\"\"Resets all headers and content for this response.\"\"\" self._headers = httputil.HTTPHeaders( { \"Server\": \"TornadoServer/%s\" % tornado.version, \"Content-Type\": \"text/html; charset=UTF-8\", \"Date\": httputil.format_timestamp(time.time()), } ) self.set_default_headers() self._write_buffer = [] # type: List[bytes] self._status_code = 200 self._reason = httputil.responses[200]"], "rank": 1}
{"nl": "python custom type from list comprehension", "code": ["def list_of(cls): \"\"\" Returns a function that checks that each element in a list is of a specific type. \"\"\" return lambda l: isinstance(l, list) and all(isinstance(x, cls) for x in l)", "def is_iterable(etype) -> bool: \"\"\" Determine whether etype is a List or other iterable \"\"\" return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)", "def build_list_type_validator(item_validator): \"\"\"Return a function which validates that the value is a list of items which are validated using item_validator. \"\"\" def validate_list_of_type(value): return [item_validator(item) for item in validate_list(value)] return validate_list_of_type", "def flatten(l, types=(list, float)): \"\"\" Flat nested list of lists into a single list. \"\"\" l = [item if isinstance(item, types) else [item] for item in l] return [item for sublist in l for item in sublist]", "def toList(variable, types=(basestring, int, float, )): \"\"\"Converts a variable of type string, int, float to a list, containing the variable as the only element. :param variable: any python object :type variable: (str, int, float, others) :returns: [variable] or variable \"\"\" if isinstance(variable, types): return [variable] else: return variable"], "rank": 31}
{"nl": "returns random number in standardn normal distribution python", "code": ["def sample_normal(mean, var, rng): \"\"\"Sample from independent normal distributions Each element is an independent normal distribution. Parameters ---------- mean : numpy.ndarray Means of the normal distribution. Shape --> (batch_num, sample_dim) var : numpy.ndarray Variance of the normal distribution. Shape --> (batch_num, sample_dim) rng : numpy.random.RandomState Returns ------- ret : numpy.ndarray The sampling result. Shape --> (batch_num, sample_dim) \"\"\" ret = numpy.sqrt(var) * rng.randn(*mean.shape) + mean return ret", "def rlognormal(mu, tau, size=None): \"\"\" Return random lognormal variates. \"\"\" return np.random.lognormal(mu, np.sqrt(1. / tau), size)", "def rnormal(mu, tau, size=None): \"\"\" Random normal variates. \"\"\" return np.random.normal(mu, 1. / np.sqrt(tau), size)", "def _gauss(mean: int, sigma: int) -> int: \"\"\" Creates a variation from a base value Args: mean: base value sigma: gaussian sigma Returns: random value \"\"\" return int(random.gauss(mean, sigma))", "def EvalGaussianPdf(x, mu, sigma): \"\"\"Computes the unnormalized PDF of the normal distribution. x: value mu: mean sigma: standard deviation returns: float probability density \"\"\" return scipy.stats.norm.pdf(x, mu, sigma)"], "rank": 2}
{"nl": "record training time python", "code": ["def on_train_end(self, logs): \"\"\" Print training time at end of training \"\"\" duration = timeit.default_timer() - self.train_start print('done, took {:.3f} seconds'.format(duration))", "def main(argv=None): \"\"\"Run a Tensorflow model on the Iris dataset.\"\"\" args = parse_arguments(sys.argv if argv is None else argv) tf.logging.set_verbosity(tf.logging.INFO) learn_runner.run( experiment_fn=get_experiment_fn(args), output_dir=args.job_dir)", "def start_task(self, task): \"\"\"Begin logging of a task Stores the time this task was started in order to return time lapsed when `complete_task` is called. Parameters ---------- task : str Name of the task to be started \"\"\" self.info(\"Calculating {}...\".format(task)) self.tasks[task] = self.timer()", "def after_epoch(self, **_) -> None: \"\"\"Save/override the latest model after every epoch.\"\"\" SaveEvery.save_model(model=self._model, name_suffix=self._OUTPUT_NAME, on_failure=self._on_save_failure)", "def speedtest(func, *args, **kwargs): \"\"\" Test the speed of a function. \"\"\" n = 100 start = time.time() for i in range(n): func(*args,**kwargs) end = time.time() return (end-start)/n"], "rank": 1}
{"nl": "python how to get stem of filename", "code": ["def remove_ext(fname): \"\"\"Removes the extension from a filename \"\"\" bn = os.path.basename(fname) return os.path.splitext(bn)[0]", "def slugify_filename(filename): \"\"\" Slugify filename \"\"\" name, ext = os.path.splitext(filename) slugified = get_slugified_name(name) return slugified + ext", "def splitext_no_dot(filename): \"\"\" Wrap os.path.splitext to return the name and the extension without the '.' (e.g., csv instead of .csv) \"\"\" name, ext = os.path.splitext(filename) ext = ext.lower() return name, ext.strip('.')", "def guess_title(basename): \"\"\" Attempt to guess the title from the filename \"\"\" base, _ = os.path.splitext(basename) return re.sub(r'[ _-]+', r' ', base).title()", "def add_suffix(fullname, suffix): \"\"\" Add suffix to a full file name\"\"\" name, ext = os.path.splitext(fullname) return name + '_' + suffix + ext"], "rank": 4}
{"nl": "python direct all print output to log file", "code": ["def log_all(self, file): \"\"\"Log all data received from RFLink to file.\"\"\" global rflink_log if file == None: rflink_log = None else: log.debug('logging to: %s', file) rflink_log = open(file, 'a')", "def log_no_newline(self, msg): \"\"\" print the message to the predefined log file without newline \"\"\" self.print2file(self.logfile, False, False, msg)", "def pout(msg, log=None): \"\"\"Print 'msg' to stdout, and option 'log' at info level.\"\"\" _print(msg, sys.stdout, log_func=log.info if log else None)", "def dump_to_log(self, logger): \"\"\"Send the cmd info and collected stdout to logger.\"\"\" logger.error(\"Execution ended in %s for cmd %s\", self._retcode, self._cmd) for line in self._collected_stdout: logger.error(STDOUT_LOG_PREFIX + line)", "def pylog(self, *args, **kwargs): \"\"\"Display all available logging information.\"\"\" printerr(self.name, args, kwargs, traceback.format_exc())"], "rank": 2}
{"nl": "how to get contents fromtext file python", "code": ["def read_text_from_file(path: str) -> str: \"\"\" Reads text file contents \"\"\" with open(path) as text_file: content = text_file.read() return content", "def _read_text(self, filename): \"\"\" Helper that reads the UTF-8 content of the specified file, or None if the file doesn't exist. This returns a unicode string. \"\"\" with io.open(filename, 'rt', encoding='utf-8') as f: return f.read()", "def read(*args): \"\"\"Reads complete file contents.\"\"\" return io.open(os.path.join(HERE, *args), encoding=\"utf-8\").read()", "def str_from_file(path): \"\"\" Return file contents as string. \"\"\" with open(path) as f: s = f.read().strip() return s", "def read(fname): \"\"\"Quick way to read a file content.\"\"\" content = None with open(os.path.join(here, fname)) as f: content = f.read() return content"], "rank": 1}
{"nl": "how to remove the duplicates in list in python", "code": ["def remove_list_duplicates(lista, unique=False): \"\"\" Remove duplicated elements in a list. Args: lista: List with elements to clean duplicates. \"\"\" result = [] allready = [] for elem in lista: if elem not in result: result.append(elem) else: allready.append(elem) if unique: for elem in allready: result = list(filter((elem).__ne__, result)) return result", "def purge_duplicates(list_in): \"\"\"Remove duplicates from list while preserving order. Parameters ---------- list_in: Iterable Returns ------- list List of first occurences in order \"\"\" _list = [] for item in list_in: if item not in _list: _list.append(item) return _list", "def remove_duplicates(lst): \"\"\" Emulate what a Python ``set()`` does, but keeping the element's order. \"\"\" dset = set() return [l for l in lst if l not in dset and not dset.add(l)]", "def find_duplicates(l: list) -> set: \"\"\" Return the duplicates in a list. The function relies on https://stackoverflow.com/questions/9835762/find-and-list-duplicates-in-a-list . Parameters ---------- l : list Name Returns ------- set Duplicated values >>> find_duplicates([1,2,3]) set() >>> find_duplicates([1,2,1]) {1} \"\"\" return set([x for x in l if l.count(x) > 1])", "def dedupe_list(seq): \"\"\" Utility function to remove duplicates from a list :param seq: The sequence (list) to deduplicate :return: A list with original duplicates removed \"\"\" seen = set() return [x for x in seq if not (x in seen or seen.add(x))]"], "rank": 3}
{"nl": "how to conduct a delaunay triangulation python", "code": ["def delaunay_2d(self, tol=1e-05, alpha=0.0, offset=1.0, bound=False): \"\"\"Apply a delaunay 2D filter along the best fitting plane. This extracts the grid's points and perfoms the triangulation on those alone. \"\"\" return PolyData(self.points).delaunay_2d(tol=tol, alpha=alpha, offset=offset, bound=bound)", "def computeDelaunayTriangulation(points): \"\"\" Takes a list of point objects (which must have x and y fields). Returns a list of 3-tuples: the indices of the points that form a Delaunay triangle. \"\"\" siteList = SiteList(points) context = Context() context.triangulate = True voronoi(siteList,context) return context.triangles", "def to_bipartite_matrix(A): \"\"\"Returns the adjacency matrix of a bipartite graph whose biadjacency matrix is `A`. `A` must be a NumPy array. If `A` has **m** rows and **n** columns, then the returned matrix has **m + n** rows and columns. \"\"\" m, n = A.shape return four_blocks(zeros(m, m), A, A.T, zeros(n, n))", "def get_triangles(graph: DiGraph) -> SetOfNodeTriples: \"\"\"Get a set of triples representing the 3-cycles from a directional graph. Each 3-cycle is returned once, with nodes in sorted order. \"\"\" return { tuple(sorted([a, b, c], key=str)) for a, b in graph.edges() for c in graph.successors(b) if graph.has_edge(c, a) }", "def multivariate_normal_tril(x, dims, layer_fn=tf.compat.v1.layers.dense, loc_fn=lambda x: x, scale_fn=tril_with_diag_softplus_and_shift, name=None): \"\"\"Constructs a trainable `tfd.MultivariateNormalTriL` distribution. This function creates a MultivariateNormal (MVN) with lower-triangular scale matrix. By default the MVN is parameterized via affine transformation of input tensor `x`. Using default args, this function is mathematically equivalent to: ```none Y = MVN(loc=matmul(W, x) + b, scale_tril=f(reshape_tril(matmul(M, x) + c))) where, W in R^[d, n] M in R^[d*(d+1)/2, n] b in R^d c in R^d f(S) = set_diag(S, softplus(matrix_diag_part(S)) + 1e-5) ``` Observe that `f` makes the diagonal of the triangular-lower scale matrix be positive and no smaller than `1e-5`. #### Examples ```python # This example fits a multilinear regression loss. import tensorflow as tf import tensorflow_probability as tfp # Create fictitious training data. dtype = np.float32 n = 3000 # number of samples x_size = 4 # size of single x y_size = 2 # size of single y def make_training_data(): np.random.seed(142) x = np.random.randn(n, x_size).astype(dtype) w = np.random.randn(x_size, y_size).astype(dtype) b = np.random.randn(1, y_size).astype(dtype) true_mean = np.tensordot(x, w, axes=[[-1], [0]]) + b noise = np.random.randn(n, y_size).astype(dtype) y = true_mean + noise return y, x y, x = make_training_data() # Build TF graph for fitting MVNTriL maximum likelihood estimator. mvn = tfp.trainable_distributions.multivariate_normal_tril(x, dims=y_size) loss = -tf.reduce_mean(mvn.log_prob(y)) train_op = tf.train.AdamOptimizer(learning_rate=2.**-3).minimize(loss) mse = tf.reduce_mean(tf.squared_difference(y, mvn.mean())) init_op = tf.global_variables_initializer() # Run graph 1000 times. num_steps = 1000 loss_ = np.zeros(num_steps) # Style: `_` to indicate sess.run result. mse_ = np.zeros(num_steps) with tf.Session() as sess: sess.run(init_op) for it in xrange(loss_.size): _, loss_[it], mse_[it] = sess.run([train_op, loss, mse]) if it % 200 == 0 or it == loss_.size - 1: print(\"iteration:{} loss:{} mse:{}\".format(it, loss_[it], mse_[it])) # ==> iteration:0 loss:38.2020797729 mse:4.17175960541 # iteration:200 loss:2.90179634094 mse:0.990987896919 # iteration:400 loss:2.82727336884 mse:0.990926623344 # iteration:600 loss:2.82726788521 mse:0.990926682949 # iteration:800 loss:2.82726788521 mse:0.990926682949 # iteration:999 loss:2.82726788521 mse:0.990926682949 ``` Args: x: `Tensor` with floating type. Must have statically defined rank and statically known right-most dimension. dims: Scalar, `int`, `Tensor` indicated the MVN event size, i.e., the created MVN will be distribution over length-`dims` vectors. layer_fn: Python `callable` which takes input `x` and `int` scalar `d` and returns a transformation of `x` with shape `tf.concat([tf.shape(x)[:-1], [d]], axis=0)`. Default value: `tf.layers.dense`. loc_fn: Python `callable` which transforms the `loc` parameter. Takes a (batch of) length-`dims` vectors and returns a `Tensor` of same shape and `dtype`. Default value: `lambda x: x`. scale_fn: Python `callable` which transforms the `scale` parameters. Takes a (batch of) length-`dims * (dims + 1) / 2` vectors and returns a lower-triangular `Tensor` of same batch shape with rightmost dimensions having shape `[dims, dims]`. Default value: `tril_with_diag_softplus_and_shift`. name: A `name_scope` name for operations created by this function. Default value: `None` (i.e., \"multivariate_normal_tril\"). Returns: mvntril: An instance of `tfd.MultivariateNormalTriL`. \"\"\" with tf.compat.v1.name_scope(name, 'multivariate_normal_tril', [x, dims]): x = tf.convert_to_tensor(value=x, name='x') x = layer_fn(x, dims + dims * (dims + 1) // 2) return tfd.MultivariateNormalTriL( loc=loc_fn(x[..., :dims]), scale_tril=scale_fn(x[..., dims:]))"], "rank": 2}
{"nl": "python how to check dtype", "code": ["def make_kind_check(python_types, numpy_kind): \"\"\" Make a function that checks whether a scalar or array is of a given kind (e.g. float, int, datetime, timedelta). \"\"\" def check(value): if hasattr(value, 'dtype'): return value.dtype.kind == numpy_kind return isinstance(value, python_types) return check", "def is_numeric_dtype(dtype): \"\"\"Return ``True`` if ``dtype`` is a numeric type.\"\"\" dtype = np.dtype(dtype) return np.issubsctype(getattr(dtype, 'base', None), np.number)", "def is_string_dtype(arr_or_dtype): \"\"\" Check whether the provided array or dtype is of the string dtype. Parameters ---------- arr_or_dtype : array-like The array or dtype to check. Returns ------- boolean Whether or not the array or dtype is of the string dtype. Examples -------- >>> is_string_dtype(str) True >>> is_string_dtype(object) True >>> is_string_dtype(int) False >>> >>> is_string_dtype(np.array(['a', 'b'])) True >>> is_string_dtype(pd.Series([1, 2])) False \"\"\" # TODO: gh-15585: consider making the checks stricter. def condition(dtype): return dtype.kind in ('O', 'S', 'U') and not is_period_dtype(dtype) return _is_dtype(arr_or_dtype, condition)", "def _isstring(dtype): \"\"\"Given a numpy dtype, determines whether it is a string. Returns True if the dtype is string or unicode. \"\"\" return dtype.type == numpy.unicode_ or dtype.type == numpy.string_", "def infer_dtype_from(val, pandas_dtype=False): \"\"\" interpret the dtype from a scalar or array. This is a convenience routines to infer dtype from a scalar or an array Parameters ---------- pandas_dtype : bool, default False whether to infer dtype including pandas extension types. If False, scalar/array belongs to pandas extension types is inferred as object \"\"\" if is_scalar(val): return infer_dtype_from_scalar(val, pandas_dtype=pandas_dtype) return infer_dtype_from_array(val, pandas_dtype=pandas_dtype)"], "rank": 3}
{"nl": "cast object of type bytes to string python", "code": ["def to_str(obj): \"\"\"Attempts to convert given object to a string object \"\"\" if not isinstance(obj, str) and PY3 and isinstance(obj, bytes): obj = obj.decode('utf-8') return obj if isinstance(obj, string_types) else str(obj)", "def bytes_to_str(s, encoding='utf-8'): \"\"\"Returns a str if a bytes object is given.\"\"\" if six.PY3 and isinstance(s, bytes): return s.decode(encoding) return s", "def to_str(s): \"\"\" Convert bytes and non-string into Python 3 str \"\"\" if isinstance(s, bytes): s = s.decode('utf-8') elif not isinstance(s, str): s = str(s) return s", "def to_binary(s, encoding='utf8'): \"\"\"Portable cast function. In python 2 the ``str`` function which is used to coerce objects to bytes does not accept an encoding argument, whereas python 3's ``bytes`` function requires one. :param s: object to be converted to binary_type :return: binary_type instance, representing s. \"\"\" if PY3: # pragma: no cover return s if isinstance(s, binary_type) else binary_type(s, encoding=encoding) return binary_type(s)", "def ub_to_str(string): \"\"\" converts py2 unicode / py3 bytestring into str Args: string (unicode, byte_string): string to be converted Returns: (str) \"\"\" if not isinstance(string, str): if six.PY2: return str(string) else: return string.decode() return string"], "rank": 1}
{"nl": "python remove repeated elements in list", "code": ["def get_uniques(l): \"\"\" Returns a list with no repeated elements. \"\"\" result = [] for i in l: if i not in result: result.append(i) return result", "def unique(list): \"\"\" Returns a copy of the list without duplicates. \"\"\" unique = []; [unique.append(x) for x in list if x not in unique] return unique", "def deduplicate(list_object): \"\"\"Rebuild `list_object` removing duplicated and keeping order\"\"\" new = [] for item in list_object: if item not in new: new.append(item) return new", "def remove_list_duplicates(lista, unique=False): \"\"\" Remove duplicated elements in a list. Args: lista: List with elements to clean duplicates. \"\"\" result = [] allready = [] for elem in lista: if elem not in result: result.append(elem) else: allready.append(elem) if unique: for elem in allready: result = list(filter((elem).__ne__, result)) return result", "def purge_duplicates(list_in): \"\"\"Remove duplicates from list while preserving order. Parameters ---------- list_in: Iterable Returns ------- list List of first occurences in order \"\"\" _list = [] for item in list_in: if item not in _list: _list.append(item) return _list"], "rank": 6}
{"nl": "python smooth an array", "code": ["def smooth_array(array, amount=1): \"\"\" Returns the nearest-neighbor (+/- amount) smoothed array. This does not modify the array or slice off the funny end points. \"\"\" if amount==0: return array # we have to store the old values in a temp array to keep the # smoothing from affecting the smoothing new_array = _n.array(array) for n in range(len(array)): new_array[n] = smooth(array, n, amount) return new_array", "def smooth_image(image, sigma, sigma_in_physical_coordinates=True, FWHM=False, max_kernel_width=32): \"\"\" Smooth an image ANTsR function: `smoothImage` Arguments --------- image Image to smooth sigma Smoothing factor. Can be scalar, in which case the same sigma is applied to each dimension, or a vector of length dim(inimage) to specify a unique smoothness for each dimension. sigma_in_physical_coordinates : boolean If true, the smoothing factor is in millimeters; if false, it is in pixels. FWHM : boolean If true, sigma is interpreted as the full-width-half-max (FWHM) of the filter, not the sigma of a Gaussian kernel. max_kernel_width : scalar Maximum kernel width Returns ------- ANTsImage Example ------- >>> import ants >>> image = ants.image_read( ants.get_ants_data('r16')) >>> simage = ants.smooth_image(image, (1.2,1.5)) \"\"\" if image.components == 1: return _smooth_image_helper(image, sigma, sigma_in_physical_coordinates, FWHM, max_kernel_width) else: imagelist = utils.split_channels(image) newimages = [] for image in imagelist: newimage = _smooth_image_helper(image, sigma, sigma_in_physical_coordinates, FWHM, max_kernel_width) newimages.append(newimage) return utils.merge_channels(newimages)", "def normalize(data): \"\"\" Function to normalize data to have mean 0 and unity standard deviation (also called z-transform) Parameters ---------- data : numpy.ndarray Returns ------- numpy.ndarray z-transform of input array \"\"\" data = data.astype(float) data -= data.mean() return data / data.std()", "def _rescale_array(self, array, scale, zero): \"\"\" Scale the input array \"\"\" if scale != 1.0: sval = numpy.array(scale, dtype=array.dtype) array *= sval if zero != 0.0: zval = numpy.array(zero, dtype=array.dtype) array += zval", "def sinwave(n=4,inc=.25): \"\"\" Returns a DataFrame with the required format for a surface (sine wave) plot Parameters: ----------- n : int Ranges for X and Y axis (-n,n) n_y : int Size of increment along the axis \"\"\" x=np.arange(-n,n,inc) y=np.arange(-n,n,inc) X,Y=np.meshgrid(x,y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R)/(.5*R) return pd.DataFrame(Z,index=x,columns=y)"], "rank": 1}
{"nl": "python dictionary get case insensistive key", "code": ["def get_case_insensitive_dict_key(d: Dict, k: str) -> Optional[str]: \"\"\" Within the dictionary ``d``, find a key that matches (in case-insensitive fashion) the key ``k``, and return it (or ``None`` if there isn't one). \"\"\" for key in d.keys(): if k.lower() == key.lower(): return key return None", "def __contains__ (self, key): \"\"\"Check lowercase key item.\"\"\" assert isinstance(key, basestring) return dict.__contains__(self, key.lower())", "def __contains__(self, key): \"\"\" Invoked when determining whether a specific key is in the dictionary using `key in d`. The key is looked up case-insensitively. \"\"\" k = self._real_key(key) return k in self._data", "def get_key_by_value(dictionary, search_value): \"\"\" searchs a value in a dicionary and returns the key of the first occurrence :param dictionary: dictionary to search in :param search_value: value to search for \"\"\" for key, value in dictionary.iteritems(): if value == search_value: return ugettext(key)", "def contains_case_insensitive(adict, akey): \"\"\"Check if key is in adict. The search is case insensitive.\"\"\" for key in adict: if key.lower() == akey.lower(): return True return False"], "rank": 1}
{"nl": "key in sorted function python", "code": ["def sort_key(x): \"\"\" >>> sort_key(('name', ('ROUTE', 'URL'))) -3 \"\"\" name, (r, u) = x return - len(u) + u.count('}') * 100", "def sort_func(self, key): \"\"\"Sorting logic for `Quantity` objects.\"\"\" if key == self._KEYS.VALUE: return 'aaa' if key == self._KEYS.SOURCE: return 'zzz' return key", "def sort_key(val): \"\"\"Sort key for sorting keys in grevlex order.\"\"\" return numpy.sum((max(val)+1)**numpy.arange(len(val)-1, -1, -1)*val)", "def transcript_sort_key(transcript): \"\"\" Key function used to sort transcripts. Taking the negative of protein sequence length and nucleotide sequence length so that the transcripts with longest sequences come first in the list. This couldn't be accomplished with `reverse=True` since we're also sorting by transcript name (which places TP53-001 before TP53-002). \"\"\" return ( -len(transcript.protein_sequence), -len(transcript.sequence), transcript.name )", "def sorted_by(key: Callable[[raw_types.Qid], Any]) -> 'QubitOrder': \"\"\"A basis that orders qubits ascending based on a key function. Args: key: A function that takes a qubit and returns a key value. The basis will be ordered ascending according to these key values. Returns: A basis that orders qubits ascending based on a key function. \"\"\" return QubitOrder(lambda qubits: tuple(sorted(qubits, key=key)))"], "rank": 2}
{"nl": "indexes of sorted list python", "code": ["def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def _index_ordering(redshift_list): \"\"\" :param redshift_list: list of redshifts :return: indexes in acending order to be evaluated (from z=0 to z=z_source) \"\"\" redshift_list = np.array(redshift_list) sort_index = np.argsort(redshift_list) return sort_index", "def bisect_index(a, x): \"\"\" Find the leftmost index of an element in a list using binary search. Parameters ---------- a: list A sorted list. x: arbitrary The element. Returns ------- int The index. \"\"\" i = bisect.bisect_left(a, x) if i != len(a) and a[i] == x: return i raise ValueError", "def insort_no_dup(lst, item): \"\"\" If item is not in lst, add item to list at its sorted position \"\"\" import bisect ix = bisect.bisect_left(lst, item) if lst[ix] != item: lst[ix:ix] = [item]", "def issorted(list_, op=operator.le): \"\"\" Determines if a list is sorted Args: list_ (list): op (func): sorted operation (default=operator.le) Returns: bool : True if the list is sorted \"\"\" return all(op(list_[ix], list_[ix + 1]) for ix in range(len(list_) - 1))"], "rank": 2}
{"nl": "using color in python printouts", "code": ["def cprint(string, fg=None, bg=None, end='\\n', target=sys.stdout): \"\"\"Print a colored string to the target handle. fg and bg specify foreground- and background colors, respectively. The remaining keyword arguments are the same as for Python's built-in print function. Colors are returned to their defaults before the function returns. \"\"\" _color_manager.set_color(fg, bg) target.write(string + end) target.flush() # Needed for Python 3.x _color_manager.set_defaults()", "def write_color(string, name, style='normal', when='auto'): \"\"\" Write the given colored string to standard out. \"\"\" write(color(string, name, style, when))", "def printc(cls, txt, color=colors.red): \"\"\"Print in color.\"\"\" print(cls.color_txt(txt, color))", "def set(cls, color): \"\"\" Sets the terminal to the passed color. :param color: one of the availabe colors. \"\"\" sys.stdout.write(cls.colors.get(color, cls.colors['RESET']))", "def sprint(text, *colors): \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\" return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text"], "rank": 1}
{"nl": "how to check 2 strings are the same in python", "code": ["def eqstr(a, b): \"\"\" Determine whether two strings are equivalent. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/eqstr_c.html :param a: Arbitrary character string. :type a: str :param b: Arbitrary character string. :type b: str :return: True if A and B are equivalent. :rtype: bool \"\"\" return bool(libspice.eqstr_c(stypes.stringToCharP(a), stypes.stringToCharP(b)))", "def compare(string1, string2): \"\"\"Compare two strings while protecting against timing attacks :param str string1: the first string :param str string2: the second string :returns: True if the strings are equal, False if not :rtype: :obj:`bool` \"\"\" if len(string1) != len(string2): return False result = True for c1, c2 in izip(string1, string2): result &= c1 == c2 return result", "def indexes_equal(a: Index, b: Index) -> bool: \"\"\" Are two indexes equal? Checks by comparing ``str()`` versions of them. (AM UNSURE IF THIS IS ENOUGH.) \"\"\" return str(a) == str(b)", "def compare(self, first, second): \"\"\" Case in-sensitive comparison of two strings. Required arguments: * first - The first string to compare. * second - The second string to compare. \"\"\" if first.lower() == second.lower(): return True else: return False", "def samefile(a: str, b: str) -> bool: \"\"\"Check if two pathes represent the same file.\"\"\" try: return os.path.samefile(a, b) except OSError: return os.path.normpath(a) == os.path.normpath(b)"], "rank": 3}
{"nl": "python check that can open file", "code": ["def is_readable(filename): \"\"\"Check if file is a regular file and is readable.\"\"\" return os.path.isfile(filename) and os.access(filename, os.R_OK)", "def is_filelike(ob): \"\"\"Check for filelikeness of an object. Needed to distinguish it from file names. Returns true if it has a read or a write method. \"\"\" if hasattr(ob, 'read') and callable(ob.read): return True if hasattr(ob, 'write') and callable(ob.write): return True return False", "def _is_readable(self, obj): \"\"\"Check if the argument is a readable file-like object.\"\"\" try: read = getattr(obj, 'read') except AttributeError: return False else: return is_method(read, max_arity=1)", "def is_readable(fp, size=1): \"\"\" Check if the file-like object is readable. :param fp: file-like object :param size: byte size :return: bool \"\"\" read_size = len(fp.read(size)) fp.seek(-read_size, 1) return read_size == size", "def open_file(file, mode): \"\"\"Open a file. :arg file: file-like or path-like object. :arg str mode: ``mode`` argument for :func:`open`. \"\"\" if hasattr(file, \"read\"): return file if hasattr(file, \"open\"): return file.open(mode) return open(file, mode)"], "rank": 1}
{"nl": "python calculate log likelihood normal distributino", "code": ["def normal_log_q(self,z): \"\"\" The unnormalized log posterior components for mean-field normal family (the quantity we want to approximate) RAO-BLACKWELLIZED! \"\"\" means, scale = self.get_means_and_scales() return ss.norm.logpdf(z,loc=means,scale=scale)", "def lognorm(x, mu, sigma=1.0): \"\"\" Log-normal function from scipy \"\"\" return stats.lognorm(sigma, scale=mu).pdf(x)", "def ln_norm(x, mu, sigma=1.0): \"\"\" Natural log of scipy norm function truncated at zero \"\"\" return np.log(stats.norm(loc=mu, scale=sigma).pdf(x))", "def glog(x,l = 2): \"\"\" Generalised logarithm :param x: number :param p: number added befor logarithm \"\"\" return np.log((x+np.sqrt(x**2+l**2))/2)/np.log(l)", "def log(x): \"\"\" Natural logarithm \"\"\" if isinstance(x, UncertainFunction): mcpts = np.log(x._mcpts) return UncertainFunction(mcpts) else: return np.log(x)"], "rank": 3}
{"nl": "python make directory exists", "code": ["def ensure_dir_exists(directory): \"\"\"Se asegura de que un directorio exista.\"\"\" if directory and not os.path.exists(directory): os.makedirs(directory)", "def ensure_dir(f): \"\"\" Ensure a a file exists and if not make the relevant path \"\"\" d = os.path.dirname(f) if not os.path.exists(d): os.makedirs(d)", "def makedirs(path): \"\"\" Create directories if they do not exist, otherwise do nothing. Return path for convenience \"\"\" if not os.path.isdir(path): os.makedirs(path) return path", "def safe_mkdir_for(path, clean=False): \"\"\"Ensure that the parent directory for a file is present. If it's not there, create it. If it is, no-op. \"\"\" safe_mkdir(os.path.dirname(path), clean=clean)", "def check_create_folder(filename): \"\"\"Check if the folder exisits. If not, create the folder\"\"\" os.makedirs(os.path.dirname(filename), exist_ok=True)"], "rank": 2}
{"nl": "python redirect stdout on screen and to file", "code": ["def redirect_output(fileobj): \"\"\"Redirect standard out to file.\"\"\" old = sys.stdout sys.stdout = fileobj try: yield fileobj finally: sys.stdout = old", "def redirect_stdout(new_stdout): \"\"\"Redirect the stdout Args: new_stdout (io.StringIO): New stdout to use instead \"\"\" old_stdout, sys.stdout = sys.stdout, new_stdout try: yield None finally: sys.stdout = old_stdout", "def redirect_std(): \"\"\" Connect stdin/stdout to controlling terminal even if the scripts input and output were redirected. This is useful in utilities based on termenu. \"\"\" stdin = sys.stdin stdout = sys.stdout if not sys.stdin.isatty(): sys.stdin = open_raw(\"/dev/tty\", \"r\", 0) if not sys.stdout.isatty(): sys.stdout = open_raw(\"/dev/tty\", \"w\", 0) return stdin, stdout", "def pstd(self, *args, **kwargs): \"\"\" Console to STDOUT \"\"\" kwargs['file'] = self.out self.print(*args, **kwargs) sys.stdout.flush()", "def sys_pipes_forever(encoding=_default_encoding): \"\"\"Redirect all C output to sys.stdout/err This is not a context manager; it turns on C-forwarding permanently. \"\"\" global _mighty_wurlitzer if _mighty_wurlitzer is None: _mighty_wurlitzer = sys_pipes(encoding) _mighty_wurlitzer.__enter__()"], "rank": 1}
{"nl": "python closing db connection", "code": ["def _close(self): \"\"\" Closes the client connection to the database. \"\"\" if self.connection: with self.wrap_database_errors: self.connection.client.close()", "def close( self ): \"\"\" Close the db and release memory \"\"\" if self.db is not None: self.db.commit() self.db.close() self.db = None return", "def cleanup(self, app): \"\"\"Close all connections.\"\"\" if hasattr(self.database.obj, 'close_all'): self.database.close_all()", "def close_database_session(session): \"\"\"Close connection with the database\"\"\" try: session.close() except OperationalError as e: raise DatabaseError(error=e.orig.args[1], code=e.orig.args[0])", "def unlock(self): \"\"\"Closes the session to the database.\"\"\" if not hasattr(self, 'session'): raise RuntimeError('Error detected! The session that you want to close does not exist any more!') logger.debug(\"Closed database session of '%s'\" % self._database) self.session.close() del self.session"], "rank": 4}
{"nl": "python request cookie get", "code": ["def _get_data(self): \"\"\" Extracts the session data from cookie. \"\"\" cookie = self.adapter.cookies.get(self.name) return self._deserialize(cookie) if cookie else {}", "def parse_cookies(self, req, name, field): \"\"\"Pull the value from the cookiejar.\"\"\" return core.get_value(req.COOKIES, name, field)", "def cookies(self) -> Dict[str, str]: \"\"\"The parsed cookies attached to this request.\"\"\" cookies = SimpleCookie() cookies.load(self.headers.get('Cookie', '')) return {key: cookie.value for key, cookie in cookies.items()}", "def get_csrf_token(response): \"\"\" Extract the CSRF token out of the \"Set-Cookie\" header of a response. \"\"\" cookie_headers = [ h.decode('ascii') for h in response.headers.getlist(\"Set-Cookie\") ] if not cookie_headers: return None csrf_headers = [ h for h in cookie_headers if h.startswith(\"csrftoken=\") ] if not csrf_headers: return None match = re.match(\"csrftoken=([^ ;]+);\", csrf_headers[-1]) return match.group(1)", "def parse_cookies_str(cookies): \"\"\" parse cookies str to dict :param cookies: cookies str :type cookies: str :return: cookie dict :rtype: dict \"\"\" cookie_dict = {} for record in cookies.split(\";\"): key, value = record.strip().split(\"=\", 1) cookie_dict[key] = value return cookie_dict"], "rank": 2}
{"nl": "delete empty elements in list python3", "code": ["def clear_list_value(self, value): \"\"\" Clean the argument value to eliminate None or Falsy values if needed. \"\"\" # Don't go any further: this value is empty. if not value: return self.empty_value # Clean empty items if wanted if self.clean_empty: value = [v for v in value if v] return value or self.empty_value", "def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def remove_empty_text(utterances: List[Utterance]) -> List[Utterance]: \"\"\"Remove empty utterances from a list of utterances Args: utterances: The list of utterance we are processing \"\"\" return [utter for utter in utterances if utter.text.strip() != \"\"]", "def de_blank(val): \"\"\"Remove blank elements in `val` and return `ret`\"\"\" ret = list(val) if type(val) == list: for idx, item in enumerate(val): if item.strip() == '': ret.remove(item) else: ret[idx] = item.strip() return ret", "def unique(seq): \"\"\"Return the unique elements of a collection even if those elements are unhashable and unsortable, like dicts and sets\"\"\" cleaned = [] for each in seq: if each not in cleaned: cleaned.append(each) return cleaned"], "rank": 5}
{"nl": "python how to dump to json file", "code": ["def save(self, fname): \"\"\" Saves the dictionary in json format :param fname: file to save to \"\"\" with open(fname, 'wb') as f: json.dump(self, f)", "def _write_json(file, contents): \"\"\"Write a dict to a JSON file.\"\"\" with open(file, 'w') as f: return json.dump(contents, f, indent=2, sort_keys=True)", "def _write_json(obj, path): # type: (object, str) -> None \"\"\"Writes a serializeable object as a JSON file\"\"\" with open(path, 'w') as f: json.dump(obj, f)", "def _serialize_json(obj, fp): \"\"\" Serialize ``obj`` as a JSON formatted stream to ``fp`` \"\"\" json.dump(obj, fp, indent=4, default=serialize)", "def json_pretty_dump(obj, filename): \"\"\" Serialize obj as a JSON formatted stream to the given filename ( pretty printing version) \"\"\" with open(filename, \"wt\") as fh: json.dump(obj, fh, indent=4, sort_keys=4)"], "rank": 1}
{"nl": "python redirect stdout to 2 places", "code": ["def redirect_stdout(new_stdout): \"\"\"Redirect the stdout Args: new_stdout (io.StringIO): New stdout to use instead \"\"\" old_stdout, sys.stdout = sys.stdout, new_stdout try: yield None finally: sys.stdout = old_stdout", "def redirect_output(fileobj): \"\"\"Redirect standard out to file.\"\"\" old = sys.stdout sys.stdout = fileobj try: yield fileobj finally: sys.stdout = old", "def redirect_std(): \"\"\" Connect stdin/stdout to controlling terminal even if the scripts input and output were redirected. This is useful in utilities based on termenu. \"\"\" stdin = sys.stdin stdout = sys.stdout if not sys.stdin.isatty(): sys.stdin = open_raw(\"/dev/tty\", \"r\", 0) if not sys.stdout.isatty(): sys.stdout = open_raw(\"/dev/tty\", \"w\", 0) return stdin, stdout", "def __exit__(self, *args): \"\"\"Redirect stdout back to the original stdout.\"\"\" sys.stdout = self._orig self._devnull.close()", "def sys_pipes_forever(encoding=_default_encoding): \"\"\"Redirect all C output to sys.stdout/err This is not a context manager; it turns on C-forwarding permanently. \"\"\" global _mighty_wurlitzer if _mighty_wurlitzer is None: _mighty_wurlitzer = sys_pipes(encoding) _mighty_wurlitzer.__enter__()"], "rank": 1}
{"nl": "python 2to3 whole directory", "code": ["def data_directory(): \"\"\"Return the absolute path to the directory containing the package data.\"\"\" package_directory = os.path.abspath(os.path.dirname(__file__)) return os.path.join(package_directory, \"data\")", "def getpackagepath(): \"\"\" *Get the root path for this python package - used in unit testing code* \"\"\" moduleDirectory = os.path.dirname(__file__) packagePath = os.path.dirname(__file__) + \"/../\" return packagePath", "def get_system_root_directory(): \"\"\" Get system root directory (application installed root directory) Returns ------- string A full path \"\"\" root = os.path.dirname(__file__) root = os.path.dirname(root) root = os.path.abspath(root) return root", "def get_files(dir_name): \"\"\"Simple directory walker\"\"\" return [(os.path.join('.', d), [os.path.join(d, f) for f in files]) for d, _, files in os.walk(dir_name)]", "def listfolderpath(p): \"\"\" generator of list folder in the path. folders only \"\"\" for entry in scandir.scandir(p): if entry.is_dir(): yield entry.path"], "rank": 10}
{"nl": "python how to skip subsequent lines", "code": ["def _skip_section(self): \"\"\"Skip a section\"\"\" self._last = self._f.readline() while len(self._last) > 0 and len(self._last[0].strip()) == 0: self._last = self._f.readline()", "def _skip_newlines(self): \"\"\"Increment over newlines.\"\"\" while self._cur_token['type'] is TT.lbreak and not self._finished: self._increment()", "def _skip_frame(self): \"\"\"Skip the next time frame\"\"\" for line in self._f: if line == 'ITEM: ATOMS\\n': break for i in range(self.num_atoms): next(self._f)", "def iter_lines(file_like: Iterable[str]) -> Generator[str, None, None]: \"\"\" Helper for iterating only nonempty lines without line breaks\"\"\" for line in file_like: line = line.rstrip('\\r\\n') if line: yield line", "def _skip_frame(self): \"\"\"Skip a single frame from the trajectory\"\"\" size = self.read_size() for i in range(size+1): line = self._f.readline() if len(line) == 0: raise StopIteration"], "rank": 1}
{"nl": "python get git branch name", "code": ["def get_git_branch(git_path='git'): \"\"\"Returns the name of the current git branch \"\"\" branch_match = call((git_path, 'rev-parse', '--symbolic-full-name', 'HEAD')) if branch_match == \"HEAD\": return None else: return os.path.basename(branch_match)", "def get_current_branch(): \"\"\" Return the current branch \"\"\" cmd = [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"] output = subprocess.check_output(cmd, stderr=subprocess.STDOUT) return output.strip().decode(\"utf-8\")", "def branches(): # type: () -> List[str] \"\"\" Return a list of branches in the current repo. Returns: list[str]: A list of branches in the current repo. \"\"\" out = shell.run( 'git branch', capture=True, never_pretend=True ).stdout.strip() return [x.strip('* \\t\\n') for x in out.splitlines()]", "def branches(self): \"\"\"All branches in a list\"\"\" result = self.git(self.default + ['branch', '-a', '--no-color']) return [l.strip(' *\\n') for l in result.split('\\n') if l.strip(' *\\n')]", "def get_last_commit(git_path=None): \"\"\" Get the HEAD commit SHA1 of repository in current dir. \"\"\" if git_path is None: git_path = GIT_PATH line = get_last_commit_line(git_path) revision_id = line.split()[1] return revision_id"], "rank": 1}
{"nl": "python how to check if email and password exist", "code": ["def check_auth(email, password): \"\"\"Check if a username/password combination is valid. \"\"\" try: user = User.get(User.email == email) except User.DoesNotExist: return False return password == user.password", "def user_exists(username): \"\"\"Check if a user exists\"\"\" try: pwd.getpwnam(username) user_exists = True except KeyError: user_exists = False return user_exists", "def check_auth(username, pwd): \"\"\"This function is called to check if a username / password combination is valid. \"\"\" cfg = get_current_config() return username == cfg[\"dashboard_httpauth\"].split( \":\")[0] and pwd == cfg[\"dashboard_httpauth\"].split(\":\")[1]", "def is_password_valid(password): \"\"\" Check if a password is valid \"\"\" pattern = re.compile(r\"^.{4,75}$\") return bool(pattern.match(password))", "def is_valid(email): \"\"\"Email address validation method. :param email: Email address to be saved. :type email: basestring :returns: True if email address is correct, False otherwise. :rtype: bool \"\"\" if isinstance(email, basestring) and EMAIL_RE.match(email): return True return False"], "rank": 1}
{"nl": "get the date from a string python", "code": ["def _read_date_from_string(str1): \"\"\" Reads the date from a string in the format YYYY/MM/DD and returns :class: datetime.date \"\"\" full_date = [int(x) for x in str1.split('/')] return datetime.date(full_date[0], full_date[1], full_date[2])", "def get_date(date): \"\"\" Get the date from a value that could be a date object or a string. :param date: The date object or string. :returns: The date object. \"\"\" if type(date) is str: return datetime.strptime(date, '%Y-%m-%d').date() else: return date", "def string_to_date(value): \"\"\" Return a Python date that corresponds to the specified string representation. @param value: string representation of a date. @return: an instance ``datetime.datetime`` represented by the string. \"\"\" if isinstance(value, datetime.date): return value return dateutil.parser.parse(value).date()", "def deserialize_date(string): \"\"\" Deserializes string to date. :param string: str. :type string: str :return: date. :rtype: date \"\"\" try: from dateutil.parser import parse return parse(string).date() except ImportError: return string", "def parse(self, s): \"\"\" Parses a date string formatted like ``YYYY-MM-DD``. \"\"\" return datetime.datetime.strptime(s, self.date_format).date()"], "rank": 1}
{"nl": "python get index of lowest value in list", "code": ["def find_lt(a, x): \"\"\"Find rightmost value less than x.\"\"\" i = bs.bisect_left(a, x) if i: return i - 1 raise ValueError", "def find_lt(a, x): \"\"\"Find rightmost value less than x\"\"\" i = bisect.bisect_left(a, x) if i: return a[i-1] raise ValueError", "def find_le(a, x): \"\"\"Find rightmost value less than or equal to x.\"\"\" i = bs.bisect_right(a, x) if i: return i - 1 raise ValueError", "def find_ge(a, x): \"\"\"Find leftmost item greater than or equal to x.\"\"\" i = bs.bisect_left(a, x) if i != len(a): return i raise ValueError", "def bisect_index(a, x): \"\"\" Find the leftmost index of an element in a list using binary search. Parameters ---------- a: list A sorted list. x: arbitrary The element. Returns ------- int The index. \"\"\" i = bisect.bisect_left(a, x) if i != len(a) and a[i] == x: return i raise ValueError"], "rank": 3}
{"nl": "python write lines on new line next", "code": ["def write_line(self, line, count=1): \"\"\"writes the line and count newlines after the line\"\"\" self.write(line) self.write_newlines(count)", "def ensure_newline(self): \"\"\" use before any custom printing when using the progress iter to ensure your print statement starts on a new line instead of at the end of a progress line \"\"\" DECTCEM_SHOW = '\\033[?25h' # show cursor AT_END = DECTCEM_SHOW + '\\n' if not self._cursor_at_newline: self.write(AT_END) self._cursor_at_newline = True", "def printOut(value, end='\\n'): \"\"\" This function prints the given String immediately and flushes the output. \"\"\" sys.stdout.write(value) sys.stdout.write(end) sys.stdout.flush()", "def stdoutwriteline(*args): \"\"\" @type args: tuple @return: None \"\"\" s = \"\" for i in args: s += str(i) + \" \" s = s.strip() sys.stdout.write(str(s) + \"\\n\") sys.stdout.flush() return s", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0"], "rank": 1}
{"nl": "int to bool in python", "code": ["def is_int(value): \"\"\"Return `True` if ``value`` is an integer.\"\"\" if isinstance(value, bool): return False try: int(value) return True except (ValueError, TypeError): return False", "def to_bool(value: Any) -> bool: \"\"\"Convert string or other Python object to boolean. **Rationalle** Passing flags is one of the most common cases of using environment vars and as values are strings we need to have an easy way to convert them to boolean Python value. Without this function int or float string values can be converted as false positives, e.g. ``bool('0') => True``, but using this function ensure that digit flag be properly converted to boolean value. :param value: String or other value. \"\"\" return bool(strtobool(value) if isinstance(value, str) else value)", "def to_bool(value): # type: (Any) -> bool \"\"\" Convert a value into a bool but handle \"truthy\" strings eg, yes, true, ok, y \"\"\" if isinstance(value, _compat.string_types): return value.upper() in ('Y', 'YES', 'T', 'TRUE', '1', 'OK') return bool(value)", "def boolean(flag): \"\"\" Convert string in boolean \"\"\" s = flag.lower() if s in ('1', 'yes', 'true'): return True elif s in ('0', 'no', 'false'): return False raise ValueError('Unknown flag %r' % s)", "def _cast_boolean(value): \"\"\" Helper to convert config values to boolean as ConfigParser do. \"\"\" _BOOLEANS = {'1': True, 'yes': True, 'true': True, 'on': True, '0': False, 'no': False, 'false': False, 'off': False, '': False} value = str(value) if value.lower() not in _BOOLEANS: raise ValueError('Not a boolean: %s' % value) return _BOOLEANS[value.lower()]"], "rank": 1}
{"nl": "python numpy array how to return rows not include nan", "code": ["def nan_pixels(self): \"\"\" Return an array of the NaN pixels. Returns ------- :obj:`numpy.ndarray` Nx2 array of the NaN pixels \"\"\" nan_px = np.where(np.isnan(np.sum(self.raw_data, axis=2))) nan_px = np.c_[nan_px[0], nan_px[1]] return nan_px", "def remove_na_arraylike(arr): \"\"\" Return array-like containing only true/non-NaN values, possibly empty. \"\"\" if is_extension_array_dtype(arr): return arr[notna(arr)] else: return arr[notna(lib.values_from_object(arr))]", "def remove_nans_1D(*args) -> tuple: \"\"\"Remove nans in a set of 1D arrays. Removes indicies in all arrays if any array is nan at that index. All input arrays must have the same size. Parameters ---------- args : 1D arrays Returns ------- tuple Tuple of 1D arrays in same order as given, with nan indicies removed. \"\"\" vals = np.isnan(args[0]) for a in args: vals |= np.isnan(a) return tuple(np.array(a)[~vals] for a in args)", "def table_nan_locs(table): \"\"\" from http://stackoverflow.com/a/14033137/623735 # gets the indices of the rows with nan values in a dataframe pd.isnull(df).any(1).nonzero()[0] \"\"\" ans = [] for rownum, row in enumerate(table): try: if pd.isnull(row).any(): colnums = pd.isnull(row).nonzero()[0] ans += [(rownum, colnum) for colnum in colnums] except AttributeError: # table is really just a sequence of scalars if pd.isnull(row): ans += [(rownum, 0)] return ans", "def count_rows_with_nans(X): \"\"\"Count the number of rows in 2D arrays that contain any nan values.\"\"\" if X.ndim == 2: return np.where(np.isnan(X).sum(axis=1) != 0, 1, 0).sum()"], "rank": 6}
{"nl": "comparing sets python with boolean and set", "code": ["def __eq__(self, anotherset): \"\"\"Tests on itemlist equality\"\"\" if not isinstance(anotherset, LR0ItemSet): raise TypeError if len(self.itemlist) != len(anotherset.itemlist): return False for element in self.itemlist: if element not in anotherset.itemlist: return False return True", "def issuperset(self, other): \"\"\"Report whether this RangeSet contains another set.\"\"\" self._binary_sanity_check(other) return set.issuperset(self, other)", "def __iand__(self, other): \"\"\"Intersect this flag with ``other`` in-place. \"\"\" self.known &= other.known self.active &= other.active return self", "def isetdiff_flags(list1, list2): \"\"\" move to util_iter \"\"\" set2 = set(list2) return (item not in set2 for item in list1)", "def equal(list1, list2): \"\"\" takes flags returns indexes of True values \"\"\" return [item1 == item2 for item1, item2 in broadcast_zip(list1, list2)]"], "rank": 4}
{"nl": "python cast string to custom type", "code": ["def autoconvert(string): \"\"\"Try to convert variables into datatypes.\"\"\" for fn in (boolify, int, float): try: return fn(string) except ValueError: pass return string", "def get_truetype(value): \"\"\"Convert a string to a pythonized parameter.\"\"\" if value in [\"true\", \"True\", \"y\", \"Y\", \"yes\"]: return True if value in [\"false\", \"False\", \"n\", \"N\", \"no\"]: return False if value.isdigit(): return int(value) return str(value)", "def convert_value(bind, value): \"\"\" Type casting. \"\"\" type_name = get_type(bind) try: return typecast.cast(type_name, value) except typecast.ConverterError: return value", "def convert(self, value, _type): \"\"\" Convert instances of textx types and match rules to python types. \"\"\" return self.type_convertors.get(_type, lambda x: x)(value)", "def _cast_to_type(self, value): \"\"\" Convert the value to its string representation\"\"\" if isinstance(value, str) or value is None: return value return str(value)"], "rank": 26}
{"nl": "filter a dictionary in python and only return the key", "code": ["def filter_dict(d, keys): \"\"\" Creates a new dict from an existing dict that only has the given keys \"\"\" return {k: v for k, v in d.items() if k in keys}", "def filter_dict_by_key(d, keys): \"\"\"Filter the dict *d* to remove keys not in *keys*.\"\"\" return {k: v for k, v in d.items() if k in keys}", "def dict_pick(dictionary, allowed_keys): \"\"\" Return a dictionary only with keys found in `allowed_keys` \"\"\" return {key: value for key, value in viewitems(dictionary) if key in allowed_keys}", "def _pick_attrs(attrs, keys): \"\"\" Return attrs with keys in keys list \"\"\" return dict((k, v) for k, v in attrs.items() if k in keys)", "def _(f, x): \"\"\" filter for dict, note `f` should have signature: `f::key->value->bool` \"\"\" return {k: v for k, v in x.items() if f(k, v)}"], "rank": 1}
{"nl": "datetime to epoch python2", "code": ["def _dt_to_epoch(dt): \"\"\"Convert datetime to epoch seconds.\"\"\" try: epoch = dt.timestamp() except AttributeError: # py2 epoch = (dt - datetime(1970, 1, 1)).total_seconds() return epoch", "def _DateToEpoch(date): \"\"\"Converts python datetime to epoch microseconds.\"\"\" tz_zero = datetime.datetime.utcfromtimestamp(0) diff_sec = int((date - tz_zero).total_seconds()) return diff_sec * 1000000", "def AmericanDateToEpoch(self, date_str): \"\"\"Take a US format date and return epoch.\"\"\" try: epoch = time.strptime(date_str, \"%m/%d/%Y\") return int(calendar.timegm(epoch)) * 1000000 except ValueError: return 0", "def session_to_epoch(timestamp): \"\"\" converts Synergy Timestamp for session to UTC zone seconds since epoch \"\"\" utc_timetuple = datetime.strptime(timestamp, SYNERGY_SESSION_PATTERN).replace(tzinfo=None).utctimetuple() return calendar.timegm(utc_timetuple)", "def convertDatetime(t): \"\"\" Converts the specified datetime object into its appropriate protocol value. This is the number of milliseconds from the epoch. \"\"\" epoch = datetime.datetime.utcfromtimestamp(0) delta = t - epoch millis = delta.total_seconds() * 1000 return int(millis)"], "rank": 1}
{"nl": "python how to remove extra spaces in a string", "code": ["def strip_spaces(s): \"\"\" Strip excess spaces from a string \"\"\" return u\" \".join([c for c in s.split(u' ') if c])", "def strip_spaces(x): \"\"\" Strips spaces :param x: :return: \"\"\" x = x.replace(b' ', b'') x = x.replace(b'\\t', b'') return x", "def normalize_value(text): \"\"\" This removes newlines and multiple spaces from a string. \"\"\" result = text.replace('\\n', ' ') result = re.subn('[ ]{2,}', ' ', result)[0] return result", "def remove_bad(string): \"\"\" remove problem characters from string \"\"\" remove = [':', ',', '(', ')', ' ', '|', ';', '\\''] for c in remove: string = string.replace(c, '_') return string", "def lowstrip(term): \"\"\"Convert to lowercase and strip spaces\"\"\" term = re.sub('\\s+', ' ', term) term = term.lower() return term"], "rank": 6}
{"nl": "running python on your webserver", "code": ["def web(host, port): \"\"\"Start web application\"\"\" from .webserver.web import get_app get_app().run(host=host, port=port)", "def start(): \"\"\"Starts the web server.\"\"\" global app bottle.run(app, host=conf.WebHost, port=conf.WebPort, debug=conf.WebAutoReload, reloader=conf.WebAutoReload, quiet=conf.WebQuiet)", "def run(context, port): \"\"\" Run the Webserver/SocketIO and app \"\"\" global ctx ctx = context app.run(port=port)", "def server(port): \"\"\"Start the Django dev server.\"\"\" args = ['python', 'manage.py', 'runserver'] if port: args.append(port) run.main(args)", "def launch_server(): \"\"\"Launches the django server at 127.0.0.1:8000 \"\"\" print(os.path.dirname(os.path.abspath(__file__))) cur_dir = os.getcwd() path = os.path.dirname(os.path.abspath(__file__)) run = True os.chdir(path) os.system('python manage.py runserver --nostatic') os.chdir(cur_dir)"], "rank": 1}
{"nl": "python check if date is valid", "code": ["def valid_date(x: str) -> bool: \"\"\" Retrun ``True`` if ``x`` is a valid YYYYMMDD date; otherwise return ``False``. \"\"\" try: if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT): raise ValueError return True except ValueError: return False", "def datetime_is_iso(date_str): \"\"\"Attempts to parse a date formatted in ISO 8601 format\"\"\" try: if len(date_str) > 10: dt = isodate.parse_datetime(date_str) else: dt = isodate.parse_date(date_str) return True, [] except: # Any error qualifies as not ISO format return False, ['Datetime provided is not in a valid ISO 8601 format']", "def is_date(thing): \"\"\"Checks if the given thing represents a date :param thing: The object to check if it is a date :type thing: arbitrary object :returns: True if we have a date object :rtype: bool \"\"\" # known date types date_types = (datetime.datetime, datetime.date, DateTime) return isinstance(thing, date_types)", "def _validate_date_str(str_): \"\"\"Validate str as a date and return string version of date\"\"\" if not str_: return None # Convert to datetime so we can validate it's a real date that exists then # convert it back to the string. try: date = datetime.strptime(str_, DATE_FMT) except ValueError: msg = 'Invalid date format, should be YYYY-MM-DD' raise argparse.ArgumentTypeError(msg) return date.strftime(DATE_FMT)", "def is_date_type(cls): \"\"\"Return True if the class is a date type.\"\"\" if not isinstance(cls, type): return False return issubclass(cls, date) and not issubclass(cls, datetime)"], "rank": 1}
{"nl": "python milliseconds delta time to float", "code": ["def datetime_delta_to_ms(delta): \"\"\" Given a datetime.timedelta object, return the delta in milliseconds \"\"\" delta_ms = delta.days * 24 * 60 * 60 * 1000 delta_ms += delta.seconds * 1000 delta_ms += delta.microseconds / 1000 delta_ms = int(delta_ms) return delta_ms", "def timedelta2millisecond(td): \"\"\"Get milliseconds from a timedelta.\"\"\" milliseconds = td.days * 24 * 60 * 60 * 1000 milliseconds += td.seconds * 1000 milliseconds += td.microseconds / 1000 return milliseconds", "def datetime_to_ms(dt): \"\"\" Converts a datetime to a millisecond accuracy timestamp \"\"\" seconds = calendar.timegm(dt.utctimetuple()) return seconds * 1000 + int(dt.microsecond / 1000)", "def ms_to_datetime(ms): \"\"\" Converts a millisecond accuracy timestamp to a datetime \"\"\" dt = datetime.datetime.utcfromtimestamp(ms / 1000) return dt.replace(microsecond=(ms % 1000) * 1000).replace(tzinfo=pytz.utc)", "def convertDatetime(t): \"\"\" Converts the specified datetime object into its appropriate protocol value. This is the number of milliseconds from the epoch. \"\"\" epoch = datetime.datetime.utcfromtimestamp(0) delta = t - epoch millis = delta.total_seconds() * 1000 return int(millis)"], "rank": 1}
{"nl": "python reusing an iterator", "code": ["def __iter__(self): \"\"\"Define a generator function and return it\"\"\" def generator(): for i, obj in enumerate(self._sequence): if i >= self._limit: break yield obj raise StopIteration return generator", "def reset(self): \"\"\" Resets the iterator to the start. Any remaining values in the current iteration are discarded. \"\"\" self.__iterator, self.__saved = itertools.tee(self.__saved)", "def _fill(self): \"\"\"Advance the iterator without returning the old head.\"\"\" try: self._head = self._iterable.next() except StopIteration: self._head = None", "def peekiter(iterable): \"\"\"Return first row and also iterable with same items as original\"\"\" it = iter(iterable) one = next(it) def gen(): \"\"\"Generator that returns first and proxy other items from source\"\"\" yield one while True: yield next(it) return (one, gen())", "def __iter__(self): \"\"\"Iterate through all elements. Multiple copies will be returned if they exist. \"\"\" for value, count in self.counts(): for _ in range(count): yield value"], "rank": 2}
{"nl": "finding the median in python 3", "code": ["def median(data): \"\"\"Calculate the median of a list.\"\"\" data.sort() num_values = len(data) half = num_values // 2 if num_values % 2: return data[half] return 0.5 * (data[half-1] + data[half])", "def median(data): \"\"\" Return the median of numeric data, unsing the \"mean of middle two\" method. If ``data`` is empty, ``0`` is returned. Examples -------- >>> median([1, 3, 5]) 3.0 When the number of data points is even, the median is interpolated: >>> median([1, 3, 5, 7]) 4.0 \"\"\" if len(data) == 0: return None data = sorted(data) return float((data[len(data) // 2] + data[(len(data) - 1) // 2]) / 2.)", "def getMedian(numericValues): \"\"\" Gets the median of a list of values Returns a float/int \"\"\" theValues = sorted(numericValues) if len(theValues) % 2 == 1: return theValues[(len(theValues) + 1) / 2 - 1] else: lower = theValues[len(theValues) / 2 - 1] upper = theValues[len(theValues) / 2] return (float(lower + upper)) / 2", "def median_high(data): \"\"\"Return the high median of data. When the number of data points is odd, the middle value is returned. When it is even, the larger of the two middle values is returned. \"\"\" data = sorted(data) n = len(data) if n == 0: raise StatisticsError(\"no median for empty data\") return data[n // 2]", "def median(ls): \"\"\" Takes a list and returns the median. \"\"\" ls = sorted(ls) return ls[int(floor(len(ls)/2.0))]"], "rank": 4}
{"nl": "how to delete the element in list at a particular index in python", "code": ["def pop(self, index=-1): \"\"\"Remove and return the item at index.\"\"\" value = self._list.pop(index) del self._dict[value] return value", "def DeleteIndex(self, index): \"\"\" Remove a spent coin based on its index. Args: index (int): \"\"\" to_remove = None for i in self.Items: if i.index == index: to_remove = i if to_remove: self.Items.remove(to_remove)", "def __delitem__(self, key): \"\"\"Remove item with given key from the mapping. Runs in O(n), unless removing last item, then in O(1). \"\"\" index, value = self._dict.pop(key) key2, value2 = self._list.pop(index) assert key == key2 assert value is value2 self._fix_indices_after_delete(index)", "def remove_elements(target, indices): \"\"\"Remove multiple elements from a list and return result. This implementation is faster than the alternative below. Also note the creation of a new list to avoid altering the original. We don't have any current use for the original intact list, but may in the future...\"\"\" copied = list(target) for index in reversed(indices): del copied[index] return copied", "def remove(parent, idx): \"\"\"Remove a value from a dict.\"\"\" if isinstance(parent, dict): del parent[idx] elif isinstance(parent, list): del parent[int(idx)] else: raise JSONPathError(\"Invalid path for operation\")"], "rank": 1}
{"nl": "how to test if two files are the same in python", "code": ["def is_same_file (filename1, filename2): \"\"\"Check if filename1 and filename2 point to the same file object. There can be false negatives, ie. the result is False, but it is the same file anyway. Reason is that network filesystems can create different paths to the same physical file. \"\"\" if filename1 == filename2: return True if os.name == 'posix': return os.path.samefile(filename1, filename2) return is_same_filename(filename1, filename2)", "def samefile(a: str, b: str) -> bool: \"\"\"Check if two pathes represent the same file.\"\"\" try: return os.path.samefile(a, b) except OSError: return os.path.normpath(a) == os.path.normpath(b)", "def cmp_contents(filename1, filename2): \"\"\" Returns True if contents of the files are the same Parameters ---------- filename1 : str filename of first file to compare filename2 : str filename of second file to compare Returns ------- tf : bool True if binary contents of `filename1` is same as binary contents of `filename2`, False otherwise. \"\"\" with open_readable(filename1, 'rb') as fobj: contents1 = fobj.read() with open_readable(filename2, 'rb') as fobj: contents2 = fobj.read() return contents1 == contents2", "def file_uptodate(fname, cmp_fname): \"\"\"Check if a file exists, is non-empty and is more recent than cmp_fname. \"\"\" try: return (file_exists(fname) and file_exists(cmp_fname) and getmtime(fname) >= getmtime(cmp_fname)) except OSError: return False", "def compare(left, right): \"\"\" yields EVENT,ENTRY pairs describing the differences between left and right, which are filenames for a pair of zip files \"\"\" with open_zip(left) as l: with open_zip(right) as r: return compare_zips(l, r)"], "rank": 1}
{"nl": "add a header to a csv in python", "code": ["def printheader(h=None): \"\"\"Print the header for the CSV table.\"\"\" writer = csv.writer(sys.stdout) writer.writerow(header_fields(h))", "def writeCSV(data, headers, csvFile): \"\"\"Write data with column headers to a CSV.\"\"\" with open(csvFile, \"wb\") as f: writer = csv.writer(f, delimiter=\",\") writer.writerow(headers) writer.writerows(data)", "def generate_header(headerfields, oldheader, group_by_field): \"\"\"Returns a header as a list, ready to write to TSV file\"\"\" fieldtypes = ['peptidefdr', 'peptidepep', 'nopsms', 'proteindata', 'precursorquant', 'isoquant'] return generate_general_header(headerfields, fieldtypes, peptabledata.HEADER_PEPTIDE, oldheader, group_by_field)", "def draw_header(self, stream, header): \"\"\"Draw header with underline\"\"\" stream.writeln('=' * (len(header) + 4)) stream.writeln('| ' + header + ' |') stream.writeln('=' * (len(header) + 4)) stream.writeln()", "def format_header_cell(val): \"\"\" Formats given header column. This involves changing '_Px_' to '(', '_xP_' to ')' and all other '_' to spaces. \"\"\" return re.sub('_', ' ', re.sub(r'(_Px_)', '(', re.sub(r'(_xP_)', ')', str(val) )))"], "rank": 2}
{"nl": "python delete logfile still in use", "code": ["def DeleteLog() -> None: \"\"\"Delete log file.\"\"\" if os.path.exists(Logger.FileName): os.remove(Logger.FileName)", "def safe_delete(filename): \"\"\"Delete a file safely. If it's not present, no-op.\"\"\" try: os.unlink(filename) except OSError as e: if e.errno != errno.ENOENT: raise", "def stoplog(self): \"\"\" Stop logging. @return: 1 on success and 0 on error @rtype: integer \"\"\" if self._file_logger: self.logger.removeHandler(_file_logger) self._file_logger = None return 1", "def close_log(log, verbose=True): \"\"\"Close log This method closes and active logging.Logger instance. Parameters ---------- log : logging.Logger Logging instance \"\"\" if verbose: print('Closing log file:', log.name) # Send closing message. log.info('The log file has been closed.') # Remove all handlers from log. [log.removeHandler(handler) for handler in log.handlers]", "def cleanup(self): \"\"\"Clean up any temporary files.\"\"\" for file in glob.glob(self.basename + '*'): os.unlink(file)"], "rank": 1}
{"nl": "truncate a number after certain amount of decimals in python", "code": ["def truncate(value: Decimal, n_digits: int) -> Decimal: \"\"\"Truncates a value to a number of decimals places\"\"\" return Decimal(math.trunc(value * (10 ** n_digits))) / (10 ** n_digits)", "def price_rounding(price, decimals=2): \"\"\"Takes a decimal price and rounds to a number of decimal places\"\"\" try: exponent = D('.' + decimals * '0') except InvalidOperation: # Currencies with no decimal places, ex. JPY, HUF exponent = D() return price.quantize(exponent, rounding=ROUND_UP)", "def round_to_x_digits(number, digits): \"\"\" Returns 'number' rounded to 'digits' digits. \"\"\" return round(number * math.pow(10, digits)) / math.pow(10, digits)", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0", "def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)"], "rank": 1}
{"nl": "join list of empty strings and strings python", "code": ["def join(mapping, bind, values): \"\"\" Merge all the strings. Put space between them. \"\"\" return [' '.join([six.text_type(v) for v in values if v is not None])]", "def commajoin_as_strings(iterable): \"\"\" Join the given iterable with ',' \"\"\" return _(u',').join((six.text_type(i) for i in iterable))", "def list_to_str(lst): \"\"\" Turn a list into a comma- and/or and-separated string. Parameters ---------- lst : :obj:`list` A list of strings to join into a single string. Returns ------- str_ : :obj:`str` A string with commas and/or ands separating th elements from ``lst``. \"\"\" if len(lst) == 1: str_ = lst[0] elif len(lst) == 2: str_ = ' and '.join(lst) elif len(lst) > 2: str_ = ', '.join(lst[:-1]) str_ += ', and {0}'.format(lst[-1]) else: raise ValueError('List of length 0 provided.') return str_", "def _py2_and_3_joiner(sep, joinable): \"\"\" Allow '\\n'.join(...) statements to work in Py2 and Py3. :param sep: :param joinable: :return: \"\"\" if ISPY3: sep = bytes(sep, DEFAULT_ENCODING) joined = sep.join(joinable) return joined.decode(DEFAULT_ENCODING) if ISPY3 else joined", "def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res"], "rank": 1}
{"nl": "how to make upper and lower case string the same in python", "code": ["def clean(some_string, uppercase=False): \"\"\" helper to clean up an input string \"\"\" if uppercase: return some_string.strip().upper() else: return some_string.strip().lower()", "def _lower(string): \"\"\"Custom lower string function. Examples: FooBar -> foo_bar \"\"\" if not string: return \"\" new_string = [string[0].lower()] for char in string[1:]: if char.isupper(): new_string.append(\"_\") new_string.append(char.lower()) return \"\".join(new_string)", "def is_equal_strings_ignore_case(first, second): \"\"\"The function compares strings ignoring case\"\"\" if first and second: return first.upper() == second.upper() else: return not (first or second)", "def _upper(val_list): \"\"\" :param val_list: a list of strings :return: a list of upper-cased strings \"\"\" res = [] for ele in val_list: res.append(ele.upper()) return res", "def upcaseTokens(s,l,t): \"\"\"Helper parse action to convert tokens to upper case.\"\"\" return [ tt.upper() for tt in map(_ustr,t) ]"], "rank": 1}
{"nl": "python count whitespace characters", "code": ["def _count_leading_whitespace(text): \"\"\"Returns the number of characters at the beginning of text that are whitespace.\"\"\" idx = 0 for idx, char in enumerate(text): if not char.isspace(): return idx return idx + 1", "def _check_whitespace(string): \"\"\" Make sure thre is no whitespace in the given string. Will raise a ValueError if whitespace is detected \"\"\" if string.count(' ') + string.count('\\t') + string.count('\\n') > 0: raise ValueError(INSTRUCTION_HAS_WHITESPACE)", "def GetIndentLevel(line): \"\"\"Return the number of leading spaces in line. Args: line: A string to check. Returns: An integer count of leading spaces, possibly zero. \"\"\" indent = Match(r'^( *)\\S', line) if indent: return len(indent.group(1)) else: return 0", "def _get_indent_length(line): \"\"\"Return the length of the indentation on the given token's line.\"\"\" result = 0 for char in line: if char == \" \": result += 1 elif char == \"\\t\": result += _TAB_LENGTH else: break return result", "def indentsize(line): \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\" expline = string.expandtabs(line) return len(expline) - len(string.lstrip(expline))"], "rank": 1}
{"nl": "does cos and sin in python use degrees or radians", "code": ["def cos_sin_deg(deg): \"\"\"Return the cosine and sin for the given angle in degrees, with special-case handling of multiples of 90 for perfect right angles \"\"\" deg = deg % 360.0 if deg == 90.0: return 0.0, 1.0 elif deg == 180.0: return -1.0, 0 elif deg == 270.0: return 0, -1.0 rad = math.radians(deg) return math.cos(rad), math.sin(rad)", "def hmsToDeg(h, m, s): \"\"\"Convert RA hours, minutes, seconds into an angle in degrees.\"\"\" return h * degPerHMSHour + m * degPerHMSMin + s * degPerHMSSec", "def cart2pol(x, y): \"\"\"Cartesian to Polar coordinates conversion.\"\"\" theta = np.arctan2(y, x) rho = np.hypot(x, y) return theta, rho", "def _cosine(a, b): \"\"\" Return the len(a & b) / len(a) \"\"\" return 1. * len(a & b) / (math.sqrt(len(a)) * math.sqrt(len(b)))", "def get_lons_from_cartesian(x__, y__): \"\"\"Get longitudes from cartesian coordinates. \"\"\" return rad2deg(arccos(x__ / sqrt(x__ ** 2 + y__ ** 2))) * sign(y__)"], "rank": 1}
{"nl": "python add to two values", "code": ["def generic_add(a, b): \"\"\"Simple function to add two numbers\"\"\" logger.debug('Called generic_add({}, {})'.format(a, b)) return a + b", "def generic_add(a, b): print \"\"\"Simple function to add two numbers\"\"\" logger.info('Called generic_add({}, {})'.format(a, b)) return a + b", "def __add__(self, other): \"\"\"Handle the `+` operator.\"\"\" return self._handle_type(other)(self.value + other.value)", "def __add__(self, other): \"\"\"Left addition.\"\"\" return chaospy.poly.collection.arithmetics.add(self, other)", "def __add__(self, other): \"\"\"Concatenate two InferenceData objects.\"\"\" return concat(self, other, copy=True, inplace=False)"], "rank": 3}
{"nl": "create conda environment for python 2", "code": ["def update(packages, env=None, user=None): \"\"\" Update conda packages in a conda env Attributes ---------- packages: list of packages comma delimited \"\"\" packages = ' '.join(packages.split(',')) cmd = _create_conda_cmd('update', args=[packages, '--yes', '-q'], env=env, user=user) return _execcmd(cmd, user=user)", "def create_conda_env(sandbox_dir, env_name, dependencies, options=()): \"\"\" Create a conda environment inside the current sandbox for the given list of dependencies and options. Parameters ---------- sandbox_dir : str env_name : str dependencies : list List of conda specs options List of additional options to pass to conda. Things like [\"-c\", \"conda-forge\"] Returns ------- (env_dir, env_name) \"\"\" env_dir = os.path.join(sandbox_dir, env_name) cmdline = [\"conda\", \"create\", \"--yes\", \"--copy\", \"--quiet\", \"-p\", env_dir] + list(options) + dependencies log.info(\"Creating conda environment: \") log.info(\" command line: %s\", cmdline) subprocess.check_call(cmdline, stderr=subprocess.PIPE, stdout=subprocess.PIPE) log.debug(\"Environment created\") return env_dir, env_name", "def _find_conda(): \"\"\"Find the conda executable robustly across conda versions. Returns ------- conda : str Path to the conda executable. Raises ------ IOError If the executable cannot be found in either the CONDA_EXE environment variable or in the PATH. Notes ----- In POSIX platforms in conda >= 4.4, conda can be set up as a bash function rather than an executable. (This is to enable the syntax ``conda activate env-name``.) In this case, the environment variable ``CONDA_EXE`` contains the path to the conda executable. In other cases, we use standard search for the appropriate name in the PATH. See https://github.com/airspeed-velocity/asv/issues/645 for more details. \"\"\" if 'CONDA_EXE' in os.environ: conda = os.environ['CONDA_EXE'] else: conda = util.which('conda') return conda", "def get_environment_info() -> dict: \"\"\" Information about Cauldron and its Python interpreter. :return: A dictionary containing information about the Cauldron and its Python environment. This information is useful when providing feedback and bug reports. \"\"\" data = _environ.systems.get_system_data() data['cauldron'] = _environ.package_settings.copy() return data", "def init(): \"\"\" Execute init tasks for all components (virtualenv, pip). \"\"\" print(yellow(\"# Setting up environment...\\n\", True)) virtualenv.init() virtualenv.update_requirements() print(green(\"\\n# DONE.\", True)) print(green(\"Type \") + green(\"activate\", True) + green(\" to enable your virtual environment.\"))"], "rank": 2}
{"nl": "python check two image same", "code": ["def is_same_shape(self, other_im, check_channels=False): \"\"\" Checks if two images have the same height and width (and optionally channels). Parameters ---------- other_im : :obj:`Image` image to compare check_channels : bool whether or not to check equality of the channels Returns ------- bool True if the images are the same shape, False otherwise \"\"\" if self.height == other_im.height and self.width == other_im.width: if check_channels and self.channels != other_im.channels: return False return True return False", "def _is_image_sequenced(image): \"\"\"Determine if the image is a sequenced image.\"\"\" try: image.seek(1) image.seek(0) result = True except EOFError: result = False return result", "def _validate_image_rank(self, img_array): \"\"\" Images must be either 2D or 3D. \"\"\" if img_array.ndim == 1 or img_array.ndim > 3: msg = \"{0}D imagery is not allowed.\".format(img_array.ndim) raise IOError(msg)", "def is_progressive(image): \"\"\" Check to see if an image is progressive. \"\"\" if not isinstance(image, Image.Image): # Can only check PIL images for progressive encoding. return False return ('progressive' in image.info) or ('progression' in image.info)", "def is_empty(self): \"\"\"Checks for an empty image. \"\"\" if(((self.channels == []) and (not self.shape == (0, 0))) or ((not self.channels == []) and (self.shape == (0, 0)))): raise RuntimeError(\"Channels-shape mismatch.\") return self.channels == [] and self.shape == (0, 0)"], "rank": 1}
{"nl": "python prettyprint object with str", "code": ["def _get_pretty_string(obj): \"\"\"Return a prettier version of obj Parameters ---------- obj : object Object to pretty print Returns ------- s : str Pretty print object repr \"\"\" sio = StringIO() pprint.pprint(obj, stream=sio) return sio.getvalue()", "def pretty(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Pretty print the object's representation. \"\"\" stream = StringIO() printer = RepresentationPrinter(stream, verbose, max_width, newline) printer.pretty(obj) printer.flush() return stream.getvalue()", "def pformat(object, indent=1, width=80, depth=None): \"\"\"Format a Python object into a pretty-printed representation.\"\"\" return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)", "def pprint(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Like `pretty` but print to stdout. \"\"\" printer = RepresentationPrinter(sys.stdout, verbose, max_width, newline) printer.pretty(obj) printer.flush() sys.stdout.write(newline) sys.stdout.flush()", "def pprint(self, stream=None, indent=1, width=80, depth=None): \"\"\" Pretty print the underlying literal Python object \"\"\" pp.pprint(to_literal(self), stream, indent, width, depth)"], "rank": 1}
{"nl": "python ndarray fast iterate", "code": ["def _npiter(arr): \"\"\"Wrapper for iterating numpy array\"\"\" for a in np.nditer(arr, flags=[\"refs_ok\"]): c = a.item() if c is not None: yield c", "def array(self): \"\"\" The underlying array of shape (N, L, I) \"\"\" return numpy.array([self[sid].array for sid in sorted(self)])", "def A(*a): \"\"\"convert iterable object into numpy array\"\"\" return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]", "def _izip(*iterables): \"\"\" Iterate through multiple lists or arrays of equal size \"\"\" # This izip routine is from itertools # izip('ABCD', 'xy') --> Ax By iterators = map(iter, iterables) while iterators: yield tuple(map(next, iterators))", "def _indexes(arr): \"\"\" Returns the list of all indexes of the given array. Currently works for one and two-dimensional arrays \"\"\" myarr = np.array(arr) if myarr.ndim == 1: return list(range(len(myarr))) elif myarr.ndim == 2: return tuple(itertools.product(list(range(arr.shape[0])), list(range(arr.shape[1])))) else: raise NotImplementedError('Only supporting arrays of dimension 1 and 2 as yet.')"], "rank": 1}
{"nl": "python2 get value from dict with default value", "code": ["def dict_pop_or(d, key, default=None): \"\"\" Try popping a key from a dict. Instead of raising KeyError, just return the default value. \"\"\" val = default with suppress(KeyError): val = d.pop(key) return val", "def _get_config_or_default(self, key, default, as_type=lambda x: x): \"\"\"Return a main config value, or default if it does not exist.\"\"\" if self.main_config.has_option(self.main_section, key): return as_type(self.main_config.get(self.main_section, key)) return default", "def get_prop_value(name, props, default=None): # type: (str, Dict[str, Any], Any) -> Any \"\"\" Returns the value of a property or the default one :param name: Name of a property :param props: Dictionary of properties :param default: Default value :return: The value of the property or the default one \"\"\" if not props: return default try: return props[name] except KeyError: return default", "def get_value(key, obj, default=missing): \"\"\"Helper for pulling a keyed value off various types of objects\"\"\" if isinstance(key, int): return _get_value_for_key(key, obj, default) return _get_value_for_keys(key.split('.'), obj, default)", "def _defaultdict(dct, fallback=_illegal_character): \"\"\"Wraps the given dictionary such that the given fallback function will be called when a nonexistent key is accessed. \"\"\" out = defaultdict(lambda: fallback) for k, v in six.iteritems(dct): out[k] = v return out"], "rank": 4}
{"nl": "python iterator of a dictionary", "code": ["def itervalues(d, **kw): \"\"\"Return an iterator over the values of a dictionary.\"\"\" if not PY2: return iter(d.values(**kw)) return d.itervalues(**kw)", "def iteritems(data, **kwargs): \"\"\"Iterate over dict items.\"\"\" return iter(data.items(**kwargs)) if IS_PY3 else data.iteritems(**kwargs)", "def iterate_items(dictish): \"\"\" Return a consistent (key, value) iterable on dict-like objects, including lists of tuple pairs. Example: >>> list(iterate_items({'a': 1})) [('a', 1)] >>> list(iterate_items([('a', 1), ('b', 2)])) [('a', 1), ('b', 2)] \"\"\" if hasattr(dictish, 'iteritems'): return dictish.iteritems() if hasattr(dictish, 'items'): return dictish.items() return dictish", "def iterlists(self): \"\"\"Like :meth:`items` but returns an iterator.\"\"\" for key, values in dict.iteritems(self): yield key, list(values)", "def uniform_iterator(sequence): \"\"\"Uniform (key, value) iteration on a `dict`, or (idx, value) on a `list`.\"\"\" if isinstance(sequence, abc.Mapping): return six.iteritems(sequence) else: return enumerate(sequence)"], "rank": 1}
{"nl": "how to load a string file in python", "code": ["def FromString(s, **kwargs): \"\"\"Like FromFile, but takes a string.\"\"\" f = StringIO.StringIO(s) return FromFile(f, **kwargs)", "def Load(file): \"\"\" Loads a model from specified file \"\"\" with open(file, 'rb') as file: model = dill.load(file) return model", "def loads(s, model=None, parser=None): \"\"\"Deserialize s (a str) to a Python object.\"\"\" with StringIO(s) as f: return load(f, model=model, parser=parser)", "def load_feature(fname, language): \"\"\" Load and parse a feature file. \"\"\" fname = os.path.abspath(fname) feat = parse_file(fname, language) return feat", "def loads(cls, s): \"\"\" Load an instance of this class from YAML. \"\"\" with closing(StringIO(s)) as fileobj: return cls.load(fileobj)"], "rank": 4}
{"nl": "split on multiple tokens python split", "code": ["def split(text: str) -> List[str]: \"\"\"Split a text into a list of tokens. :param text: the text to split :return: tokens \"\"\" return [word for word in SEPARATOR.split(text) if word.strip(' \\t')]", "def multi_split(s, split): # type: (S, Iterable[S]) -> List[S] \"\"\"Splits on multiple given separators.\"\"\" for r in split: s = s.replace(r, \"|\") return [i for i in s.split(\"|\") if len(i) > 0]", "def tokenize_list(self, text): \"\"\" Split a text into separate words. \"\"\" return [self.get_record_token(record) for record in self.analyze(text)]", "def split_on(s, sep=\" \"): \"\"\"Split s by sep, unless it's inside a quote.\"\"\" pattern = '''((?:[^%s\"']|\"[^\"]*\"|'[^']*')+)''' % sep return [_strip_speechmarks(t) for t in re.split(pattern, s)[1::2]]", "def tokenize(self, s): \"\"\"Return a list of token strings from the given sentence. :param string s: The sentence string to tokenize. :rtype: iter(str) \"\"\" return [s[start:end] for start, end in self.span_tokenize(s)]"], "rank": 1}
{"nl": "generate white noise in python", "code": ["def normal_noise(points): \"\"\"Init a noise variable.\"\"\" return np.random.rand(1) * np.random.randn(points, 1) \\ + random.sample([2, -2], 1)", "def uniform_noise(points): \"\"\"Init a uniform noise variable.\"\"\" return np.random.rand(1) * np.random.uniform(points, 1) \\ + random.sample([2, -2], 1)", "def add_noise(Y, sigma): \"\"\"Adds noise to Y\"\"\" return Y + np.random.normal(0, sigma, Y.shape)", "def gaussian_noise(x, severity=1): \"\"\"Gaussian noise corruption to images. Args: x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255]. severity: integer, severity of corruption. Returns: numpy array, image with uint8 pixels in [0,255]. Added Gaussian noise. \"\"\" c = [.08, .12, 0.18, 0.26, 0.38][severity - 1] x = np.array(x) / 255. x_clip = np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255 return around_and_astype(x_clip)", "def synthesize(self, duration): \"\"\" Synthesize white noise Args: duration (numpy.timedelta64): The duration of the synthesized sound \"\"\" sr = self.samplerate.samples_per_second seconds = duration / Seconds(1) samples = np.random.uniform(low=-1., high=1., size=int(sr * seconds)) return AudioSamples(samples, self.samplerate)"], "rank": 1}
{"nl": "connect to python ftp server host", "code": ["def connect(): \"\"\"Connect to FTP server, login and return an ftplib.FTP instance.\"\"\" ftp_class = ftplib.FTP if not SSL else ftplib.FTP_TLS ftp = ftp_class(timeout=TIMEOUT) ftp.connect(HOST, PORT) ftp.login(USER, PASSWORD) if SSL: ftp.prot_p() # secure data connection return ftp", "def connect(host, port, username, password): \"\"\"Connect and login to an FTP server and return ftplib.FTP object.\"\"\" # Instantiate ftplib client session = ftplib.FTP() # Connect to host without auth session.connect(host, port) # Authenticate connection session.login(username, password) return session", "def _connect(self, servers): \"\"\" connect to the given server, e.g.: \\\\connect localhost:4200 \"\"\" self._do_connect(servers.split(' ')) self._verify_connection(verbose=True)", "def write(url, content, **args): \"\"\"Put an object into a ftps URL.\"\"\" with FTPSResource(url, **args) as resource: resource.write(content)", "def _send_file(self, filename): \"\"\" Sends a file via FTP. \"\"\" # pylint: disable=E1101 ftp = ftplib.FTP(host=self.host) ftp.login(user=self.user, passwd=self.password) ftp.set_pasv(True) ftp.storbinary(\"STOR %s\" % os.path.basename(filename), file(filename, 'rb'))"], "rank": 2}
{"nl": "extract number of channels in an image python", "code": ["def numchannels(samples:np.ndarray) -> int: \"\"\" return the number of channels present in samples samples: a numpy array as returned by sndread for multichannel audio, samples is always interleaved, meaning that samples[n] returns always a frame, which is either a single scalar for mono audio, or an array for multichannel audio. \"\"\" if len(samples.shape) == 1: return 1 else: return samples.shape[1]", "def bitdepth(self): \"\"\"The number of bits per sample in the audio encoding (an int). Only available for certain file formats (zero where unavailable). \"\"\" if hasattr(self.mgfile.info, 'bits_per_sample'): return self.mgfile.info.bits_per_sample return 0", "def array_size(x, axis): \"\"\"Calculate the size of `x` along `axis` dimensions only.\"\"\" axis_shape = x.shape if axis is None else tuple(x.shape[a] for a in axis) return max(numpy.prod(axis_shape), 1)", "def rank(tensor: BKTensor) -> int: \"\"\"Return the number of dimensions of a tensor\"\"\" if isinstance(tensor, np.ndarray): return len(tensor.shape) return len(tensor[0].size())", "def array_dim(arr): \"\"\"Return the size of a multidimansional array. \"\"\" dim = [] while True: try: dim.append(len(arr)) arr = arr[0] except TypeError: return dim"], "rank": 1}
{"nl": "how to create a sort key in python", "code": ["def sort_key(x): \"\"\" >>> sort_key(('name', ('ROUTE', 'URL'))) -3 \"\"\" name, (r, u) = x return - len(u) + u.count('}') * 100", "def naturalsortkey(s): \"\"\"Natural sort order\"\"\" return [int(part) if part.isdigit() else part for part in re.split('([0-9]+)', s)]", "def sort_func(self, key): \"\"\"Sorting logic for `Quantity` objects.\"\"\" if key == self._KEYS.VALUE: return 'aaa' if key == self._KEYS.SOURCE: return 'zzz' return key", "def sort_key(val): \"\"\"Sort key for sorting keys in grevlex order.\"\"\" return numpy.sum((max(val)+1)**numpy.arange(len(val)-1, -1, -1)*val)", "def transcript_sort_key(transcript): \"\"\" Key function used to sort transcripts. Taking the negative of protein sequence length and nucleotide sequence length so that the transcripts with longest sequences come first in the list. This couldn't be accomplished with `reverse=True` since we're also sorting by transcript name (which places TP53-001 before TP53-002). \"\"\" return ( -len(transcript.protein_sequence), -len(transcript.sequence), transcript.name )"], "rank": 1}
{"nl": "calculate mid points between numbers python", "code": ["def _mid(pt1, pt2): \"\"\" (Point, Point) -> Point Return the point that lies in between the two input points. \"\"\" (x0, y0), (x1, y1) = pt1, pt2 return 0.5 * (x0 + x1), 0.5 * (y0 + y1)", "def _infer_interval_breaks(coord): \"\"\" >>> _infer_interval_breaks(np.arange(5)) array([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5]) Taken from xarray.plotting.plot module \"\"\" coord = np.asarray(coord) deltas = 0.5 * (coord[1:] - coord[:-1]) first = coord[0] - deltas[0] last = coord[-1] + deltas[-1] return np.r_[[first], coord[:-1] + deltas, [last]]", "def nearest_intersection_idx(a, b): \"\"\"Determine the index of the point just before two lines with common x values. Parameters ---------- a : array-like 1-dimensional array of y-values for line 1 b : array-like 1-dimensional array of y-values for line 2 Returns ------- An array of indexes representing the index of the values just before the intersection(s) of the two lines. \"\"\" # Difference in the two y-value sets difference = a - b # Determine the point just before the intersection of the lines # Will return multiple points for multiple intersections sign_change_idx, = np.nonzero(np.diff(np.sign(difference))) return sign_change_idx", "def _interval_to_bound_points(array): \"\"\" Helper function which returns an array with the Intervals' boundaries. \"\"\" array_boundaries = np.array([x.left for x in array]) array_boundaries = np.concatenate( (array_boundaries, np.array([array[-1].right]))) return array_boundaries", "def get_bound(pts): \"\"\"Compute a minimal rectangle that covers all the points.\"\"\" (x0, y0, x1, y1) = (INF, INF, -INF, -INF) for (x, y) in pts: x0 = min(x0, x) y0 = min(y0, y) x1 = max(x1, x) y1 = max(y1, y) return (x0, y0, x1, y1)"], "rank": 1}
{"nl": "call a list of dictinaries data from ajax in python flask", "code": ["def convert_ajax_data(self, field_data): \"\"\" Due to the way Angular organizes it model, when this Form data is sent using Ajax, then for this kind of widget, the sent data has to be converted into a format suitable for Django's Form validation. \"\"\" data = [key for key, val in field_data.items() if val] return data", "def get_active_ajax_datatable(self): \"\"\" Returns a single datatable according to the hint GET variable from an AJAX request. \"\"\" data = getattr(self.request, self.request.method) datatables_dict = self.get_datatables(only=data['datatable']) return list(datatables_dict.values())[0]", "def http_request_json(*args, **kwargs): \"\"\" See: http_request \"\"\" ret, status = http_request(*args, **kwargs) return json.loads(ret), status", "def list(self,table, **kparams): \"\"\" get a collection of records by table name. returns a dict (the json map) for python 3.4 \"\"\" result = self.table_api_get(table, **kparams) return self.to_records(result, table)", "def jsonify(resource): \"\"\"Return a Flask ``Response`` object containing a JSON representation of *resource*. :param resource: The resource to act as the basis of the response \"\"\" response = flask.jsonify(resource.to_dict()) response = add_link_headers(response, resource.links()) return response"], "rank": 1}
{"nl": "python function compare length of 2 strings", "code": ["def count_string_diff(a,b): \"\"\"Return the number of characters in two strings that don't exactly match\"\"\" shortest = min(len(a), len(b)) return sum(a[i] != b[i] for i in range(shortest))", "def compare(string1, string2): \"\"\"Compare two strings while protecting against timing attacks :param str string1: the first string :param str string2: the second string :returns: True if the strings are equal, False if not :rtype: :obj:`bool` \"\"\" if len(string1) != len(string2): return False result = True for c1, c2 in izip(string1, string2): result &= c1 == c2 return result", "def long_substring(str_a, str_b): \"\"\" Looks for a longest common string between any two given strings passed :param str_a: str :param str_b: str Big Thanks to Pulkit Kathuria(@kevincobain2000) for the function The function is derived from jProcessing toolkit suite \"\"\" data = [str_a, str_b] substr = '' if len(data) > 1 and len(data[0]) > 0: for i in range(len(data[0])): for j in range(len(data[0])-i+1): if j > len(substr) and all(data[0][i:i+j] in x for x in data): substr = data[0][i:i+j] return substr.strip()", "def hamming_distance(str1, str2): \"\"\"Calculate the Hamming distance between two bit strings Args: str1 (str): First string. str2 (str): Second string. Returns: int: Distance between strings. Raises: VisualizationError: Strings not same length \"\"\" if len(str1) != len(str2): raise VisualizationError('Strings not same length.') return sum(s1 != s2 for s1, s2 in zip(str1, str2))", "def check_length(value, length): \"\"\" Checks length of value @param value: value to check @type value: C{str} @param length: length checking for @type length: C{int} @return: None when check successful @raise ValueError: check failed \"\"\" _length = len(value) if _length != length: raise ValueError(\"length must be %d, not %d\" % \\ (length, _length))"], "rank": 1}
{"nl": "protobuf python dictionary f dictionary", "code": ["def MessageToDict(message, including_default_value_fields=False, preserving_proto_field_name=False): \"\"\"Converts protobuf message to a JSON dictionary. Args: message: The protocol buffers message instance to serialize. including_default_value_fields: If True, singular primitive fields, repeated fields, and map fields will always be serialized. If False, only serialize non-empty fields. Singular message fields and oneof fields are not affected by this option. preserving_proto_field_name: If True, use the original proto field names as defined in the .proto file. If False, convert the field names to lowerCamelCase. Returns: A dict representation of the JSON formatted protocol buffer message. \"\"\" printer = _Printer(including_default_value_fields, preserving_proto_field_name) # pylint: disable=protected-access return printer._MessageToJsonObject(message)", "def _dict_to_proto(py_dict, proto): \"\"\" Converts a python dictionary to the proto supplied :param py_dict: The dictionary to convert :type py_dict: dict :param proto: The proto object to merge with dictionary :type proto: protobuf :return: A parsed python dictionary in provided proto format :raises: ParseError: On JSON parsing problems. \"\"\" dict_json_str = json.dumps(py_dict) return json_format.Parse(dict_json_str, proto)", "def toJson(protoObject, indent=None): \"\"\" Serialises a protobuf object as json \"\"\" # Using the internal method because this way we can reformat the JSON js = json_format.MessageToDict(protoObject, False) return json.dumps(js, indent=indent)", "def metadata(self): \"\"\"google.protobuf.Message: the current operation metadata.\"\"\" if not self._operation.HasField(\"metadata\"): return None return protobuf_helpers.from_any_pb( self._metadata_type, self._operation.metadata )", "def WritePythonFile(file_descriptor, package, version, printer): \"\"\"Write the given extended file descriptor to out.\"\"\" _WriteFile(file_descriptor, package, version, _ProtoRpcPrinter(printer))"], "rank": 1}
{"nl": "python get sort index numpy array", "code": ["def argsort_indices(a, axis=-1): \"\"\"Like argsort, but returns an index suitable for sorting the the original array even if that array is multidimensional \"\"\" a = np.asarray(a) ind = list(np.ix_(*[np.arange(d) for d in a.shape])) ind[axis] = a.argsort(axis) return tuple(ind)", "def arglexsort(arrays): \"\"\" Returns the indices of the lexicographical sorting order of the supplied arrays. \"\"\" dtypes = ','.join(array.dtype.str for array in arrays) recarray = np.empty(len(arrays[0]), dtype=dtypes) for i, array in enumerate(arrays): recarray['f%s' % i] = array return recarray.argsort()", "def unsort_vector(data, indices_of_increasing): \"\"\"Upermutate 1-D data that is sorted by indices_of_increasing.\"\"\" return numpy.array([data[indices_of_increasing.index(i)] for i in range(len(data))])", "def _index_ordering(redshift_list): \"\"\" :param redshift_list: list of redshifts :return: indexes in acending order to be evaluated (from z=0 to z=z_source) \"\"\" redshift_list = np.array(redshift_list) sort_index = np.argsort(redshift_list) return sort_index", "def rank(self): \"\"\"how high in sorted list each key is. inverse permutation of sorter, such that sorted[rank]==keys\"\"\" r = np.empty(self.size, np.int) r[self.sorter] = np.arange(self.size) return r"], "rank": 1}
{"nl": "python how to lemmatizer", "code": ["def register_modele(self, modele: Modele): \"\"\" Register a modele onto the lemmatizer :param modele: Modele to register \"\"\" self.lemmatiseur._modeles[modele.gr()] = modele", "def lemmatize(self, text, best_guess=True, return_frequencies=False): \"\"\"Lemmatize all tokens in a string or a list. A string is first tokenized using punkt. Throw a type error if the input is neither a string nor a list. \"\"\" if isinstance(text, str): tokens = wordpunct_tokenize(text) elif isinstance(text, list): tokens= text else: raise TypeError(\"lemmatize only works with strings or lists of string tokens.\") return [self._lemmatize_token(token, best_guess, return_frequencies) for token in tokens]", "def has_synset(word: str) -> list: \"\"\"\" Returns a list of synsets of a word after lemmatization. \"\"\" return wn.synsets(lemmatize(word, neverstem=True))", "def clean_text_by_sentences(text, language=\"english\", additional_stopwords=None): \"\"\" Tokenizes a given text into sentences, applying filters and lemmatizing them. Returns a SyntacticUnit list. \"\"\" init_textcleanner(language, additional_stopwords) original_sentences = split_sentences(text) filtered_sentences = filter_words(original_sentences) return merge_syntactic_units(original_sentences, filtered_sentences)", "def get_wordnet_syns(word): \"\"\" Utilize wordnet (installed with nltk) to get synonyms for words word is the input word returns a list of unique synonyms \"\"\" synonyms = [] regex = r\"_\" pat = re.compile(regex) synset = nltk.wordnet.wordnet.synsets(word) for ss in synset: for swords in ss.lemma_names: synonyms.append(pat.sub(\" \", swords.lower())) synonyms = f7(synonyms) return synonyms"], "rank": 1}
{"nl": "python histogram get number of bins", "code": ["def shape(self) -> Tuple[int, ...]: \"\"\"Shape of histogram's data. Returns ------- One-element tuple with the number of bins along each axis. \"\"\" return tuple(bins.bin_count for bins in self._binnings)", "def _histplot_bins(column, bins=100): \"\"\"Helper to get bins for histplot.\"\"\" col_min = np.min(column) col_max = np.max(column) return range(col_min, col_max + 2, max((col_max - col_min) // bins, 1))", "def get_bin_indices(self, values): \"\"\"Returns index tuple in histogram of bin which contains value\"\"\" return tuple([self.get_axis_bin_index(values[ax_i], ax_i) for ax_i in range(self.dimensions)])", "def lon_lat_bins(bb, coord_bin_width): \"\"\" Define bin edges for disaggregation histograms. Given bins data as provided by :func:`collect_bin_data`, this function finds edges of histograms, taking into account maximum and minimum values of magnitude, distance and coordinates as well as requested sizes/numbers of bins. \"\"\" west, south, east, north = bb west = numpy.floor(west / coord_bin_width) * coord_bin_width east = numpy.ceil(east / coord_bin_width) * coord_bin_width lon_extent = get_longitudinal_extent(west, east) lon_bins, _, _ = npoints_between( west, 0, 0, east, 0, 0, numpy.round(lon_extent / coord_bin_width + 1)) lat_bins = coord_bin_width * numpy.arange( int(numpy.floor(south / coord_bin_width)), int(numpy.ceil(north / coord_bin_width) + 1)) return lon_bins, lat_bins", "def get_bin_edges_from_axis(axis) -> np.ndarray: \"\"\" Get bin edges from a ROOT hist axis. Note: Doesn't include over- or underflow bins! Args: axis (ROOT.TAxis): Axis from which the bin edges should be extracted. Returns: Array containing the bin edges. \"\"\" # Don't include over- or underflow bins bins = range(1, axis.GetNbins() + 1) # Bin edges bin_edges = np.empty(len(bins) + 1) bin_edges[:-1] = [axis.GetBinLowEdge(i) for i in bins] bin_edges[-1] = axis.GetBinUpEdge(axis.GetNbins()) return bin_edges"], "rank": 1}
{"nl": "check if array contains integer python", "code": ["def is_int_vector(l): r\"\"\"Checks if l is a numpy array of integers \"\"\" if isinstance(l, np.ndarray): if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'): return True return False", "def is_integer_array(val): \"\"\" Checks whether a variable is a numpy integer array. Parameters ---------- val The variable to check. Returns ------- bool True if the variable is a numpy integer array. Otherwise False. \"\"\" return is_np_array(val) and issubclass(val.dtype.type, np.integer)", "def is_iterable_of_int(l): r\"\"\" Checks if l is iterable and contains only integral types \"\"\" if not is_iterable(l): return False return all(is_int(value) for value in l)", "def is_integer(dtype): \"\"\"Returns whether this is a (non-quantized) integer type.\"\"\" dtype = tf.as_dtype(dtype) if hasattr(dtype, 'is_integer'): return dtype.is_integer return np.issubdtype(np.dtype(dtype), np.integer)", "def contains_all(self, array): \"\"\"Test if `array` is an array of real numbers.\"\"\" dtype = getattr(array, 'dtype', None) if dtype is None: dtype = np.result_type(*array) return is_real_dtype(dtype)"], "rank": 1}
{"nl": "best way to get key against a value from python dictionary", "code": ["def get_key_by_value(dictionary, search_value): \"\"\" searchs a value in a dicionary and returns the key of the first occurrence :param dictionary: dictionary to search in :param search_value: value to search for \"\"\" for key, value in dictionary.iteritems(): if value == search_value: return ugettext(key)", "def __contains__(self, key): \"\"\" Invoked when determining whether a specific key is in the dictionary using `key in d`. The key is looked up case-insensitively. \"\"\" k = self._real_key(key) return k in self._data", "def value_for_key(membersuite_object_data, key): \"\"\"Return the value for `key` of membersuite_object_data. \"\"\" key_value_dicts = { d['Key']: d['Value'] for d in membersuite_object_data[\"Fields\"][\"KeyValueOfstringanyType\"]} return key_value_dicts[key]", "def _find_value(key, *args): \"\"\"Find a value for 'key' in any of the objects given as 'args'\"\"\" for arg in args: v = _get_value(arg, key) if v is not None: return v", "def get_case_insensitive_dict_key(d: Dict, k: str) -> Optional[str]: \"\"\" Within the dictionary ``d``, find a key that matches (in case-insensitive fashion) the key ``k``, and return it (or ``None`` if there isn't one). \"\"\" for key in d.keys(): if k.lower() == key.lower(): return key return None"], "rank": 1}
{"nl": "python datetime to microseconds since epoch", "code": ["def _DateToEpoch(date): \"\"\"Converts python datetime to epoch microseconds.\"\"\" tz_zero = datetime.datetime.utcfromtimestamp(0) diff_sec = int((date - tz_zero).total_seconds()) return diff_sec * 1000000", "def convertDatetime(t): \"\"\" Converts the specified datetime object into its appropriate protocol value. This is the number of milliseconds from the epoch. \"\"\" epoch = datetime.datetime.utcfromtimestamp(0) delta = t - epoch millis = delta.total_seconds() * 1000 return int(millis)", "def _dt_to_epoch(dt): \"\"\"Convert datetime to epoch seconds.\"\"\" try: epoch = dt.timestamp() except AttributeError: # py2 epoch = (dt - datetime(1970, 1, 1)).total_seconds() return epoch", "def timestamp_to_microseconds(timestamp): \"\"\"Convert a timestamp string into a microseconds value :param timestamp :return time in microseconds \"\"\" timestamp_str = datetime.datetime.strptime(timestamp, ISO_DATETIME_REGEX) epoch_time_secs = calendar.timegm(timestamp_str.timetuple()) epoch_time_mus = epoch_time_secs * 1e6 + timestamp_str.microsecond return epoch_time_mus", "def datetime_to_ms(dt): \"\"\" Converts a datetime to a millisecond accuracy timestamp \"\"\" seconds = calendar.timegm(dt.utctimetuple()) return seconds * 1000 + int(dt.microsecond / 1000)"], "rank": 1}
{"nl": "how to know the proxies in a browser python", "code": ["def _GetProxies(self): \"\"\"Gather a list of proxies to use.\"\"\" # Detect proxies from the OS environment. result = client_utils.FindProxies() # Also try to connect directly if all proxies fail. result.append(\"\") # Also try all proxies configured in the config system. result.extend(config.CONFIG[\"Client.proxy_servers\"]) return result", "def enable_proxy(self, host, port): \"\"\"Enable a default web proxy\"\"\" self.proxy = [host, _number(port)] self.proxy_enabled = True", "def _internet_on(address): \"\"\" Check to see if the internet is on by pinging a set address. :param address: the IP or address to hit :return: a boolean - true if can be reached, false if not. \"\"\" try: urllib2.urlopen(address, timeout=1) return True except urllib2.URLError as err: return False", "def _prepare_proxy(self, conn): \"\"\" Establish tunnel connection early, because otherwise httplib would improperly set Host: header to proxy's IP:port. \"\"\" conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers) conn.connect()", "def url(self): \"\"\" The url of this window \"\"\" with switch_window(self._browser, self.name): return self._browser.url"], "rank": 1}
{"nl": "how to execute async in python", "code": ["def apply(self, func, args=(), kwds=dict()): \"\"\"Equivalent of the apply() builtin function. It blocks till the result is ready.\"\"\" return self.apply_async(func, args, kwds).get()", "def _run_sync(self, method: Callable, *args, **kwargs) -> Any: \"\"\" Utility method to run commands synchronously for testing. \"\"\" if self.loop.is_running(): raise RuntimeError(\"Event loop is already running.\") if not self.is_connected: self.loop.run_until_complete(self.connect()) task = asyncio.Task(method(*args, **kwargs), loop=self.loop) result = self.loop.run_until_complete(task) self.loop.run_until_complete(self.quit()) return result", "def run_task(func): \"\"\" Decorator to wrap an async function in an event loop. Use for main sync interface methods. \"\"\" def _wrapped(*a, **k): loop = asyncio.get_event_loop() return loop.run_until_complete(func(*a, **k)) return _wrapped", "async def async_run(self) -> None: \"\"\" Asynchronously run the worker, does not close connections. Useful when testing. \"\"\" self.main_task = self.loop.create_task(self.main()) await self.main_task", "def asynchronous(function, event): \"\"\" Runs the function asynchronously taking care of exceptions. \"\"\" thread = Thread(target=synchronous, args=(function, event)) thread.daemon = True thread.start()"], "rank": 2}
{"nl": "cross validation python without sklearn", "code": ["def check_cv(self, y): \"\"\"Resolve which cross validation strategy is used.\"\"\" y_arr = None if self.stratified: # Try to convert y to numpy for sklearn's check_cv; if conversion # doesn't work, still try. try: y_arr = to_numpy(y) except (AttributeError, TypeError): y_arr = y if self._is_float(self.cv): return self._check_cv_float() return self._check_cv_non_float(y_arr)", "def local_accuracy(X_train, y_train, X_test, y_test, attr_test, model_generator, metric, trained_model): \"\"\" The how well do the features plus a constant base rate sum up to the model output. \"\"\" X_train, X_test = to_array(X_train, X_test) # how many features to mask assert X_train.shape[1] == X_test.shape[1] # keep nkeep top features and re-train the model for each test explanation yp_test = trained_model.predict(X_test) return metric(yp_test, strip_list(attr_test).sum(1))", "def partial_fit(self, X, y=None, classes=None, **fit_params): \"\"\"Fit the module. If the module is initialized, it is not re-initialized, which means that this method should be used if you want to continue training a model (warm start). Parameters ---------- X : input data, compatible with skorch.dataset.Dataset By default, you should be able to pass: * numpy arrays * torch tensors * pandas DataFrame or Series * scipy sparse CSR matrices * a dictionary of the former three * a list/tuple of the former three * a Dataset If this doesn't work with your data, you have to pass a ``Dataset`` that can deal with the data. y : target data, compatible with skorch.dataset.Dataset The same data types as for ``X`` are supported. If your X is a Dataset that contains the target, ``y`` may be set to None. classes : array, sahpe (n_classes,) Solely for sklearn compatibility, currently unused. **fit_params : dict Additional parameters passed to the ``forward`` method of the module and to the ``self.train_split`` call. \"\"\" if not self.initialized_: self.initialize() self.notify('on_train_begin', X=X, y=y) try: self.fit_loop(X, y, **fit_params) except KeyboardInterrupt: pass self.notify('on_train_end', X=X, y=y) return self", "def AddAccuracy(model, softmax, label): \"\"\"Adds an accuracy op to the model\"\"\" accuracy = brew.accuracy(model, [softmax, label], \"accuracy\") return accuracy", "def cat_acc(y_true, y_pred): \"\"\"Categorical accuracy \"\"\" return np.mean(y_true.argmax(axis=1) == y_pred.argmax(axis=1))"], "rank": 1}
{"nl": "remove zeros from a list in python", "code": ["def _remove_blank(l): \"\"\" Removes trailing zeros in the list of integers and returns a new list of integers\"\"\" ret = [] for i, _ in enumerate(l): if l[i] == 0: break ret.append(l[i]) return ret", "def __remove_trailing_zeros(self, collection): \"\"\"Removes trailing zeroes from indexable collection of numbers\"\"\" index = len(collection) - 1 while index >= 0 and collection[index] == 0: index -= 1 return collection[:index + 1]", "def _trim_zeros_complex(str_complexes, na_rep='NaN'): \"\"\" Separates the real and imaginary parts from the complex number, and executes the _trim_zeros_float method on each of those. \"\"\" def separate_and_trim(str_complex, na_rep): num_arr = str_complex.split('+') return (_trim_zeros_float([num_arr[0]], na_rep) + ['+'] + _trim_zeros_float([num_arr[1][:-1]], na_rep) + ['j']) return [''.join(separate_and_trim(x, na_rep)) for x in str_complexes]", "def de_blank(val): \"\"\"Remove blank elements in `val` and return `ret`\"\"\" ret = list(val) if type(val) == list: for idx, item in enumerate(val): if item.strip() == '': ret.remove(item) else: ret[idx] = item.strip() return ret", "def clean_with_zeros(self,x): \"\"\" set nan and inf rows from x to zero\"\"\" x[~np.any(np.isnan(x) | np.isinf(x),axis=1)] = 0 return x"], "rank": 1}
{"nl": "python grab every n elements", "code": ["def split_every(n, iterable): \"\"\"Returns a generator that spits an iteratable into n-sized chunks. The last chunk may have less than n elements. See http://stackoverflow.com/a/22919323/503377.\"\"\" items = iter(iterable) return itertools.takewhile(bool, (list(itertools.islice(items, n)) for _ in itertools.count()))", "def split_every(iterable, n): # TODO: Remove this, or make it return a generator. \"\"\" A generator of n-length chunks of an input iterable \"\"\" i = iter(iterable) piece = list(islice(i, n)) while piece: yield piece piece = list(islice(i, n))", "def partition_all(n, iterable): \"\"\"Partition a list into equally sized pieces, including last smaller parts http://stackoverflow.com/questions/5129102/python-equivalent-to-clojures-partition-all \"\"\" it = iter(iterable) while True: chunk = list(itertools.islice(it, n)) if not chunk: break yield chunk", "def chunks(iterable, n): \"\"\"Yield successive n-sized chunks from iterable object. https://stackoverflow.com/a/312464 \"\"\" for i in range(0, len(iterable), n): yield iterable[i:i + n]", "def chunk_list(l, n): \"\"\"Return `n` size lists from a given list `l`\"\"\" return [l[i:i + n] for i in range(0, len(l), n)]"], "rank": 2}
{"nl": "how to do dot product vectors in python", "code": ["def dotproduct(X, Y): \"\"\"Return the sum of the element-wise product of vectors x and y. >>> dotproduct([1, 2, 3], [1000, 100, 10]) 1230 \"\"\" return sum([x * y for x, y in zip(X, Y)])", "def dot(self, w): \"\"\"Return the dotproduct between self and another vector.\"\"\" return sum([x * y for x, y in zip(self, w)])", "def dot_product(self, other): \"\"\" Return the dot product of the given vectors. \"\"\" return self.x * other.x + self.y * other.y", "def dot_v3(v, w): \"\"\"Return the dotproduct of two vectors.\"\"\" return sum([x * y for x, y in zip(v, w)])", "def dot(a, b): \"\"\"Take arrays `a` and `b` and form the dot product between the last axis of `a` and the first of `b`. \"\"\" b = numpy.asarray(b) return numpy.dot(a, b.reshape(b.shape[0], -1)).reshape(a.shape[:-1] + b.shape[1:])"], "rank": 4}
{"nl": "python garbage collector how to delete unnecessary", "code": ["def cleanup(self): \"\"\"Forcefully delete objects from memory In an ideal world, this shouldn't be necessary. Garbage collection guarantees that anything without reference is automatically removed. However, because this application is designed to be run multiple times from the same interpreter process, extra case must be taken to ensure there are no memory leaks. Explicitly deleting objects shines a light on where objects may still be referenced in the form of an error. No errors means this was uneccesary, but that's ok. \"\"\" for instance in self.context: del(instance) for plugin in self.plugins: del(plugin)", "def clean_all(self, args): \"\"\"Delete all build components; the package cache, package builds, bootstrap builds and distributions.\"\"\" self.clean_dists(args) self.clean_builds(args) self.clean_download_cache(args)", "def clean(): \"\"\"clean - remove build artifacts.\"\"\" run('rm -rf build/') run('rm -rf dist/') run('rm -rf puzzle.egg-info') run('find . -name __pycache__ -delete') run('find . -name *.pyc -delete') run('find . -name *.pyo -delete') run('find . -name *~ -delete') log.info('cleaned up')", "def cleanup_storage(*args): \"\"\"Clean up processes after SIGTERM or SIGINT is received.\"\"\" ShardedClusters().cleanup() ReplicaSets().cleanup() Servers().cleanup() sys.exit(0)", "def cleanup(self): \"\"\"Clean up any temporary files.\"\"\" for file in glob.glob(self.basename + '*'): os.unlink(file)"], "rank": 30}
{"nl": "python check if row contains none", "code": ["def is_valid_row(cls, row): \"\"\"Indicates whether or not the given row contains valid data.\"\"\" for k in row.keys(): if row[k] is None: return False return True", "def _not_none(items): \"\"\"Whether the item is a placeholder or contains a placeholder.\"\"\" if not isinstance(items, (tuple, list)): items = (items,) return all(item is not _none for item in items)", "def non_zero_row(arr): \"\"\" 0. Empty row returns False. >>> arr = array([]) >>> non_zero_row(arr) False 1. Row with a zero returns False. >>> arr = array([1, 4, 3, 0, 5, -1, -2]) >>> non_zero_row(arr) False 2. Row with no zeros returns True. >>> arr = array([-1, -0.1, 0.001, 2]) >>> non_zero_row(arr) True :param arr: array :type arr: numpy array :return empty: If row is completely free of zeros :rtype empty: bool \"\"\" if len(arr) == 0: return False for item in arr: if item == 0: return False return True", "def is_empty_shape(sh: ShExJ.Shape) -> bool: \"\"\" Determine whether sh has any value \"\"\" return sh.closed is None and sh.expression is None and sh.extra is None and \\ sh.semActs is None", "def step_table_made(self): \"\"\"check if the step table exists\"\"\" try: empty = self.step_table.empty except AttributeError: empty = True return not empty"], "rank": 1}
{"nl": "how to remove border in image in python", "code": ["def border(self): \"\"\"Region formed by taking border elements. :returns: :class:`jicimagelib.region.Region` \"\"\" border_array = self.bitmap - self.inner.bitmap return Region(border_array)", "def _trim(image): \"\"\"Trim a PIL image and remove white space.\"\"\" background = PIL.Image.new(image.mode, image.size, image.getpixel((0, 0))) diff = PIL.ImageChops.difference(image, background) diff = PIL.ImageChops.add(diff, diff, 2.0, -100) bbox = diff.getbbox() if bbox: image = image.crop(bbox) return image", "def add_bg(img, padding, color=COL_WHITE): \"\"\" Adds a padding to the given image as background of specified color :param img: Input image. :param padding: constant padding around the image. :param color: background color that needs to filled for the newly padded region. :return: New image with background. \"\"\" img = gray3(img) h, w, d = img.shape new_img = np.ones((h + 2*padding, w + 2*padding, d)) * color[:d] new_img = new_img.astype(np.uint8) set_img_box(new_img, (padding, padding, w, h), img) return new_img", "def inpaint(self): \"\"\" Replace masked-out elements in an array using an iterative image inpainting algorithm. \"\"\" import inpaint filled = inpaint.replace_nans(np.ma.filled(self.raster_data, np.NAN).astype(np.float32), 3, 0.01, 2) self.raster_data = np.ma.masked_invalid(filled)", "def show(data, negate=False): \"\"\"Show the stretched data. \"\"\" from PIL import Image as pil data = np.array((data - data.min()) * 255.0 / (data.max() - data.min()), np.uint8) if negate: data = 255 - data img = pil.fromarray(data) img.show()"], "rank": 1}
{"nl": "python regex substitute from dictionary", "code": ["def substitute(dict_, source): \"\"\" Perform re.sub with the patterns in the given dict Args: dict_: {pattern: repl} source: str \"\"\" d_esc = (re.escape(k) for k in dict_.keys()) pattern = re.compile('|'.join(d_esc)) return pattern.sub(lambda x: dict_[x.group()], source)", "def replace_all(text, dic): \"\"\"Takes a string and dictionary. replaces all occurrences of i with j\"\"\" for i, j in dic.iteritems(): text = text.replace(i, j) return text", "def fmt_subst(regex, subst): \"\"\"Replace regex with string.\"\"\" return lambda text: re.sub(regex, subst, text) if text else text", "def replace_variables(self, source: str, variables: dict) -> str: \"\"\"Replace {{variable-name}} with stored value.\"\"\" try: replaced = re.sub( \"{{(.*?)}}\", lambda m: variables.get(m.group(1), \"\"), source ) except TypeError: replaced = source return replaced", "def template_substitute(text, **kwargs): \"\"\" Replace placeholders in text by using the data mapping. Other placeholders that is not represented by data is left untouched. :param text: Text to search and replace placeholders. :param data: Data mapping/dict for placeholder key and values. :return: Potentially modified text with replaced placeholders. \"\"\" for name, value in kwargs.items(): placeholder_pattern = \"{%s}\" % name if placeholder_pattern in text: text = text.replace(placeholder_pattern, value) return text"], "rank": 1}
{"nl": "python change directory decorate", "code": ["def change_dir(directory): \"\"\" Wraps a function to run in a given directory. \"\"\" def cd_decorator(func): @wraps(func) def wrapper(*args, **kwargs): org_path = os.getcwd() os.chdir(directory) func(*args, **kwargs) os.chdir(org_path) return wrapper return cd_decorator", "def dir_path(dir): \"\"\"with dir_path(path) to change into a directory.\"\"\" old_dir = os.getcwd() os.chdir(dir) yield os.chdir(old_dir)", "def in_directory(path): \"\"\"Context manager (with statement) that changes the current directory during the context. \"\"\" curdir = os.path.abspath(os.curdir) os.chdir(path) yield os.chdir(curdir)", "def mkdir(dir, enter): \"\"\"Create directory with template for topic of the current environment \"\"\" if not os.path.exists(dir): os.makedirs(dir)", "def makedirs(path, mode=0o777, exist_ok=False): \"\"\"A wrapper of os.makedirs().\"\"\" os.makedirs(path, mode, exist_ok)"], "rank": 1}
{"nl": "python register a service", "code": ["def register_service(self, service): \"\"\" Register service into the system. Called by Services. \"\"\" if service not in self.services: self.services.append(service)", "def _replace_service_arg(self, name, index, args): \"\"\" Replace index in list with service \"\"\" args[index] = self.get_instantiated_service(name)", "def build_service_class(metadata): \"\"\"Generate a service class for the service contained in the specified metadata class.\"\"\" i = importlib.import_module(metadata) service = i.service env = get_jinja_env() service_template = env.get_template('service.py.jinja2') with open(api_path(service.name.lower()), 'w') as t: t.write(service_template.render(service_md=service))", "def add(self, entity): \"\"\" Adds the supplied dict as a new entity \"\"\" result = self._http_req('connections', method='POST', payload=entity) status = result['status'] if not status==201: raise ServiceRegistryError(status,\"Couldn't add entity\") self.debug(0x01,result) return result['decoded']", "def __call__(self, factory_name, *args, **kwargs): \"\"\"Create object.\"\"\" return self.factories[factory_name](*args, **kwargs)"], "rank": 1}
{"nl": "how to make a str all lowercasein python", "code": ["def _lower(string): \"\"\"Custom lower string function. Examples: FooBar -> foo_bar \"\"\" if not string: return \"\" new_string = [string[0].lower()] for char in string[1:]: if char.isupper(): new_string.append(\"_\") new_string.append(char.lower()) return \"\".join(new_string)", "def lowercase_chars(string: any) -> str: \"\"\"Return all (and only) the lowercase chars in the given string.\"\"\" return ''.join([c if c.islower() else '' for c in str(string)])", "def snake_case(a_string): \"\"\"Returns a snake cased version of a string. :param a_string: any :class:`str` object. Usage: >>> snake_case('FooBar') \"foo_bar\" \"\"\" partial = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', a_string) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', partial).lower()", "def clean(some_string, uppercase=False): \"\"\" helper to clean up an input string \"\"\" if uppercase: return some_string.strip().upper() else: return some_string.strip().lower()", "def camel_to_snake_case(string): \"\"\"Converts 'string' presented in camel case to snake case. e.g.: CamelCase => snake_case \"\"\" s = _1.sub(r'\\1_\\2', string) return _2.sub(r'\\1_\\2', s).lower()"], "rank": 14}
{"nl": "python 3 slice function", "code": ["def Slice(a, begin, size): \"\"\" Slicing op. \"\"\" return np.copy(a)[[slice(*tpl) for tpl in zip(begin, begin+size)]],", "def make_strslice(lineno, s, lower, upper): \"\"\" Wrapper: returns String Slice node \"\"\" return symbols.STRSLICE.make_node(lineno, s, lower, upper)", "def is_full_slice(obj, l): \"\"\" We have a full length slice. \"\"\" return (isinstance(obj, slice) and obj.start == 0 and obj.stop == l and obj.step is None)", "def iget_list_column_slice(list_, start=None, stop=None, stride=None): \"\"\" iterator version of get_list_column \"\"\" if isinstance(start, slice): slice_ = start else: slice_ = slice(start, stop, stride) return (row[slice_] for row in list_)", "def make_slice_strings(cls, slice_key): \"\"\" Converts the given slice key to start and size query parts. \"\"\" start = slice_key.start size = slice_key.stop - start return (str(start), str(size))"], "rank": 1}
{"nl": "apply a list of functions python", "code": ["def compose(*funcs): \"\"\"compose a list of functions\"\"\" return lambda x: reduce(lambda v, f: f(v), reversed(funcs), x)", "def compose(func_list): \"\"\" composion of preprocessing functions \"\"\" def f(G, bim): for func in func_list: G, bim = func(G, bim) return G, bim return f", "def compose(*parameter_functions): \"\"\"Composes multiple modification functions in order. Args: *parameter_functions: The functions to compose. Returns: A parameter modification function that consists of applying all the provided functions. \"\"\" def composed_fn(var_name, variable, phase): for fn in parameter_functions: variable = fn(var_name, variable, phase) return variable return composed_fn", "def map(cls, iterable, func, *a, **kw): \"\"\" Iterable-first replacement of Python's built-in `map()` function. \"\"\" return cls(func(x, *a, **kw) for x in iterable)", "def map_wrap(f): \"\"\"Wrap standard function to easily pass into 'map' processing. \"\"\" @functools.wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) return wrapper"], "rank": 1}
{"nl": "how to add size on the python", "code": ["def calculate_size(name, count): \"\"\" Calculates the request payload size\"\"\" data_size = 0 data_size += calculate_size_str(name) data_size += INT_SIZE_IN_BYTES return data_size", "def calculate_size(name, max_size): \"\"\" Calculates the request payload size\"\"\" data_size = 0 data_size += calculate_size_str(name) data_size += INT_SIZE_IN_BYTES return data_size", "def calculate_size(name, function): \"\"\" Calculates the request payload size\"\"\" data_size = 0 data_size += calculate_size_str(name) data_size += calculate_size_data(function) return data_size", "def calculate_size(name, replace_existing_values): \"\"\" Calculates the request payload size\"\"\" data_size = 0 data_size += calculate_size_str(name) data_size += BOOLEAN_SIZE_IN_BYTES return data_size", "def calculate_size(name, timeout): \"\"\" Calculates the request payload size\"\"\" data_size = 0 data_size += calculate_size_str(name) data_size += LONG_SIZE_IN_BYTES return data_size"], "rank": 4}
{"nl": "how to clear a frame in python", "code": ["def clearImg(self): \"\"\"Clears the current image\"\"\" self.img.setImage(np.array([[0]])) self.img.image = None", "def clear(self): \"\"\"Clear the displayed image.\"\"\" self._imgobj = None try: # See if there is an image on the canvas self.canvas.delete_object_by_tag(self._canvas_img_tag) self.redraw() except KeyError: pass", "def _clear(self): \"\"\" Helper that clears the composition. \"\"\" draw = ImageDraw.Draw(self._background_image) draw.rectangle(self._device.bounding_box, fill=\"black\") del draw", "def forget_coords(self): \"\"\"Forget all loaded coordinates.\"\"\" self.w.ntotal.set_text('0') self.coords_dict.clear() self.redo()", "def clear(self): \"\"\" Clear screen and go to 0,0 \"\"\" # Erase current output first. self.erase() # Send \"Erase Screen\" command and go to (0, 0). output = self.output output.erase_screen() output.cursor_goto(0, 0) output.flush() self.request_absolute_cursor_position()"], "rank": 6}
{"nl": "python not (condition1 and condition2)", "code": ["def _not(condition=None, **kwargs): \"\"\" Return the opposite of input condition. :param condition: condition to process. :result: not condition. :rtype: bool \"\"\" result = True if condition is not None: result = not run(condition, **kwargs) return result", "def _or(ctx, *logical): \"\"\" Returns TRUE if any argument is TRUE \"\"\" for arg in logical: if conversions.to_boolean(arg, ctx): return True return False", "def notin(arg, values): \"\"\" Like isin, but checks whether this expression's value(s) are not contained in the passed values. See isin docs for full usage. \"\"\" op = ops.NotContains(arg, values) return op.to_expr()", "def logical_or(self, other): \"\"\"logical_or(t) = self(t) or other(t).\"\"\" return self.operation(other, lambda x, y: int(x or y))", "def BROADCAST_FILTER_NOT(func): \"\"\" Composes the passed filters into an and-joined filter. \"\"\" return lambda u, command, *args, **kwargs: not func(u, command, *args, **kwargs)"], "rank": 1}
{"nl": "python check valid attribute names", "code": ["def _validate_key(self, key): \"\"\"Returns a boolean indicating if the attribute name is valid or not\"\"\" return not any([key.startswith(i) for i in self.EXCEPTIONS])", "def required_attributes(element, *attributes): \"\"\"Check element for required attributes. Raise ``NotValidXmlException`` on error. :param element: ElementTree element :param attributes: list of attributes names to check :raises NotValidXmlException: if some argument is missing \"\"\" if not reduce(lambda still_valid, param: still_valid and param in element.attrib, attributes, True): raise NotValidXmlException(msg_err_missing_attributes(element.tag, *attributes))", "def assert_valid_name(name: str) -> str: \"\"\"Uphold the spec rules about naming.\"\"\" error = is_valid_name_error(name) if error: raise error return name", "def name_is_valid(name): \"\"\"Return True if the dataset name is valid. The name can only be 80 characters long. Valid characters: Alpha numeric characters [0-9a-zA-Z] Valid special characters: - _ . \"\"\" # The name can only be 80 characters long. if len(name) > MAX_NAME_LENGTH: return False return bool(NAME_VALID_CHARS_REGEX.match(name))", "def is_valid_varname(varname): \"\"\" Checks syntax and validity of a variable name \"\"\" if not isinstance(varname, six.string_types): return False match_obj = re.match(varname_regex, varname) valid_syntax = match_obj is not None valid_name = not keyword.iskeyword(varname) isvalid = valid_syntax and valid_name return isvalid"], "rank": 1}
{"nl": "python get the date of creation of file", "code": ["def get_creation_datetime(filepath): \"\"\" Get the date that a file was created. Parameters ---------- filepath : str Returns ------- creation_datetime : datetime.datetime or None \"\"\" if platform.system() == 'Windows': return datetime.fromtimestamp(os.path.getctime(filepath)) else: stat = os.stat(filepath) try: return datetime.fromtimestamp(stat.st_birthtime) except AttributeError: # We're probably on Linux. No easy way to get creation dates here, # so we'll settle for when its content was last modified. return None", "def datetime_created(self): \"\"\"Returns file group's create aware *datetime* in UTC format.\"\"\" if self.info().get('datetime_created'): return dateutil.parser.parse(self.info()['datetime_created'])", "def creation_time(self): \"\"\"dfdatetime.Filetime: creation time or None if not set.\"\"\" timestamp = self._fsntfs_attribute.get_creation_time_as_integer() return dfdatetime_filetime.Filetime(timestamp=timestamp)", "def get_access_datetime(filepath): \"\"\" Get the last time filepath was accessed. Parameters ---------- filepath : str Returns ------- access_datetime : datetime.datetime \"\"\" import tzlocal tz = tzlocal.get_localzone() mtime = datetime.fromtimestamp(os.path.getatime(filepath)) return mtime.replace(tzinfo=tz)", "def fileModifiedTimestamp(fname): \"\"\"return \"YYYY-MM-DD\" when the file was modified.\"\"\" modifiedTime=os.path.getmtime(fname) stamp=time.strftime('%Y-%m-%d', time.localtime(modifiedTime)) return stamp"], "rank": 1}
{"nl": "make values as strings in a list in python", "code": ["def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res", "def list_to_str(list, separator=','): \"\"\" >>> list = [0, 0, 7] >>> list_to_str(list) '0,0,7' \"\"\" list = [str(x) for x in list] return separator.join(list)", "def list_i2str(ilist): \"\"\" Convert an integer list into a string list. \"\"\" slist = [] for el in ilist: slist.append(str(el)) return slist", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids", "def _tostr(self,obj): \"\"\" converts a object to list, if object is a list, it creates a comma seperated string. \"\"\" if not obj: return '' if isinstance(obj, list): return ', '.join(map(self._tostr, obj)) return str(obj)"], "rank": 8}
{"nl": "how to check if memory leak in python program", "code": ["def memory_full(): \"\"\"Check if the memory is too full for further caching.\"\"\" current_process = psutil.Process(os.getpid()) return (current_process.memory_percent() > config.MAXIMUM_CACHE_MEMORY_PERCENTAGE)", "def memory_used(self): \"\"\"To know the allocated memory at function termination. ..versionadded:: 4.1 This property might return None if the function is still running. This function should help to show memory leaks or ram greedy code. \"\"\" if self._end_memory: memory_used = self._end_memory - self._start_memory return memory_used else: return None", "def _EnforceProcessMemoryLimit(self, memory_limit): \"\"\"Enforces a process memory limit. Args: memory_limit (int): maximum number of bytes the process is allowed to allocate, where 0 represents no limit and None a default of 4 GiB. \"\"\" # Resource is not supported on Windows. if resource: if memory_limit is None: memory_limit = 4 * 1024 * 1024 * 1024 elif memory_limit == 0: memory_limit = resource.RLIM_INFINITY resource.setrlimit(resource.RLIMIT_DATA, (memory_limit, memory_limit))", "def _request_limit_reached(exception): \"\"\" Checks if exception was raised because of too many executed requests. (This is a temporal solution and will be changed in later package versions.) :param exception: Exception raised during download :type exception: Exception :return: True if exception is caused because too many requests were executed at once and False otherwise :rtype: bool \"\"\" return isinstance(exception, requests.HTTPError) and \\ exception.response.status_code == requests.status_codes.codes.TOO_MANY_REQUESTS", "def get_free_memory_win(): \"\"\"Return current free memory on the machine for windows. Warning : this script is really not robust Return in MB unit \"\"\" stat = MEMORYSTATUSEX() ctypes.windll.kernel32.GlobalMemoryStatusEx(ctypes.byref(stat)) return int(stat.ullAvailPhys / 1024 / 1024)"], "rank": 1}
{"nl": "python assert data type check if array type", "code": ["def _valid_other_type(x, types): \"\"\" Do all elements of x have a type from types? \"\"\" return all(any(isinstance(el, t) for t in types) for el in np.ravel(x))", "def make_kind_check(python_types, numpy_kind): \"\"\" Make a function that checks whether a scalar or array is of a given kind (e.g. float, int, datetime, timedelta). \"\"\" def check(value): if hasattr(value, 'dtype'): return value.dtype.kind == numpy_kind return isinstance(value, python_types) return check", "def is_iterable(value): \"\"\"must be an iterable (list, array, tuple)\"\"\" return isinstance(value, np.ndarray) or isinstance(value, list) or isinstance(value, tuple), value", "def contains_all(self, array): \"\"\"Test if `array` is an array of real numbers.\"\"\" dtype = getattr(array, 'dtype', None) if dtype is None: dtype = np.result_type(*array) return is_real_dtype(dtype)", "def _assert_is_type(name, value, value_type): \"\"\"Assert that a value must be a given type.\"\"\" if not isinstance(value, value_type): if type(value_type) is tuple: types = ', '.join(t.__name__ for t in value_type) raise ValueError('{0} must be one of ({1})'.format(name, types)) else: raise ValueError('{0} must be {1}' .format(name, value_type.__name__))"], "rank": 5}
{"nl": "how to check column value is null python", "code": ["def _notnull(expr): \"\"\" Return a sequence or scalar according to the input indicating if the values are not null. :param expr: sequence or scalar :return: sequence or scalar \"\"\" if isinstance(expr, SequenceExpr): return NotNull(_input=expr, _data_type=types.boolean) elif isinstance(expr, Scalar): return NotNull(_input=expr, _value_type=types.boolean)", "def is_not_null(df: DataFrame, col_name: str) -> bool: \"\"\" Return ``True`` if the given DataFrame has a column of the given name (string), and there exists at least one non-NaN value in that column; return ``False`` otherwise. \"\"\" if ( isinstance(df, pd.DataFrame) and col_name in df.columns and df[col_name].notnull().any() ): return True else: return False", "def _none_value(self): \"\"\"Get an appropriate \"null\" value for this field's type. This is used internally when setting the field to None. \"\"\" if self.out_type == int: return 0 elif self.out_type == float: return 0.0 elif self.out_type == bool: return False elif self.out_type == six.text_type: return u''", "def count_nulls(self, field): \"\"\" Count the number of null values in a column \"\"\" try: n = self.df[field].isnull().sum() except KeyError: self.warning(\"Can not find column\", field) return except Exception as e: self.err(e, \"Can not count nulls\") return self.ok(\"Found\", n, \"nulls in column\", field)", "def selectnotnone(table, field, complement=False): \"\"\"Select rows where the given field is not `None`.\"\"\" return select(table, field, lambda v: v is not None, complement=complement)"], "rank": 2}
{"nl": "python sql server query date to datetime object", "code": ["def from_pydatetime(cls, pydatetime): \"\"\" Creates sql datetime2 object from Python datetime object ignoring timezone @param pydatetime: Python datetime object @return: sql datetime2 object \"\"\" return cls(date=Date.from_pydate(pydatetime.date), time=Time.from_pytime(pydatetime.time))", "def _datetime_to_date(arg): \"\"\" convert datetime/str to date :param arg: :return: \"\"\" _arg = parse(arg) if isinstance(_arg, datetime.datetime): _arg = _arg.date() return _arg", "def date_to_datetime(x): \"\"\"Convert a date into a datetime\"\"\" if not isinstance(x, datetime) and isinstance(x, date): return datetime.combine(x, time()) return x", "def get_date(date): \"\"\" Get the date from a value that could be a date object or a string. :param date: The date object or string. :returns: The date object. \"\"\" if type(date) is str: return datetime.strptime(date, '%Y-%m-%d').date() else: return date", "def date_to_datetime(d): \"\"\" >>> date_to_datetime(date(2000, 1, 2)) datetime.datetime(2000, 1, 2, 0, 0) >>> date_to_datetime(datetime(2000, 1, 2, 3, 4, 5)) datetime.datetime(2000, 1, 2, 3, 4, 5) \"\"\" if not isinstance(d, datetime): d = datetime.combine(d, datetime.min.time()) return d"], "rank": 1}
{"nl": "python json print tree", "code": ["def prettyprint(d): \"\"\"Print dicttree in Json-like format. keys are sorted \"\"\" print(json.dumps(d, sort_keys=True, indent=4, separators=(\",\" , \": \")))", "def as_tree(context): \"\"\"Return info about an object's members as JSON\"\"\" tree = _build_tree(context, 2, 1) if type(tree) == dict: tree = [tree] return Response(content_type='application/json', body=json.dumps(tree))", "def pretty_dict_str(d, indent=2): \"\"\"shows JSON indented representation of d\"\"\" b = StringIO() write_pretty_dict_str(b, d, indent=indent) return b.getvalue()", "def to_json(data): \"\"\"Return data as a JSON string.\"\"\" return json.dumps(data, default=lambda x: x.__dict__, sort_keys=True, indent=4)", "def pprint(j, no_pretty): \"\"\" Prints as formatted JSON \"\"\" if not no_pretty: click.echo( json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=(\",\", \": \")) ) else: click.echo(j)"], "rank": 1}
{"nl": "how to give x axis limits in python plot", "code": ["def set_xlimits(self, min=None, max=None): \"\"\"Set limits for the x-axis. :param min: minimum value to be displayed. If None, it will be calculated. :param max: maximum value to be displayed. If None, it will be calculated. \"\"\" self.limits['xmin'] = min self.limits['xmax'] = max", "def set_xlimits_widgets(self, set_min=True, set_max=True): \"\"\"Populate axis limits GUI with current plot values.\"\"\" xmin, xmax = self.tab_plot.ax.get_xlim() if set_min: self.w.x_lo.set_text('{0}'.format(xmin)) if set_max: self.w.x_hi.set_text('{0}'.format(xmax))", "def set_xlimits(self, row, column, min=None, max=None): \"\"\"Set x-axis limits of a subplot. :param row,column: specify the subplot. :param min: minimal axis value :param max: maximum axis value \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_xlimits(min, max)", "def _set_axis_limits(self, which, lims, d, scale, reverse=False): \"\"\"Private method for setting axis limits. Sets the axis limits on each axis for an individual plot. Args: which (str): The indicator of which part of the plots to adjust. This currently handles `x` and `y`. lims (len-2 list of floats): The limits for the axis. d (float): Amount to increment by between the limits. scale (str): Scale of the axis. Either `log` or `lin`. reverse (bool, optional): If True, reverse the axis tick marks. Default is False. \"\"\" setattr(self.limits, which + 'lims', lims) setattr(self.limits, 'd' + which, d) setattr(self.limits, which + 'scale', scale) if reverse: setattr(self.limits, 'reverse_' + which + '_axis', True) return", "def set_ylim(self, xlims, dx, xscale, reverse=False): \"\"\"Set y limits for plot. This will set the limits for the y axis for the specific plot. Args: ylims (len-2 list of floats): The limits for the axis. dy (float): Amount to increment by between the limits. yscale (str): Scale of the axis. Either `log` or `lin`. reverse (bool, optional): If True, reverse the axis tick marks. Default is False. \"\"\" self._set_axis_limits('y', xlims, dx, xscale, reverse) return"], "rank": 3}
{"nl": "compute l2 norm in python", "code": ["def l2_norm(params): \"\"\"Computes l2 norm of params by flattening them into a vector.\"\"\" flattened, _ = flatten(params) return np.dot(flattened, flattened)", "def l2_norm(arr): \"\"\" The l2 norm of an array is is defined as: sqrt(||x||), where ||x|| is the dot product of the vector. \"\"\" arr = np.asarray(arr) return np.sqrt(np.dot(arr.ravel().squeeze(), arr.ravel().squeeze()))", "def ln_norm(x, mu, sigma=1.0): \"\"\" Natural log of scipy norm function truncated at zero \"\"\" return np.log(stats.norm(loc=mu, scale=sigma).pdf(x))", "def normalize(v, axis=None, eps=1e-10): \"\"\"L2 Normalize along specified axes.\"\"\" return v / max(anorm(v, axis=axis, keepdims=True), eps)", "def _norm(self, x): \"\"\"Compute the safe norm.\"\"\" return tf.sqrt(tf.reduce_sum(tf.square(x), keepdims=True, axis=-1) + 1e-7)"], "rank": 1}
{"nl": "how to log a variable without knowing its type in python", "code": ["def info(self, text): \"\"\" Ajout d'un message de log de type INFO \"\"\" self.logger.info(\"{}{}\".format(self.message_prefix, text))", "def info(self, message, *args, **kwargs): \"\"\"More important level : default for print and save \"\"\" self._log(logging.INFO, message, *args, **kwargs)", "def debug(self, text): \"\"\" Ajout d'un message de log de type DEBUG \"\"\" self.logger.debug(\"{}{}\".format(self.message_prefix, text))", "def warn(self, text): \"\"\" Ajout d'un message de log de type WARN \"\"\" self.logger.warn(\"{}{}\".format(self.message_prefix, text))", "def pylog(self, *args, **kwargs): \"\"\"Display all available logging information.\"\"\" printerr(self.name, args, kwargs, traceback.format_exc())"], "rank": 1}
{"nl": "python map, delete key", "code": ["def __delitem__(self, key): \"\"\"Remove item with given key from the mapping. Runs in O(n), unless removing last item, then in O(1). \"\"\" index, value = self._dict.pop(key) key2, value2 = self._list.pop(index) assert key == key2 assert value is value2 self._fix_indices_after_delete(index)", "def delete(self, row): \"\"\"Delete a track value\"\"\" i = self._get_key_index(row) del self.keys[i]", "def __delitem__ (self, key): \"\"\"Remove key from dict.\"\"\" self._keys.remove(key) super(ListDict, self).__delitem__(key)", "def pop (self, key): \"\"\"Remove key from dict and return value.\"\"\" if key in self._keys: self._keys.remove(key) super(ListDict, self).pop(key)", "def delete_entry(self, key): \"\"\"Delete an object from the redis table\"\"\" pipe = self.client.pipeline() pipe.srem(self.keys_container, key) pipe.delete(key) pipe.execute()"], "rank": 8}
{"nl": "dictionarry type from python to c++", "code": ["def struct2dict(struct): \"\"\"convert a ctypes structure to a dictionary\"\"\" return {x: getattr(struct, x) for x in dict(struct._fields_).keys()}", "def conv_dict(self): \"\"\"dictionary of conversion\"\"\" return dict(integer=self.integer, real=self.real, no_type=self.no_type)", "def get_ctype(rtype, cfunc, *args): \"\"\" Call a C function that takes a pointer as its last argument and return the C object that it contains after the function has finished. :param rtype: C data type is filled by the function :param cfunc: C function to call :param args: Arguments to call function with :return: A pointer to the specified data type \"\"\" val_p = backend.ffi.new(rtype) args = args + (val_p,) cfunc(*args) return val_p[0]", "def _tool_to_dict(tool): \"\"\"Parse a tool definition into a cwl2wdl style dictionary. \"\"\" out = {\"name\": _id_to_name(tool.tool[\"id\"]), \"baseCommand\": \" \".join(tool.tool[\"baseCommand\"]), \"arguments\": [], \"inputs\": [_input_to_dict(i) for i in tool.tool[\"inputs\"]], \"outputs\": [_output_to_dict(o) for o in tool.tool[\"outputs\"]], \"requirements\": _requirements_to_dict(tool.requirements + tool.hints), \"stdin\": None, \"stdout\": None} return out", "def translate_dict(cls, val): \"\"\"Translate dicts to scala Maps\"\"\" escaped = ', '.join( [\"{} -> {}\".format(cls.translate_str(k), cls.translate(v)) for k, v in val.items()] ) return 'Map({})'.format(escaped)"], "rank": 1}
{"nl": "axes3d view setting python 3", "code": ["def plot3d_init(fignum): \"\"\" initializes 3D plot \"\"\" from mpl_toolkits.mplot3d import Axes3D fig = plt.figure(fignum) ax = fig.add_subplot(111, projection='3d') return ax", "def _axes(self): \"\"\"Set the _force_vertical flag when rendering axes\"\"\" self.view._force_vertical = True super(HorizontalGraph, self)._axes() self.view._force_vertical = False", "def surface(self, zdata, **kwargs): \"\"\"Show a 3D surface plot. Extra keyword arguments are passed to `SurfacePlot()`. Parameters ---------- zdata : array-like A 2D array of the surface Z values. \"\"\" self._configure_3d() surf = scene.SurfacePlot(z=zdata, **kwargs) self.view.add(surf) self.view.camera.set_range() return surf", "def activate_subplot(numPlot): \"\"\"Make subplot *numPlot* active on the canvas. Use this if a simple ``subplot(numRows, numCols, numPlot)`` overwrites the subplot instead of activating it. \"\"\" # see http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg07156.html from pylab import gcf, axes numPlot -= 1 # index is 0-based, plots are 1-based return axes(gcf().get_axes()[numPlot])", "def oplot(self, x, y, **kw): \"\"\"generic plotting method, overplotting any existing plot \"\"\" self.panel.oplot(x, y, **kw)"], "rank": 1}
{"nl": "python get month start and end date by month and year", "code": ["def get_month_start_end_day(): \"\"\" Get the month start date a nd end date \"\"\" t = date.today() n = mdays[t.month] return (date(t.year, t.month, 1), date(t.year, t.month, n))", "def last_month(): \"\"\" Return start and end date of this month. \"\"\" since = TODAY + delta(day=1, months=-1) until = since + delta(months=1) return Date(since), Date(until)", "def get_year_start(day=None): \"\"\"Returns January 1 of the given year.\"\"\" day = add_timezone(day or datetime.date.today()) return day.replace(month=1).replace(day=1)", "def from_years_range(start_year, end_year): \"\"\"Transform a range of years (two ints) to a DateRange object.\"\"\" start = datetime.date(start_year, 1 , 1) end = datetime.date(end_year, 12 , 31) return DateRange(start, end)", "def start_of_month(val): \"\"\" Return a new datetime.datetime object with values that represent a start of a month. :param val: Date to ... :type val: datetime.datetime | datetime.date :rtype: datetime.datetime \"\"\" if type(val) == date: val = datetime.fromordinal(val.toordinal()) return start_of_day(val).replace(day=1)"], "rank": 1}
{"nl": "python set that can take repeats", "code": ["def update(self, iterable): \"\"\" Return a new PSet with elements in iterable added >>> s1 = s(1, 2) >>> s1.update([3, 4, 4]) pset([1, 2, 3, 4]) \"\"\" e = self.evolver() for element in iterable: e.add(element) return e.persistent()", "def unique(iterable): \"\"\" Returns a list copy in which each item occurs only once (in-order). \"\"\" seen = set() return [x for x in iterable if x not in seen and not seen.add(x)]", "def pset(iterable=(), pre_size=8): \"\"\" Creates a persistent set from iterable. Optionally takes a sizing parameter equivalent to that used for :py:func:`pmap`. >>> s1 = pset([1, 2, 3, 2]) >>> s1 pset([1, 2, 3]) \"\"\" if not iterable: return _EMPTY_PSET return PSet._from_iterable(iterable, pre_size=pre_size)", "def unique_iter(seq): \"\"\" See http://www.peterbe.com/plog/uniqifiers-benchmark Originally f8 written by Dave Kirby \"\"\" seen = set() return [x for x in seq if x not in seen and not seen.add(x)]", "def unique(iterable): \"\"\"Filter out duplicate items from an iterable\"\"\" seen = set() for item in iterable: if item not in seen: seen.add(item) yield item"], "rank": 26}
{"nl": "python flask request get form name", "code": ["def parse_form(self, req, name, field): \"\"\"Pull a form value from the request.\"\"\" return core.get_value(req.POST, name, field)", "def parse_form(self, req, name, field): \"\"\"Pull a form value from the request.\"\"\" return get_value(req.body_arguments, name, field)", "def get_path_from_query_string(req): \"\"\"Gets path from query string Args: req (flask.request): Request object from Flask Returns: path (str): Value of \"path\" parameter from query string Raises: exceptions.UserError: If \"path\" is not found in query string \"\"\" if req.args.get('path') is None: raise exceptions.UserError('Path not found in query string') return req.args.get('path')", "def get(key, default=None): \"\"\" return the key from the request \"\"\" data = get_form() or get_query_string() return data.get(key, default)", "def parse_querystring(self, req, name, field): \"\"\"Pull a querystring value from the request.\"\"\" return core.get_value(req.args, name, field)"], "rank": 2}
{"nl": "python for comprehension sum", "code": ["def _accumulate(sequence, func): \"\"\" Python2 accumulate implementation taken from https://docs.python.org/3/library/itertools.html#itertools.accumulate \"\"\" iterator = iter(sequence) total = next(iterator) yield total for element in iterator: total = func(total, element) yield total", "def lcumsum (inlist): \"\"\" Returns a list consisting of the cumulative sum of the items in the passed list. Usage: lcumsum(inlist) \"\"\" newlist = copy.deepcopy(inlist) for i in range(1,len(newlist)): newlist[i] = newlist[i] + newlist[i-1] return newlist", "def _cumprod(l): \"\"\"Cumulative product of a list. Args: l: a list of integers Returns: a list with one more element (starting with 1) \"\"\" ret = [1] for item in l: ret.append(ret[-1] * item) return ret", "def cumsum(inlist): \"\"\" Returns a list consisting of the cumulative sum of the items in the passed list. Usage: lcumsum(inlist) \"\"\" newlist = copy.deepcopy(inlist) for i in range(1, len(newlist)): newlist[i] = newlist[i] + newlist[i - 1] return newlist", "def query_sum(queryset, field): \"\"\" Let the DBMS perform a sum on a queryset \"\"\" return queryset.aggregate(s=models.functions.Coalesce(models.Sum(field), 0))['s']"], "rank": 1}
{"nl": "python draw image frombyte array", "code": ["def from_bytes(cls, b): \"\"\"Create :class:`PNG` from raw bytes. :arg bytes b: The raw bytes of the PNG file. :rtype: :class:`PNG` \"\"\" im = cls() im.chunks = list(parse_chunks(b)) im.init() return im", "def to_bytes(self): \"\"\"Convert the entire image to bytes. :rtype: bytes \"\"\" chunks = [PNG_SIGN] chunks.extend(c[1] for c in self.chunks) return b\"\".join(chunks)", "def be_array_from_bytes(fmt, data): \"\"\" Reads an array from bytestring with big-endian data. \"\"\" arr = array.array(str(fmt), data) return fix_byteorder(arr)", "def get_buffer(self, data_np, header, format, output=None): \"\"\"Get image as a buffer in (format). Format should be 'jpeg', 'png', etc. \"\"\" if not have_pil: raise Exception(\"Install PIL to use this method\") image = PILimage.fromarray(data_np) buf = output if buf is None: buf = BytesIO() image.save(buf, format) return buf", "def barray(iterlines): \"\"\" Array of bytes \"\"\" lst = [line.encode('utf-8') for line in iterlines] arr = numpy.array(lst) return arr"], "rank": 1}
{"nl": "python get file last modified time datetime", "code": ["def get_time(filename): \"\"\" Get the modified time for a file as a datetime instance \"\"\" ts = os.stat(filename).st_mtime return datetime.datetime.utcfromtimestamp(ts)", "def last_modified_date(filename): \"\"\"Last modified timestamp as a UTC datetime\"\"\" mtime = os.path.getmtime(filename) dt = datetime.datetime.utcfromtimestamp(mtime) return dt.replace(tzinfo=pytz.utc)", "def get_access_datetime(filepath): \"\"\" Get the last time filepath was accessed. Parameters ---------- filepath : str Returns ------- access_datetime : datetime.datetime \"\"\" import tzlocal tz = tzlocal.get_localzone() mtime = datetime.fromtimestamp(os.path.getatime(filepath)) return mtime.replace(tzinfo=tz)", "def fileModifiedTimestamp(fname): \"\"\"return \"YYYY-MM-DD\" when the file was modified.\"\"\" modifiedTime=os.path.getmtime(fname) stamp=time.strftime('%Y-%m-%d', time.localtime(modifiedTime)) return stamp", "def last_modified_time(path): \"\"\" Get the last modified time of path as a Timestamp. \"\"\" return pd.Timestamp(os.path.getmtime(path), unit='s', tz='UTC')"], "rank": 1}
{"nl": "python first line from file", "code": ["def getfirstline(file, default): \"\"\" Returns the first line of a file. \"\"\" with open(file, 'rb') as fh: content = fh.readlines() if len(content) == 1: return content[0].decode('utf-8').strip('\\n') return default", "def rAsciiLine(ifile): \"\"\"Returns the next non-blank line in an ASCII file.\"\"\" _line = ifile.readline().strip() while len(_line) == 0: _line = ifile.readline().strip() return _line", "def get_lines(handle, line): \"\"\" Get zero-indexed line from an open file-like. \"\"\" for i, l in enumerate(handle): if i == line: return l", "def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line", "def get_list_from_file(file_name): \"\"\"read the lines from a file into a list\"\"\" with open(file_name, mode='r', encoding='utf-8') as f1: lst = f1.readlines() return lst"], "rank": 1}
{"nl": "python break list into batches of 50", "code": ["def batch(items, size): \"\"\"Batches a list into a list of lists, with sub-lists sized by a specified batch size.\"\"\" return [items[x:x + size] for x in xrange(0, len(items), size)]", "def chunked_list(_list, _chunk_size=50): \"\"\" Break lists into small lists for processing:w \"\"\" for i in range(0, len(_list), _chunk_size): yield _list[i:i + _chunk_size]", "def _split_batches(self, data, batch_size): \"\"\"Yield successive n-sized chunks from l.\"\"\" for i in range(0, len(data), batch_size): yield data[i : i + batch_size]", "def batch(input_iter, batch_size=32): \"\"\"Batches data from an iterator that returns single items at a time.\"\"\" input_iter = iter(input_iter) next_ = list(itertools.islice(input_iter, batch_size)) while next_: yield next_ next_ = list(itertools.islice(input_iter, batch_size))", "def chunked(l, n): \"\"\"Chunk one big list into few small lists.\"\"\" return [l[i:i + n] for i in range(0, len(l), n)]"], "rank": 1}
{"nl": "how to see all python versions", "code": ["def all_versions(req): \"\"\"Get all versions of req from PyPI.\"\"\" import requests url = \"https://pypi.python.org/pypi/\" + req + \"/json\" return tuple(requests.get(url).json()[\"releases\"].keys())", "def check_python_version(): \"\"\"Check if the currently running Python version is new enough.\"\"\" # Required due to multiple with statements on one line req_version = (2, 7) cur_version = sys.version_info if cur_version >= req_version: print(\"Python version... %sOK%s (found %s, requires %s)\" % (Bcolors.OKGREEN, Bcolors.ENDC, str(platform.python_version()), str(req_version[0]) + \".\" + str(req_version[1]))) else: print(\"Python version... %sFAIL%s (found %s, requires %s)\" % (Bcolors.FAIL, Bcolors.ENDC, str(cur_version), str(req_version)))", "def version(): \"\"\" View the current version of the CLI. \"\"\" import pkg_resources version = pkg_resources.require(PROJECT_NAME)[0].version floyd_logger.info(version)", "def list_backends(_): \"\"\"List all available backends.\"\"\" backends = [b.__name__ for b in available_backends()] print('\\n'.join(backends))", "def _list_available_rest_versions(self): \"\"\"Return a list of the REST API versions supported by the array\"\"\" url = \"https://{0}/api/api_version\".format(self._target) data = self._request(\"GET\", url, reestablish_session=False) return data[\"version\"]"], "rank": 1}
{"nl": "how to sort a list in alphabetic order python", "code": ["def sort_nicely(l): \"\"\"Sort the given list in the way that humans expect.\"\"\" convert = lambda text: int(text) if text.isdigit() else text alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)] l.sort(key=alphanum_key)", "def natural_sort(list, key=lambda s:s): \"\"\" Sort the list into natural alphanumeric order. \"\"\" def get_alphanum_key_func(key): convert = lambda text: int(text) if text.isdigit() else text return lambda s: [convert(c) for c in re.split('([0-9]+)', key(s))] sort_key = get_alphanum_key_func(key) list.sort(key=sort_key)", "def natural_sort(list_to_sort: Iterable[str]) -> List[str]: \"\"\" Sorts a list of strings case insensitively as well as numerically. For example: ['a1', 'A2', 'a3', 'A11', 'a22'] To sort a list in place, don't call this method, which makes a copy. Instead, do this: my_list.sort(key=natural_keys) :param list_to_sort: the list being sorted :return: the list sorted naturally \"\"\" return sorted(list_to_sort, key=natural_keys)", "def csort(objs, key): \"\"\"Order-preserving sorting function.\"\"\" idxs = dict((obj, i) for (i, obj) in enumerate(objs)) return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))", "def sort_by_name(self): \"\"\"Sort list elements by name.\"\"\" super(JSSObjectList, self).sort(key=lambda k: k.name)"], "rank": 2}
{"nl": "python test path exists", "code": ["def _file_exists(path, filename): \"\"\"Checks if the filename exists under the path.\"\"\" return os.path.isfile(os.path.join(path, filename))", "def executable_exists(executable): \"\"\"Test if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False", "def is_valid_folder(parser, arg): \"\"\"Check if arg is a valid file that already exists on the file system.\"\"\" arg = os.path.abspath(arg) if not os.path.isdir(arg): parser.error(\"The folder %s does not exist!\" % arg) else: return arg", "def is_valid_file(parser, arg): \"\"\"Check if arg is a valid file that already exists on the file system.\"\"\" arg = os.path.abspath(arg) if not os.path.exists(arg): parser.error(\"The file %s does not exist!\" % arg) else: return arg", "def require(executable: str, explanation: str = \"\") -> None: \"\"\" Ensures that the external tool is available. Asserts upon failure. \"\"\" assert shutil.which(executable), \"Need {!r} on the PATH.{}\".format( executable, \"\\n\" + explanation if explanation else \"\")"], "rank": 7}
{"nl": "python check if object has attribute and if it is not none", "code": ["def hasattrs(object, *names): \"\"\" Takes in an object and a variable length amount of named attributes, and checks to see if the object has each property. If any of the attributes are missing, this returns false. :param object: an object that may or may not contain the listed attributes :param names: a variable amount of attribute names to check for :return: True if the object contains each named attribute, false otherwise \"\"\" for name in names: if not hasattr(object, name): return False return True", "def _is_initialized(self, entity): \"\"\"Internal helper to ask if the entity has a value for this Property. This returns False if a value is stored but it is None. \"\"\" return (not self._required or ((self._has_value(entity) or self._default is not None) and self._get_value(entity) is not None))", "def instance_contains(container, item): \"\"\"Search into instance attributes, properties and return values of no-args methods.\"\"\" return item in (member for _, member in inspect.getmembers(container))", "def check_attribute_exists(instance): \"\"\" Additional check for the dimension model, to ensure that attributes given as the key and label attribute on the dimension exist. \"\"\" attributes = instance.get('attributes', {}).keys() if instance.get('key_attribute') not in attributes: return False label_attr = instance.get('label_attribute') if label_attr and label_attr not in attributes: return False return True", "def is_all_field_none(self): \"\"\" :rtype: bool \"\"\" if self._type_ is not None: return False if self._value is not None: return False if self._name is not None: return False return True"], "rank": 1}
{"nl": "changing from list to string in python", "code": ["def list_to_str(list, separator=','): \"\"\" >>> list = [0, 0, 7] >>> list_to_str(list) '0,0,7' \"\"\" list = [str(x) for x in list] return separator.join(list)", "def list_i2str(ilist): \"\"\" Convert an integer list into a string list. \"\"\" slist = [] for el in ilist: slist.append(str(el)) return slist", "def _tostr(self,obj): \"\"\" converts a object to list, if object is a list, it creates a comma seperated string. \"\"\" if not obj: return '' if isinstance(obj, list): return ', '.join(map(self._tostr, obj)) return str(obj)", "def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids"], "rank": 1}
{"nl": "easy way to check is a boolean has changed in python", "code": ["def changed(self, *value): \"\"\"Checks whether the value has changed since the last call.\"\"\" if self._last_checked_value != value: self._last_checked_value = value return True return False", "def _check_and_convert_bools(self): \"\"\"Replace boolean variables by the characters 'F'/'T' \"\"\" replacements = { True: 'T', False: 'F', } for key in self.bools: if isinstance(self[key], bool): self[key] = replacements[self[key]]", "def process_bool_arg(arg): \"\"\" Determine True/False from argument \"\"\" if isinstance(arg, bool): return arg elif isinstance(arg, basestring): if arg.lower() in [\"true\", \"1\"]: return True elif arg.lower() in [\"false\", \"0\"]: return False", "def boolean(value): \"\"\" Configuration-friendly boolean type converter. Supports both boolean-valued and string-valued inputs (e.g. from env vars). \"\"\" if isinstance(value, bool): return value if value == \"\": return False return strtobool(value)", "def _isbool(string): \"\"\" >>> _isbool(True) True >>> _isbool(\"False\") True >>> _isbool(1) False \"\"\" return isinstance(string, _bool_type) or\\ (isinstance(string, (_binary_type, _text_type)) and string in (\"True\", \"False\"))"], "rank": 40}
{"nl": "get result from cursor python", "code": ["def query_fetch_one(self, query, values): \"\"\" Executes a db query, gets the first value, and closes the connection. \"\"\" self.cursor.execute(query, values) retval = self.cursor.fetchone() self.__close_db() return retval", "async def fetchall(self) -> Iterable[sqlite3.Row]: \"\"\"Fetch all remaining rows.\"\"\" return await self._execute(self._cursor.fetchall)", "def _dictfetchall(self, cursor): \"\"\" Return all rows from a cursor as a dict. \"\"\" columns = [col[0] for col in cursor.description] return [ dict(zip(columns, row)) for row in cursor.fetchall() ]", "def select(self, cmd, *args, **kwargs): \"\"\" Execute the SQL command and return the data rows as tuples \"\"\" self.cursor.execute(cmd, *args, **kwargs) return self.cursor.fetchall()", "def execute(self, cmd, *args, **kwargs): \"\"\" Execute the SQL command and return the data rows as tuples \"\"\" self.cursor.execute(cmd, *args, **kwargs)"], "rank": 5}
{"nl": "python get current loggers", "code": ["def _get_loggers(): \"\"\"Return list of Logger classes.\"\"\" from .. import loader modules = loader.get_package_modules('logger') return list(loader.get_plugins(modules, [_Logger]))", "def __getLogger(cls): \"\"\" Get the logger for this object. :returns: (Logger) A Logger object. \"\"\" if cls.__logger is None: cls.__logger = opf_utils.initLogger(cls) return cls.__logger", "def register_logging_factories(loader): \"\"\" Registers default factories for logging standard package. :param loader: Loader where you want register default logging factories \"\"\" loader.register_factory(logging.Logger, LoggerFactory) loader.register_factory(logging.Handler, LoggingHandlerFactory)", "def pylog(self, *args, **kwargs): \"\"\"Display all available logging information.\"\"\" printerr(self.name, args, kwargs, traceback.format_exc())", "def find_console_handler(logger): \"\"\"Return a stream handler, if it exists.\"\"\" for handler in logger.handlers: if (isinstance(handler, logging.StreamHandler) and handler.stream == sys.stderr): return handler"], "rank": 1}
{"nl": "python parse date strptime day of month no padd", "code": ["def parse_date(s): \"\"\"Fast %Y-%m-%d parsing.\"\"\" try: return datetime.date(int(s[:4]), int(s[5:7]), int(s[8:10])) except ValueError: # other accepted format used in one-day data set return datetime.datetime.strptime(s, '%d %B %Y').date()", "def parse(self, s): \"\"\" Parses a date string formatted like ``YYYY-MM-DD``. \"\"\" return datetime.datetime.strptime(s, self.date_format).date()", "def convert_date(date): \"\"\"Convert string to datetime object.\"\"\" date = convert_month(date, shorten=False) clean_string = convert_string(date) return datetime.strptime(clean_string, DATE_FMT.replace('-',''))", "def parse_date(s): \"\"\" Parse a date using dateutil.parser.parse if available, falling back to datetime.datetime.strptime if not \"\"\" if isinstance(s, (datetime.datetime, datetime.date)): return s try: from dateutil.parser import parse except ImportError: parse = lambda d: datetime.datetime.strptime(d, \"%Y-%m-%d\") return parse(s)", "def get_just_date(self): \"\"\"Parses just date from date-time :return: Just day, month and year (setting hours to 00:00:00) \"\"\" return datetime.datetime( self.date_time.year, self.date_time.month, self.date_time.day )"], "rank": 2}
{"nl": "how to return the type of an object in python", "code": ["def typename(obj): \"\"\"Returns the type of obj as a string. More descriptive and specific than type(obj), and safe for any object, unlike __class__.\"\"\" if hasattr(obj, '__class__'): return getattr(obj, '__class__').__name__ else: return type(obj).__name__", "def object_type_repr(obj): \"\"\"Returns the name of the object's type. For some recognized singletons the name of the object is returned instead. (For example for `None` and `Ellipsis`). \"\"\" if obj is None: return 'None' elif obj is Ellipsis: return 'Ellipsis' if obj.__class__.__module__ == '__builtin__': name = obj.__class__.__name__ else: name = obj.__class__.__module__ + '.' + obj.__class__.__name__ return '%s object' % name", "def getTypeStr(_type): r\"\"\"Gets the string representation of the given type. \"\"\" if isinstance(_type, CustomType): return str(_type) if hasattr(_type, '__name__'): return _type.__name__ return ''", "def _get_type(self, value): \"\"\"Get the data type for *value*.\"\"\" if value is None: return type(None) elif type(value) in int_types: return int elif type(value) in float_types: return float elif isinstance(value, binary_type): return binary_type else: return text_type", "def _api_type(self, value): \"\"\" Returns the API type of the given value based on its python type. \"\"\" if isinstance(value, six.string_types): return 'string' elif isinstance(value, six.integer_types): return 'integer' elif type(value) is datetime.datetime: return 'date'"], "rank": 10}
{"nl": "how to get the first and last index of element in list python", "code": ["def _rindex(mylist: Sequence[T], x: T) -> int: \"\"\"Index of the last occurrence of x in the sequence.\"\"\" return len(mylist) - mylist[::-1].index(x) - 1", "def _get_item_position(self, idx): \"\"\"Return a tuple of (start, end) indices of an item from its index.\"\"\" start = 0 if idx == 0 else self._index[idx - 1] + 1 end = self._index[idx] return start, end", "def index(self, elem): \"\"\"Find the index of elem in the reversed iterator.\"\"\" return _coconut.len(self._iter) - self._iter.index(elem) - 1", "def find_last_sublist(list_, sublist): \"\"\"Given a list, find the last occurance of a sublist within it. Returns: Index where the sublist starts, or None if there is no match. \"\"\" for i in reversed(range(len(list_) - len(sublist) + 1)): if list_[i] == sublist[0] and list_[i:i + len(sublist)] == sublist: return i return None", "def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None"], "rank": 7}
{"nl": "how to get a document in a collection using mongoengine api server in python", "code": ["def find_one(cls, *args, **kw): \"\"\"Get a single document from the collection this class is bound to. Additional arguments are processed according to `_prepare_find` prior to passing to PyMongo, where positional parameters are interpreted as query fragments, parametric keyword arguments combined, and other keyword arguments passed along with minor transformation. Automatically calls `to_mongo` with the retrieved data. https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find_one \"\"\" if len(args) == 1 and not isinstance(args[0], Filter): args = (getattr(cls, cls.__pk__) == args[0], ) Doc, collection, query, options = cls._prepare_find(*args, **kw) result = Doc.from_mongo(collection.find_one(query, **options)) return result", "def find(self, *args, **kwargs): \"\"\"Same as :meth:`pymongo.collection.Collection.find`, except it returns the right document class. \"\"\" return Cursor(self, *args, wrap=self.document_class, **kwargs)", "def find_one(self, query): \"\"\"Find one wrapper with conversion to dictionary :param dict query: A Mongo query \"\"\" mongo_response = yield self.collection.find_one(query) raise Return(self._obj_cursor_to_dictionary(mongo_response))", "def find_one(cls, *args, **kwargs): \"\"\"Run a find_one on this model's collection. The arguments to ``Model.find_one`` are the same as to ``pymongo.Collection.find_one``.\"\"\" database, collection = cls._collection_key.split('.') return current()[database][collection].find_one(*args, **kwargs)", "def __getitem__(self, name): \"\"\" A pymongo-like behavior for dynamically obtaining a collection of documents \"\"\" if name not in self._collections: self._collections[name] = Collection(self.db, name) return self._collections[name]"], "rank": 1}
{"nl": "doctype html parse python", "code": ["def iterparse(source, tag, clear=False, events=None): \"\"\" iterparse variant that supports 'tag' parameter (like lxml), yields elements and clears nodes after parsing. \"\"\" for event, elem in ElementTree.iterparse(source, events=events): if elem.tag == tag: yield elem if clear: elem.clear()", "def strip_html(string, keep_tag_content=False): \"\"\" Remove html code contained into the given string. :param string: String to manipulate. :type string: str :param keep_tag_content: True to preserve tag content, False to remove tag and its content too (default). :type keep_tag_content: bool :return: String with html removed. :rtype: str \"\"\" r = HTML_TAG_ONLY_RE if keep_tag_content else HTML_RE return r.sub('', string)", "def parse(text, showToc=True): \"\"\"Returns HTML from MediaWiki markup\"\"\" p = Parser(show_toc=showToc) return p.parse(text)", "def html_to_text(content): \"\"\" Converts html content to plain text \"\"\" text = None h2t = html2text.HTML2Text() h2t.ignore_links = False text = h2t.handle(content) return text", "def do_striptags(value): \"\"\"Strip SGML/XML tags and replace adjacent whitespace by one space. \"\"\" if hasattr(value, '__html__'): value = value.__html__() return Markup(unicode(value)).striptags()"], "rank": 3}
{"nl": "python full screen adjust to the screen", "code": ["def trigger_fullscreen_action(self, fullscreen): \"\"\" Toggle fullscreen from outside the GUI, causes the GUI to updated and run all its actions. \"\"\" action = self.action_group.get_action('fullscreen') action.set_active(fullscreen)", "def screen(self, width, height, colorDepth): \"\"\" @summary: record resize event of screen (maybe first event) @param width: {int} width of screen @param height: {int} height of screen @param colorDepth: {int} colorDepth \"\"\" screenEvent = ScreenEvent() screenEvent.width.value = width screenEvent.height.value = height screenEvent.colorDepth.value = colorDepth self.rec(screenEvent)", "def update_screen(self): \"\"\"Refresh the screen. You don't need to override this except to update only small portins of the screen.\"\"\" self.clock.tick(self.FPS) pygame.display.update()", "def display(self): \"\"\" Get screen width and height \"\"\" w, h = self.session.window_size() return Display(w*self.scale, h*self.scale)", "def match_aspect_to_viewport(self): \"\"\"Updates Camera.aspect to match the viewport's aspect ratio.\"\"\" viewport = self.viewport self.aspect = float(viewport.width) / viewport.height"], "rank": 3}
{"nl": "how to set default values in dict python", "code": ["def setdefaults(dct, defaults): \"\"\"Given a target dct and a dict of {key:default value} pairs, calls setdefault for all of those pairs.\"\"\" for key in defaults: dct.setdefault(key, defaults[key]) return dct", "def setDictDefaults (d, defaults): \"\"\"Sets all defaults for the given dictionary to those contained in a second defaults dictionary. This convenience method calls: d.setdefault(key, value) for each key and value in the given defaults dictionary. \"\"\" for key, val in defaults.items(): d.setdefault(key, val) return d", "def set_default(self, key, value): \"\"\"Set the default value for this key. Default only used when no value is provided by the user via arg, config or env. \"\"\" k = self._real_key(key.lower()) self._defaults[k] = value", "def setdefault(self, name: str, default: Any=None) -> Any: \"\"\"Set an attribute with a default value.\"\"\" return self.__dict__.setdefault(name, default)", "def setdefault(obj, field, default): \"\"\"Set an object's field to default if it doesn't have a value\"\"\" setattr(obj, field, getattr(obj, field, default))"], "rank": 2}
{"nl": "image segmentation kmeans python", "code": ["def cluster_kmeans(data, n_clusters, **kwargs): \"\"\" Identify clusters using K - Means algorithm. Parameters ---------- data : array_like array of size [n_samples, n_features]. n_clusters : int The number of clusters expected in the data. Returns ------- dict boolean array for each identified cluster. \"\"\" km = cl.KMeans(n_clusters, **kwargs) kmf = km.fit(data) labels = kmf.labels_ return labels, [np.nan]", "def fit(self, X): \"\"\" Apply KMeans Clustering X: dataset with feature vectors \"\"\" self.centers_, self.labels_, self.sse_arr_, self.n_iter_ = \\ _kmeans(X, self.n_clusters, self.max_iter, self.n_trials, self.tol)", "def classify_clusters(points, n=10): \"\"\" Return an array of K-Means cluster classes for an array of `shapely.geometry.Point` objects. \"\"\" arr = [[p.x, p.y] for p in points.values] clf = KMeans(n_clusters=n) clf.fit(arr) classes = clf.predict(arr) return classes", "def region_from_segment(image, segment): \"\"\"given a segment (rectangle) and an image, returns it's corresponding subimage\"\"\" x, y, w, h = segment return image[y:y + h, x:x + w]", "def _centroids(n_clusters: int, points: List[List[float]]) -> List[List[float]]: \"\"\" Return n_clusters centroids of points \"\"\" k_means = KMeans(n_clusters=n_clusters) k_means.fit(points) closest, _ = pairwise_distances_argmin_min(k_means.cluster_centers_, points) return list(map(list, np.array(points)[closest.tolist()]))"], "rank": 4}
{"nl": "test for empty python dictionary", "code": ["def _remove_empty_items(d, required): \"\"\"Return a new dict with any empty items removed. Note that this is not a deep check. If d contains a dictionary which itself contains empty items, those are never checked. This method exists to make to_serializable() functions cleaner. We could revisit this some day, but for now, the serialized objects are stripped of empty values to keep the output YAML more compact. Args: d: a dictionary required: list of required keys (for example, TaskDescriptors always emit the \"task-id\", even if None) Returns: A dictionary with empty items removed. \"\"\" new_dict = {} for k, v in d.items(): if k in required: new_dict[k] = v elif isinstance(v, int) or v: # \"if v\" would suppress emitting int(0) new_dict[k] = v return new_dict", "def check_empty_dict(GET_dict): \"\"\" Returns True if the GET querstring contains on values, but it can contain empty keys. This is better than doing not bool(request.GET) as an empty key will return True \"\"\" empty = True for k, v in GET_dict.items(): # Don't disable on p(age) or 'all' GET param if v and k != 'p' and k != 'all': empty = False return empty", "def nonull_dict(self): \"\"\"Like dict, but does not hold any null values. :return: \"\"\" return {k: v for k, v in six.iteritems(self.dict) if v and k != '_codes'}", "def _clean_dict(target_dict, whitelist=None): \"\"\" Convenience function that removes a dicts keys that have falsy values \"\"\" assert isinstance(target_dict, dict) return { ustr(k).strip(): ustr(v).strip() for k, v in target_dict.items() if v not in (None, Ellipsis, [], (), \"\") and (not whitelist or k in whitelist) }", "def get_single_item(d): \"\"\"Get an item from a dict which contains just one item.\"\"\" assert len(d) == 1, 'Single-item dict must have just one item, not %d.' % len(d) return next(six.iteritems(d))"], "rank": 4}
{"nl": "python logging rotating file handler by date", "code": ["def timed_rotating_file_handler(name, logname, filename, when='h', interval=1, backupCount=0, encoding=None, delay=False, utc=False): \"\"\" A Bark logging handler logging output to a named file. At intervals specified by the 'when', the file will be rotated, under control of 'backupCount'. Similar to logging.handlers.TimedRotatingFileHandler. \"\"\" return wrap_log_handler(logging.handlers.TimedRotatingFileHandler( filename, when=when, interval=interval, backupCount=backupCount, encoding=encoding, delay=delay, utc=utc))", "def init_rotating_logger(level, logfile, max_files, max_bytes): \"\"\"Initializes a rotating logger It also makes sure that any StreamHandler is removed, so as to avoid stdout/stderr constipation issues \"\"\" logging.basicConfig() root_logger = logging.getLogger() log_format = \"[%(asctime)s] [%(levelname)s] %(filename)s: %(message)s\" root_logger.setLevel(level) handler = RotatingFileHandler(logfile, maxBytes=max_bytes, backupCount=max_files) handler.setFormatter(logging.Formatter(fmt=log_format, datefmt=date_format)) root_logger.addHandler(handler) for handler in root_logger.handlers: root_logger.debug(\"Associated handlers - \" + str(handler)) if isinstance(handler, logging.StreamHandler): root_logger.debug(\"Removing StreamHandler: \" + str(handler)) root_logger.handlers.remove(handler)", "def __init__(self, filename, mode, encoding=None): \"\"\"Use the specified filename for streamed logging.\"\"\" FileHandler.__init__(self, filename, mode, encoding) self.mode = mode self.encoding = encoding", "def should_rollover(self, record: LogRecord) -> bool: \"\"\" Determine if rollover should occur. record is not used, as we are just comparing times, but it is needed so the method signatures are the same \"\"\" t = int(time.time()) if t >= self.rollover_at: return True return False", "def setupLogFile(self): \"\"\"Set up the logging file for a new session- include date and some whitespace\"\"\" self.logWrite(\"\\n###############################################\") self.logWrite(\"calcpkg.py log from \" + str(datetime.datetime.now())) self.changeLogging(True)"], "rank": 1}
{"nl": "python get type of values in a columns", "code": ["def _get_column_types(self, data): \"\"\"Get a list of the data types for each column in *data*.\"\"\" columns = list(zip_longest(*data)) return [self._get_column_type(column) for column in columns]", "def dtypes(self): \"\"\"Returns all column names and their data types as a list. >>> df.dtypes [('age', 'int'), ('name', 'string')] \"\"\" return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]", "def datatype(dbtype, description, cursor): \"\"\"Google AppEngine Helper to convert a data type into a string.\"\"\" dt = cursor.db.introspection.get_field_type(dbtype, description) if type(dt) is tuple: return dt[0] else: return dt", "def _getTypename(self, defn): \"\"\" Returns the SQL typename required to store the given FieldDefinition \"\"\" return 'REAL' if defn.type.float or 'TIME' in defn.type.name or defn.dntoeu else 'INTEGER'", "def type(self): \"\"\"Returns type of the data for the given FeatureType.\"\"\" if self is FeatureType.TIMESTAMP: return list if self is FeatureType.BBOX: return BBox return dict"], "rank": 1}
{"nl": "remove from index object in python", "code": ["def _remove_from_index(index, obj): \"\"\"Removes object ``obj`` from the ``index``.\"\"\" try: index.value_map[indexed_value(index, obj)].remove(obj.id) except KeyError: pass", "def detach_index(self, name): \"\"\" :param name: :return: \"\"\" assert type(name) == str if name in self._indexes: del self._indexes[name]", "def remove_index(self): \"\"\"Remove Elasticsearch index associated to the campaign\"\"\" self.index_client.close(self.index_name) self.index_client.delete(self.index_name)", "def delete_index(index): \"\"\"Delete index entirely (removes all documents and mapping).\"\"\" logger.info(\"Deleting search index: '%s'\", index) client = get_client() return client.indices.delete(index=index)", "def remove_item(self, item): \"\"\" Remove (and un-index) an object :param item: object to remove :type item: alignak.objects.item.Item :return: None \"\"\" self.unindex_item(item) self.items.pop(item.uuid, None)"], "rank": 6}
{"nl": "plot linear regression python on existing plot", "code": ["def _linear_seaborn_(self, label=None, style=None, opts=None): \"\"\" Returns a Seaborn linear regression plot \"\"\" xticks, yticks = self._get_ticks(opts) try: fig = sns.lmplot(self.x, self.y, data=self.df) fig = self._set_with_height(fig, opts) return fig except Exception as e: self.err(e, self.linear_, \"Can not draw linear regression chart\")", "def linregress(x, y, return_stats=False): \"\"\"linear regression calculation Parameters ---- x : independent variable (series) y : dependent variable (series) return_stats : returns statistical values as well if required (bool) Returns ---- list of parameters (and statistics) \"\"\" a1, a0, r_value, p_value, stderr = scipy.stats.linregress(x, y) retval = a1, a0 if return_stats: retval += r_value, p_value, stderr return retval", "def linearRegressionAnalysis(series): \"\"\" Returns factor and offset of linear regression function by least squares method. \"\"\" n = safeLen(series) sumI = sum([i for i, v in enumerate(series) if v is not None]) sumV = sum([v for i, v in enumerate(series) if v is not None]) sumII = sum([i * i for i, v in enumerate(series) if v is not None]) sumIV = sum([i * v for i, v in enumerate(series) if v is not None]) denominator = float(n * sumII - sumI * sumI) if denominator == 0: return None else: factor = (n * sumIV - sumI * sumV) / denominator / series.step offset = sumII * sumV - sumIV * sumI offset = offset / denominator - factor * series.start return factor, offset", "def fit_linear(X, y): \"\"\" Uses OLS to fit the regression. \"\"\" model = linear_model.LinearRegression() model.fit(X, y) return model", "def plot(self): \"\"\"Plot the empirical histogram versus best-fit distribution's PDF.\"\"\" plt.plot(self.bin_edges, self.hist, self.bin_edges, self.best_pdf)"], "rank": 1}
{"nl": "python gzip decompress stream", "code": ["def load_streams(chunks): \"\"\" Given a gzipped stream of data, yield streams of decompressed data. \"\"\" chunks = peekable(chunks) while chunks: if six.PY3: dc = zlib.decompressobj(wbits=zlib.MAX_WBITS | 16) else: dc = zlib.decompressobj(zlib.MAX_WBITS | 16) yield load_stream(dc, chunks) if dc.unused_data: chunks = peekable(itertools.chain((dc.unused_data,), chunks))", "def ungzip_data(input_data): \"\"\"Return a string of data after gzip decoding :param the input gziped data :return the gzip decoded data\"\"\" buf = StringIO(input_data) f = gzip.GzipFile(fileobj=buf) return f", "def __init__(self, stream_start): \"\"\"Initializes a gzip member decompressor wrapper. Args: stream_start (int): offset to the compressed stream within the containing file object. \"\"\" self._decompressor = zlib_decompressor.DeflateDecompressor() self.last_read = stream_start self.uncompressed_offset = 0 self._compressed_data = b''", "def open(name=None, fileobj=None, closefd=True): \"\"\" Use all decompressor possible to make the stream \"\"\" return Guesser().open(name=name, fileobj=fileobj, closefd=closefd)", "def get_gzipped_contents(input_file): \"\"\" Returns a gzipped version of a previously opened file's buffer. \"\"\" zbuf = StringIO() zfile = GzipFile(mode=\"wb\", compresslevel=6, fileobj=zbuf) zfile.write(input_file.read()) zfile.close() return ContentFile(zbuf.getvalue())"], "rank": 1}
{"nl": "python cut string by length", "code": ["def split_len(s, length): \"\"\"split string *s* into list of strings no longer than *length*\"\"\" return [s[i:i+length] for i in range(0, len(s), length)]", "def truncate_string(value, max_width=None): \"\"\"Truncate string values.\"\"\" if isinstance(value, text_type) and max_width is not None and len(value) > max_width: return value[:max_width] return value", "def trunc(obj, max, left=0): \"\"\" Convert `obj` to string, eliminate newlines and truncate the string to `max` characters. If there are more characters in the string add ``...`` to the string. With `left=True`, the string can be truncated at the beginning. @note: Does not catch exceptions when converting `obj` to string with `str`. >>> trunc('This is a long text.', 8) This ... >>> trunc('This is a long text.', 8, left) ...text. \"\"\" s = str(obj) s = s.replace('\\n', '|') if len(s) > max: if left: return '...'+s[len(s)-max+3:] else: return s[:(max-3)]+'...' else: return s", "def fsliceafter(astr, sub): \"\"\"Return the slice after at sub in string astr\"\"\" findex = astr.find(sub) return astr[findex + len(sub):]", "def _split_str(s, n): \"\"\" split string into list of strings by specified number. \"\"\" length = len(s) return [s[i:i + n] for i in range(0, length, n)]"], "rank": 1}
{"nl": "converter string to json python", "code": ["def serialize_json_string(self, value): \"\"\" Tries to load an encoded json string back into an object :param json_string: :return: \"\"\" # Check if the value might be a json string if not isinstance(value, six.string_types): return value # Make sure it starts with a brace if not value.startswith('{') or value.startswith('['): return value # Try to load the string try: return json.loads(value) except: return value", "def string(value) -> str: \"\"\" string dict/object/value to JSON \"\"\" return system_json.dumps(Json(value).safe_object(), ensure_ascii=False)", "def from_json(cls, s): \"\"\" Restores the object from the given JSON. :param s: the JSON string to parse :type s: str :return: the \"\"\" d = json.loads(s) return get_dict_handler(d[\"type\"])(d)", "def json(body, charset='utf-8', **kwargs): \"\"\"Takes JSON formatted data, converting it into native Python objects\"\"\" return json_converter.loads(text(body, charset=charset))", "def json_decode(data): \"\"\" Decodes the given JSON as primitives \"\"\" if isinstance(data, six.binary_type): data = data.decode('utf-8') return json.loads(data)"], "rank": 4}
{"nl": "python image margin padding", "code": ["def margin(text): r\"\"\"Add a margin to both ends of each line in the string. Example: >>> margin('line1\\nline2') ' line1 \\n line2 ' \"\"\" lines = str(text).split('\\n') return '\\n'.join(' {} '.format(l) for l in lines)", "def __call__(self, img): \"\"\" Args: img (PIL Image): Image to be padded. Returns: PIL Image: Padded image. \"\"\" return F.pad(img, self.padding, self.fill, self.padding_mode)", "def get_margin(length): \"\"\"Add enough tabs to align in two columns\"\"\" if length > 23: margin_left = \"\\t\" chars = 1 elif length > 15: margin_left = \"\\t\\t\" chars = 2 elif length > 7: margin_left = \"\\t\\t\\t\" chars = 3 else: margin_left = \"\\t\\t\\t\\t\" chars = 4 return margin_left", "def calculate_top_margin(self): \"\"\" Calculate the margin in pixels above the plot area, setting border_top. \"\"\" self.border_top = 5 if self.show_graph_title: self.border_top += self.title_font_size self.border_top += 5 if self.show_graph_subtitle: self.border_top += self.subtitle_font_size", "def add_bg(img, padding, color=COL_WHITE): \"\"\" Adds a padding to the given image as background of specified color :param img: Input image. :param padding: constant padding around the image. :param color: background color that needs to filled for the newly padded region. :return: New image with background. \"\"\" img = gray3(img) h, w, d = img.shape new_img = np.ones((h + 2*padding, w + 2*padding, d)) * color[:d] new_img = new_img.astype(np.uint8) set_img_box(new_img, (padding, padding, w, h), img) return new_img"], "rank": 2}
{"nl": "remove item from series python", "code": ["def remove_series(self, series): \"\"\"Removes a :py:class:`.Series` from the chart. :param Series series: The :py:class:`.Series` to remove. :raises ValueError: if you try to remove the last\\ :py:class:`.Series`.\"\"\" if len(self.all_series()) == 1: raise ValueError(\"Cannot remove last series from %s\" % str(self)) self._all_series.remove(series) series._chart = None", "def remove_item(self, item): \"\"\" Remove (and un-index) an object :param item: object to remove :type item: alignak.objects.item.Item :return: None \"\"\" self.unindex_item(item) self.items.pop(item.uuid, None)", "def remove_element(self, e): \"\"\"Remove element `e` from model \"\"\" if e.label is not None: self.elementdict.pop(e.label) self.elementlist.remove(e)", "def series_index(self, series): \"\"\" Return the integer index of *series* in this sequence. \"\"\" for idx, s in enumerate(self): if series is s: return idx raise ValueError('series not in chart data object')", "def DeleteIndex(self, index): \"\"\" Remove a spent coin based on its index. Args: index (int): \"\"\" to_remove = None for i in self.Items: if i.index == index: to_remove = i if to_remove: self.Items.remove(to_remove)"], "rank": 1}
{"nl": "check if address is valid python", "code": ["def is_valid_ip(ip_address): \"\"\" Check Validity of an IP address \"\"\" valid = True try: socket.inet_aton(ip_address.strip()) except: valid = False return valid", "def is_valid_ipv6(ip_str): \"\"\" Check the validity of an IPv6 address \"\"\" try: socket.inet_pton(socket.AF_INET6, ip_str) except socket.error: return False return True", "def __validate_email(self, email): \"\"\"Checks if a string looks like an email address\"\"\" e = re.match(self.EMAIL_ADDRESS_REGEX, email, re.UNICODE) if e: return email else: error = \"Invalid email address: \" + str(email) msg = self.GRIMOIRELAB_INVALID_FORMAT % {'error': error} raise InvalidFormatError(cause=msg)", "def is_valid_email(email): \"\"\" Check if email is valid \"\"\" pattern = re.compile(r'[\\w\\.-]+@[\\w\\.-]+[.]\\w+') return bool(pattern.match(email))", "def is_valid(email): \"\"\"Email address validation method. :param email: Email address to be saved. :type email: basestring :returns: True if email address is correct, False otherwise. :rtype: bool \"\"\" if isinstance(email, basestring) and EMAIL_RE.match(email): return True return False"], "rank": 2}
{"nl": "replace multiple things in a string python", "code": ["def replace(s, replace): \"\"\"Replace multiple values in a string\"\"\" for r in replace: s = s.replace(*r) return s", "def replaceStrs(s, *args): r\"\"\"Replace all ``(frm, to)`` tuples in `args` in string `s`. >>> replaceStrs(\"nothing is better than warm beer\", ... ('nothing','warm beer'), ('warm beer','nothing')) 'warm beer is better than nothing' \"\"\" if args == (): return s mapping = dict((frm, to) for frm, to in args) return re.sub(\"|\".join(map(re.escape, mapping.keys())), lambda match:mapping[match.group(0)], s)", "def multiple_replace(string, replacements): # type: (str, Dict[str,str]) -> str \"\"\"Simultaneously replace multiple strigns in a string Args: string (str): Input string replacements (Dict[str,str]): Replacements dictionary Returns: str: String with replacements \"\"\" pattern = re.compile(\"|\".join([re.escape(k) for k in sorted(replacements, key=len, reverse=True)]), flags=re.DOTALL) return pattern.sub(lambda x: replacements[x.group(0)], string)", "def multi_replace(instr, search_list=[], repl_list=None): \"\"\" Does a string replace with a list of search and replacements TODO: rename \"\"\" repl_list = [''] * len(search_list) if repl_list is None else repl_list for ser, repl in zip(search_list, repl_list): instr = instr.replace(ser, repl) return instr", "def fmt_subst(regex, subst): \"\"\"Replace regex with string.\"\"\" return lambda text: re.sub(regex, subst, text) if text else text"], "rank": 4}
{"nl": "python logging format brace bracket", "code": ["def format(self, record, *args, **kwargs): \"\"\" Format a message in the log Act like the normal format, but indent anything that is a newline within the message. \"\"\" return logging.Formatter.format( self, record, *args, **kwargs).replace('\\n', '\\n' + ' ' * 8)", "def pylog(self, *args, **kwargs): \"\"\"Display all available logging information.\"\"\" printerr(self.name, args, kwargs, traceback.format_exc())", "def print_log(value_color=\"\", value_noncolor=\"\"): \"\"\"set the colors for text.\"\"\" HEADER = '\\033[92m' ENDC = '\\033[0m' print(HEADER + value_color + ENDC + str(value_noncolor))", "def debug(self, text): \"\"\" Ajout d'un message de log de type DEBUG \"\"\" self.logger.debug(\"{}{}\".format(self.message_prefix, text))", "def info(self, message, *args, **kwargs): \"\"\"More important level : default for print and save \"\"\" self._log(logging.INFO, message, *args, **kwargs)"], "rank": 1}
{"nl": "python strip new lines while reading file", "code": ["def iter_lines(file_like: Iterable[str]) -> Generator[str, None, None]: \"\"\" Helper for iterating only nonempty lines without line breaks\"\"\" for line in file_like: line = line.rstrip('\\r\\n') if line: yield line", "def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line", "def get_stripped_file_lines(filename): \"\"\" Return lines of a file with whitespace removed \"\"\" try: lines = open(filename).readlines() except FileNotFoundError: fatal(\"Could not open file: {!r}\".format(filename)) return [line.strip() for line in lines]", "def file_lines(bblfile:str) -> iter: \"\"\"Yield lines found in given file\"\"\" with open(bblfile) as fd: yield from (line.rstrip() for line in fd if line.rstrip())", "def lines(input): \"\"\"Remove comments and empty lines\"\"\" for raw_line in input: line = raw_line.strip() if line and not line.startswith('#'): yield strip_comments(line)"], "rank": 3}
{"nl": "how to determine the index of an object on a list python", "code": ["def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def find_geom(geom, geoms): \"\"\" Returns the index of a geometry in a list of geometries avoiding expensive equality checks of `in` operator. \"\"\" for i, g in enumerate(geoms): if g is geom: return i", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def get_list_index(lst, index_or_name): \"\"\" Return the index of an element in the list. Args: lst (list): The list. index_or_name (int or str): The value of the reference element, or directly its numeric index. Returns: (int) The index of the element in the list. \"\"\" if isinstance(index_or_name, six.integer_types): return index_or_name return lst.index(index_or_name)"], "rank": 3}
{"nl": "how to remove repeated numbers in a list in python", "code": ["def deduplicate(list_object): \"\"\"Rebuild `list_object` removing duplicated and keeping order\"\"\" new = [] for item in list_object: if item not in new: new.append(item) return new", "def dedupe_list(seq): \"\"\" Utility function to remove duplicates from a list :param seq: The sequence (list) to deduplicate :return: A list with original duplicates removed \"\"\" seen = set() return [x for x in seq if not (x in seen or seen.add(x))]", "def de_duplicate(items): \"\"\"Remove any duplicate item, preserving order >>> de_duplicate([1, 2, 1, 2]) [1, 2] \"\"\" result = [] for item in items: if item not in result: result.append(item) return result", "def remove_list_duplicates(lista, unique=False): \"\"\" Remove duplicated elements in a list. Args: lista: List with elements to clean duplicates. \"\"\" result = [] allready = [] for elem in lista: if elem not in result: result.append(elem) else: allready.append(elem) if unique: for elem in allready: result = list(filter((elem).__ne__, result)) return result", "def get_uniques(l): \"\"\" Returns a list with no repeated elements. \"\"\" result = [] for i in l: if i not in result: result.append(i) return result"], "rank": 2}
{"nl": "reversing a dictionary in python", "code": ["def inverse(d): \"\"\" reverse the k:v pairs \"\"\" output = {} for k, v in unwrap(d).items(): output[v] = output.get(v, []) output[v].append(k) return output", "def reverse_mapping(mapping): \"\"\" For every key, value pair, return the mapping for the equivalent value, key pair >>> reverse_mapping({'a': 'b'}) == {'b': 'a'} True \"\"\" keys, values = zip(*mapping.items()) return dict(zip(values, keys))", "def invertDictMapping(d): \"\"\" Invert mapping of dictionary (i.e. map values to list of keys) \"\"\" inv_map = {} for k, v in d.items(): inv_map[v] = inv_map.get(v, []) inv_map[v].append(k) return inv_map", "def inverted_dict(d): \"\"\"Return a dict with swapped keys and values >>> inverted_dict({0: ('a', 'b'), 1: 'cd'}) == {'cd': 1, ('a', 'b'): 0} True \"\"\" return dict((force_hashable(v), k) for (k, v) in viewitems(dict(d)))", "def invert(dict_): \"\"\"Return an inverted dictionary, where former values are keys and former keys are values. .. warning:: If more than one key maps to any given value in input dictionary, it is undefined which one will be chosen for the result. :param dict_: Dictionary to swap keys and values in :return: Inverted dictionary \"\"\" ensure_mapping(dict_) return dict_.__class__(izip(itervalues(dict_), iterkeys(dict_)))"], "rank": 4}
{"nl": "python disable output buffer", "code": ["def disable_stdout_buffering(): \"\"\"This turns off stdout buffering so that outputs are immediately materialized and log messages show up before the program exits\"\"\" stdout_orig = sys.stdout sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0) # NOTE(brandyn): This removes the original stdout return stdout_orig", "def linebuffered_stdout(): \"\"\" Always line buffer stdout so pipes and redirects are CLI friendly. \"\"\" if sys.stdout.line_buffering: return sys.stdout orig = sys.stdout new = type(orig)(orig.buffer, encoding=orig.encoding, errors=orig.errors, line_buffering=True) new.mode = orig.mode return new", "def suppress_stdout(): \"\"\" Context manager that suppresses stdout. Examples: >>> with suppress_stdout(): ... print('Test print') >>> print('test') test \"\"\" save_stdout = sys.stdout sys.stdout = DevNull() yield sys.stdout = save_stdout", "def flush(self): \"\"\" Ensure all logging output has been flushed \"\"\" if len(self._buffer) > 0: self.logger.log(self.level, self._buffer) self._buffer = str()", "def __clear_buffers(self): \"\"\"Clears the input and output buffers\"\"\" try: self._port.reset_input_buffer() self._port.reset_output_buffer() except AttributeError: #pySerial 2.7 self._port.flushInput() self._port.flushOutput()"], "rank": 1}
{"nl": "python get an unique id", "code": ["def generate_id(self): \"\"\"Generate a fresh id\"\"\" if self.use_repeatable_ids: self.repeatable_id_counter += 1 return 'autobaked-{}'.format(self.repeatable_id_counter) else: return str(uuid4())", "def _unique_id(self, prefix): \"\"\" Generate a unique (within the graph) identifer internal to graph generation. \"\"\" _id = self._id_gen self._id_gen += 1 return prefix + str(_id)", "def generate_unique_host_id(): \"\"\"Generate a unique ID, that is somewhat guaranteed to be unique among all instances running at the same time.\"\"\" host = \".\".join(reversed(socket.gethostname().split(\".\"))) pid = os.getpid() return \"%s.%d\" % (host, pid)", "def generate_id(): \"\"\"Generate new UUID\"\"\" # TODO: Use six.string_type to Py3 compat try: return unicode(uuid1()).replace(u\"-\", u\"\") except NameError: return str(uuid1()).replace(u\"-\", u\"\")", "def _get_random_id(): \"\"\" Get a random (i.e., unique) string identifier\"\"\" symbols = string.ascii_uppercase + string.ascii_lowercase + string.digits return ''.join(random.choice(symbols) for _ in range(15))"], "rank": 3}
{"nl": "python logging create blank line", "code": ["def format(self, record, *args, **kwargs): \"\"\" Format a message in the log Act like the normal format, but indent anything that is a newline within the message. \"\"\" return logging.Formatter.format( self, record, *args, **kwargs).replace('\\n', '\\n' + ' ' * 8)", "def info(self, message, *args, **kwargs): \"\"\"More important level : default for print and save \"\"\" self._log(logging.INFO, message, *args, **kwargs)", "def log_no_newline(self, msg): \"\"\" print the message to the predefined log file without newline \"\"\" self.print2file(self.logfile, False, False, msg)", "def write(self, text): \"\"\"Write text. An additional attribute terminator with a value of None is added to the logging record to indicate that StreamHandler should not add a newline.\"\"\" self.logger.log(self.loglevel, text, extra={'terminator': None})", "def info(self, text): \"\"\" Ajout d'un message de log de type INFO \"\"\" self.logger.info(\"{}{}\".format(self.message_prefix, text))"], "rank": 4}
{"nl": "check specific header in python", "code": ["def required_header(header): \"\"\"Function that verify if the header parameter is a essential header :param header: A string represented a header :returns: A boolean value that represent if the header is required \"\"\" if header in IGNORE_HEADERS: return False if header.startswith('HTTP_') or header == 'CONTENT_TYPE': return True return False", "def get_header(request, header_service): \"\"\"Return request's 'X_POLYAXON_...:' header, as a bytestring. Hide some test client ickyness where the header can be unicode. \"\"\" service = request.META.get('HTTP_{}'.format(header_service), b'') if isinstance(service, str): # Work around django test client oddness service = service.encode(HTTP_HEADER_ENCODING) return service", "def header_status(header): \"\"\"Parse HTTP status line, return status (int) and reason.\"\"\" status_line = header[:header.find('\\r')] # 'HTTP/1.1 200 OK' -> (200, 'OK') fields = status_line.split(None, 2) return int(fields[1]), fields[2]", "def fetch_header(self): \"\"\"Make a header request to the endpoint.\"\"\" query = self.query().add_query_parameter(req='header') return self._parse_messages(self.get_query(query).content)[0]", "def get_from_headers(request, key): \"\"\"Try to read a value named ``key`` from the headers. \"\"\" value = request.headers.get(key) return to_native(value)"], "rank": 1}
{"nl": "get common values in a dictionary python", "code": ["def compare(dicts): \"\"\"Compare by iteration\"\"\" common_members = {} common_keys = reduce(lambda x, y: x & y, map(dict.keys, dicts)) for k in common_keys: common_members[k] = list( reduce(lambda x, y: x & y, [set(d[k]) for d in dicts])) return common_members", "def intersect(d1, d2): \"\"\"Intersect dictionaries d1 and d2 by key *and* value.\"\"\" return dict((k, d1[k]) for k in d1 if k in d2 and d1[k] == d2[k])", "def most_common(items): \"\"\" Wanted functionality from Counters (new in Python 2.7). \"\"\" counts = {} for i in items: counts.setdefault(i, 0) counts[i] += 1 return max(six.iteritems(counts), key=operator.itemgetter(1))", "def compare_dict(da, db): \"\"\" Compare differencs from two dicts \"\"\" sa = set(da.items()) sb = set(db.items()) diff = sa & sb return dict(sa - diff), dict(sb - diff)", "def is_same_dict(d1, d2): \"\"\"Test two dictionary is equal on values. (ignore order) \"\"\" for k, v in d1.items(): if isinstance(v, dict): is_same_dict(v, d2[k]) else: assert d1[k] == d2[k] for k, v in d2.items(): if isinstance(v, dict): is_same_dict(v, d1[k]) else: assert d1[k] == d2[k]"], "rank": 1}
{"nl": "python random permutation of a set", "code": ["def endless_permutations(N, random_state=None): \"\"\" Generate an endless sequence of random integers from permutations of the set [0, ..., N). If we call this N times, we will sweep through the entire set without replacement, on the (N+1)th call a new permutation will be created, etc. Parameters ---------- N: int the length of the set random_state: int or RandomState, optional random seed Yields ------ int: a random int from the set [0, ..., N) \"\"\" generator = check_random_state(random_state) while True: batch_inds = generator.permutation(N) for b in batch_inds: yield b", "def random_choice(sequence): \"\"\" Same as :meth:`random.choice`, but also supports :class:`set` type to be passed as sequence. \"\"\" return random.choice(tuple(sequence) if isinstance(sequence, set) else sequence)", "def circ_permutation(items): \"\"\"Calculate the circular permutation for a given list of items.\"\"\" permutations = [] for i in range(len(items)): permutations.append(items[i:] + items[:i]) return permutations", "def rand_elem(seq, n=None): \"\"\"returns a random element from seq n times. If n is None, it continues indefinitly\"\"\" return map(random.choice, repeat(seq, n) if n is not None else repeat(seq))", "def make_unique_ngrams(s, n): \"\"\"Make a set of unique n-grams from a string.\"\"\" return set(s[i:i + n] for i in range(len(s) - n + 1))"], "rank": 1}
{"nl": "unhasable type list in python to replace with a hashable list", "code": ["def dedupe(items): \"\"\"Remove duplicates from a sequence (of hashable items) while maintaining order. NOTE: This only works if items in the list are hashable types. Taken from the Python Cookbook, 3rd ed. Such a great book! \"\"\" seen = set() for item in items: if item not in seen: yield item seen.add(item)", "def dedupe_list(l): \"\"\"Remove duplicates from a list preserving the order. We might be tempted to use the list(set(l)) idiom, but it doesn't preserve the order, which hinders testability and does not work for lists with unhashable elements. \"\"\" result = [] for el in l: if el not in result: result.append(el) return result", "def hash_iterable(it): \"\"\"Perform a O(1) memory hash of an iterable of arbitrary length. hash(tuple(it)) creates a temporary tuple containing all values from it which could be a problem if it is large. See discussion at: https://groups.google.com/forum/#!msg/python-ideas/XcuC01a8SYs/e-doB9TbDwAJ \"\"\" hash_value = hash(type(it)) for value in it: hash_value = hash((hash_value, value)) return hash_value", "def omnihash(obj): \"\"\" recursively hash unhashable objects \"\"\" if isinstance(obj, set): return hash(frozenset(omnihash(e) for e in obj)) elif isinstance(obj, (tuple, list)): return hash(tuple(omnihash(e) for e in obj)) elif isinstance(obj, dict): return hash(frozenset((k, omnihash(v)) for k, v in obj.items())) else: return hash(obj)", "def _my_hash(arg_list): # type: (List[Any]) -> int \"\"\"Simple helper hash function\"\"\" res = 0 for arg in arg_list: res = res * 31 + hash(arg) return res"], "rank": 2}
{"nl": "python get variable by name locals globals", "code": ["def getvariable(name): \"\"\"Get the value of a local variable somewhere in the call stack.\"\"\" import inspect fr = inspect.currentframe() try: while fr: fr = fr.f_back vars = fr.f_locals if name in vars: return vars[name] except: pass return None", "def caller_locals(): \"\"\"Get the local variables in the caller's frame.\"\"\" import inspect frame = inspect.currentframe() try: return frame.f_back.f_back.f_locals finally: del frame", "def get_frame_locals(stepback=0): \"\"\"Returns locals dictionary from a given frame. :param int stepback: :rtype: dict \"\"\" with Frame(stepback=stepback) as frame: locals_dict = frame.f_locals return locals_dict", "def get_parent_var(name, global_ok=False, default=None, skip_frames=0): \"\"\" Directly gets a variable from a parent frame-scope. Returns -------- Any The content of the variable found by the given name, or None. \"\"\" scope = get_parent_scope_from_var(name, global_ok=global_ok, skip_frames=skip_frames + 1) if not scope: return default if name in scope.locals: return scope.locals.get(name, default) return scope.globals.get(name, default)", "def get_var(name, factory=None): \"\"\"Gets a global variable given its name. If factory is not None and the variable is not set, factory is a callable that will set the variable. If not set, returns None. \"\"\" if name not in _VARS and factory is not None: _VARS[name] = factory() return _VARS.get(name)"], "rank": 1}
{"nl": "python split string ever n characters", "code": ["def _split_str(s, n): \"\"\" split string into list of strings by specified number. \"\"\" length = len(s) return [s[i:i + n] for i in range(0, length, n)]", "def split_string(text, chars_per_string): \"\"\" Splits one string into multiple strings, with a maximum amount of `chars_per_string` characters per string. This is very useful for splitting one giant message into multiples. :param text: The text to split :param chars_per_string: The number of characters per line the text is split into. :return: The splitted text as a list of strings. \"\"\" return [text[i:i + chars_per_string] for i in range(0, len(text), chars_per_string)]", "def schunk(string, size): \"\"\"Splits string into n sized chunks.\"\"\" return [string[i:i+size] for i in range(0, len(string), size)]", "def split_len(s, length): \"\"\"split string *s* into list of strings no longer than *length*\"\"\" return [s[i:i+length] for i in range(0, len(s), length)]", "def group(data, num): \"\"\" Split data into chunks of num chars each \"\"\" return [data[i:i+num] for i in range(0, len(data), num)]"], "rank": 1}
{"nl": "python logging set verbosity", "code": ["def set_verbosity(verbosity): \"\"\"Banana banana \"\"\" Logger._verbosity = min(max(0, WARNING - verbosity), 2) debug(\"Verbosity set to %d\" % (WARNING - Logger._verbosity), 'logging')", "def logger(message, level=10): \"\"\"Handle logging.\"\"\" logging.getLogger(__name__).log(level, str(message))", "def logv(msg, *args, **kwargs): \"\"\" Print out a log message, only if verbose mode. \"\"\" if settings.VERBOSE: log(msg, *args, **kwargs)", "def set_log_level(logger_name: str, log_level: str, propagate: bool = False): \"\"\"Set the log level of the specified logger.\"\"\" log = logging.getLogger(logger_name) log.propagate = propagate log.setLevel(log_level)", "def log(self, level, msg=None, *args, **kwargs): \"\"\"Writes log out at any arbitray level.\"\"\" return self._log(level, msg, args, kwargs)"], "rank": 1}
{"nl": "set a pixel python", "code": ["def setPixel(self, x, y, color): \"\"\"Set the pixel at (x,y) to the integers in sequence 'color'.\"\"\" return _fitz.Pixmap_setPixel(self, x, y, color)", "def get_pixel(framebuf, x, y): \"\"\"Get the color of a given pixel\"\"\" index = (y >> 3) * framebuf.stride + x offset = y & 0x07 return (framebuf.buf[index] >> offset) & 0x01", "def setHSV(self, pixel, hsv): \"\"\"Set single pixel to HSV tuple\"\"\" color = conversions.hsv2rgb(hsv) self._set_base(pixel, color)", "def set(self): \"\"\"Set the color as current OpenGL color \"\"\" glColor4f(self.r, self.g, self.b, self.a)", "def new(self, size, fill): \"\"\"Return a new Image instance filled with a color.\"\"\" return Image(PIL.Image.new(\"RGB\", size, fill))"], "rank": 1}
{"nl": "discover file extension python", "code": ["def infer_format(filename:str) -> str: \"\"\"Return extension identifying format of given filename\"\"\" _, ext = os.path.splitext(filename) return ext", "def get_file_extension_type(filename): \"\"\" Return the group associated to the file :param filename: :return: str \"\"\" ext = get_file_extension(filename) if ext: for name, group in EXTENSIONS.items(): if ext in group: return name return \"OTHER\"", "def get_file_extension(filename): \"\"\" Return the extension if the filename has it. None if not. :param filename: The filename. :return: Extension or None. \"\"\" filename_x = filename.split('.') if len(filename_x) > 1: if filename_x[-1].strip() is not '': return filename_x[-1] return None", "def get_abi3_suffix(): \"\"\"Return the file extension for an abi3-compliant Extension()\"\"\" for suffix, _, _ in (s for s in imp.get_suffixes() if s[2] == imp.C_EXTENSION): if '.abi3' in suffix: # Unix return suffix elif suffix == '.pyd': # Windows return suffix", "def guess_extension(amimetype, normalize=False): \"\"\" Tries to guess extension for a mimetype. @param amimetype: name of a mimetype @time amimetype: string @return: the extension @rtype: string \"\"\" ext = _mimes.guess_extension(amimetype) if ext and normalize: # Normalize some common magic mis-interpreation ext = {'.asc': '.txt', '.obj': '.bin'}.get(ext, ext) from invenio.legacy.bibdocfile.api_normalizer import normalize_format return normalize_format(ext) return ext"], "rank": 7}
{"nl": "python draw line in control", "code": ["def _add_line_segment(self, x, y): \"\"\"Add a |_LineSegment| operation to the drawing sequence.\"\"\" self._drawing_operations.append(_LineSegment.new(self, x, y))", "def vline(self, x, y, height, color): \"\"\"Draw a vertical line up to a given length.\"\"\" self.rect(x, y, 1, height, color, fill=True)", "def hline(self, x, y, width, color): \"\"\"Draw a horizontal line up to a given length.\"\"\" self.rect(x, y, width, 1, color, fill=True)", "def _draw_lines_internal(self, coords, colour, bg): \"\"\"Helper to draw lines connecting a set of nodes that are scaled for the Screen.\"\"\" for i, (x, y) in enumerate(coords): if i == 0: self._screen.move(x, y) else: self._screen.draw(x, y, colour=colour, bg=bg, thin=True)", "def polyline(self, arr): \"\"\"Draw a set of lines\"\"\" for i in range(0, len(arr) - 1): self.line(arr[i][0], arr[i][1], arr[i + 1][0], arr[i + 1][1])"], "rank": 3}
{"nl": "python check if a valid date", "code": ["def valid_date(x: str) -> bool: \"\"\" Retrun ``True`` if ``x`` is a valid YYYYMMDD date; otherwise return ``False``. \"\"\" try: if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT): raise ValueError return True except ValueError: return False", "def is_date(thing): \"\"\"Checks if the given thing represents a date :param thing: The object to check if it is a date :type thing: arbitrary object :returns: True if we have a date object :rtype: bool \"\"\" # known date types date_types = (datetime.datetime, datetime.date, DateTime) return isinstance(thing, date_types)", "def datetime_is_iso(date_str): \"\"\"Attempts to parse a date formatted in ISO 8601 format\"\"\" try: if len(date_str) > 10: dt = isodate.parse_datetime(date_str) else: dt = isodate.parse_date(date_str) return True, [] except: # Any error qualifies as not ISO format return False, ['Datetime provided is not in a valid ISO 8601 format']", "def is_date_type(cls): \"\"\"Return True if the class is a date type.\"\"\" if not isinstance(cls, type): return False return issubclass(cls, date) and not issubclass(cls, datetime)", "def _validate_date_str(str_): \"\"\"Validate str as a date and return string version of date\"\"\" if not str_: return None # Convert to datetime so we can validate it's a real date that exists then # convert it back to the string. try: date = datetime.strptime(str_, DATE_FMT) except ValueError: msg = 'Invalid date format, should be YYYY-MM-DD' raise argparse.ArgumentTypeError(msg) return date.strftime(DATE_FMT)"], "rank": 1}
{"nl": "delete spaces and non number terms from a string python", "code": ["def strip_spaces(s): \"\"\" Strip excess spaces from a string \"\"\" return u\" \".join([c for c in s.split(u' ') if c])", "def remove_bad(string): \"\"\" remove problem characters from string \"\"\" remove = [':', ',', '(', ')', ' ', '|', ';', '\\''] for c in remove: string = string.replace(c, '_') return string", "def sanitize_word(s): \"\"\"Remove non-alphanumerical characters from metric word. And trim excessive underscores. \"\"\" s = re.sub('[^\\w-]+', '_', s) s = re.sub('__+', '_', s) return s.strip('_')", "def lowstrip(term): \"\"\"Convert to lowercase and strip spaces\"\"\" term = re.sub('\\s+', ' ', term) term = term.lower() return term", "def normalize_value(text): \"\"\" This removes newlines and multiple spaces from a string. \"\"\" result = text.replace('\\n', ' ') result = re.subn('[ ]{2,}', ' ', result)[0] return result"], "rank": 9}
{"nl": "python django foreach list to str with ,", "code": ["def commajoin_as_strings(iterable): \"\"\" Join the given iterable with ',' \"\"\" return _(u',').join((six.text_type(i) for i in iterable))", "def _tostr(self,obj): \"\"\" converts a object to list, if object is a list, it creates a comma seperated string. \"\"\" if not obj: return '' if isinstance(obj, list): return ', '.join(map(self._tostr, obj)) return str(obj)", "def seq_to_str(obj, sep=\",\"): \"\"\" Given a sequence convert it to a comma separated string. If, however, the argument is a single object, return its string representation. \"\"\" if isinstance(obj, string_classes): return obj elif isinstance(obj, (list, tuple)): return sep.join([str(x) for x in obj]) else: return str(obj)", "def list_to_str(list, separator=','): \"\"\" >>> list = [0, 0, 7] >>> list_to_str(list) '0,0,7' \"\"\" list = [str(x) for x in list] return separator.join(list)", "def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res"], "rank": 1}
{"nl": "python django postgres flow", "code": ["def install_postgres(user=None, dbname=None, password=None): \"\"\"Install Postgres on remote\"\"\" execute(pydiploy.django.install_postgres_server, user=user, dbname=dbname, password=password)", "def psql(sql, show=True): \"\"\" Runs SQL against the project's database. \"\"\" out = postgres('psql -c \"%s\"' % sql) if show: print_command(sql) return out", "def get_pg_connection(host, user, port, password, database, ssl={}): \"\"\" PostgreSQL connection \"\"\" return psycopg2.connect(host=host, user=user, port=port, password=password, dbname=database, sslmode=ssl.get('sslmode', None), sslcert=ssl.get('sslcert', None), sslkey=ssl.get('sslkey', None), sslrootcert=ssl.get('sslrootcert', None), )", "def execute(self, sql, params=None): \"\"\"Just a pointer to engine.execute \"\"\" # wrap in a transaction to ensure things are committed # https://github.com/smnorris/pgdata/issues/3 with self.engine.begin() as conn: result = conn.execute(sql, params) return result", "def create_db_schema(cls, cur, schema_name): \"\"\" Create Postgres schema script and execute it on cursor \"\"\" create_schema_script = \"CREATE SCHEMA {0} ;\\n\".format(schema_name) cur.execute(create_schema_script)"], "rank": 1}
{"nl": "how to filter an image using a mask in python", "code": ["def filter_greys_using_image(image, target): \"\"\"Filter out any values in target not in image :param image: image containing values to appear in filtered image :param target: the image to filter :rtype: 2d :class:`numpy.ndarray` containing only value in image and with the same dimensions as target \"\"\" maskbase = numpy.array(range(256), dtype=numpy.uint8) mask = numpy.where(numpy.in1d(maskbase, numpy.unique(image)), maskbase, 0) return mask[target]", "def filtered_image(self, im): \"\"\"Returns a filtered image after applying the Fourier-space filters\"\"\" q = np.fft.fftn(im) for k,v in self.filters: q[k] -= v return np.real(np.fft.ifftn(q))", "def inpaint(self): \"\"\" Replace masked-out elements in an array using an iterative image inpainting algorithm. \"\"\" import inpaint filled = inpaint.replace_nans(np.ma.filled(self.raster_data, np.NAN).astype(np.float32), 3, 0.01, 2) self.raster_data = np.ma.masked_invalid(filled)", "def filter_contour(imageFile, opFile): \"\"\" convert an image by applying a contour \"\"\" im = Image.open(imageFile) im1 = im.filter(ImageFilter.CONTOUR) im1.save(opFile)", "def zoomed_scaled_array_around_mask(self, mask, buffer=1): \"\"\"Extract the 2D region of an array corresponding to the rectangle encompassing all unmasked values. This is used to extract and visualize only the region of an image that is used in an analysis. Parameters ---------- mask : mask.Mask The mask around which the scaled array is extracted. buffer : int The buffer of pixels around the extraction. \"\"\" return self.new_with_array(array=array_util.extracted_array_2d_from_array_2d_and_coordinates( array_2d=self, y0=mask.zoom_region[0]-buffer, y1=mask.zoom_region[1]+buffer, x0=mask.zoom_region[2]-buffer, x1=mask.zoom_region[3]+buffer))"], "rank": 1}
{"nl": "python longest consecutive ones group", "code": ["def get_longest_orf(orfs): \"\"\"Find longest ORF from the given list of ORFs.\"\"\" sorted_orf = sorted(orfs, key=lambda x: len(x['sequence']), reverse=True)[0] return sorted_orf", "def longest_run_1d(arr): \"\"\"Return the length of the longest consecutive run of identical values. Parameters ---------- arr : bool array Input array Returns ------- int Length of longest run. \"\"\" v, rl = rle_1d(arr)[:2] return np.where(v, rl, 0).max()", "def longest_run(da, dim='time'): \"\"\"Return the length of the longest consecutive run of True values. Parameters ---------- arr : N-dimensional array (boolean) Input array dim : Xarray dimension (default = 'time') Dimension along which to calculate consecutive run Returns ------- N-dimensional array (int) Length of longest run of True values along dimension \"\"\" d = rle(da, dim=dim) rl_long = d.max(dim=dim) return rl_long", "def long_substr(data): \"\"\"Return the longest common substring in a list of strings. Credit: http://stackoverflow.com/questions/2892931/longest-common-substring-from-more-than-two-strings-python \"\"\" substr = '' if len(data) > 1 and len(data[0]) > 0: for i in range(len(data[0])): for j in range(len(data[0])-i+1): if j > len(substr) and all(data[0][i:i+j] in x for x in data): substr = data[0][i:i+j] elif len(data) == 1: substr = data[0] return substr", "def long_substring(str_a, str_b): \"\"\" Looks for a longest common string between any two given strings passed :param str_a: str :param str_b: str Big Thanks to Pulkit Kathuria(@kevincobain2000) for the function The function is derived from jProcessing toolkit suite \"\"\" data = [str_a, str_b] substr = '' if len(data) > 1 and len(data[0]) > 0: for i in range(len(data[0])): for j in range(len(data[0])-i+1): if j > len(substr) and all(data[0][i:i+j] in x for x in data): substr = data[0][i:i+j] return substr.strip()"], "rank": 2}
{"nl": "python parsing yaml file", "code": ["def _ParseYamlFromFile(filedesc): \"\"\"Parses given YAML file.\"\"\" content = filedesc.read() return yaml.Parse(content) or collections.OrderedDict()", "def load_yaml(yaml_file: str) -> Any: \"\"\" Load YAML from file. :param yaml_file: path to YAML file :return: content of the YAML as dict/list \"\"\" with open(yaml_file, 'r') as file: return ruamel.yaml.load(file, ruamel.yaml.RoundTripLoader)", "def _parse_config(config_file_path): \"\"\" Parse Config File from yaml file. \"\"\" config_file = open(config_file_path, 'r') config = yaml.load(config_file) config_file.close() return config", "def load_yaml(file): \"\"\"If pyyaml > 5.1 use full_load to avoid warning\"\"\" if hasattr(yaml, \"full_load\"): return yaml.full_load(file) else: return yaml.load(file)", "def Parse(text): \"\"\"Parses a YAML source into a Python object. Args: text: A YAML source to parse. Returns: A Python data structure corresponding to the YAML source. \"\"\" precondition.AssertType(text, Text) if compatibility.PY2: text = text.encode(\"utf-8\") return yaml.safe_load(text)"], "rank": 2}
{"nl": "python get list of methods on object", "code": ["def get_method_names(obj): \"\"\" Gets names of all methods implemented in specified object. :param obj: an object to introspect. :return: a list with method names. \"\"\" method_names = [] for method_name in dir(obj): method = getattr(obj, method_name) if MethodReflector._is_method(method, method_name): method_names.append(method_name) return method_names", "def get_methods(*objs): \"\"\" Return the names of all callable attributes of an object\"\"\" return set( attr for obj in objs for attr in dir(obj) if not attr.startswith('_') and callable(getattr(obj, attr)) )", "def get_all_attributes(klass_or_instance): \"\"\"Get all attribute members (attribute, property style method). \"\"\" pairs = list() for attr, value in inspect.getmembers( klass_or_instance, lambda x: not inspect.isroutine(x)): if not (attr.startswith(\"__\") or attr.endswith(\"__\")): pairs.append((attr, value)) return pairs", "def __dir__(self): u\"\"\"Returns a list of children and available helper methods.\"\"\" return sorted(self.keys() | {m for m in dir(self.__class__) if m.startswith('to_')})", "def get_action_methods(self): \"\"\" return a list of methods on this class for executing actions. methods are return as a list of (name, func) tuples \"\"\" return [(name, getattr(self, name)) for name, _ in Action.get_command_types()]"], "rank": 2}
{"nl": "how to covert a string column to a float in python", "code": ["def comma_converter(float_string): \"\"\"Convert numbers to floats whether the decimal point is '.' or ','\"\"\" trans_table = maketrans(b',', b'.') return float(float_string.translate(trans_table))", "def _convert_to_float_if_possible(s): \"\"\" A small helper function to convert a string to a numeric value if appropriate :param s: the string to be converted :type s: str \"\"\" try: ret = float(s) except (ValueError, TypeError): ret = s return ret", "def covstr(s): \"\"\" convert string to int or float. \"\"\" try: ret = int(s) except ValueError: ret = float(s) return ret", "def energy_string_to_float( string ): \"\"\" Convert a string of a calculation energy, e.g. '-1.2345 eV' to a float. Args: string (str): The string to convert. Return (float) \"\"\" energy_re = re.compile( \"(-?\\d+\\.\\d+)\" ) return float( energy_re.match( string ).group(0) )", "def parse_float(float_str): \"\"\"Parse a string of the form 305.48b into a Python float. The terminal letter, if present, indicates e.g. billions.\"\"\" factor = __get_factor(float_str) if factor != 1: float_str = float_str[:-1] try: return float(float_str.replace(',', '')) * factor except ValueError: return None"], "rank": 1}
{"nl": "printing data type off all columns in data frame in python", "code": ["def dtypes(self): \"\"\"Returns all column names and their data types as a list. >>> df.dtypes [('age', 'int'), ('name', 'string')] \"\"\" return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]", "def get_obj_cols(df): \"\"\" Returns names of 'object' columns in the DataFrame. \"\"\" obj_cols = [] for idx, dt in enumerate(df.dtypes): if dt == 'object' or is_category(dt): obj_cols.append(df.columns.values[idx]) return obj_cols", "def _get_column_types(self, data): \"\"\"Get a list of the data types for each column in *data*.\"\"\" columns = list(zip_longest(*data)) return [self._get_column_type(column) for column in columns]", "def _maybe_pandas_data(data, feature_names, feature_types): \"\"\" Extract internal data from pd.DataFrame for DMatrix data \"\"\" if not isinstance(data, DataFrame): return data, feature_names, feature_types data_dtypes = data.dtypes if not all(dtype.name in PANDAS_DTYPE_MAPPER for dtype in data_dtypes): bad_fields = [data.columns[i] for i, dtype in enumerate(data_dtypes) if dtype.name not in PANDAS_DTYPE_MAPPER] msg = \"\"\"DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields \"\"\" raise ValueError(msg + ', '.join(bad_fields)) if feature_names is None: if isinstance(data.columns, MultiIndex): feature_names = [ ' '.join([str(x) for x in i]) for i in data.columns ] else: feature_names = data.columns.format() if feature_types is None: feature_types = [PANDAS_DTYPE_MAPPER[dtype.name] for dtype in data_dtypes] data = data.values.astype('float') return data, feature_names, feature_types", "def printComparison(results, class_or_prop): \"\"\" print(out the results of the comparison using a nice table) \"\"\" data = [] Row = namedtuple('Row',[class_or_prop,'VALIDATED']) for k,v in sorted(results.items(), key=lambda x: x[1]): data += [Row(k, str(v))] pprinttable(data)"], "rank": 1}
{"nl": "multiply matrix with different constant in python", "code": ["def matrixTimesVector(MM, aa): \"\"\" :param MM: A matrix of size 3x3 :param aa: A vector of size 3 :return: A vector of size 3 which is the product of the matrix by the vector \"\"\" bb = np.zeros(3, np.float) for ii in range(3): bb[ii] = np.sum(MM[ii, :] * aa) return bb", "def multiply(traj): \"\"\"Sophisticated simulation of multiplication\"\"\" z=traj.x*traj.y traj.f_add_result('z',z=z, comment='I am the product of two reals!')", "def __rmatmul__(self, other): \"\"\" Matrix multiplication using binary `@` operator in Python>=3.5. \"\"\" return self.T.dot(np.transpose(other)).T", "def normalize_matrix(matrix): \"\"\"Fold all values of the matrix into [0, 1].\"\"\" abs_matrix = np.abs(matrix.copy()) return abs_matrix / abs_matrix.max()", "def vectorsToMatrix(aa, bb): \"\"\" Performs the vector multiplication of the elements of two vectors, constructing the 3x3 matrix. :param aa: One vector of size 3 :param bb: Another vector of size 3 :return: A 3x3 matrix M composed of the products of the elements of aa and bb : M_ij = aa_i * bb_j \"\"\" MM = np.zeros([3, 3], np.float) for ii in range(3): for jj in range(3): MM[ii, jj] = aa[ii] * bb[jj] return MM"], "rank": 3}
{"nl": "permutations in python with three arguements", "code": ["def product(*args, **kwargs): \"\"\" Yields all permutations with replacement: list(product(\"cat\", repeat=2)) => [(\"c\", \"c\"), (\"c\", \"a\"), (\"c\", \"t\"), (\"a\", \"c\"), (\"a\", \"a\"), (\"a\", \"t\"), (\"t\", \"c\"), (\"t\", \"a\"), (\"t\", \"t\")] \"\"\" p = [[]] for iterable in map(tuple, args) * kwargs.get(\"repeat\", 1): p = [x + [y] for x in p for y in iterable] for p in p: yield tuple(p)", "def distinct_permutations(iterable): \"\"\"Yield successive distinct permutations of the elements in *iterable*. >>> sorted(distinct_permutations([1, 0, 1])) [(0, 1, 1), (1, 0, 1), (1, 1, 0)] Equivalent to ``set(permutations(iterable))``, except duplicates are not generated and thrown away. For larger input sequences this is much more efficient. Duplicate permutations arise when there are duplicated elements in the input iterable. The number of items returned is `n! / (x_1! * x_2! * ... * x_n!)`, where `n` is the total number of items input, and each `x_i` is the count of a distinct item in the input sequence. \"\"\" def make_new_permutations(permutations, e): \"\"\"Internal helper function. The output permutations are built up by adding element *e* to the current *permutations* at every possible position. The key idea is to keep repeated elements (reverse) ordered: if e1 == e2 and e1 is before e2 in the iterable, then all permutations with e1 before e2 are ignored. \"\"\" for permutation in permutations: for j in range(len(permutation)): yield permutation[:j] + [e] + permutation[j:] if permutation[j] == e: break else: yield permutation + [e] permutations = [[]] for e in iterable: permutations = make_new_permutations(permutations, e) return (tuple(t) for t in permutations)", "def pool_args(function, sequence, kwargs): \"\"\"Return a single iterator of n elements of lists of length 3, given a sequence of len n.\"\"\" return zip(itertools.repeat(function), sequence, itertools.repeat(kwargs))", "def circ_permutation(items): \"\"\"Calculate the circular permutation for a given list of items.\"\"\" permutations = [] for i in range(len(items)): permutations.append(items[i:] + items[:i]) return permutations", "def endless_permutations(N, random_state=None): \"\"\" Generate an endless sequence of random integers from permutations of the set [0, ..., N). If we call this N times, we will sweep through the entire set without replacement, on the (N+1)th call a new permutation will be created, etc. Parameters ---------- N: int the length of the set random_state: int or RandomState, optional random seed Yields ------ int: a random int from the set [0, ..., N) \"\"\" generator = check_random_state(random_state) while True: batch_inds = generator.permutation(N) for b in batch_inds: yield b"], "rank": 1}
{"nl": "type not equal to string python", "code": ["def is_unicode(string): \"\"\"Validates that the object itself is some kinda string\"\"\" str_type = str(type(string)) if str_type.find('str') > 0 or str_type.find('unicode') > 0: return True return False", "def validate_string(option, value): \"\"\"Validates that 'value' is an instance of `basestring` for Python 2 or `str` for Python 3. \"\"\" if isinstance(value, string_type): return value raise TypeError(\"Wrong type for %s, value must be \" \"an instance of %s\" % (option, string_type.__name__))", "def isstring(value): \"\"\"Report whether the given value is a byte or unicode string.\"\"\" classes = (str, bytes) if pyutils.PY3 else basestring # noqa: F821 return isinstance(value, classes)", "def isString(s): \"\"\"Convenience method that works with all 2.x versions of Python to determine whether or not something is stringlike.\"\"\" try: return isinstance(s, unicode) or isinstance(s, basestring) except NameError: return isinstance(s, str)", "def visit_Str(self, node): \"\"\" Set the pythonic string type. \"\"\" self.result[node] = self.builder.NamedType(pytype_to_ctype(str))"], "rank": 2}
{"nl": "how to modify print function in python", "code": ["def raw_print(*args, **kw): \"\"\"Raw print to sys.__stdout__, otherwise identical interface to print().\"\"\" print(*args, sep=kw.get('sep', ' '), end=kw.get('end', '\\n'), file=sys.__stdout__) sys.__stdout__.flush()", "def pstd(self, *args, **kwargs): \"\"\" Console to STDOUT \"\"\" kwargs['file'] = self.out self.print(*args, **kwargs) sys.stdout.flush()", "def print(cls, *args, **kwargs): \"\"\"Print synchronized.\"\"\" # pylint: disable=protected-access with _shared._PRINT_LOCK: print(*args, **kwargs) _sys.stdout.flush()", "def prin(*args, **kwargs): r\"\"\"Like ``print``, but a function. I.e. prints out all arguments as ``print`` would do. Specify output stream like this:: print('ERROR', `out=\"sys.stderr\"``). \"\"\" print >> kwargs.get('out',None), \" \".join([str(arg) for arg in args])", "def _show(self, message, indent=0, enable_verbose=True): # pragma: no cover \"\"\"Message printer. \"\"\" if enable_verbose: print(\" \" * indent + message)"], "rank": 5}
{"nl": "using dot file with python graphviz", "code": ["def plot_dot_graph(graph, filename=None): \"\"\" Plots a graph in graphviz dot notation. :param graph: the dot notation graph :type graph: str :param filename: the (optional) file to save the generated plot to. The extension determines the file format. :type filename: str \"\"\" if not plot.pygraphviz_available: logger.error(\"Pygraphviz is not installed, cannot generate graph plot!\") return if not plot.PIL_available: logger.error(\"PIL is not installed, cannot display graph plot!\") return agraph = AGraph(graph) agraph.layout(prog='dot') if filename is None: filename = tempfile.mktemp(suffix=\".png\") agraph.draw(filename) image = Image.open(filename) image.show()", "def cmd_dot(conf: Config): \"\"\"Print out a neat targets dependency tree based on requested targets. Use graphviz to render the dot file, e.g.: > ybt dot :foo :bar | dot -Tpng -o graph.png \"\"\" build_context = BuildContext(conf) populate_targets_graph(build_context, conf) if conf.output_dot_file is None: write_dot(build_context, conf, sys.stdout) else: with open(conf.output_dot_file, 'w') as out_file: write_dot(build_context, conf, out_file)", "def _text_to_graphiz(self, text): \"\"\"create a graphviz graph from text\"\"\" dot = Source(text, format='svg') return dot.pipe().decode('utf-8')", "def to_dotfile(self): \"\"\" Writes a DOT graphviz file of the domain structure, and returns the filename\"\"\" domain = self.get_domain() filename = \"%s.dot\" % (self.__class__.__name__) nx.write_dot(domain, filename) return filename", "def export_to_dot(self, filename: str = 'output') -> None: \"\"\" Export the graph to the dot file \"filename.dot\". \"\"\" with open(filename + '.dot', 'w') as output: output.write(self.as_dot())"], "rank": 2}
{"nl": "python lamba filter with or", "code": ["def BROADCAST_FILTER_NOT(func): \"\"\" Composes the passed filters into an and-joined filter. \"\"\" return lambda u, command, *args, **kwargs: not func(u, command, *args, **kwargs)", "def filter(self, f, operator=\"and\"): \"\"\" Add a filter to the query Takes a Filter object, or a filterable DSL object. \"\"\" if self._filtered: self._filter_dsl.filter(f) else: self._build_filtered_query(f, operator) return self", "def logical_or(self, other): \"\"\"logical_or(t) = self(t) or other(t).\"\"\" return self.operation(other, lambda x, y: int(x or y))", "def _or(ctx, *logical): \"\"\" Returns TRUE if any argument is TRUE \"\"\" for arg in logical: if conversions.to_boolean(arg, ctx): return True return False", "def exclude(self, *args, **kwargs) -> \"QuerySet\": \"\"\" Same as .filter(), but with appends all args with NOT \"\"\" return self._filter_or_exclude(negate=True, *args, **kwargs)"], "rank": 1}
{"nl": "check index mongod python", "code": ["def can_elasticsearch(record): \"\"\"Check if a given record is indexed. :param record: A record object. :returns: If the record is indexed returns `True`, otherwise `False`. \"\"\" search = request._methodview.search_class() search = search.get_record(str(record.id)) return search.count() == 1", "def ensure_index(self, key, unique=False): \"\"\"Wrapper for pymongo.Collection.ensure_index \"\"\" return self.collection.ensure_index(key, unique=unique)", "def validate_multiindex(self, obj): \"\"\"validate that we can store the multi-index; reset and return the new object \"\"\" levels = [l if l is not None else \"level_{0}\".format(i) for i, l in enumerate(obj.index.names)] try: return obj.reset_index(), levels except ValueError: raise ValueError(\"duplicate names/columns in the multi-index when \" \"storing as a table\")", "def __init__(self, collection, index_type_obj): \"\"\" Constructs wrapper for general index creation and deletion :param collection Collection :param index_type_obj BaseIndex Object of a index sub-class \"\"\" self.collection = collection self.index_type_obj = index_type_obj", "def find_mapping(es_url, index): \"\"\" Find the mapping given an index \"\"\" mapping = None backend = find_perceval_backend(es_url, index) if backend: mapping = backend.get_elastic_mappings() if mapping: logging.debug(\"MAPPING FOUND:\\n%s\", json.dumps(json.loads(mapping['items']), indent=True)) return mapping"], "rank": 2}
{"nl": "how to load data from url with python", "code": ["def get(url): \"\"\"Recieving the JSON file from uulm\"\"\" response = urllib.request.urlopen(url) data = response.read() data = data.decode(\"utf-8\") data = json.loads(data) return data", "def get_jsonparsed_data(url): \"\"\"Receive the content of ``url``, parse it as JSON and return the object. \"\"\" response = urlopen(url) data = response.read().decode('utf-8') return json.loads(data)", "def url_read_text(url, verbose=True): r\"\"\" Directly reads text data from url \"\"\" data = url_read(url, verbose) text = data.decode('utf8') return text", "def _get_url(url): \"\"\"Retrieve requested URL\"\"\" try: data = HTTP_SESSION.get(url, stream=True) data.raise_for_status() except requests.exceptions.RequestException as exc: raise FetcherException(exc) return data", "def wget(url): \"\"\" Download the page into a string \"\"\" import urllib.parse request = urllib.request.urlopen(url) filestring = request.read() return filestring"], "rank": 1}
{"nl": "python check if field exists in sql table", "code": ["def has_table(self, name): \"\"\"Return ``True`` if the table *name* exists in the database.\"\"\" return len(self.sql(\"SELECT name FROM sqlite_master WHERE type='table' AND name=?\", parameters=(name,), asrecarray=False, cache=False)) > 0", "def has_field(mc, field_name): \"\"\" detect if a model has a given field has :param field_name: :param mc: :return: \"\"\" try: mc._meta.get_field(field_name) except FieldDoesNotExist: return False return True", "def column_exists(cr, table, column): \"\"\" Check whether a certain column exists \"\"\" cr.execute( 'SELECT count(attname) FROM pg_attribute ' 'WHERE attrelid = ' '( SELECT oid FROM pg_class WHERE relname = %s ) ' 'AND attname = %s', (table, column)) return cr.fetchone()[0] == 1", "def _tableExists(self, tableName): cursor=_conn.execute(\"\"\" SELECT * FROM sqlite_master WHERE name ='{0}' and type='table'; \"\"\".format(tableName)) exists = cursor.fetchone() is not None cursor.close() return exists", "def table_exists(cursor, tablename, schema='public'): query = \"\"\" SELECT EXISTS ( SELECT 1 FROM information_schema.tables WHERE table_schema = %s AND table_name = %s )\"\"\" cursor.execute(query, (schema, tablename)) res = cursor.fetchone()[0] return res"], "rank": 3}
{"nl": "matplotlib python remove ticks", "code": ["def clear_matplotlib_ticks(self, axis=\"both\"): \"\"\"Clears the default matplotlib ticks.\"\"\" ax = self.get_axes() plotting.clear_matplotlib_ticks(ax=ax, axis=axis)", "def clean_axis(axis): \"\"\"Remove ticks, tick labels, and frame from axis\"\"\" axis.get_xaxis().set_ticks([]) axis.get_yaxis().set_ticks([]) for spine in list(axis.spines.values()): spine.set_visible(False)", "def axes_off(ax): \"\"\"Get rid of all axis ticks, lines, etc. \"\"\" ax.set_frame_on(False) ax.axes.get_yaxis().set_visible(False) ax.axes.get_xaxis().set_visible(False)", "def _hide_tick_lines_and_labels(axis): \"\"\" Set visible property of ticklines and ticklabels of an axis to False \"\"\" for item in axis.get_ticklines() + axis.get_ticklabels(): item.set_visible(False)", "def clear(self): \"\"\" clear plot \"\"\" self.axes.cla() self.conf.ntrace = 0 self.conf.xlabel = '' self.conf.ylabel = '' self.conf.title = ''"], "rank": 1}
{"nl": "using aparser from python shell", "code": ["def cli_run(): \"\"\"docstring for argparse\"\"\" parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow') parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+') parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda') args = parser.parse_args() main(args)", "def doc_parser(): \"\"\"Utility function to allow getting the arguments for a single command, for Sphinx documentation\"\"\" parser = argparse.ArgumentParser( prog='ambry', description='Ambry {}. Management interface for ambry, libraries ' 'and repositories. '.format(ambry._meta.__version__)) return parser", "def parsed_args(): parser = argparse.ArgumentParser(description=\"\"\"python runtime functions\"\"\", epilog=\"\") parser.add_argument('command',nargs='*', help=\"Name of the function to run with arguments\") args = parser.parse_args() return (args, parser)", "def build_parser(): \"\"\"Build the script's argument parser.\"\"\" parser = argparse.ArgumentParser(description=\"The IOTile task supervisor\") parser.add_argument('-c', '--config', help=\"config json with options\") parser.add_argument('-v', '--verbose', action=\"count\", default=0, help=\"Increase logging verbosity\") return parser", "def apply_argument_parser(argumentsParser, options=None): \"\"\" Apply the argument parser. \"\"\" if options is not None: args = argumentsParser.parse_args(options) else: args = argumentsParser.parse_args() return args"], "rank": 10}
{"nl": "python how to retrieve an anchor tag", "code": ["def get_anchor_href(markup): \"\"\" Given HTML markup, return a list of hrefs for each anchor tag. \"\"\" soup = BeautifulSoup(markup, 'lxml') return ['%s' % link.get('href') for link in soup.find_all('a')]", "def links(cls, page): \"\"\"return all links on a page, including potentially rel= links.\"\"\" for match in cls.HREF_RE.finditer(page): yield cls.href_match_to_url(match)", "def get_element_with_id(self, id): \"\"\"Return the element with the specified ID.\"\"\" # Should we maintain a hashmap of ids to make this more efficient? Probably overkill. # TODO: Elements can contain nested elements (captions, footnotes, table cells, etc.) return next((el for el in self.elements if el.id == id), None)", "def remove_this_tlink(self,tlink_id): \"\"\" Removes the tlink for the given tlink identifier @type tlink_id: string @param tlink_id: the tlink identifier to be removed \"\"\" for tlink in self.get_tlinks(): if tlink.get_id() == tlink_id: self.node.remove(tlink.get_node()) break", "def get_short_url(self): \"\"\" Returns short version of topic url (without page number) \"\"\" return reverse('post_short_url', args=(self.forum.slug, self.slug, self.id))"], "rank": 1}
{"nl": "python decode and print base64 string", "code": ["def toBase64(s): \"\"\"Represent string / bytes s as base64, omitting newlines\"\"\" if isinstance(s, str): s = s.encode(\"utf-8\") return binascii.b2a_base64(s)[:-1]", "def decode_bytes(string): \"\"\" Decodes a given base64 string into bytes. :param str string: The string to decode :return: The decoded bytes :rtype: bytes \"\"\" if is_string_type(type(string)): string = bytes(string, \"utf-8\") return base64.decodebytes(string)", "def calc_base64(s): \"\"\"Return base64 encoded binarystring.\"\"\" s = compat.to_bytes(s) s = compat.base64_encodebytes(s).strip() # return bytestring return compat.to_native(s)", "def decode_base64(data: str) -> bytes: \"\"\"Decode base64, padding being optional. :param data: Base64 data as an ASCII byte string :returns: The decoded byte string. \"\"\" missing_padding = len(data) % 4 if missing_padding != 0: data += \"=\" * (4 - missing_padding) return base64.decodebytes(data.encode(\"utf-8\"))", "def bytes_base64(x): \"\"\"Turn bytes into base64\"\"\" if six.PY2: return base64.encodestring(x).replace('\\n', '') return base64.encodebytes(bytes_encode(x)).replace(b'\\n', b'')"], "rank": 1}
{"nl": "python 2to 3 script", "code": ["def command_py2to3(args): \"\"\" Apply '2to3' tool (Python2 to Python3 conversion tool) to Python sources. \"\"\" from lib2to3.main import main sys.exit(main(\"lib2to3.fixes\", args=args.sources))", "def setup_detect_python2(): \"\"\" Call this before using the refactoring tools to create them on demand if needed. \"\"\" if None in [RTs._rt_py2_detect, RTs._rtp_py2_detect]: RTs._rt_py2_detect = RefactoringTool(py2_detect_fixers) RTs._rtp_py2_detect = RefactoringTool(py2_detect_fixers, {'print_function': True})", "def bytes_hack(buf): \"\"\" Hacky workaround for old installs of the library on systems without python-future that were keeping the 2to3 update from working after auto-update. \"\"\" ub = None if sys.version_info > (3,): ub = buf else: ub = bytes(buf) return ub", "def supports_py3(project_name): \"\"\"Check with PyPI if a project supports Python 3.\"\"\" log = logging.getLogger(\"ciu\") log.info(\"Checking {} ...\".format(project_name)) request = requests.get(\"https://pypi.org/pypi/{}/json\".format(project_name)) if request.status_code >= 400: log = logging.getLogger(\"ciu\") log.warning(\"problem fetching {}, assuming ported ({})\".format( project_name, request.status_code)) return True response = request.json() return any(c.startswith(\"Programming Language :: Python :: 3\") for c in response[\"info\"][\"classifiers\"])", "def upgrade(directory, sql, tag, x_arg, revision): \"\"\"Upgrade to a later version\"\"\" _upgrade(directory, revision, sql, tag, x_arg)"], "rank": 1}
{"nl": "adding noise to images python", "code": ["def shot_noise(x, severity=1): \"\"\"Shot noise corruption to images. Args: x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255]. severity: integer, severity of corruption. Returns: numpy array, image with uint8 pixels in [0,255]. Added shot noise. \"\"\" c = [60, 25, 12, 5, 3][severity - 1] x = np.array(x) / 255. x_clip = np.clip(np.random.poisson(x * c) / float(c), 0, 1) * 255 return around_and_astype(x_clip)", "def gaussian_noise(x, severity=1): \"\"\"Gaussian noise corruption to images. Args: x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255]. severity: integer, severity of corruption. Returns: numpy array, image with uint8 pixels in [0,255]. Added Gaussian noise. \"\"\" c = [.08, .12, 0.18, 0.26, 0.38][severity - 1] x = np.array(x) / 255. x_clip = np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255 return around_and_astype(x_clip)", "def add_noise(Y, sigma): \"\"\"Adds noise to Y\"\"\" return Y + np.random.normal(0, sigma, Y.shape)", "def uniform_noise(points): \"\"\"Init a uniform noise variable.\"\"\" return np.random.rand(1) * np.random.uniform(points, 1) \\ + random.sample([2, -2], 1)", "def normal_noise(points): \"\"\"Init a noise variable.\"\"\" return np.random.rand(1) * np.random.randn(points, 1) \\ + random.sample([2, -2], 1)"], "rank": 1}
{"nl": "sum results of a query python sqlalchemy", "code": ["def query_sum(queryset, field): \"\"\" Let the DBMS perform a sum on a queryset \"\"\" return queryset.aggregate(s=models.functions.Coalesce(models.Sum(field), 0))['s']", "def get_count(self, query): \"\"\" Returns a number of query results. This is faster than .count() on the query \"\"\" count_q = query.statement.with_only_columns( [func.count()]).order_by(None) count = query.session.execute(count_q).scalar() return count", "def count_rows(self, table, cols='*'): \"\"\"Get the number of rows in a particular table.\"\"\" query = 'SELECT COUNT({0}) FROM {1}'.format(join_cols(cols), wrap(table)) result = self.fetch(query) return result if result is not None else 0", "def count_rows(self, table_name): \"\"\"Return the number of entries in a table by counting them.\"\"\" self.table_must_exist(table_name) query = \"SELECT COUNT (*) FROM `%s`\" % table_name.lower() self.own_cursor.execute(query) return int(self.own_cursor.fetchone()[0])", "def querySQL(self, sql, args=()): \"\"\"For use with SELECT (or SELECT-like PRAGMA) statements. \"\"\" if self.debug: result = timeinto(self.queryTimes, self._queryandfetch, sql, args) else: result = self._queryandfetch(sql, args) return result"], "rank": 2}
{"nl": "python default value if null", "code": ["def _none_value(self): \"\"\"Get an appropriate \"null\" value for this field's type. This is used internally when setting the field to None. \"\"\" if self.out_type == int: return 0 elif self.out_type == float: return 0.0 elif self.out_type == bool: return False elif self.out_type == six.text_type: return u''", "def safe_int(val, default=None): \"\"\" Returns int() of val if val is not convertable to int use default instead :param val: :param default: \"\"\" try: val = int(val) except (ValueError, TypeError): val = default return val", "def min_or_none(val1, val2): \"\"\"Returns min(val1, val2) returning None only if both values are None\"\"\" return min(val1, val2, key=lambda x: sys.maxint if x is None else x)", "def set_if_empty(self, param, default): \"\"\" Set the parameter to the default if it doesn't exist \"\"\" if not self.has(param): self.set(param, default)", "def setdefault(obj, field, default): \"\"\"Set an object's field to default if it doesn't have a value\"\"\" setattr(obj, field, getattr(obj, field, default))"], "rank": 2}
{"nl": "first few lines of a file python print", "code": ["def getLinesFromLogFile(stream): \"\"\" Returns all lines written to the passed in stream \"\"\" stream.flush() stream.seek(0) lines = stream.readlines() return lines", "def head(filename, n=10): \"\"\" prints the top `n` lines of a file \"\"\" with freader(filename) as fr: for _ in range(n): print(fr.readline().strip())", "def lint_file(in_file, out_file=None): \"\"\"Helps remove extraneous whitespace from the lines of a file :param file in_file: A readable file or file-like :param file out_file: A writable file or file-like \"\"\" for line in in_file: print(line.strip(), file=out_file)", "def file_lines(bblfile:str) -> iter: \"\"\"Yield lines found in given file\"\"\" with open(bblfile) as fd: yield from (line.rstrip() for line in fd if line.rstrip())", "def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line"], "rank": 2}
{"nl": "how do i tell something to print 6 lines in the paragraph in python", "code": ["def indented_show(text, howmany=1): \"\"\"Print a formatted indented text. \"\"\" print(StrTemplate.pad_indent(text=text, howmany=howmany))", "def indent(block, spaces): \"\"\" indents paragraphs of text for rst formatting \"\"\" new_block = '' for line in block.split('\\n'): new_block += spaces + line + '\\n' return new_block", "def paragraph(separator='\\n\\n', wrap_start='', wrap_end='', html=False, sentences_quantity=3): \"\"\"Return a random paragraph.\"\"\" return paragraphs(quantity=1, separator=separator, wrap_start=wrap_start, wrap_end=wrap_end, html=html, sentences_quantity=sentences_quantity)", "def _write_separator(self): \"\"\" Inserts a horizontal (commented) line tot the generated code. \"\"\" tmp = self._page_width - ((4 * self.__indent_level) + 2) self._write_line('# ' + ('-' * tmp))", "def straight_line_show(title, length=100, linestyle=\"=\", pad=0): \"\"\"Print a formatted straight line. \"\"\" print(StrTemplate.straight_line( title=title, length=length, linestyle=linestyle, pad=pad))"], "rank": 56}
{"nl": "3 dimension convolution of cnn with python numpy", "code": ["def conv2d(x_input, w_matrix): \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\" return tf.nn.conv2d(x_input, w_matrix, strides=[1, 1, 1, 1], padding='SAME')", "def conv3x3(in_channels, out_channels, stride=1): \"\"\" 3x3 convolution with padding. Original code has had bias turned off, because Batch Norm would remove the bias either way \"\"\" return nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)", "def conv1x1(in_planes, out_planes, stride=1): \"\"\"1x1 convolution\"\"\" return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)", "def conv_block(inputs, filters, dilation_rates_and_kernel_sizes, **kwargs): \"\"\"A block of standard 2d convolutions.\"\"\" return conv_block_internal(conv, inputs, filters, dilation_rates_and_kernel_sizes, **kwargs)", "def convolve_fft(array, kernel): \"\"\" Convolve an array with a kernel using FFT. Implemntation based on the convolve_fft function from astropy. https://github.com/astropy/astropy/blob/master/astropy/convolution/convolve.py \"\"\" array = np.asarray(array, dtype=np.complex) kernel = np.asarray(kernel, dtype=np.complex) if array.ndim != kernel.ndim: raise ValueError(\"Image and kernel must have same number of \" \"dimensions\") array_shape = array.shape kernel_shape = kernel.shape new_shape = np.array(array_shape) + np.array(kernel_shape) array_slices = [] kernel_slices = [] for (new_dimsize, array_dimsize, kernel_dimsize) in zip( new_shape, array_shape, kernel_shape): center = new_dimsize - (new_dimsize + 1) // 2 array_slices += [slice(center - array_dimsize // 2, center + (array_dimsize + 1) // 2)] kernel_slices += [slice(center - kernel_dimsize // 2, center + (kernel_dimsize + 1) // 2)] array_slices = tuple(array_slices) kernel_slices = tuple(kernel_slices) if not np.all(new_shape == array_shape): big_array = np.zeros(new_shape, dtype=np.complex) big_array[array_slices] = array else: big_array = array if not np.all(new_shape == kernel_shape): big_kernel = np.zeros(new_shape, dtype=np.complex) big_kernel[kernel_slices] = kernel else: big_kernel = kernel array_fft = np.fft.fftn(big_array) kernel_fft = np.fft.fftn(np.fft.ifftshift(big_kernel)) rifft = np.fft.ifftn(array_fft * kernel_fft) return rifft[array_slices].real"], "rank": 3}
{"nl": "using mask in images python", "code": ["def _rgbtomask(self, obj): \"\"\"Convert RGB arrays from mask canvas object back to boolean mask.\"\"\" dat = obj.get_image().get_data() # RGB arrays return dat.sum(axis=2).astype(np.bool)", "def filter_greys_using_image(image, target): \"\"\"Filter out any values in target not in image :param image: image containing values to appear in filtered image :param target: the image to filter :rtype: 2d :class:`numpy.ndarray` containing only value in image and with the same dimensions as target \"\"\" maskbase = numpy.array(range(256), dtype=numpy.uint8) mask = numpy.where(numpy.in1d(maskbase, numpy.unique(image)), maskbase, 0) return mask[target]", "def inpaint(self): \"\"\" Replace masked-out elements in an array using an iterative image inpainting algorithm. \"\"\" import inpaint filled = inpaint.replace_nans(np.ma.filled(self.raster_data, np.NAN).astype(np.float32), 3, 0.01, 2) self.raster_data = np.ma.masked_invalid(filled)", "def show(data, negate=False): \"\"\"Show the stretched data. \"\"\" from PIL import Image as pil data = np.array((data - data.min()) * 255.0 / (data.max() - data.min()), np.uint8) if negate: data = 255 - data img = pil.fromarray(data) img.show()", "def zoomed_scaled_array_around_mask(self, mask, buffer=1): \"\"\"Extract the 2D region of an array corresponding to the rectangle encompassing all unmasked values. This is used to extract and visualize only the region of an image that is used in an analysis. Parameters ---------- mask : mask.Mask The mask around which the scaled array is extracted. buffer : int The buffer of pixels around the extraction. \"\"\" return self.new_with_array(array=array_util.extracted_array_2d_from_array_2d_and_coordinates( array_2d=self, y0=mask.zoom_region[0]-buffer, y1=mask.zoom_region[1]+buffer, x0=mask.zoom_region[2]-buffer, x1=mask.zoom_region[3]+buffer))"], "rank": 3}
{"nl": "python flatten dict items", "code": ["def flatten_multidict(multidict): \"\"\"Return flattened dictionary from ``MultiDict``.\"\"\" return dict([(key, value if len(value) > 1 else value[0]) for (key, value) in multidict.iterlists()])", "def flatten(nested, containers=(list, tuple)): \"\"\" Flatten a nested list by yielding its scalar items. \"\"\" for item in nested: if hasattr(item, \"next\") or isinstance(item, containers): for subitem in flatten(item): yield subitem else: yield item", "def flatten_dict_join_keys(dct, join_symbol=\" \"): \"\"\" Flatten dict with defined key join symbol. :param dct: dict to flatten :param join_symbol: default value is \" \" :return: \"\"\" return dict( flatten_dict(dct, join=lambda a,b:a+join_symbol+b) )", "def _iterate_flattened_values(value): \"\"\"Provides an iterator over all values in a nested structure.\"\"\" if isinstance(value, six.string_types): yield value return if isinstance(value, collections.Mapping): value = collections.ValuesView(value) if isinstance(value, collections.Iterable): for nested_value in value: for nested_nested_value in _iterate_flattened_values(nested_value): yield nested_nested_value yield value", "def map_keys_deep(f, dct): \"\"\" Implementation of map that recurses. This tests the same keys at every level of dict and in lists :param f: 2-ary function expecting a key and value and returns a modified key :param dct: Dict for deep processing :return: Modified dct with matching props mapped \"\"\" return _map_deep(lambda k, v: [f(k, v), v], dct)"], "rank": 1}
{"nl": "incorrect header check python", "code": ["def required_header(header): \"\"\"Function that verify if the header parameter is a essential header :param header: A string represented a header :returns: A boolean value that represent if the header is required \"\"\" if header in IGNORE_HEADERS: return False if header.startswith('HTTP_') or header == 'CONTENT_TYPE': return True return False", "def _check_fpos(self, fp_, fpos, offset, block): \"\"\"Check file position matches blocksize\"\"\" if (fp_.tell() + offset != fpos): warnings.warn(\"Actual \"+block+\" header size does not match expected\") return", "def _raise_if_wrong_file_signature(stream): \"\"\" Reads the 4 first bytes of the stream to check that is LASF\"\"\" file_sig = stream.read(len(headers.LAS_FILE_SIGNATURE)) if file_sig != headers.LAS_FILE_SIGNATURE: raise errors.PylasError( \"File Signature ({}) is not {}\".format(file_sig, headers.LAS_FILE_SIGNATURE) )", "def generate_header(headerfields, oldheader, group_by_field): \"\"\"Returns a header as a list, ready to write to TSV file\"\"\" fieldtypes = ['peptidefdr', 'peptidepep', 'nopsms', 'proteindata', 'precursorquant', 'isoquant'] return generate_general_header(headerfields, fieldtypes, peptabledata.HEADER_PEPTIDE, oldheader, group_by_field)", "def _cnx_is_empty(in_file): \"\"\"Check if cnr or cns files are empty (only have a header) \"\"\" with open(in_file) as in_handle: for i, line in enumerate(in_handle): if i > 0: return False return True"], "rank": 1}
{"nl": "how to get domain from url netloc python", "code": ["def get_domain(url): \"\"\" Get domain part of an url. For example: https://www.python.org/doc/ -> https://www.python.org \"\"\" parse_result = urlparse(url) domain = \"{schema}://{netloc}\".format( schema=parse_result.scheme, netloc=parse_result.netloc) return domain", "def parse_domain(url): \"\"\" parse the domain from the url \"\"\" domain_match = lib.DOMAIN_REGEX.match(url) if domain_match: return domain_match.group()", "def url_host(url: str) -> str: \"\"\" Parses hostname from URL. :param url: URL :return: hostname \"\"\" from urllib.parse import urlparse res = urlparse(url) return res.netloc.split(':')[0] if res.netloc else ''", "def top_level(url, fix_protocol=True): \"\"\"Extract the top level domain from an URL.\"\"\" ext = tld.get_tld(url, fix_protocol=fix_protocol) toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split( ext)[0] + ext return toplevel", "def get_site_name(request): \"\"\"Return the domain:port part of the URL without scheme. Eg: facebook.com, 127.0.0.1:8080, etc. \"\"\" urlparts = request.urlparts return ':'.join([urlparts.hostname, str(urlparts.port)])"], "rank": 3}
{"nl": "how to add noise to an image using python", "code": ["def shot_noise(x, severity=1): \"\"\"Shot noise corruption to images. Args: x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255]. severity: integer, severity of corruption. Returns: numpy array, image with uint8 pixels in [0,255]. Added shot noise. \"\"\" c = [60, 25, 12, 5, 3][severity - 1] x = np.array(x) / 255. x_clip = np.clip(np.random.poisson(x * c) / float(c), 0, 1) * 255 return around_and_astype(x_clip)", "def gaussian_noise(x, severity=1): \"\"\"Gaussian noise corruption to images. Args: x: numpy array, uncorrupted image, assumed to have uint8 pixel in [0,255]. severity: integer, severity of corruption. Returns: numpy array, image with uint8 pixels in [0,255]. Added Gaussian noise. \"\"\" c = [.08, .12, 0.18, 0.26, 0.38][severity - 1] x = np.array(x) / 255. x_clip = np.clip(x + np.random.normal(size=x.shape, scale=c), 0, 1) * 255 return around_and_astype(x_clip)", "def add_noise(Y, sigma): \"\"\"Adds noise to Y\"\"\" return Y + np.random.normal(0, sigma, Y.shape)", "def uniform_noise(points): \"\"\"Init a uniform noise variable.\"\"\" return np.random.rand(1) * np.random.uniform(points, 1) \\ + random.sample([2, -2], 1)", "def normal_noise(points): \"\"\"Init a noise variable.\"\"\" return np.random.rand(1) * np.random.randn(points, 1) \\ + random.sample([2, -2], 1)"], "rank": 1}
{"nl": "python csv read numpy array", "code": ["def csv_to_numpy(string_like, dtype=None): # type: (str) -> np.array \"\"\"Convert a CSV object to a numpy array. Args: string_like (str): CSV string. dtype (dtype, optional): Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually. This argument can only be used to 'upcast' the array. For downcasting, use the .astype(t) method. Returns: (np.array): numpy array \"\"\" stream = StringIO(string_like) return np.genfromtxt(stream, dtype=dtype, delimiter=',')", "def load_data(filename): \"\"\" :rtype : numpy matrix \"\"\" data = pandas.read_csv(filename, header=None, delimiter='\\t', skiprows=9) return data.as_matrix()", "def csv_to_dicts(file, header=None): \"\"\"Reads a csv and returns a List of Dicts with keys given by header row.\"\"\" with open(file) as csvfile: return [row for row in csv.DictReader(csvfile, fieldnames=header)]", "def _openResources(self): \"\"\" Uses numpy.load to open the underlying file \"\"\" arr = np.load(self._fileName, allow_pickle=ALLOW_PICKLE) check_is_an_array(arr) self._array = arr", "def read_numpy(fh, byteorder, dtype, count, offsetsize): \"\"\"Read tag data from file and return as numpy array.\"\"\" dtype = 'b' if dtype[-1] == 's' else byteorder+dtype[-1] return fh.read_array(dtype, count)"], "rank": 1}
{"nl": "implement a tree python", "code": ["def to_tree(self): \"\"\" returns a TreeLib tree \"\"\" tree = TreeLibTree() for node in self: tree.create_node(node, node.node_id, parent=node.parent) return tree", "def map_tree(visitor, tree): \"\"\"Apply function to nodes\"\"\" newn = [map_tree(visitor, node) for node in tree.nodes] return visitor(tree, newn)", "def walk_tree(root): \"\"\"Pre-order depth-first\"\"\" yield root for child in root.children: for el in walk_tree(child): yield el", "def debugTreePrint(node,pfx=\"->\"): \"\"\"Purely a debugging aid: Ascii-art picture of a tree descended from node\"\"\" print pfx,node.item for c in node.children: debugTreePrint(c,\" \"+pfx)", "def print_tree(self, indent=2): \"\"\" print_tree: prints out structure of tree Args: indent (int): What level of indentation at which to start printing Returns: None \"\"\" config.LOGGER.info(\"{indent}{data}\".format(indent=\" \" * indent, data=str(self))) for child in self.children: child.print_tree(indent + 1)"], "rank": 4}
{"nl": "print attributes of object in python", "code": ["def _repr(obj): \"\"\"Show the received object as precise as possible.\"\"\" vals = \", \".join(\"{}={!r}\".format( name, getattr(obj, name)) for name in obj._attribs) if vals: t = \"{}(name={}, {})\".format(obj.__class__.__name__, obj.name, vals) else: t = \"{}(name={})\".format(obj.__class__.__name__, obj.name) return t", "def prnt(self): \"\"\" Prints DB data representation of the object. \"\"\" print(\"= = = =\\n\\n%s object key: \\033[32m%s\\033[0m\" % (self.__class__.__name__, self.key)) pprnt(self._data or self.clean_value())", "def format_repr(obj, attributes) -> str: \"\"\"Format an object's repr method with specific attributes.\"\"\" attribute_repr = ', '.join(('{}={}'.format(attr, repr(getattr(obj, attr))) for attr in attributes)) return \"{0}({1})\".format(obj.__class__.__qualname__, attribute_repr)", "def __repr__(self): \"\"\"Return string representation of object.\"\"\" return str(self.__class__) + '(' + ', '.join([list.__repr__(d) for d in self.data]) + ')'", "def print_param_values(self_): \"\"\"Print the values of all this object's Parameters.\"\"\" self = self_.self for name,val in self.param.get_param_values(): print('%s.%s = %s' % (self.name,name,val))"], "rank": 1}
{"nl": "vs code python default indentation", "code": ["def dumped(text, level, indent=2): \"\"\"Put curly brackets round an indented text\"\"\" return indented(\"{\\n%s\\n}\" % indented(text, level + 1, indent) or \"None\", level, indent) + \"\\n\"", "def _pad(self): \"\"\"Pads the output with an amount of indentation appropriate for the number of open element. This method does nothing if the indent value passed to the constructor is falsy. \"\"\" if self._indent: self.whitespace(self._indent * len(self._open_elements))", "def PrintIndented(self, file, ident, code): \"\"\"Takes an array, add indentation to each entry and prints it.\"\"\" for entry in code: print >>file, '%s%s' % (ident, entry)", "def indent(self): \"\"\" Begins an indented block. Must be used in a 'with' code block. All calls to the logger inside of the block will be indented. \"\"\" blk = IndentBlock(self, self._indent) self._indent += 1 return blk", "def indent(text: str, num: int = 2) -> str: \"\"\"Indent a piece of text.\"\"\" lines = text.splitlines() return \"\\n\".join(indent_iterable(lines, num=num))"], "rank": 1}
{"nl": "python word similarity sementic", "code": ["def basic_word_sim(word1, word2): \"\"\" Simple measure of similarity: Number of letters in common / max length \"\"\" return sum([1 for c in word1 if c in word2]) / max(len(word1), len(word2))", "def similarity(word1: str, word2: str) -> float: \"\"\" Get cosine similarity between two words. If a word is not in the vocabulary, KeyError will be raised. :param string word1: first word :param string word2: second word :return: the cosine similarity between the two word vectors \"\"\" return _MODEL.similarity(word1, word2)", "def __similarity(s1, s2, ngrams_fn, n=3): \"\"\" The fraction of n-grams matching between two sequences Args: s1: a string s2: another string n: an int for the n in n-gram Returns: float: the fraction of n-grams matching \"\"\" ngrams1, ngrams2 = set(ngrams_fn(s1, n=n)), set(ngrams_fn(s2, n=n)) matches = ngrams1.intersection(ngrams2) return 2 * len(matches) / (len(ngrams1) + len(ngrams2))", "def tanimoto_set_similarity(x: Iterable[X], y: Iterable[X]) -> float: \"\"\"Calculate the tanimoto set similarity.\"\"\" a, b = set(x), set(y) union = a | b if not union: return 0.0 return len(a & b) / len(union)", "def _calculate_similarity(c): \"\"\"Get a similarity matrix of % of shared sequence :param c: cluster object :return ma: similarity matrix \"\"\" ma = {} for idc in c: set1 = _get_seqs(c[idc]) [ma.update({(idc, idc2): _common(set1, _get_seqs(c[idc2]), idc, idc2)}) for idc2 in c if idc != idc2 and (idc2, idc) not in ma] # logger.debug(\"_calculate_similarity_ %s\" % ma) return ma"], "rank": 1}
{"nl": "python read json iterate", "code": ["def read(self): \"\"\"Iterate over all JSON input (Generator)\"\"\" for line in self.io.read(): with self.parse_line(line) as j: yield j", "def json_iter (path): \"\"\" iterator for JSON-per-line in a file pattern \"\"\" with open(path, 'r') as f: for line in f.readlines(): yield json.loads(line)", "def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def _read_json_file(self, json_file): \"\"\" Helper function to read JSON file as OrderedDict \"\"\" self.log.debug(\"Reading '%s' JSON file...\" % json_file) with open(json_file, 'r') as f: return json.load(f, object_pairs_hook=OrderedDict)", "def load(cls, fp, **kwargs): \"\"\"wrapper for :py:func:`json.load`\"\"\" json_obj = json.load(fp, **kwargs) return parse(cls, json_obj)"], "rank": 2}
{"nl": "python subplot not able to set xticklabels", "code": ["def show_xticklabels(self, row, column): \"\"\"Show the x-axis tick labels for a subplot. :param row,column: specify the subplot. \"\"\" subplot = self.get_subplot_at(row, column) subplot.show_xticklabels()", "def show_yticklabels(self, row, column): \"\"\"Show the y-axis tick labels for a subplot. :param row,column: specify the subplot. \"\"\" subplot = self.get_subplot_at(row, column) subplot.show_yticklabels()", "def set_axis_options(self, row, column, text): \"\"\"Set additionnal options as plain text.\"\"\" subplot = self.get_subplot_at(row, column) subplot.set_axis_options(text)", "def add_xlabel(self, text=None): \"\"\" Add a label to the x-axis. \"\"\" x = self.fit.meta['independent'] if not text: text = '$' + x['tex_symbol'] + r'$ $(\\si{' + x['siunitx'] + r'})$' self.plt.set_xlabel(text)", "def set_xticks_for_all(self, row_column_list=None, ticks=None): \"\"\"Manually specify the x-axis tick values. :param row_column_list: a list containing (row, column) tuples to specify the subplots, or None to indicate *all* subplots. :type row_column_list: list or None :param ticks: list of tick values. \"\"\" if row_column_list is None: self.ticks['x'] = ticks else: for row, column in row_column_list: self.set_xticks(row, column, ticks)"], "rank": 1}
{"nl": "how to read an image file into python using its path", "code": ["def load_image(fname): \"\"\" read an image from file - PIL doesnt close nicely \"\"\" with open(fname, \"rb\") as f: i = Image.open(fname) #i.load() return i", "def get_image(self, source): \"\"\" Given a file-like object, loads it up into a PIL.Image object and returns it. :param file source: A file-like object to load the image from. :rtype: PIL.Image :returns: The loaded image. \"\"\" buf = StringIO(source.read()) return Image.open(buf)", "def _load_texture(file_name, resolver): \"\"\" Load a texture from a file into a PIL image. \"\"\" file_data = resolver.get(file_name) image = PIL.Image.open(util.wrap_as_stream(file_data)) return image", "def open_as_pillow(filename): \"\"\" This way can delete file immediately \"\"\" with __sys_open(filename, 'rb') as f: data = BytesIO(f.read()) return Image.open(data)", "def imdecode(image_path): \"\"\"Return BGR image read by opencv\"\"\" import os assert os.path.exists(image_path), image_path + ' not found' im = cv2.imread(image_path) return im"], "rank": 1}
{"nl": "argparse python add subparser to subparser", "code": ["def sub(name, func,**kwarg): \"\"\" Add subparser \"\"\" sp = subparsers.add_parser(name, **kwarg) sp.set_defaults(func=func) sp.arg = sp.add_argument return sp", "def set_subparsers_args(self, *args, **kwargs): \"\"\" Sets args and kwargs that are passed when creating a subparsers group in an argparse.ArgumentParser i.e. when calling argparser.ArgumentParser.add_subparsers \"\"\" self.subparsers_args = args self.subparsers_kwargs = kwargs", "def add_to_parser(self, parser): \"\"\" Adds the argument to an argparse.ArgumentParser instance @param parser An argparse.ArgumentParser instance \"\"\" kwargs = self._get_kwargs() args = self._get_args() parser.add_argument(*args, **kwargs)", "def register_action(action): \"\"\" Adds an action to the parser cli. :param action(BaseAction): a subclass of the BaseAction class \"\"\" sub = _subparsers.add_parser(action.meta('cmd'), help=action.meta('help')) sub.set_defaults(cmd=action.meta('cmd')) for (name, arg) in action.props().items(): sub.add_argument(arg.name, arg.flag, **arg.options) _actions[action.meta('cmd')] = action", "def add_option(self, *args, **kwargs): \"\"\"Add optparse or argparse option depending on CmdHelper initialization.\"\"\" if self.parseTool == 'argparse': if args and args[0] == '': # no short option args = args[1:] return self.parser.add_argument(*args, **kwargs) else: return self.parser.add_option(*args, **kwargs)"], "rank": 2}
{"nl": "remove white spaces from string in python", "code": ["def strip_spaces(s): \"\"\" Strip excess spaces from a string \"\"\" return u\" \".join([c for c in s.split(u' ') if c])", "def lowstrip(term): \"\"\"Convert to lowercase and strip spaces\"\"\" term = re.sub('\\s+', ' ', term) term = term.lower() return term", "def remove_bad(string): \"\"\" remove problem characters from string \"\"\" remove = [':', ',', '(', ')', ' ', '|', ';', '\\''] for c in remove: string = string.replace(c, '_') return string", "def unpunctuate(s, *, char_blacklist=string.punctuation): \"\"\" Remove punctuation from string s. \"\"\" # remove punctuation s = \"\".join(c for c in s if c not in char_blacklist) # remove consecutive spaces return \" \".join(filter(None, s.split(\" \")))", "def _sanitize(text): \"\"\"Return sanitized Eidos text field for human readability.\"\"\" d = {'-LRB-': '(', '-RRB-': ')'} return re.sub('|'.join(d.keys()), lambda m: d[m.group(0)], text)"], "rank": 1}
{"nl": "python utc time to local time", "code": ["def datetime_local_to_utc(local): \"\"\" Simple function to convert naive :std:`datetime.datetime` object containing local time to a naive :std:`datetime.datetime` object with UTC time. \"\"\" timestamp = time.mktime(local.timetuple()) return datetime.datetime.utcfromtimestamp(timestamp)", "def convert_2_utc(self, datetime_, timezone): \"\"\"convert to datetime to UTC offset.\"\"\" datetime_ = self.tz_mapper[timezone].localize(datetime_) return datetime_.astimezone(pytz.UTC)", "def localize(dt): \"\"\"Localize a datetime object to local time.\"\"\" if dt.tzinfo is UTC: return (dt + LOCAL_UTC_OFFSET).replace(tzinfo=None) # No TZ info so not going to assume anything, return as-is. return dt", "def timestamp_to_datetime(cls, time_stamp, localized=True): \"\"\" Converts a UTC timestamp to a datetime.datetime.\"\"\" ret = datetime.datetime.utcfromtimestamp(time_stamp) if localized: ret = localize(ret, pytz.utc) return ret", "def to_utc(self, dt): \"\"\"Convert any timestamp to UTC (with tzinfo).\"\"\" if dt.tzinfo is None: return dt.replace(tzinfo=self.utc) return dt.astimezone(self.utc)"], "rank": 1}
{"nl": "python check for files edited within time", "code": ["def has_changed (filename): \"\"\"Check if filename has changed since the last check. If this is the first check, assume the file is changed.\"\"\" key = os.path.abspath(filename) mtime = get_mtime(key) if key not in _mtime_cache: _mtime_cache[key] = mtime return True return mtime > _mtime_cache[key]", "def example_write_file_to_disk_if_changed(): \"\"\" Try to remove all comments from a file, and save it if changes were made. \"\"\" my_file = FileAsObj('/tmp/example_file.txt') my_file.rm(my_file.egrep('^#')) if my_file.changed: my_file.save()", "def need_update(a, b): \"\"\" Check if file a is newer than file b and decide whether or not to update file b. Can generalize to two lists. \"\"\" a = listify(a) b = listify(b) return any((not op.exists(x)) for x in b) or \\ all((os.stat(x).st_size == 0 for x in b)) or \\ any(is_newer_file(x, y) for x in a for y in b)", "def on_modified(self, event): \"\"\"Function called everytime a new file is modified. Args: event: Event to process. \"\"\" self._logger.debug('Detected modify event on watched path: %s', event.src_path) self._process_event(event)", "def should_rollover(self, record: LogRecord) -> bool: \"\"\" Determine if rollover should occur. record is not used, as we are just comparing times, but it is needed so the method signatures are the same \"\"\" t = int(time.time()) if t >= self.rollover_at: return True return False"], "rank": 1}
{"nl": "python pprint a long string", "code": ["def _short_repr(obj): \"\"\"Helper function returns a truncated repr() of an object.\"\"\" stringified = pprint.saferepr(obj) if len(stringified) > 200: return '%s... (%d bytes)' % (stringified[:200], len(stringified)) return stringified", "def pformat(o, indent=1, width=80, depth=None): \"\"\"Format a Python o into a pretty-printed representation.\"\"\" return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(o)", "def pprint(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Like `pretty` but print to stdout. \"\"\" printer = RepresentationPrinter(sys.stdout, verbose, max_width, newline) printer.pretty(obj) printer.flush() sys.stdout.write(newline) sys.stdout.flush()", "def pprint(self, stream=None, indent=1, width=80, depth=None): \"\"\" Pretty print the underlying literal Python object \"\"\" pp.pprint(to_literal(self), stream, indent, width, depth)", "def pretty(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Pretty print the object's representation. \"\"\" stream = StringIO() printer = RepresentationPrinter(stream, verbose, max_width, newline) printer.pretty(obj) printer.flush() return stream.getvalue()"], "rank": 1}
{"nl": "close window python gui", "code": ["def closing_plugin(self, cancelable=False): \"\"\"Perform actions before parent main window is closed\"\"\" self.dialog_manager.close_all() self.shell.exit_interpreter() return True", "def closeEvent(self, event): \"\"\" Called when closing this window. \"\"\" logger.debug(\"closeEvent\") self.argosApplication.saveSettingsIfNeeded() self.finalize() self.argosApplication.removeMainWindow(self) event.accept() logger.debug(\"closeEvent accepted\")", "def closeEvent(self, e): \"\"\"Qt slot when the window is closed.\"\"\" if self._closed: return res = self.emit('close') # Discard the close event if False is returned by one of the callback # functions. if False in res: # pragma: no cover e.ignore() return super(GUI, self).closeEvent(e) self._closed = True", "def quit(self): \"\"\" Quits the application (called when the last window is closed) \"\"\" logger.debug(\"ArgosApplication.quit called\") assert len(self.mainWindows) == 0, \\ \"Bug: still {} windows present at application quit!\".format(len(self.mainWindows)) self.qApplication.quit()", "def closeEvent(self, e): \"\"\"Qt slot when the window is closed.\"\"\" self.emit('close_widget') super(DockWidget, self).closeEvent(e)"], "rank": 1}
{"nl": "python read file json with", "code": ["def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def open_json(file_name): \"\"\" returns json contents as string \"\"\" with open(file_name, \"r\") as json_data: data = json.load(json_data) return data", "def json_get_data(filename): \"\"\"Get data from json file \"\"\" with open(filename) as fp: json_data = json.load(fp) return json_data return False", "def read_json(location): \"\"\"Open and load JSON from file. location (Path): Path to JSON file. RETURNS (dict): Loaded JSON content. \"\"\" location = ensure_path(location) with location.open('r', encoding='utf8') as f: return ujson.load(f)", "def load_from_file(cls, file_path: str): \"\"\" Read and reconstruct the data from a JSON file. \"\"\" with open(file_path, \"r\") as f: data = json.load(f) item = cls.decode(data=data) return item"], "rank": 2}
{"nl": "how to load and execute a sql file in python", "code": ["def _get_sql(filename): \"\"\"Returns the contents of the sql file from the given ``filename``.\"\"\" with open(os.path.join(SQL_DIR, filename), 'r') as f: return f.read()", "def load_files(files): \"\"\"Load and execute a python file.\"\"\" for py_file in files: LOG.debug(\"exec %s\", py_file) execfile(py_file, globals(), locals())", "def Load(file): \"\"\" Loads a model from specified file \"\"\" with open(file, 'rb') as file: model = dill.load(file) return model", "def execfile(fname, variables): \"\"\" This is builtin in python2, but we have to roll our own on py3. \"\"\" with open(fname) as f: code = compile(f.read(), fname, 'exec') exec(code, variables)", "def sql(self, sql: str, *qmark_params, **named_params): \"\"\" :deprecated: use self.statement to execute properly-formatted sql statements \"\"\" statement = SingleSqlStatement(sql) return self.statement(statement).execute(*qmark_params, **named_params)"], "rank": 1}
{"nl": "iterate over file names in directory python", "code": ["def directory_files(path): \"\"\"Yield directory file names.\"\"\" for entry in os.scandir(path): if not entry.name.startswith('.') and entry.is_file(): yield entry.name", "def get_files(dir_name): \"\"\"Simple directory walker\"\"\" return [(os.path.join('.', d), [os.path.join(d, f) for f in files]) for d, _, files in os.walk(dir_name)]", "def get_all_files(folder): \"\"\" Generator that loops through all absolute paths of the files within folder Parameters ---------- folder: str Root folder start point for recursive search. Yields ------ fpath: str Absolute path of one file in the folders \"\"\" for path, dirlist, filelist in os.walk(folder): for fn in filelist: yield op.join(path, fn)", "def recursively_get_files_from_directory(directory): \"\"\" Return all filenames under recursively found in a directory \"\"\" return [ os.path.join(root, filename) for root, directories, filenames in os.walk(directory) for filename in filenames ]", "def list_files(directory): \"\"\"Returns all files in a given directory \"\"\" return [f for f in pathlib.Path(directory).iterdir() if f.is_file() and not f.name.startswith('.')]"], "rank": 1}
{"nl": "how to sort the columns in python in data frame", "code": ["def sort_data(data, cols): \"\"\"Sort `data` rows and order columns\"\"\" return data.sort_values(cols)[cols + ['value']].reset_index(drop=True)", "def _preprocess(df): \"\"\" given a DataFrame where records are stored row-wise, rearrange it such that records are stored column-wise. \"\"\" df = df.stack() df.index.rename([\"id\", \"time\"], inplace=True) # .reset_index() df.name = \"value\" df = df.reset_index() return df", "def _rows_sort(self, rows): \"\"\" Returns a list of rows sorted by start and end date. :param list[dict[str,T]] rows: The list of rows. :rtype: list[dict[str,T]] \"\"\" return sorted(rows, key=lambda row: (row[self._key_start_date], row[self._key_end_date]))", "def sort_data(x, y): \"\"\"Sort the data.\"\"\" xy = sorted(zip(x, y)) x, y = zip(*xy) return x, y", "def sortlevel(self, level=None, ascending=True, sort_remaining=None): \"\"\" For internal compatibility with with the Index API. Sort the Index. This is for compat with MultiIndex Parameters ---------- ascending : boolean, default True False to sort in descending order level, sort_remaining are compat parameters Returns ------- Index \"\"\" return self.sort_values(return_indexer=True, ascending=ascending)"], "rank": 1}
{"nl": "opencv resize keep ratio python", "code": ["def resize(im, short, max_size): \"\"\" only resize input image to target size and return scale :param im: BGR image input by opencv :param short: one dimensional size (the short side) :param max_size: one dimensional max size (the long side) :return: resized image (NDArray) and scale (float) \"\"\" im_shape = im.shape im_size_min = np.min(im_shape[0:2]) im_size_max = np.max(im_shape[0:2]) im_scale = float(short) / float(im_size_min) # prevent bigger axis from being more than max_size: if np.round(im_scale * im_size_max) > max_size: im_scale = float(max_size) / float(im_size_max) im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR) return im, im_scale", "def scale_min(im, targ, interpolation=cv2.INTER_AREA): \"\"\" Scale the image so that the smallest axis is of size targ. Arguments: im (array): image targ (int): target size \"\"\" r,c,*_ = im.shape ratio = targ/min(r,c) sz = (scale_to(c, ratio, targ), scale_to(r, ratio, targ)) return cv2.resize(im, sz, interpolation=interpolation)", "def zoom_cv(x,z): \"\"\" Zoom the center of image x by a factor of z+1 while retaining the original image size and proportion. \"\"\" if z==0: return x r,c,*_ = x.shape M = cv2.getRotationMatrix2D((c/2,r/2),0,z+1.) return cv2.warpAffine(x,M,(c,r))", "def resize(self): \"\"\" Get target size for a cropped image and do the resizing if we got anything usable. \"\"\" resized_size = self.get_resized_size() if not resized_size: return self.image = self.image.resize(resized_size, Image.ANTIALIAS)", "def resetScale(self): \"\"\"Resets the scale on this image. Correctly aligns time scale, undoes manual scaling\"\"\" self.img.scale(1./self.imgScale[0], 1./self.imgScale[1]) self.imgScale = (1.,1.)"], "rank": 2}
{"nl": "how to concatenate output in same file python", "code": ["def build_output(self, fout): \"\"\"Squash self.out into string. Join every line in self.out with a new line and write the result to the output file. \"\"\" fout.write('\\n'.join([s for s in self.out]))", "def write_file(filename, content): \"\"\"Create the file with the given content\"\"\" print 'Generating {0}'.format(filename) with open(filename, 'wb') as out_f: out_f.write(content)", "def merge_pdfs(pdf_filepaths, out_filepath): \"\"\" Merge all the PDF files in `pdf_filepaths` in a new PDF file `out_filepath`. Parameters ---------- pdf_filepaths: list of str Paths to PDF files. out_filepath: str Path to the result PDF file. Returns ------- path: str The output file path. \"\"\" merger = PdfFileMerger() for pdf in pdf_filepaths: merger.append(PdfFileReader(open(pdf, 'rb'))) merger.write(out_filepath) return out_filepath", "def _replace_file(path, content): \"\"\"Writes a file if it doesn't already exist with the same content. This is useful because cargo uses timestamps to decide whether to compile things.\"\"\" if os.path.exists(path): with open(path, 'r') as f: if content == f.read(): print(\"Not overwriting {} because it is unchanged\".format(path), file=sys.stderr) return with open(path, 'w') as f: f.write(content)", "def output_dir(self, *args) -> str: \"\"\" Directory where to store output \"\"\" return os.path.join(self.project_dir, 'output', *args)"], "rank": 1}
{"nl": "check file size in python", "code": ["def is_readable(fp, size=1): \"\"\" Check if the file-like object is readable. :param fp: file-like object :param size: byte size :return: bool \"\"\" read_size = len(fp.read(size)) fp.seek(-read_size, 1) return read_size == size", "def _check_fpos(self, fp_, fpos, offset, block): \"\"\"Check file position matches blocksize\"\"\" if (fp_.tell() + offset != fpos): warnings.warn(\"Actual \"+block+\" header size does not match expected\") return", "def check_max_filesize(chosen_file, max_size): \"\"\" Checks file sizes for host \"\"\" if os.path.getsize(chosen_file) > max_size: return False else: return True", "def get_file_size(filename): \"\"\" Get the file size of a given file :param filename: string: pathname of a file :return: human readable filesize \"\"\" if os.path.isfile(filename): return convert_size(os.path.getsize(filename)) return None", "def get_size_in_bytes(self, handle): \"\"\"Return the size in bytes.\"\"\" fpath = self._fpath_from_handle(handle) return os.stat(fpath).st_size"], "rank": 4}
{"nl": "cast timestamp datatype python", "code": ["def execute_cast_simple_literal_to_timestamp(op, data, type, **kwargs): \"\"\"Cast integer and strings to timestamps\"\"\" return pd.Timestamp(data, tz=type.timezone)", "def dt_to_ts(value): \"\"\" If value is a datetime, convert to timestamp \"\"\" if not isinstance(value, datetime): return value return calendar.timegm(value.utctimetuple()) + value.microsecond / 1000000.0", "def convert_timestamp(timestamp): \"\"\" Converts bokehJS timestamp to datetime64. \"\"\" datetime = dt.datetime.utcfromtimestamp(timestamp/1000.) return np.datetime64(datetime.replace(tzinfo=None))", "def convert_tstamp(response): \"\"\" Convert a Stripe API timestamp response (unix epoch) to a native datetime. :rtype: datetime \"\"\" if response is None: # Allow passing None to convert_tstamp() return response # Overrides the set timezone to UTC - I think... tz = timezone.utc if settings.USE_TZ else None return datetime.datetime.fromtimestamp(response, tz)", "def timestamp_to_datetime(cls, dt, dt_format=DATETIME_FORMAT): \"\"\"Convert unix timestamp to human readable date/time string\"\"\" return cls.convert_datetime(cls.get_datetime(dt), dt_format=dt_format)"], "rank": 8}
{"nl": "python setcurrentindex qcombobox changing the index, but not the value", "code": ["def _updateItemComboBoxIndex(self, item, column, num): \"\"\"Callback for comboboxes: notifies us that a combobox for the given item and column has changed\"\"\" item._combobox_current_index[column] = num item._combobox_current_value[column] = item._combobox_option_list[column][num][0]", "def _selectItem(self, index): \"\"\"Select item in the list \"\"\" self._selectedIndex = index self.setCurrentIndex(self.model().createIndex(index, 0))", "def get_current_item(self): \"\"\"Returns (first) selected item or None\"\"\" l = self.selectedIndexes() if len(l) > 0: return self.model().get_item(l[0])", "def onchange(self, value): \"\"\"Called when a new DropDownItem gets selected. \"\"\" log.debug('combo box. selected %s' % value) self.select_by_value(value) return (value, )", "def select_default(self): \"\"\" Resets the combo box to the original \"selected\" value from the constructor (or the first value if no selected value was specified). \"\"\" if self._default is None: if not self._set_option_by_index(0): utils.error_format(self.description + \"\\n\" + \"Unable to select default option as the Combo is empty\") else: if not self._set_option(self._default): utils.error_format( self.description + \"\\n\" + \"Unable to select default option as it doesnt exist in the Combo\")"], "rank": 1}
{"nl": "python enum not json serializable", "code": ["def __init__(self, enum_obj: Any) -> None: \"\"\"Initialize attributes for informative output. :param enum_obj: Enum object. \"\"\" if enum_obj: self.name = enum_obj self.items = ', '.join([str(i) for i in enum_obj]) else: self.items = ''", "def json_serial(obj): \"\"\"JSON serializer for objects not serializable by default json code\"\"\" if isinstance(obj, LegipyModel): return obj.to_json() elif isinstance(obj, (datetime.date, datetime.datetime)): return obj.isoformat() raise TypeError(\"Type {0} not serializable\".format(repr(type(obj))))", "def json_serialize(obj): \"\"\" Simple generic JSON serializer for common objects. \"\"\" if isinstance(obj, datetime): return obj.isoformat() if hasattr(obj, 'id'): return jsonify(obj.id) if hasattr(obj, 'name'): return jsonify(obj.name) raise TypeError('{0} is not JSON serializable'.format(obj))", "def describe_enum_value(enum_value): \"\"\"Build descriptor for Enum instance. Args: enum_value: Enum value to provide descriptor for. Returns: Initialized EnumValueDescriptor instance describing the Enum instance. \"\"\" enum_value_descriptor = EnumValueDescriptor() enum_value_descriptor.name = six.text_type(enum_value.name) enum_value_descriptor.number = enum_value.number return enum_value_descriptor", "def to_json(self) -> Mapping: \"\"\"Return the properties of this :class:`Sample` as JSON serializable. \"\"\" return {str(x): str(y) for x, y in self.items()}"], "rank": 33}
{"nl": "python how to check if method is overload", "code": ["def _is_override(meta, method): \"\"\"Checks whether given class or instance method has been marked with the ``@override`` decorator. \"\"\" from taipan.objective.modifiers import _OverriddenMethod return isinstance(method, _OverriddenMethod)", "def isroutine(object): \"\"\"Return true if the object is any kind of function or method.\"\"\" return (isbuiltin(object) or isfunction(object) or ismethod(object) or ismethoddescriptor(object))", "def is_descriptor_class(desc, include_abstract=False): r\"\"\"Check calculatable descriptor class or not. Returns: bool \"\"\" return ( isinstance(desc, type) and issubclass(desc, Descriptor) and (True if include_abstract else not inspect.isabstract(desc)) )", "def method(func): \"\"\"Wrap a function as a method.\"\"\" attr = abc.abstractmethod(func) attr.__imethod__ = True return attr", "def __is_bound_method(method): \"\"\"Return ``True`` if the `method` is a bound method (attached to an class instance. Args: method: A method or function type object. \"\"\" if not(hasattr(method, \"__func__\") and hasattr(method, \"__self__\")): return False # Bound methods have a __self__ attribute pointing to the owner instance return six.get_method_self(method) is not None"], "rank": 1}
{"nl": "python iterate through queryset", "code": ["def _unordered_iterator(self): \"\"\" Return the value of each QuerySet, but also add the '#' property to each return item. \"\"\" for i, qs in zip(self._queryset_idxs, self._querysets): for item in qs: setattr(item, '#', i) yield item", "def paginate(self, request, offset=0, limit=None): \"\"\"Paginate queryset.\"\"\" return self.collection.offset(offset).limit(limit), self.collection.count()", "def items(self, limit=0): \"\"\"Return iterator for items in each page\"\"\" i = ItemIterator(self.iterator) i.limit = limit return i", "def autopage(self): \"\"\"Iterate through results from all pages. :return: all results :rtype: generator \"\"\" while self.items: yield from self.items self.items = self.fetch_next()", "def __iter__(self): \"\"\"Define a generator function and return it\"\"\" def generator(): for i, obj in enumerate(self._sequence): if i >= self._limit: break yield obj raise StopIteration return generator"], "rank": 1}
{"nl": "how to create a char array in python ctypes", "code": ["def bytes_to_c_array(data): \"\"\" Make a C array using the given string. \"\"\" chars = [ \"'{}'\".format(encode_escape(i)) for i in decode_escape(data) ] return ', '.join(chars) + ', 0'", "def c_array(ctype, values): \"\"\"Convert a python string to c array.\"\"\" if isinstance(values, np.ndarray) and values.dtype.itemsize == ctypes.sizeof(ctype): return (ctype * len(values)).from_buffer_copy(values) return (ctype * len(values))(*values)", "def getBuffer(x): \"\"\" Copy @x into a (modifiable) ctypes byte array \"\"\" b = bytes(x) return (c_ubyte * len(b)).from_buffer_copy(bytes(x))", "def c_str(string): \"\"\"\"Convert a python string to C string.\"\"\" if not isinstance(string, str): string = string.decode('ascii') return ctypes.c_char_p(string.encode('utf-8'))", "def cint8_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)): return np.fromiter(cptr, dtype=np.int8, count=length) else: raise RuntimeError('Expected int pointer')"], "rank": 6}
{"nl": "python get last occurrence in string", "code": ["def findLastCharIndexMatching(text, func): \"\"\" Return index of last character in string for which func(char) evaluates to True. \"\"\" for i in range(len(text) - 1, -1, -1): if func(text[i]): return i", "def _rindex(mylist: Sequence[T], x: T) -> int: \"\"\"Index of the last occurrence of x in the sequence.\"\"\" return len(mylist) - mylist[::-1].index(x) - 1", "def find_last_sublist(list_, sublist): \"\"\"Given a list, find the last occurance of a sublist within it. Returns: Index where the sublist starts, or None if there is no match. \"\"\" for i in reversed(range(len(list_) - len(sublist) + 1)): if list_[i] == sublist[0] and list_[i:i + len(sublist)] == sublist: return i return None", "def find_first_in_list(txt: str, str_list: [str]) -> int: # type: ignore \"\"\" Returns the index of the earliest occurence of an item from a list in a string Ex: find_first_in_list('foobar', ['bar', 'fin']) -> 3 \"\"\" start = len(txt) + 1 for item in str_list: if start > txt.find(item) > -1: start = txt.find(item) return start if len(txt) + 1 > start > -1 else -1", "def _find_first_of(line, substrings): \"\"\"Find earliest occurrence of one of substrings in line. Returns pair of index and found substring, or (-1, None) if no occurrences of any of substrings were found in line. \"\"\" starts = ((line.find(i), i) for i in substrings) found = [(i, sub) for i, sub in starts if i != -1] if found: return min(found) else: return -1, None"], "rank": 4}
{"nl": "how to round a float to an int in python", "code": ["def intround(value): \"\"\"Given a float returns a rounded int. Should give the same result on both Py2/3 \"\"\" return int(decimal.Decimal.from_float( value).to_integral_value(decimal.ROUND_HALF_EVEN))", "def round_float(f, digits, rounding=ROUND_HALF_UP): \"\"\" Accurate float rounding from http://stackoverflow.com/a/15398691. \"\"\" return Decimal(str(f)).quantize(Decimal(10) ** (-1 * digits), rounding=rounding)", "def proper_round(n): \"\"\" rounds float to closest int :rtype: int :param n: float \"\"\" return int(n) + (n / abs(n)) * int(abs(n - int(n)) >= 0.5) if n != 0 else 0", "def floor(self): \"\"\"Round `x` and `y` down to integers.\"\"\" return Point(int(math.floor(self.x)), int(math.floor(self.y)))", "def round_to_float(number, precision): \"\"\"Round a float to a precision\"\"\" rounded = Decimal(str(floor((number + precision / 2) // precision)) ) * Decimal(str(precision)) return float(rounded)"], "rank": 1}
{"nl": "get mouse coordinates python", "code": ["def mouse_get_pos(): \"\"\" :return: \"\"\" p = POINT() AUTO_IT.AU3_MouseGetPos(ctypes.byref(p)) return p.x, p.y", "def get_mouse_location(self): \"\"\" Get the current mouse location (coordinates and screen number). :return: a namedtuple with ``x``, ``y`` and ``screen_num`` fields \"\"\" x = ctypes.c_int(0) y = ctypes.c_int(0) screen_num = ctypes.c_int(0) _libxdo.xdo_get_mouse_location( self._xdo, ctypes.byref(x), ctypes.byref(y), ctypes.byref(screen_num)) return mouse_location(x.value, y.value, screen_num.value)", "def _position(): \"\"\"Returns the current xy coordinates of the mouse cursor as a two-integer tuple by calling the GetCursorPos() win32 function. Returns: (x, y) tuple of the current xy coordinates of the mouse cursor. \"\"\" cursor = POINT() ctypes.windll.user32.GetCursorPos(ctypes.byref(cursor)) return (cursor.x, cursor.y)", "def __init__(self, pos, cell, motion, cellmotion): self.pos = pos \"\"\"(x, y) position of the mouse on the screen. type: (int, int)\"\"\" self.cell = cell \"\"\"(x, y) position of the mouse snapped to a cell on the root console. type: (int, int)\"\"\" self.motion = motion \"\"\"(x, y) motion of the mouse on the screen. type: (int, int)\"\"\" self.cellmotion = cellmotion \"\"\"(x, y) mostion of the mouse moving over cells on the root console. type: (int, int)\"\"\"", "def on_mouse_motion(self, x, y, dx, dy): \"\"\" Pyglet specific mouse motion callback. Forwards and traslates the event to the example \"\"\" # Screen coordinates relative to the lower-left corner # so we have to flip the y axis to make this consistent with # other window libraries self.example.mouse_position_event(x, self.buffer_height - y)"], "rank": 1}
{"nl": "how to delete python paths", "code": ["def clean_py_files(path): \"\"\" Removes all .py files. :param path: the path :return: None \"\"\" for dirname, subdirlist, filelist in os.walk(path): for f in filelist: if f.endswith('py'): os.remove(os.path.join(dirname, f))", "def delete_all_eggs(self): \"\"\" delete all the eggs in the directory specified \"\"\" path_to_delete = os.path.join(self.egg_directory, \"lib\", \"python\") if os.path.exists(path_to_delete): shutil.rmtree(path_to_delete)", "def cli(ctx, project_dir): \"\"\"Clean the previous generated files.\"\"\" exit_code = SCons(project_dir).clean() ctx.exit(exit_code)", "def clean(): \"\"\"clean - remove build artifacts.\"\"\" run('rm -rf build/') run('rm -rf dist/') run('rm -rf puzzle.egg-info') run('find . -name __pycache__ -delete') run('find . -name *.pyc -delete') run('find . -name *.pyo -delete') run('find . -name *~ -delete') log.info('cleaned up')", "def remove_examples_all(): \"\"\"remove arduino/examples/all directory. :rtype: None \"\"\" d = examples_all_dir() if d.exists(): log.debug('remove %s', d) d.rmtree() else: log.debug('nothing to remove: %s', d)"], "rank": 2}
{"nl": "python every time take n items from list using yield", "code": ["def stretch(iterable, n=2): r\"\"\"Repeat each item in `iterable` `n` times. Example: >>> list(stretch(range(3), 2)) [0, 0, 1, 1, 2, 2] \"\"\" times = range(n) for item in iterable: for i in times: yield item", "def __init__(self, function): \"\"\"function: to be called with each stream element as its only argument \"\"\" super(takewhile, self).__init__() self.function = function", "def split_every(iterable, n): # TODO: Remove this, or make it return a generator. \"\"\" A generator of n-length chunks of an input iterable \"\"\" i = iter(iterable) piece = list(islice(i, n)) while piece: yield piece piece = list(islice(i, n))", "def split_every(n, iterable): \"\"\"Returns a generator that spits an iteratable into n-sized chunks. The last chunk may have less than n elements. See http://stackoverflow.com/a/22919323/503377.\"\"\" items = iter(iterable) return itertools.takewhile(bool, (list(itertools.islice(items, n)) for _ in itertools.count()))", "def pool_args(function, sequence, kwargs): \"\"\"Return a single iterator of n elements of lists of length 3, given a sequence of len n.\"\"\" return zip(itertools.repeat(function), sequence, itertools.repeat(kwargs))"], "rank": 4}
{"nl": "python method objects by name", "code": ["def FindMethodByName(self, name): \"\"\"Searches for the specified method, and returns its descriptor.\"\"\" for method in self.methods: if name == method.name: return method return None", "def get_action_methods(self): \"\"\" return a list of methods on this class for executing actions. methods are return as a list of (name, func) tuples \"\"\" return [(name, getattr(self, name)) for name, _ in Action.get_command_types()]", "def get_method_names(obj): \"\"\" Gets names of all methods implemented in specified object. :param obj: an object to introspect. :return: a list with method names. \"\"\" method_names = [] for method_name in dir(obj): method = getattr(obj, method_name) if MethodReflector._is_method(method, method_name): method_names.append(method_name) return method_names", "def __getattr__(self, name): \"\"\"Return wrapper to named api method.\"\"\" return functools.partial(self._obj.request, self._api_prefix + name)", "def get_http_method(self, method): \"\"\"Gets the http method that will be called from the requests library\"\"\" return self.http_methods[method](self.url, **self.http_method_args)"], "rank": 1}
{"nl": "python split strings into list of lines", "code": ["def split_multiline(value): \"\"\"Split a multiline string into a list, excluding blank lines.\"\"\" return [element for element in (line.strip() for line in value.split('\\n')) if element]", "def split_strings_in_list_retain_spaces(orig_list): \"\"\" Function to split every line in a list, and retain spaces for a rejoin :param orig_list: Original list :return: A List with split lines \"\"\" temp_list = list() for line in orig_list: line_split = __re.split(r'(\\s+)', line) temp_list.append(line_split) return temp_list", "def cleanLines(source, lineSep=os.linesep): \"\"\" :param source: some iterable source (list, file, etc) :param lineSep: string of separators (chars) that must be removed :return: list of non empty lines with removed separators \"\"\" stripped = (line.strip(lineSep) for line in source) return (line for line in stripped if len(line) != 0)", "def split_into_sentences(s): \"\"\"Split text into list of sentences.\"\"\" s = re.sub(r\"\\s+\", \" \", s) s = re.sub(r\"[\\\\.\\\\?\\\\!]\", \"\\n\", s) return s.split(\"\\n\")", "def multi_split(s, split): # type: (S, Iterable[S]) -> List[S] \"\"\"Splits on multiple given separators.\"\"\" for r in split: s = s.replace(r, \"|\") return [i for i in s.split(\"|\") if len(i) > 0]"], "rank": 1}
{"nl": "python index for first column name", "code": ["def _get_col_index(name): \"\"\"Convert column name to index.\"\"\" index = string.ascii_uppercase.index col = 0 for c in name.upper(): col = col * 26 + index(c) + 1 return col", "def index(self, value): \"\"\" Return the smallest index of the row(s) with this column equal to value. \"\"\" for i in xrange(len(self.parentNode)): if getattr(self.parentNode[i], self.Name) == value: return i raise ValueError(value)", "def ColumnToIndex (col): \"\"\"convert column to index. Eg: ConvertInIndex(\"AB\") = 28\"\"\" ndx = 0 for c in col: ndx = ndx * 26 + ord(c.upper()) - 64 return ndx", "def getRowCurrentIndex(self): \"\"\" Returns the index of column 0 of the current item in the underlying model. See also the notes at the top of this module on current item vs selected item(s). \"\"\" curIndex = self.currentIndex() col0Index = curIndex.sibling(curIndex.row(), 0) return col0Index", "def standard_db_name(file_column_name): \"\"\"return a standard name by following rules: 1. find all regular expression partners ((IDs)|(ID)|([A-Z][a-z]+)|([A-Z]{2,})) 2. lower very part and join again with _ This method is only used if values in table[model]['columns'] are str :param str file_column_name: name of column in file :return: standard name :rtype: str \"\"\" found = id_re.findall(file_column_name) if not found: return file_column_name return '_'.join(x[0].lower() for x in found)"], "rank": 1}
{"nl": "python memoryview to structure", "code": ["def read(self, start_position: int, size: int) -> memoryview: \"\"\" Return a view into the memory \"\"\" return memoryview(self._bytes)[start_position:start_position + size]", "def memory_read(self, start_position: int, size: int) -> memoryview: \"\"\" Read and return a view of ``size`` bytes from memory starting at ``start_position``. \"\"\" return self._memory.read(start_position, size)", "def _ram_buffer(self): \"\"\"Setup the RAM buffer from the C++ code.\"\"\" # get the address of the RAM address = _LIB.Memory(self._env) # create a buffer from the contents of the address location buffer_ = ctypes.cast(address, ctypes.POINTER(RAM_VECTOR)).contents # create a NumPy array from the buffer return np.frombuffer(buffer_, dtype='uint8')", "def get_memory(self, mode): \"\"\"Return a smt bit vector that represents a memory location. \"\"\" mem = { \"pre\": self._translator.get_memory_init(), \"post\": self._translator.get_memory_curr(), } return mem[mode]", "def get_ram(self, format_ = \"nl\"): \"\"\" return a string representations of the ram \"\"\" ram = [self.ram.read(i) for i in range(self.ram.size)] return self._format_mem(ram, format_)"], "rank": 2}
{"nl": "how do i clear python cache", "code": ["def Flush(self): \"\"\"Flush all items from cache.\"\"\" while self._age: node = self._age.PopLeft() self.KillObject(node.data) self._hash = dict()", "def purge_cache(self, object_type): \"\"\" Purge the named cache of all values. If no cache exists for object_type, nothing is done \"\"\" if object_type in self.mapping: cache = self.mapping[object_type] log.debug(\"Purging [{}] cache of {} values.\".format(object_type, len(cache))) cache.purge()", "def delete(self, name): \"\"\" Deletes the named entry in the cache. :param name: the name. :return: true if it is deleted. \"\"\" if name in self._cache: del self._cache[name] self.writeCache() # TODO clean files return True return False", "def flush(self): \"\"\" Flush all unwritten data to disk. \"\"\" if self._cache_modified_count > 0: self.storage.write(self.cache) self._cache_modified_count = 0", "def __delitem__(self, resource): \"\"\"Remove resource instance from internal cache\"\"\" self.__caches[type(resource)].pop(resource.get_cache_internal_key(), None)"], "rank": 2}
{"nl": "creating a leap year function in python returning true or false", "code": ["def _is_leap_year(year): \"\"\"Determine if a year is leap year. Parameters ---------- year : numeric Returns ------- isleap : array of bools \"\"\" isleap = ((np.mod(year, 4) == 0) & ((np.mod(year, 100) != 0) | (np.mod(year, 400) == 0))) return isleap", "def get_year_start(day=None): \"\"\"Returns January 1 of the given year.\"\"\" day = add_timezone(day or datetime.date.today()) return day.replace(month=1).replace(day=1)", "def today(year=None): \"\"\"this day, last year\"\"\" return datetime.date(int(year), _date.month, _date.day) if year else _date", "def is_end_of_month(self) -> bool: \"\"\" Checks if the date is at the end of the month \"\"\" end_of_month = Datum() # get_end_of_month(value) end_of_month.end_of_month() return self.value == end_of_month.value", "def monthly(date=datetime.date.today()): \"\"\" Take a date object and return the first day of the month. \"\"\" return datetime.date(date.year, date.month, 1)"], "rank": 1}
{"nl": "python automate entering of credentials", "code": ["def get_login_credentials(args): \"\"\" Gets the login credentials from the user, if not specified while invoking the script. @param args: arguments provided to the script. \"\"\" if not args.username: args.username = raw_input(\"Enter Username: \") if not args.password: args.password = getpass.getpass(\"Enter Password: \")", "def login(self, username, password=None, token=None): \"\"\"Login user for protected API calls.\"\"\" self.session.basic_auth(username, password)", "async def login( username: str, password: str, brand: str, websession: ClientSession = None) -> API: \"\"\"Log in to the API.\"\"\" api = API(brand, websession) await api.authenticate(username, password) return api", "def login(self, user: str, passwd: str) -> None: \"\"\"Log in to instagram with given username and password and internally store session object. :raises InvalidArgumentException: If the provided username does not exist. :raises BadCredentialsException: If the provided password is wrong. :raises ConnectionException: If connection to Instagram failed. :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call :meth:`Instaloader.two_factor_login`.\"\"\" self.context.login(user, passwd)", "def get_auth(): \"\"\"Get authentication.\"\"\" import getpass user = input(\"User Name: \") # noqa pswd = getpass.getpass('Password: ') return Github(user, pswd)"], "rank": 1}
{"nl": "python if list of items is in line", "code": ["def isin_alone(elems, line): \"\"\"Check if an element from a list is the only element of a string. :type elems: list :type line: str \"\"\" found = False for e in elems: if line.strip().lower() == e.lower(): found = True break return found", "def isin(elems, line): \"\"\"Check if an element from a list is in a string. :type elems: list :type line: str \"\"\" found = False for e in elems: if e in line.lower(): found = True break return found", "def has_multiline_items(maybe_list: Optional[Sequence[str]]): \"\"\"Check whether one of the items in the list has multiple lines.\"\"\" return maybe_list and any(is_multiline(item) for item in maybe_list)", "def check_if_numbers_are_consecutive(list_): \"\"\" Returns True if numbers in the list are consecutive :param list_: list of integers :return: Boolean \"\"\" return all((True if second - first == 1 else False for first, second in zip(list_[:-1], list_[1:])))", "def obj_in_list_always(target_list, obj): \"\"\" >>> l = [1,1,1] >>> obj_in_list_always(l, 1) True >>> l.append(2) >>> obj_in_list_always(l, 1) False \"\"\" for item in set(target_list): if item is not obj: return False return True"], "rank": 2}
{"nl": "python how to multiply matrix", "code": ["def matrixTimesVector(MM, aa): \"\"\" :param MM: A matrix of size 3x3 :param aa: A vector of size 3 :return: A vector of size 3 which is the product of the matrix by the vector \"\"\" bb = np.zeros(3, np.float) for ii in range(3): bb[ii] = np.sum(MM[ii, :] * aa) return bb", "def __rmatmul__(self, other): \"\"\" Matrix multiplication using binary `@` operator in Python>=3.5. \"\"\" return self.T.dot(np.transpose(other)).T", "def vectorsToMatrix(aa, bb): \"\"\" Performs the vector multiplication of the elements of two vectors, constructing the 3x3 matrix. :param aa: One vector of size 3 :param bb: Another vector of size 3 :return: A 3x3 matrix M composed of the products of the elements of aa and bb : M_ij = aa_i * bb_j \"\"\" MM = np.zeros([3, 3], np.float) for ii in range(3): for jj in range(3): MM[ii, jj] = aa[ii] * bb[jj] return MM", "def multiply(traj): \"\"\"Sophisticated simulation of multiplication\"\"\" z=traj.x*traj.y traj.f_add_result('z',z=z, comment='I am the product of two reals!')", "def multiply(self, number): \"\"\"Return a Vector as the product of the vector and a real number.\"\"\" return self.from_list([x * number for x in self.to_list()])"], "rank": 2}
{"nl": "turn string to a list in python", "code": ["def _str_to_list(s): \"\"\"Converts a comma separated string to a list\"\"\" _list = s.split(\",\") return list(map(lambda i: i.lstrip(), _list))", "def path_to_list(pathstr): \"\"\"Conver a path string to a list of path elements.\"\"\" return [elem for elem in pathstr.split(os.path.pathsep) if elem]", "def string_to_list(string, sep=\",\", filter_empty=False): \"\"\"Transforma una string con elementos separados por `sep` en una lista.\"\"\" return [value.strip() for value in string.split(sep) if (not filter_empty or value)]", "def comma_delimited_to_list(list_param): \"\"\"Convert comma-delimited list / string into a list of strings :param list_param: Comma-delimited string :type list_param: str | unicode :return: A list of strings :rtype: list \"\"\" if isinstance(list_param, list): return list_param if isinstance(list_param, str): return list_param.split(',') else: return []", "def _str_to_list(value, separator): \"\"\"Convert a string to a list with sanitization.\"\"\" value_list = [item.strip() for item in value.split(separator)] value_list_sanitized = builtins.list(filter(None, value_list)) if len(value_list_sanitized) > 0: return value_list_sanitized else: raise ValueError('Invalid list variable.')"], "rank": 5}
{"nl": "graph corresponding to the adjacency matrix python", "code": ["def adjacency(tree): \"\"\" Construct the adjacency matrix of the tree :param tree: :return: \"\"\" dd = ids(tree) N = len(dd) A = np.zeros((N, N)) def _adj(node): if np.isscalar(node): return elif isinstance(node, tuple) and len(node) == 2: A[dd[node], dd[node[0]]] = 1 A[dd[node[0]], dd[node]] = 1 _adj(node[0]) A[dd[node], dd[node[1]]] = 1 A[dd[node[1]], dd[node]] = 1 _adj(node[1]) _adj(tree) return A", "def get_adjacent_matrix(self): \"\"\"Get adjacency matrix. Returns: :param adj: adjacency matrix :type adj: np.ndarray \"\"\" edges = self.edges num_edges = len(edges) + 1 adj = np.zeros([num_edges, num_edges]) for k in range(num_edges - 1): adj[edges[k].L, edges[k].R] = 1 adj[edges[k].R, edges[k].L] = 1 return adj", "def to_bipartite_matrix(A): \"\"\"Returns the adjacency matrix of a bipartite graph whose biadjacency matrix is `A`. `A` must be a NumPy array. If `A` has **m** rows and **n** columns, then the returned matrix has **m + n** rows and columns. \"\"\" m, n = A.shape return four_blocks(zeros(m, m), A, A.T, zeros(n, n))", "def _create_complete_graph(node_ids): \"\"\"Create a complete graph from the list of node ids. Args: node_ids: a list of node ids Returns: An undirected graph (as a networkx.Graph) \"\"\" g = nx.Graph() g.add_nodes_from(node_ids) for (i, j) in combinations(node_ids, 2): g.add_edge(i, j) return g", "def get_input_nodes(G: nx.DiGraph) -> List[str]: \"\"\" Get all input nodes from a network. \"\"\" return [n for n, d in G.in_degree() if d == 0]"], "rank": 2}
{"nl": "python most frequent element multidimension", "code": ["def most_frequent(lst): \"\"\" Returns the item that appears most frequently in the given list. \"\"\" lst = lst[:] highest_freq = 0 most_freq = None for val in unique(lst): if lst.count(val) > highest_freq: most_freq = val highest_freq = lst.count(val) return most_freq", "def nlargest(self, n=None): \"\"\"List the n most common elements and their counts. List is from the most common to the least. If n is None, the list all element counts. Run time should be O(m log m) where m is len(self) Args: n (int): The number of elements to return \"\"\" if n is None: return sorted(self.counts(), key=itemgetter(1), reverse=True) else: return heapq.nlargest(n, self.counts(), key=itemgetter(1))", "def qth_pw(self, q): \"\"\" returns the qth most probable element in the dawg. \"\"\" return heapq.nlargest(q + 2, self._T.iteritems(), key=operator.itemgetter(1))[-1]", "def most_common(items): \"\"\" Wanted functionality from Counters (new in Python 2.7). \"\"\" counts = {} for i in items: counts.setdefault(i, 0) counts[i] += 1 return max(six.iteritems(counts), key=operator.itemgetter(1))", "def _rank(self, ranking, n): \"\"\" return the first n sentences with highest ranking \"\"\" return nlargest(n, ranking, key=ranking.get)"], "rank": 16}
{"nl": "how to move a row up in python", "code": ["def move_up(lines=1, file=sys.stdout): \"\"\" Move the cursor up a number of lines. Esc[ValueA: Moves the cursor up by the specified number of lines without changing columns. If the cursor is already on the top line, ANSI.SYS ignores this sequence. \"\"\" move.up(lines).write(file=file)", "def select_up(self): \"\"\"move cursor up\"\"\" r, c = self._index self._select_index(r-1, c)", "def cursor_up(self, count=1): \"\"\" (for multiline edit). Move cursor to the previous line. \"\"\" original_column = self.preferred_column or self.document.cursor_position_col self.cursor_position += self.document.get_cursor_up_position( count=count, preferred_column=original_column) # Remember the original column for the next up/down movement. self.preferred_column = original_column", "def select_down(self): \"\"\"move cursor down\"\"\" r, c = self._index self._select_index(r+1, c)", "def auto_up(self, count=1, go_to_start_of_line_if_history_changes=False): \"\"\" If we're not on the first line (of a multiline input) go a line up, otherwise go back in history. (If nothing is selected.) \"\"\" if self.complete_state: self.complete_previous(count=count) elif self.document.cursor_position_row > 0: self.cursor_up(count=count) elif not self.selection_state: self.history_backward(count=count) # Go to the start of the line? if go_to_start_of_line_if_history_changes: self.cursor_position += self.document.get_start_of_line_position()"], "rank": 1}
{"nl": "capture output of python pprint into a file", "code": ["def py(self, output): \"\"\"Output data as a nicely-formatted python data structure\"\"\" import pprint pprint.pprint(output, stream=self.outfile)", "def pprint(o, stream=None, indent=1, width=80, depth=None): \"\"\"Pretty-print a Python o to a stream [default is sys.stdout].\"\"\" printer = PrettyPrinter( stream=stream, indent=indent, width=width, depth=depth) printer.pprint(o)", "def pprint(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Like `pretty` but print to stdout. \"\"\" printer = RepresentationPrinter(sys.stdout, verbose, max_width, newline) printer.pretty(obj) printer.flush() sys.stdout.write(newline) sys.stdout.flush()", "def json_pretty_dump(obj, filename): \"\"\" Serialize obj as a JSON formatted stream to the given filename ( pretty printing version) \"\"\" with open(filename, \"wt\") as fh: json.dump(obj, fh, indent=4, sort_keys=4)", "def pprint(self, stream=None, indent=1, width=80, depth=None): \"\"\" Pretty print the underlying literal Python object \"\"\" pp.pprint(to_literal(self), stream, indent, width, depth)"], "rank": 1}
{"nl": "custom json serialize python tuple", "code": ["def to_json(value, **kwargs): \"\"\"Return a copy of the tuple as a list If the tuple contains HasProperties instances, they are serialized. \"\"\" serial_list = [ val.serialize(**kwargs) if isinstance(val, HasProperties) else val for val in value ] return serial_list", "def json_serialize(obj): \"\"\" Simple generic JSON serializer for common objects. \"\"\" if isinstance(obj, datetime): return obj.isoformat() if hasattr(obj, 'id'): return jsonify(obj.id) if hasattr(obj, 'name'): return jsonify(obj.name) raise TypeError('{0} is not JSON serializable'.format(obj))", "def json_serial(obj): \"\"\"JSON serializer for objects not serializable by default json code\"\"\" if isinstance(obj, LegipyModel): return obj.to_json() elif isinstance(obj, (datetime.date, datetime.datetime)): return obj.isoformat() raise TypeError(\"Type {0} not serializable\".format(repr(type(obj))))", "def serialize(self, value, **kwargs): \"\"\"Serialize every item of the list.\"\"\" return [self.item_type.serialize(val, **kwargs) for val in value]", "def json_dumps(self, obj): \"\"\"Serializer for consistency\"\"\" return json.dumps(obj, sort_keys=True, indent=4, separators=(',', ': '))"], "rank": 1}
{"nl": "how to code tables in python using latex", "code": ["def get_latex_table(self, parameters=None, transpose=False, caption=None, label=\"tab:model_params\", hlines=True, blank_fill=\"--\"): # pragma: no cover \"\"\" Generates a LaTeX table from parameter summaries. Parameters ---------- parameters : list[str], optional A list of what parameters to include in the table. By default, includes all parameters transpose : bool, optional Defaults to False, which gives each column as a parameter, each chain (framework) as a row. You can swap it so that you have a parameter each row and a framework each column by setting this to True caption : str, optional If you want to generate a caption for the table through Python, use this. Defaults to an empty string label : str, optional If you want to generate a label for the table through Python, use this. Defaults to an empty string hlines : bool, optional Inserts ``\\\\hline`` before and after the header, and at the end of table. blank_fill : str, optional If a framework does not have a particular parameter, will fill that cell of the table with this string. Returns ------- str the LaTeX table. \"\"\" if parameters is None: parameters = self.parent._all_parameters for p in parameters: assert isinstance(p, str), \\ \"Generating a LaTeX table requires all parameters have labels\" num_parameters = len(parameters) num_chains = len(self.parent.chains) fit_values = self.get_summary(squeeze=False) if label is None: label = \"\" if caption is None: caption = \"\" end_text = \" \\\\\\\\ \\n\" if transpose: column_text = \"c\" * (num_chains + 1) else: column_text = \"c\" * (num_parameters + 1) center_text = \"\" hline_text = \"\\\\hline\\n\" if hlines: center_text += hline_text + \"\\t\\t\" if transpose: center_text += \" & \".join([\"Parameter\"] + [c.name for c in self.parent.chains]) + end_text if hlines: center_text += \"\\t\\t\" + hline_text for p in parameters: arr = [\"\\t\\t\" + p] for chain_res in fit_values: if p in chain_res: arr.append(self.get_parameter_text(*chain_res[p], wrap=True)) else: arr.append(blank_fill) center_text += \" & \".join(arr) + end_text else: center_text += \" & \".join([\"Model\"] + parameters) + end_text if hlines: center_text += \"\\t\\t\" + hline_text for name, chain_res in zip([c.name for c in self.parent.chains], fit_values): arr = [\"\\t\\t\" + name] for p in parameters: if p in chain_res: arr.append(self.get_parameter_text(*chain_res[p], wrap=True)) else: arr.append(blank_fill) center_text += \" & \".join(arr) + end_text if hlines: center_text += \"\\t\\t\" + hline_text final_text = get_latex_table_frame(caption, label) % (column_text, center_text) return final_text", "def print_latex(o): \"\"\"A function to generate the latex representation of sympy expressions.\"\"\" if can_print_latex(o): s = latex(o, mode='plain') s = s.replace('\\\\dag','\\\\dagger') s = s.strip('$') return '$$%s$$' % s # Fallback to the string printer return None", "def _render_table(data, fields=None): \"\"\" Helper to render a list of dictionaries as an HTML display object. \"\"\" return IPython.core.display.HTML(datalab.utils.commands.HtmlBuilder.render_table(data, fields))", "def print_display_png(o): \"\"\" A function to display sympy expression using display style LaTeX in PNG. \"\"\" s = latex(o, mode='plain') s = s.strip('$') # As matplotlib does not support display style, dvipng backend is # used here. png = latex_to_png('$$%s$$' % s, backend='dvipng') return png", "def html(header_rows): \"\"\" Convert a list of tuples describing a table into a HTML string \"\"\" name = 'table%d' % next(tablecounter) return HtmlTable([map(str, row) for row in header_rows], name).render()"], "rank": 1}
{"nl": "python how to show help", "code": ["def do_help(self, arg): \"\"\" Show help on all commands. \"\"\" print(self.response_prompt, file=self.stdout) return cmd.Cmd.do_help(self, arg)", "def help(self, level=0): \"\"\"return the usage string for available options \"\"\" self.cmdline_parser.formatter.output_level = level with _patch_optparse(): return self.cmdline_parser.format_help()", "def _help(): \"\"\" Display both SQLAlchemy and Python help statements \"\"\" statement = '%s%s' % (shelp, phelp % ', '.join(cntx_.keys())) print statement.strip()", "def help(self): \"\"\"Prints discovered resources and their associated methods. Nice when noodling in the terminal to wrap your head around Magento's insanity. \"\"\" print('Resources:') print('') for name in sorted(self._resources.keys()): methods = sorted(self._resources[name]._methods.keys()) print('{}: {}'.format(bold(name), ', '.join(methods)))", "def show_intro(self): \"\"\"Show intro to IPython help\"\"\" from IPython.core.usage import interactive_usage self.main.help.show_rich_text(interactive_usage)"], "rank": 3}
{"nl": "max heap insert python", "code": ["def _heappush_max(heap, item): \"\"\" why is this not in heapq \"\"\" heap.append(item) heapq._siftdown_max(heap, 0, len(heap) - 1)", "def _heapreplace_max(heap, item): \"\"\"Maxheap version of a heappop followed by a heappush.\"\"\" returnitem = heap[0] # raises appropriate IndexError if heap is empty heap[0] = item _siftup_max(heap, 0) return returnitem", "def heappush_max(heap, item): \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\" heap.append(item) _siftdown_max(heap, 0, len(heap) - 1)", "def _heapify_max(x): \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\" n = len(x) for i in reversed(range(n//2)): _siftup_max(x, i)", "def heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt"], "rank": 3}
{"nl": "how to check for duplicate characters in a python string", "code": ["def first_unique_char(s): \"\"\" :type s: str :rtype: int \"\"\" if (len(s) == 1): return 0 ban = [] for i in range(len(s)): if all(s[i] != s[k] for k in range(i + 1, len(s))) == True and s[i] not in ban: return i else: ban.append(s[i]) return -1", "def _check_for_duplicate_sequence_names(self, fasta_file_path): \"\"\"Test if the given fasta file contains sequences with duplicate sequence names. Parameters ---------- fasta_file_path: string path to file that is to be checked Returns ------- The name of the first duplicate sequence found, else False. \"\"\" found_sequence_names = set() for record in SeqIO.parse(fasta_file_path, 'fasta'): name = record.name if name in found_sequence_names: return name found_sequence_names.add(name) return False", "def has_jongsung(letter): \"\"\"Check whether this letter contains Jongsung\"\"\" if len(letter) != 1: raise Exception('The target string must be one letter.') if not is_hangul(letter): raise NotHangulException('The target string must be Hangul') code = lt.hangul_index(letter) return code % NUM_JONG > 0", "def count_string_diff(a,b): \"\"\"Return the number of characters in two strings that don't exactly match\"\"\" shortest = min(len(a), len(b)) return sum(a[i] != b[i] for i in range(shortest))", "def unicode_is_ascii(u_string): \"\"\"Determine if unicode string only contains ASCII characters. :param str u_string: unicode string to check. Must be unicode and not Python 2 `str`. :rtype: bool \"\"\" assert isinstance(u_string, str) try: u_string.encode('ascii') return True except UnicodeEncodeError: return False"], "rank": 1}
{"nl": "how to correct socket not define error in python", "code": ["def feed_eof(self): \"\"\"Send a potentially \"ragged\" EOF. This method will raise an SSL_ERROR_EOF exception if the EOF is unexpected. \"\"\" self._incoming.write_eof() ssldata, appdata = self.feed_ssldata(b'') assert appdata == [] or appdata == [b'']", "def _connection_failed(self, error=\"Error not specified!\"): \"\"\"Clean up after connection failure detected.\"\"\" if not self._error: LOG.error(\"Connection failed: %s\", str(error)) self._error = error", "def _is_retryable_exception(e): \"\"\"Returns True if the exception is always safe to retry. This is True if the client was never able to establish a connection to the server (for example, name resolution failed or the connection could otherwise not be initialized). Conservatively, if we can't tell whether a network connection could have been established, we return False. \"\"\" if isinstance(e, urllib3.exceptions.ProtocolError): e = e.args[1] if isinstance(e, (socket.gaierror, socket.herror)): return True if isinstance(e, socket.error) and e.errno in _RETRYABLE_SOCKET_ERRORS: return True if isinstance(e, urllib3.exceptions.NewConnectionError): return True return False", "def on_IOError(self, e): \"\"\" Handle an IOError exception. \"\"\" sys.stderr.write(\"Error: %s: \\\"%s\\\"\\n\" % (e.strerror, e.filename))", "def connection_lost(self, exc): \"\"\"Called when asyncio.Protocol loses the network connection.\"\"\" if exc is None: self.log.warning('eof from receiver?') else: self.log.warning('Lost connection to receiver: %s', exc) self.transport = None if self._connection_lost_callback: self._loop.call_soon(self._connection_lost_callback)"], "rank": 40}
{"nl": "python draft4validator validate schema", "code": ["def validate(schema, data, owner=None): \"\"\"Validate input data with input schema. :param Schema schema: schema able to validate input data. :param data: data to validate. :param Schema owner: input schema parent schema. :raises: Exception if the data is not validated. \"\"\" schema._validate(data=data, owner=owner)", "def validate(payload, schema): \"\"\"Validate `payload` against `schema`, returning an error list. jsonschema provides lots of information in it's errors, but it can be a bit of work to extract all the information. \"\"\" v = jsonschema.Draft4Validator( schema, format_checker=jsonschema.FormatChecker()) error_list = [] for error in v.iter_errors(payload): message = error.message location = '/' + '/'.join([str(c) for c in error.absolute_path]) error_list.append(message + ' at ' + location) return error_list", "def validate(self): \"\"\"Validate the configuration file.\"\"\" validator = Draft4Validator(self.SCHEMA) if not validator.is_valid(self.config): for err in validator.iter_errors(self.config): LOGGER.error(str(err.message)) validator.validate(self.config)", "def validate(datum, schema, field=None, raise_errors=True): \"\"\" Determine if a python datum is an instance of a schema. Parameters ---------- datum: Any Data being validated schema: dict Schema field: str, optional Record field being validated raise_errors: bool, optional If true, errors are raised for invalid data. If false, a simple True (valid) or False (invalid) result is returned Example:: from fastavro.validation import validate schema = {...} record = {...} validate(record, schema) \"\"\" record_type = extract_record_type(schema) result = None validator = VALIDATORS.get(record_type) if validator: result = validator(datum, schema=schema, parent_ns=field, raise_errors=raise_errors) elif record_type in SCHEMA_DEFS: result = validate(datum, schema=SCHEMA_DEFS[record_type], field=field, raise_errors=raise_errors) else: raise UnknownType(record_type) if raise_errors and result is False: raise ValidationError(ValidationErrorData(datum, schema, field)) return result", "def _validate(data, schema, ac_schema_safe=True, **options): \"\"\" See the descritpion of :func:`validate` for more details of parameters and return value. Validate target object 'data' with given schema object. \"\"\" try: jsonschema.validate(data, schema, **options) except (jsonschema.ValidationError, jsonschema.SchemaError, Exception) as exc: if ac_schema_safe: return (False, str(exc)) # Validation was failed. raise return (True, '')"], "rank": 3}
{"nl": "get number of rows in output of sql query in python", "code": ["def count_rows(self, table, cols='*'): \"\"\"Get the number of rows in a particular table.\"\"\" query = 'SELECT COUNT({0}) FROM {1}'.format(join_cols(cols), wrap(table)) result = self.fetch(query) return result if result is not None else 0", "def count_rows(self, table_name): \"\"\"Return the number of entries in a table by counting them.\"\"\" self.table_must_exist(table_name) query = \"SELECT COUNT (*) FROM `%s`\" % table_name.lower() self.own_cursor.execute(query) return int(self.own_cursor.fetchone()[0])", "def get_count(self, query): \"\"\" Returns a number of query results. This is faster than .count() on the query \"\"\" count_q = query.statement.with_only_columns( [func.count()]).order_by(None) count = query.session.execute(count_q).scalar() return count", "def count_(self): \"\"\" Returns the number of rows of the main dataframe \"\"\" try: num = len(self.df.index) except Exception as e: self.err(e, \"Can not count data\") return return num", "def execute_sql(self, query): \"\"\" Executes a given query string on an open postgres database. \"\"\" c = self.con.cursor() c.execute(query) result = [] if c.rowcount > 0: try: result = c.fetchall() except psycopg2.ProgrammingError: pass return result"], "rank": 1}
{"nl": "add index support objects python", "code": ["def to_index(self, index_type, index_name, includes=None): \"\"\" Create an index field from this field \"\"\" return IndexField(self.name, self.data_type, index_type, index_name, includes)", "def __init__(self, collection, index_type_obj): \"\"\" Constructs wrapper for general index creation and deletion :param collection Collection :param index_type_obj BaseIndex Object of a index sub-class \"\"\" self.collection = collection self.index_type_obj = index_type_obj", "def index(obj, index=INDEX_NAME, doc_type=DOC_TYPE): \"\"\" Index the given document. https://elasticsearch-py.readthedocs.io/en/master/api.html#elasticsearch.Elasticsearch.index https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html \"\"\" doc = to_dict(obj) if doc is None: return id = doc.pop('id') return es_conn.index(index, doc_type, doc, id=id)", "def validate_multiindex(self, obj): \"\"\"validate that we can store the multi-index; reset and return the new object \"\"\" levels = [l if l is not None else \"level_{0}\".format(i) for i, l in enumerate(obj.index.names)] try: return obj.reset_index(), levels except ValueError: raise ValueError(\"duplicate names/columns in the multi-index when \" \"storing as a table\")", "def update_index(index): \"\"\"Re-index every document in a named index.\"\"\" logger.info(\"Updating search index: '%s'\", index) client = get_client() responses = [] for model in get_index_models(index): logger.info(\"Updating search index model: '%s'\", model.search_doc_type) objects = model.objects.get_search_queryset(index).iterator() actions = bulk_actions(objects, index=index, action=\"index\") response = helpers.bulk(client, actions, chunk_size=get_setting(\"chunk_size\")) responses.append(response) return responses"], "rank": 1}
{"nl": "make python tuple from list of strings", "code": ["def _parse_tuple_string(argument): \"\"\" Return a tuple from parsing 'a,b,c,d' -> (a,b,c,d) \"\"\" if isinstance(argument, str): return tuple(int(p.strip()) for p in argument.split(',')) return argument", "def ensure_tuple(obj): \"\"\"Try and make the given argument into a tuple.\"\"\" if obj is None: return tuple() if isinstance(obj, Iterable) and not isinstance(obj, six.string_types): return tuple(obj) return obj,", "def ver_to_tuple(value): \"\"\" Convert version like string to a tuple of integers. \"\"\" return tuple(int(_f) for _f in re.split(r'\\D+', value) if _f)", "def as_tuple(self, value): \"\"\"Utility function which converts lists to tuples.\"\"\" if isinstance(value, list): value = tuple(value) return value", "def from_tuple(tup): \"\"\"Convert a tuple into a range with error handling. Parameters ---------- tup : tuple (len 2 or 3) The tuple to turn into a range. Returns ------- range : range The range from the tuple. Raises ------ ValueError Raised when the tuple length is not 2 or 3. \"\"\" if len(tup) not in (2, 3): raise ValueError( 'tuple must contain 2 or 3 elements, not: %d (%r' % ( len(tup), tup, ), ) return range(*tup)"], "rank": 1}
{"nl": "how to create a variable containing multiple figures python", "code": ["def strip_figures(figure): \"\"\" Strips a figure into multiple figures with a trace on each of them Parameters: ----------- figure : Figure Plotly Figure \"\"\" fig=[] for trace in figure['data']: fig.append(dict(data=[trace],layout=figure['layout'])) return fig", "def new_figure_manager_given_figure(num, figure): \"\"\" Create a new figure manager instance for the given figure. \"\"\" fig = figure frame = FigureFrameWx(num, fig) figmgr = frame.get_figure_manager() if matplotlib.is_interactive(): figmgr.frame.Show() return figmgr", "def show(self, title=''): \"\"\" Display Bloch sphere and corresponding data sets. \"\"\" self.render(title=title) if self.fig: plt.show(self.fig)", "def plot_and_save(self, **kwargs): \"\"\"Used when the plot method defined does not create a figure nor calls save_plot Then the plot method has to use self.fig\"\"\" self.fig = pyplot.figure() self.plot() self.axes = pyplot.gca() self.save_plot(self.fig, self.axes, **kwargs) pyplot.close(self.fig)", "def finish_plot(): \"\"\"Helper for plotting.\"\"\" plt.legend() plt.grid(color='0.7') plt.xlabel('x') plt.ylabel('y') plt.show()"], "rank": 1}
{"nl": "python validate url invalid characters", "code": ["def clean_url(url): \"\"\"URL Validation function\"\"\" if not url.startswith(('http://', 'https://')): url = f'http://{url}' if not URL_RE.match(url): raise BadURLException(f'{url} is not valid') return url", "def _is_valid_url(url): \"\"\" Helper function to validate that URLs are well formed, i.e that it contains a valid protocol and a valid domain. It does not actually check if the URL exists \"\"\" try: parsed = urlparse(url) mandatory_parts = [parsed.scheme, parsed.netloc] return all(mandatory_parts) except: return False", "def is_valid_url(url): \"\"\"Checks if a given string is an url\"\"\" pieces = urlparse(url) return all([pieces.scheme, pieces.netloc])", "def url_syntax_check(url): # pragma: no cover \"\"\" Check the syntax of the given URL. :param url: The URL to check the syntax for. :type url: str :return: The syntax validity. :rtype: bool .. warning:: If an empty or a non-string :code:`url` is given, we return :code:`None`. \"\"\" if url and isinstance(url, str): # The given URL is not empty nor None. # and # * The given URL is a string. # We silently load the configuration. load_config(True) return Check(url).is_url_valid() # We return None, there is nothing to check. return None", "def url_encode(url): \"\"\" Convert special characters using %xx escape. :param url: str :return: str - encoded url \"\"\" if isinstance(url, text_type): url = url.encode('utf8') return quote(url, ':/%?&=')"], "rank": 4}
{"nl": "python compare float of number to integer", "code": ["def _check_for_int(x): \"\"\" This is a compatibility function that takes a C{float} and converts it to an C{int} if the values are equal. \"\"\" try: y = int(x) except (OverflowError, ValueError): pass else: # There is no way in AMF0 to distinguish between integers and floats if x == x and y == x: return y return x", "def is_integer(value: Any) -> bool: \"\"\"Return true if a value is an integer number.\"\"\" return (isinstance(value, int) and not isinstance(value, bool)) or ( isinstance(value, float) and isfinite(value) and int(value) == value )", "def positive_int(val): \"\"\"Parse `val` into a positive integer.\"\"\" if isinstance(val, float): raise ValueError('\"{}\" must not be a float'.format(val)) val = int(val) if val >= 0: return val raise ValueError('\"{}\" must be positive'.format(val))", "def is_natural(x): \"\"\"A non-negative integer.\"\"\" try: is_integer = int(x) == x except (TypeError, ValueError): return False return is_integer and x >= 0", "def minus(*args): \"\"\"Also, converts either to ints or to floats.\"\"\" if len(args) == 1: return -to_numeric(args[0]) return to_numeric(args[0]) - to_numeric(args[1])"], "rank": 1}
{"nl": "python pdb see stack", "code": ["def set_trace(): \"\"\"Start a Pdb instance at the calling frame, with stdout routed to sys.__stdout__.\"\"\" # https://github.com/nose-devs/nose/blob/master/nose/tools/nontrivial.py pdb.Pdb(stdout=sys.__stdout__).set_trace(sys._getframe().f_back)", "def debug_on_error(type, value, tb): \"\"\"Code due to Thomas Heller - published in Python Cookbook (O'Reilley)\"\"\" traceback.print_exc(type, value, tb) print() pdb.pm()", "def user_return(self, frame, return_value): \"\"\"This function is called when a return trap is set here.\"\"\" pdb.Pdb.user_return(self, frame, return_value)", "def print_trace(self): \"\"\" Prints stack trace for current exceptions chain. \"\"\" traceback.print_exc() for tb in self.tracebacks: print tb, print ''", "def __run(self): \"\"\"Hacked run function, which installs the trace.\"\"\" sys.settrace(self.globaltrace) self.__run_backup() self.run = self.__run_backup"], "rank": 1}
{"nl": "how to raise a number to a power in python 3", "code": ["def _power(ctx, number, power): \"\"\" Returns the result of a number raised to a power \"\"\" return decimal_pow(conversions.to_decimal(number, ctx), conversions.to_decimal(power, ctx))", "def safe_pow(base, exp): \"\"\"safe version of pow\"\"\" if exp > MAX_EXPONENT: raise RuntimeError(\"Invalid exponent, max exponent is {}\".format(MAX_EXPONENT)) return base ** exp", "def is_power_of_2(num): \"\"\"Return whether `num` is a power of two\"\"\" log = math.log2(num) return int(log) == float(log)", "def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)", "def _safe_db(num, den): \"\"\"Properly handle the potential +Inf db SIR instead of raising a RuntimeWarning. \"\"\" if den == 0: return np.inf return 10 * np.log10(num / den)"], "rank": 1}
{"nl": "how to set width of bar in horizontal bar chart python", "code": ["def _change_height(self, ax, new_value): \"\"\"Make bars in horizontal bar chart thinner\"\"\" for patch in ax.patches: current_height = patch.get_height() diff = current_height - new_value # we change the bar height patch.set_height(new_value) # we recenter the bar patch.set_y(patch.get_y() + diff * .5)", "def barv(d, plt, title=None, rotation='vertical'): \"\"\"A convenience function for plotting a vertical bar plot from a Counter\"\"\" labels = sorted(d, key=d.get, reverse=True) index = range(len(labels)) plt.xticks(index, labels, rotation=rotation) plt.bar(index, [d[v] for v in labels]) if title is not None: plt.title(title)", "def seaborn_bar_(self, label=None, style=None, opts=None): \"\"\" Get a Seaborn bar chart \"\"\" try: fig = sns.barplot(self.x, self.y, palette=\"BuGn_d\") return fig except Exception as e: self.err(e, self.seaborn_bar_, \"Can not get Seaborn bar chart object\")", "def __init__(self, min_value, max_value, format=\"%(bar)s: %(percentage) 6.2f%% %(timeinfo)s\", width=40, barchar=\"#\", emptychar=\"-\", output=sys.stdout): \"\"\" :param min_value: minimum value for update(..) :param format: format specifier for the output :param width: width of the progress bar's (excluding extra text) :param barchar: character used to print the bar :param output: where to write the output to \"\"\" self.min_value = min_value self.max_value = max_value self.format = format self.width = width self.barchar = barchar self.emptychar = emptychar self.output = output self.firsttime = True self.prevtime = time.time() self.starttime = self.prevtime self.prevfraction = 0 self.firsttimedone = False self.value = self.min_value", "def vertical_percent(plot, percent=0.1): \"\"\" Using the size of the y axis, return a fraction of that size. \"\"\" plot_bottom, plot_top = plot.get_ylim() return percent * (plot_top - plot_bottom)"], "rank": 1}
{"nl": "writing javascript in python for webpage", "code": ["def add_to_js(self, name, var): \"\"\"Add an object to Javascript.\"\"\" frame = self.page().mainFrame() frame.addToJavaScriptWindowObject(name, var)", "def eval_script(self, expr): \"\"\" Evaluates a piece of Javascript in the context of the current page and returns its value. \"\"\" ret = self.conn.issue_command(\"Evaluate\", expr) return json.loads(\"[%s]\" % ret)[0]", "def static_urls_js(): \"\"\" Add global variables to JavaScript about the location and latest version of transpiled files. Usage:: {% static_urls_js %} \"\"\" if apps.is_installed('django.contrib.staticfiles'): from django.contrib.staticfiles.storage import staticfiles_storage static_base_url = staticfiles_storage.base_url else: static_base_url = PrefixNode.handle_simple(\"STATIC_URL\") transpile_base_url = urljoin(static_base_url, 'js/transpile/') return { 'static_base_url': static_base_url, 'transpile_base_url': transpile_base_url, 'version': LAST_RUN['version'] }", "def import_js(path, lib_name, globals): \"\"\"Imports from javascript source file. globals is your globals()\"\"\" with codecs.open(path_as_local(path), \"r\", \"utf-8\") as f: js = f.read() e = EvalJs() e.execute(js) var = e.context['var'] globals[lib_name] = var.to_python()", "def to_jupyter(graph: BELGraph, chart: Optional[str] = None) -> Javascript: \"\"\"Render the graph as JavaScript in a Jupyter Notebook.\"\"\" with open(os.path.join(HERE, 'render_with_javascript.js'), 'rt') as f: js_template = Template(f.read()) return Javascript(js_template.render(**_get_context(graph, chart=chart)))"], "rank": 1}
{"nl": "python two vector multiply", "code": ["def multiply(self, number): \"\"\"Return a Vector as the product of the vector and a real number.\"\"\" return self.from_list([x * number for x in self.to_list()])", "def get_scalar_product(self, other): \"\"\"Returns the scalar product of this vector with the given other vector.\"\"\" return self.x*other.x+self.y*other.y", "def matrixTimesVector(MM, aa): \"\"\" :param MM: A matrix of size 3x3 :param aa: A vector of size 3 :return: A vector of size 3 which is the product of the matrix by the vector \"\"\" bb = np.zeros(3, np.float) for ii in range(3): bb[ii] = np.sum(MM[ii, :] * aa) return bb", "def scale_v2(vec, amount): \"\"\"Return a new Vec2 with x and y from vec and multiplied by amount.\"\"\" return Vec2(vec.x * amount, vec.y * amount)", "def dot_v3(v, w): \"\"\"Return the dotproduct of two vectors.\"\"\" return sum([x * y for x, y in zip(v, w)])"], "rank": 7}
{"nl": "python dynamically read args in functions", "code": ["def parsed_args(): parser = argparse.ArgumentParser(description=\"\"\"python runtime functions\"\"\", epilog=\"\") parser.add_argument('command',nargs='*', help=\"Name of the function to run with arguments\") args = parser.parse_args() return (args, parser)", "def _correct_args(func, kwargs): \"\"\" Convert a dictionary of arguments including __argv into a list for passing to the function. \"\"\" args = inspect.getargspec(func)[0] return [kwargs[arg] for arg in args] + kwargs['__args']", "def _call(callable_obj, arg_names, namespace): \"\"\"Actually calls the callable with the namespace parsed from the command line. Args: callable_obj: a callable object arg_names: name of the function arguments namespace: the namespace object parsed from the command line \"\"\" arguments = {arg_name: getattr(namespace, arg_name) for arg_name in arg_names} return callable_obj(**arguments)", "def run(args): \"\"\"Process command line arguments and walk inputs.\"\"\" raw_arguments = get_arguments(args[1:]) process_arguments(raw_arguments) walk.run() return True", "def get_all_args(fn) -> list: \"\"\" Returns a list of all arguments for the function fn. >>> def foo(x, y, z=100): return x + y + z >>> get_all_args(foo) ['x', 'y', 'z'] \"\"\" sig = inspect.signature(fn) return list(sig.parameters)"], "rank": 1}
{"nl": "python draw line with scope and intercept", "code": ["def _add_line_segment(self, x, y): \"\"\"Add a |_LineSegment| operation to the drawing sequence.\"\"\" self._drawing_operations.append(_LineSegment.new(self, x, y))", "def _draw_lines_internal(self, coords, colour, bg): \"\"\"Helper to draw lines connecting a set of nodes that are scaled for the Screen.\"\"\" for i, (x, y) in enumerate(coords): if i == 0: self._screen.move(x, y) else: self._screen.draw(x, y, colour=colour, bg=bg, thin=True)", "def vline(self, x, y, height, color): \"\"\"Draw a vertical line up to a given length.\"\"\" self.rect(x, y, 1, height, color, fill=True)", "def polyline(self, arr): \"\"\"Draw a set of lines\"\"\" for i in range(0, len(arr) - 1): self.line(arr[i][0], arr[i][1], arr[i + 1][0], arr[i + 1][1])", "def hline(self, x, y, width, color): \"\"\"Draw a horizontal line up to a given length.\"\"\" self.rect(x, y, width, 1, color, fill=True)"], "rank": 3}
{"nl": "python good way to load a yaml file", "code": ["def load_yaml_file(file_path: str): \"\"\"Load a YAML file from path\"\"\" with codecs.open(file_path, 'r') as f: return yaml.safe_load(f)", "def load_yaml(filepath): \"\"\"Convenience function for loading yaml-encoded data from disk.\"\"\" with open(filepath) as f: txt = f.read() return yaml.load(txt)", "def load_yaml(yaml_file: str) -> Any: \"\"\" Load YAML from file. :param yaml_file: path to YAML file :return: content of the YAML as dict/list \"\"\" with open(yaml_file, 'r') as file: return ruamel.yaml.load(file, ruamel.yaml.RoundTripLoader)", "def _parse_config(config_file_path): \"\"\" Parse Config File from yaml file. \"\"\" config_file = open(config_file_path, 'r') config = yaml.load(config_file) config_file.close() return config", "def load_yaml(file): \"\"\"If pyyaml > 5.1 use full_load to avoid warning\"\"\" if hasattr(yaml, \"full_load\"): return yaml.full_load(file) else: return yaml.load(file)"], "rank": 2}
{"nl": "python detect if a file is a symbolic link", "code": ["def is_symlink(self): \"\"\" Whether this path is a symbolic link. \"\"\" try: return S_ISLNK(self.lstat().st_mode) except OSError as e: if e.errno != ENOENT: raise # Path doesn't exist return False", "def _is_target_a_directory(link, rel_target): \"\"\" If creating a symlink from link to a target, determine if target is a directory (relative to dirname(link)). \"\"\" target = os.path.join(os.path.dirname(link), rel_target) return os.path.isdir(target)", "def create_symlink(source, link_name): \"\"\" Creates symbolic link for either operating system. http://stackoverflow.com/questions/6260149/os-symlink-support-in-windows \"\"\" os_symlink = getattr(os, \"symlink\", None) if isinstance(os_symlink, collections.Callable): os_symlink(source, link_name) else: import ctypes csl = ctypes.windll.kernel32.CreateSymbolicLinkW csl.argtypes = (ctypes.c_wchar_p, ctypes.c_wchar_p, ctypes.c_uint32) csl.restype = ctypes.c_ubyte flags = 1 if os.path.isdir(source) else 0 if csl(link_name, source, flags) == 0: raise ctypes.WinError()", "def symlink(source, destination): \"\"\"Create a symbolic link\"\"\" log(\"Symlinking {} as {}\".format(source, destination)) cmd = [ 'ln', '-sf', source, destination, ] subprocess.check_call(cmd)", "def is_image(filename): \"\"\"Determine if given filename is an image.\"\"\" # note: isfile() also accepts symlinks return os.path.isfile(filename) and filename.lower().endswith(ImageExts)"], "rank": 1}
{"nl": "test if multiple variables are none python", "code": ["def _not_none(items): \"\"\"Whether the item is a placeholder or contains a placeholder.\"\"\" if not isinstance(items, (tuple, list)): items = (items,) return all(item is not _none for item in items)", "def is_none(string_, default='raise'): \"\"\" Check if a string is equivalent to None. Parameters ---------- string_ : str default : {'raise', False} Default behaviour if none of the \"None\" strings is detected. Returns ------- is_none : bool Examples -------- >>> is_none('2', default=False) False >>> is_none('undefined', default=False) True \"\"\" none = ['none', 'undefined', 'unknown', 'null', ''] if string_.lower() in none: return True elif not default: return False else: raise ValueError('The value \\'{}\\' cannot be mapped to none.' .format(string_))", "def check_empty_dict(GET_dict): \"\"\" Returns True if the GET querstring contains on values, but it can contain empty keys. This is better than doing not bool(request.GET) as an empty key will return True \"\"\" empty = True for k, v in GET_dict.items(): # Don't disable on p(age) or 'all' GET param if v and k != 'p' and k != 'all': empty = False return empty", "def is_a_sequence(var, allow_none=False): \"\"\" Returns True if var is a list or a tuple (but not a string!) \"\"\" return isinstance(var, (list, tuple)) or (var is None and allow_none)", "def is_empty_shape(sh: ShExJ.Shape) -> bool: \"\"\" Determine whether sh has any value \"\"\" return sh.closed is None and sh.expression is None and sh.extra is None and \\ sh.semActs is None"], "rank": 1}
{"nl": "delete a column in python db", "code": ["def drop_column(self, tablename: str, fieldname: str) -> int: \"\"\"Drops (deletes) a column from an existing table.\"\"\" sql = \"ALTER TABLE {} DROP COLUMN {}\".format(tablename, fieldname) log.info(sql) return self.db_exec_literal(sql)", "def wipe_table(self, table: str) -> int: \"\"\"Delete all records from a table. Use caution!\"\"\" sql = \"DELETE FROM \" + self.delimit(table) return self.db_exec(sql)", "def clear_table(dbconn, table_name): \"\"\" Delete all rows from a table :param dbconn: data base connection :param table_name: name of the table :return: \"\"\" cur = dbconn.cursor() cur.execute(\"DELETE FROM '{name}'\".format(name=table_name)) dbconn.commit()", "def delete(self, row): \"\"\"Delete a track value\"\"\" i = self._get_key_index(row) del self.keys[i]", "def delete_entry(self, key): \"\"\"Delete an object from the redis table\"\"\" pipe = self.client.pipeline() pipe.srem(self.keys_container, key) pipe.delete(key) pipe.execute()"], "rank": 1}
{"nl": "calculate the eigen values in python", "code": ["def center_eigenvalue_diff(mat): \"\"\"Compute the eigvals of mat and then find the center eigval difference.\"\"\" N = len(mat) evals = np.sort(la.eigvals(mat)) diff = np.abs(evals[N/2] - evals[N/2-1]) return diff", "def perform_pca(A): \"\"\" Computes eigenvalues and eigenvectors of covariance matrix of A. The rows of a correspond to observations, the columns to variables. \"\"\" # First subtract the mean M = (A-numpy.mean(A.T, axis=1)).T # Get eigenvectors and values of covariance matrix return numpy.linalg.eig(numpy.cov(M))", "def get_stationary_distribution(self): \"\"\"Compute the stationary distribution of states. \"\"\" # The stationary distribution is proportional to the left-eigenvector # associated with the largest eigenvalue (i.e., 1) of the transition # matrix. check_is_fitted(self, \"transmat_\") eigvals, eigvecs = np.linalg.eig(self.transmat_.T) eigvec = np.real_if_close(eigvecs[:, np.argmax(eigvals)]) return eigvec / eigvec.sum()", "def _svd(cls, matrix, num_concepts=5): \"\"\" Perform singular value decomposition for dimensionality reduction of the input matrix. \"\"\" u, s, v = svds(matrix, k=num_concepts) return u, s, v", "def _cho_factor(A, lower=True, check_finite=True): \"\"\"Implementaton of :func:`scipy.linalg.cho_factor` using a function supported in cupy.\"\"\" return cp.linalg.cholesky(A), True"], "rank": 1}
{"nl": "python read adb devices", "code": ["def list_adb_devices_by_usb_id(): \"\"\"List the usb id of all android devices connected to the computer that are detected by adb. Returns: A list of strings that are android device usb ids. Empty if there's none. \"\"\" out = adb.AdbProxy().devices(['-l']) clean_lines = new_str(out, 'utf-8').strip().split('\\n') results = [] for line in clean_lines: tokens = line.strip().split() if len(tokens) > 2 and tokens[1] == 'device': results.append(tokens[2]) return results", "def is_adb_detectable(self): \"\"\"Checks if USB is on and device is ready by verifying adb devices.\"\"\" serials = list_adb_devices() if self.serial in serials: self.log.debug('Is now adb detectable.') return True return False", "async def sysinfo(dev: Device): \"\"\"Print out system information (version, MAC addrs).\"\"\" click.echo(await dev.get_system_info()) click.echo(await dev.get_interface_information())", "def autoscan(): \"\"\"autoscan will check all of the serial ports to see if they have a matching VID:PID for a MicroPython board. \"\"\" for port in serial.tools.list_ports.comports(): if is_micropython_usb_device(port): connect_serial(port[0])", "def _get_device_id(self, bus): \"\"\" Find the device id \"\"\" _dbus = bus.get(SERVICE_BUS, PATH) devices = _dbus.devices() if self.device is None and self.device_id is None and len(devices) == 1: return devices[0] for id in devices: self._dev = bus.get(SERVICE_BUS, DEVICE_PATH + \"/%s\" % id) if self.device == self._dev.name: return id return None"], "rank": 1}
{"nl": "how to capitalize only the first letter of a string in python", "code": ["def capitalize(string): \"\"\"Capitalize a sentence. Parameters ---------- string : `str` String to capitalize. Returns ------- `str` Capitalized string. Examples -------- >>> capitalize('worD WORD WoRd') 'Word word word' \"\"\" if not string: return string if len(string) == 1: return string.upper() return string[0].upper() + string[1:].lower()", "def mixedcase(path): \"\"\"Removes underscores and capitalizes the neighbouring character\"\"\" words = path.split('_') return words[0] + ''.join(word.title() for word in words[1:])", "def fix_title_capitalization(title): \"\"\"Try to capitalize properly a title string.\"\"\" if re.search(\"[A-Z]\", title) and re.search(\"[a-z]\", title): return title word_list = re.split(' +', title) final = [word_list[0].capitalize()] for word in word_list[1:]: if word.upper() in COMMON_ACRONYMS: final.append(word.upper()) elif len(word) > 3: final.append(word.capitalize()) else: final.append(word.lower()) return \" \".join(final)", "def titleize(text): \"\"\"Capitalizes all the words and replaces some characters in the string to create a nicer looking title. \"\"\" if len(text) == 0: # if empty string, return it return text else: text = text.lower() # lower all char # delete redundant empty space chunks = [chunk[0].upper() + chunk[1:] for chunk in text.split(\" \") if len(chunk) >= 1] return \" \".join(chunks)", "def snake_to_camel(s: str) -> str: \"\"\"Convert string from snake case to camel case.\"\"\" fragments = s.split('_') return fragments[0] + ''.join(x.title() for x in fragments[1:])"], "rank": 12}
{"nl": "python function that takes a string and returns an int", "code": ["def try_cast_int(s): \"\"\"(str) -> int All the digits in a given string are concatenated and converted into a single number. \"\"\" try: temp = re.findall('\\d', str(s)) temp = ''.join(temp) return int(temp) except: return s", "def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)", "def str2int(string_with_int): \"\"\" Collect digits from a string \"\"\" return int(\"\".join([char for char in string_with_int if char in string.digits]) or 0)", "def prsint(string): \"\"\" Parse a string as an integer, encapsulating error handling. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/prsint_c.html :param string: String representing an integer. :type string: str :return: Integer value obtained by parsing string. :rtype: int \"\"\" string = stypes.stringToCharP(string) intval = ctypes.c_int() libspice.prsint_c(string, ctypes.byref(intval)) return intval.value", "def covstr(s): \"\"\" convert string to int or float. \"\"\" try: ret = int(s) except ValueError: ret = float(s) return ret"], "rank": 2}
{"nl": "python lookup and add idiom", "code": ["def get_obj(ref): \"\"\"Get object from string reference.\"\"\" oid = int(ref) return server.id2ref.get(oid) or server.id2obj[oid]", "def generate_id(self, obj): \"\"\"Generate unique document id for ElasticSearch.\"\"\" object_type = type(obj).__name__.lower() return '{}_{}'.format(object_type, self.get_object_id(obj))", "def _id(self): \"\"\"What this object is equal to.\"\"\" return (self.__class__, self.number_of_needles, self.needle_positions, self.left_end_needle)", "def __init__(self): \"\"\"Initializes an attribute container identifier.\"\"\" super(AttributeContainerIdentifier, self).__init__() self._identifier = id(self)", "def visit_Name(self, node): \"\"\" Get range for parameters for examples or false branching. \"\"\" return self.add(node, self.result[node.id])"], "rank": 241}
{"nl": "python elementtree delete namespace", "code": ["def remove_namespaces(root): \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\" for elem in root.getiterator(): if not hasattr(elem.tag, 'find'): continue i = elem.tag.find('}') if i >= 0: elem.tag = elem.tag[i + 1:] objectify.deannotate(root, cleanup_namespaces=True)", "def strip_xml_namespace(root): \"\"\"Strip out namespace data from an ElementTree. This function is recursive and will traverse all subnodes to the root element @param root: the root element @return: the same root element, minus namespace \"\"\" try: root.tag = root.tag.split('}')[1] except IndexError: pass for element in root.getchildren(): strip_xml_namespace(element)", "def recClearTag(element): \"\"\"Applies maspy.xml.clearTag() to the tag attribute of the \"element\" and recursively to all child elements. :param element: an :instance:`xml.etree.Element` \"\"\" children = element.getchildren() if len(children) > 0: for child in children: recClearTag(child) element.tag = clearTag(element.tag)", "def _strip_namespace(self, xml): \"\"\"strips any namespaces from an xml string\"\"\" p = re.compile(b\"xmlns=*[\\\"\\\"][^\\\"\\\"]*[\\\"\\\"]\") allmatches = p.finditer(xml) for match in allmatches: xml = xml.replace(match.group(), b\"\") return xml", "def remove_element(self, e): \"\"\"Remove element `e` from model \"\"\" if e.label is not None: self.elementdict.pop(e.label) self.elementlist.remove(e)"], "rank": 3}
{"nl": "what can you store in a python session", "code": ["def _session_set(self, key, value): \"\"\" Saves a value to session. \"\"\" self.session[self._session_key(key)] = value", "def save_session_to_file(self, sessionfile): \"\"\"Not meant to be used directly, use :meth:`Instaloader.save_session_to_file`.\"\"\" pickle.dump(requests.utils.dict_from_cookiejar(self._session.cookies), sessionfile)", "def _get_session(): \"\"\"Return (and memoize) a database session\"\"\" session = getattr(g, '_session', None) if session is None: session = g._session = db.session() return session", "def save(self): \"\"\"save the current session override, if session was saved earlier\"\"\" if self.path: self._saveState(self.path) else: self.saveAs()", "def save_session(self, sid, session, namespace=None): \"\"\"Store the user session for a client. The only difference with the :func:`socketio.Server.save_session` method is that when the ``namespace`` argument is not given the namespace associated with the class is used. \"\"\" return self.server.save_session( sid, session, namespace=namespace or self.namespace)"], "rank": 5}
{"nl": "wrapping python as a wrapper", "code": ["def Proxy(f): \"\"\"A helper to create a proxy method in a class.\"\"\" def Wrapped(self, *args): return getattr(self, f)(*args) return Wrapped", "def install(): \"\"\" Installs ScoutApm SQL Instrumentation by monkeypatching the `cursor` method of BaseDatabaseWrapper, to return a wrapper that instruments any calls going through it. \"\"\" @monkeypatch_method(BaseDatabaseWrapper) def cursor(original, self, *args, **kwargs): result = original(*args, **kwargs) return _DetailedTracingCursorWrapper(result, self) logger.debug(\"Monkey patched SQL\")", "def map_wrap(f): \"\"\"Wrap standard function to easily pass into 'map' processing. \"\"\" @functools.wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) return wrapper", "def cleanwrap(func): \"\"\" Wrapper for Zotero._cleanup \"\"\" def enc(self, *args, **kwargs): \"\"\" Send each item to _cleanup() \"\"\" return (func(self, item, **kwargs) for item in args) return enc", "def return_value(self, *args, **kwargs): \"\"\"Extracts the real value to be returned from the wrapping callable. :return: The value the double should return when called. \"\"\" self._called() return self._return_value(*args, **kwargs)"], "rank": 6}
{"nl": "configure a list of characters into a string python", "code": ["def delimited(items, character='|'): \"\"\"Returns a character delimited version of the provided list as a Python string\"\"\" return '|'.join(items) if type(items) in (list, tuple, set) else items", "def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res", "def list2string (inlist,delimit=' '): \"\"\" Converts a 1D list to a single long string for file output, using the string.join function. Usage: list2string (inlist,delimit=' ') Returns: the string created from inlist \"\"\" stringlist = [makestr(_) for _ in inlist] return string.join(stringlist,delimit)", "def to_unicode_repr( _letter ): \"\"\" helpful in situations where browser/app may recognize Unicode encoding in the \\u0b8e type syntax but not actual unicode glyph/code-point\"\"\" # Python 2-3 compatible return u\"u'\"+ u\"\".join( [ u\"\\\\u%04x\"%ord(l) for l in _letter ] ) + u\"'\"", "def join(mapping, bind, values): \"\"\" Merge all the strings. Put space between them. \"\"\" return [' '.join([six.text_type(v) for v in values if v is not None])]"], "rank": 1}
{"nl": "python list of self variables", "code": ["def vars_(self): \"\"\" Returns symbol instances corresponding to variables of the current scope. \"\"\" return [x for x in self[self.current_scope].values() if x.class_ == CLASS.var]", "def values(self): \"\"\"return a list of all state values\"\"\" values = [] for __, data in self.items(): values.append(data) return values", "def print_param_values(self_): \"\"\"Print the values of all this object's Parameters.\"\"\" self = self_.self for name,val in self.param.get_param_values(): print('%s.%s = %s' % (self.name,name,val))", "def types(self): \"\"\" Return a list of all the variable types that exist in the Variables object. \"\"\" output = set() for var in self.values(): if var.has_value(): output.update(var.types()) return list(output)", "def dict_self(self): \"\"\"Return the self object attributes not inherited as dict.\"\"\" return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}"], "rank": 1}
{"nl": "how to stop streaming data python", "code": ["def stop(self): \"\"\"Stop stream.\"\"\" if self.stream and self.stream.session.state != STATE_STOPPED: self.stream.stop()", "def close_stream(self): \"\"\" Closes the stream. Performs cleanup. \"\"\" self.keep_listening = False self.stream.stop() self.stream.close()", "def flush_on_close(self, stream): \"\"\"Flush tornado iostream write buffer and prevent further writes. Returns a future that resolves when the stream is flushed. \"\"\" assert get_thread_ident() == self.ioloop_thread_id # Prevent futher writes stream.KATCPServer_closing = True # Write empty message to get future that resolves when buffer is flushed return stream.write('\\n')", "def stop(self): \"\"\" Stops the video stream and resets the clock. \"\"\" logger.debug(\"Stopping playback\") # Stop the clock self.clock.stop() # Set plauyer status to ready self.status = READY", "def stop(self): \"\"\" Stops the playing thread and close \"\"\" with self.lock: self.halting = True self.go.clear()"], "rank": 1}
{"nl": "python create remote file ssh", "code": ["def send_file(self, local_path, remote_path, user='root', unix_mode=None): \"\"\"Upload a local file on the remote host. \"\"\" self.enable_user(user) return self.ssh_pool.send_file(user, local_path, remote_path, unix_mode=unix_mode)", "def send_dir(self, local_path, remote_path, user='root'): \"\"\"Upload a directory on the remote host. \"\"\" self.enable_user(user) return self.ssh_pool.send_dir(user, local_path, remote_path)", "def write(url, content, **args): \"\"\"Put an object into a ftps URL.\"\"\" with FTPSResource(url, **args) as resource: resource.write(content)", "def _send_file(self, filename): \"\"\" Sends a file via FTP. \"\"\" # pylint: disable=E1101 ftp = ftplib.FTP(host=self.host) ftp.login(user=self.user, passwd=self.password) ftp.set_pasv(True) ftp.storbinary(\"STOR %s\" % os.path.basename(filename), file(filename, 'rb'))", "def upload_file(token, channel_name, file_name): \"\"\" upload file to a channel \"\"\" slack = Slacker(token) slack.files.upload(file_name, channels=channel_name)"], "rank": 1}
{"nl": "get the first value in a series python", "code": ["def first(series, order_by=None): \"\"\" Returns the first value of a series. Args: series (pandas.Series): column to summarize. Kwargs: order_by: a pandas.Series or list of series (can be symbolic) to order the input series by before summarization. \"\"\" if order_by is not None: series = order_series_by(series, order_by) first_s = series.iloc[0] return first_s", "def get_X0(X): \"\"\" Return zero-th element of a one-element data container. \"\"\" if pandas_available and isinstance(X, pd.DataFrame): assert len(X) == 1 x = np.array(X.iloc[0]) else: x, = X return x", "def series_index(self, series): \"\"\" Return the integer index of *series* in this sequence. \"\"\" for idx, s in enumerate(self): if series is s: return idx raise ValueError('series not in chart data object')", "def _values(self): \"\"\"Getter for series values (flattened)\"\"\" return [ val for serie in self.series for val in serie.values if val is not None ]", "def yvals(self): \"\"\"All y values\"\"\" return [ val[1] for serie in self.series for val in serie.values if val[1] is not None ]"], "rank": 8}
{"nl": "fetch a variable from its name + python", "code": ["def _get_var_from_string(item): \"\"\" Get resource variable. \"\"\" modname, varname = _split_mod_var_names(item) if modname: mod = __import__(modname, globals(), locals(), [varname], -1) return getattr(mod, varname) else: return globals()[varname]", "def get_var(self, name): \"\"\" Returns the variable set with the given name. \"\"\" for var in self.vars: if var.name == name: return var else: raise ValueError", "def getSystemVariable(self, remote, name): \"\"\"Get single system variable from CCU / Homegear\"\"\" if self._server is not None: return self._server.getSystemVariable(remote, name)", "def get_property_by_name(pif, name): \"\"\"Get a property by name\"\"\" return next((x for x in pif.properties if x.name == name), None)", "def load_object_by_name(object_name): \"\"\"Load an object from a module by name\"\"\" mod_name, attr = object_name.rsplit('.', 1) mod = import_module(mod_name) return getattr(mod, attr)"], "rank": 1}
{"nl": "how to change dimensions of a window in python", "code": ["def resizeEvent(self, event): \"\"\"Reimplement Qt method\"\"\" if not self.isMaximized() and not self.fullscreen_flag: self.window_size = self.size() QMainWindow.resizeEvent(self, event) # To be used by the tour to be able to resize self.sig_resized.emit(event)", "def set_position(self, x, y, width, height): \"\"\"Set window top-left corner position and size\"\"\" SetWindowPos(self._hwnd, None, x, y, width, height, ctypes.c_uint(0))", "def setwinsize(self, rows, cols): \"\"\"Set the terminal window size of the child tty. \"\"\" self._winsize = (rows, cols) self.pty.set_size(cols, rows)", "def screen(self, width, height, colorDepth): \"\"\" @summary: record resize event of screen (maybe first event) @param width: {int} width of screen @param height: {int} height of screen @param colorDepth: {int} colorDepth \"\"\" screenEvent = ScreenEvent() screenEvent.width.value = width screenEvent.height.value = height screenEvent.colorDepth.value = colorDepth self.rec(screenEvent)", "def OnMove(self, event): \"\"\"Main window move event\"\"\" # Store window position in config position = self.main_window.GetScreenPositionTuple() config[\"window_position\"] = repr(position)"], "rank": 3}
{"nl": "python numpy array fix dtype", "code": ["def scale_dtype(arr, dtype): \"\"\"Convert an array from 0..1 to dtype, scaling up linearly \"\"\" max_int = np.iinfo(dtype).max return (arr * max_int).astype(dtype)", "def dict_to_numpy_array(d): \"\"\" Convert a dict of 1d array to a numpy recarray \"\"\" return fromarrays(d.values(), np.dtype([(str(k), v.dtype) for k, v in d.items()]))", "def astype(array, y): \"\"\"A functional form of the `astype` method. Args: array: The array or number to cast. y: An array or number, as the input, whose type should be that of array. Returns: An array or number with the same dtype as `y`. \"\"\" if isinstance(y, autograd.core.Node): return array.astype(numpy.array(y.value).dtype) return array.astype(numpy.array(y).dtype)", "def _convert_to_array(array_like, dtype): \"\"\" Convert Matrix attributes which are array-like or buffer to array. \"\"\" if isinstance(array_like, bytes): return np.frombuffer(array_like, dtype=dtype) return np.asarray(array_like, dtype=dtype)", "def to_int64(a): \"\"\"Return view of the recarray with all int32 cast to int64.\"\"\" # build new dtype and replace i4 --> i8 def promote_i4(typestr): if typestr[1:] == 'i4': typestr = typestr[0]+'i8' return typestr dtype = [(name, promote_i4(typestr)) for name,typestr in a.dtype.descr] return a.astype(dtype)"], "rank": 2}
{"nl": "python3 extending an empty diff results in none type", "code": ["def default_diff(latest_config, current_config): \"\"\"Determine if two revisions have actually changed.\"\"\" # Pop off the fields we don't care about: pop_no_diff_fields(latest_config, current_config) diff = DeepDiff( latest_config, current_config, ignore_order=True ) return diff", "def record_diff(old, new): \"\"\"Return a JSON-compatible structure capable turn the `new` record back into the `old` record. The parameters must be structures compatible with json.dumps *or* strings compatible with json.loads. Note that by design, `old == record_patch(new, record_diff(old, new))`\"\"\" old, new = _norm_json_params(old, new) return json_delta.diff(new, old, verbose=False)", "def is_changed(): \"\"\" Checks if current project has any noncommited changes. \"\"\" executed, changed_lines = execute_git('status --porcelain', output=False) merge_not_finished = mod_path.exists('.git/MERGE_HEAD') return changed_lines.strip() or merge_not_finished", "def diff(file_, imports): \"\"\"Display the difference between modules in a file and imported modules.\"\"\" modules_not_imported = compare_modules(file_, imports) logging.info(\"The following modules are in {} but do not seem to be imported: \" \"{}\".format(file_, \", \".join(x for x in modules_not_imported)))", "def files_changed(): \"\"\" Return the list of file changed in the current branch compared to `master` \"\"\" with chdir(get_root()): result = run_command('git diff --name-only master...', capture='out') changed_files = result.stdout.splitlines() # Remove empty lines return [f for f in changed_files if f]"], "rank": 1}
{"nl": "windows cmd python display width", "code": ["def size(): \"\"\"Determines the height and width of the console window Returns: tuple of int: The height in lines, then width in characters \"\"\" try: assert os != 'nt' and sys.stdout.isatty() rows, columns = os.popen('stty size', 'r').read().split() except (AssertionError, AttributeError, ValueError): # in case of failure, use dimensions of a full screen 13\" laptop rows, columns = DEFAULT_HEIGHT, DEFAULT_WIDTH return int(rows), int(columns)", "def get_size(self): \"\"\"see doc in Term class\"\"\" self.curses.setupterm() return self.curses.tigetnum('cols'), self.curses.tigetnum('lines')", "def get_width(): \"\"\"Get terminal width\"\"\" # Get terminal size ws = struct.pack(\"HHHH\", 0, 0, 0, 0) ws = fcntl.ioctl(sys.stdout.fileno(), termios.TIOCGWINSZ, ws) lines, columns, x, y = struct.unpack(\"HHHH\", ws) width = min(columns * 39 // 40, columns - 2) return width", "def get_terminal_width(): \"\"\" -> #int width of the terminal window \"\"\" # http://www.brandonrubin.me/2014/03/18/python-snippet-get-terminal-width/ command = ['tput', 'cols'] try: width = int(subprocess.check_output(command)) except OSError as e: print( \"Invalid Command '{0}': exit status ({1})\".format( command[0], e.errno)) except subprocess.CalledProcessError as e: print( \"'{0}' returned non-zero exit status: ({1})\".format( command, e.returncode)) else: return width", "def display(self): \"\"\" Get screen width and height \"\"\" w, h = self.session.window_size() return Display(w*self.scale, h*self.scale)"], "rank": 3}
{"nl": "top values in list python", "code": ["def top(self, topn=10): \"\"\" Get a list of the top ``topn`` features in this :class:`.Feature`\\. Examples -------- .. code-block:: python >>> myFeature = Feature([('the', 2), ('pine', 1), ('trapezoid', 5)]) >>> myFeature.top(1) [('trapezoid', 5)] Parameters ---------- topn : int Returns ------- list \"\"\" return [self[i] for i in argsort(list(zip(*self))[1])[::-1][:topn]]", "def get_top_priority(self): \"\"\"Pops the element that has the top (smallest) priority. :returns: element with the top (smallest) priority. :raises: IndexError -- Priority queue is empty. \"\"\" if self.is_empty(): raise IndexError(\"Priority queue is empty.\") _, _, element = heapq.heappop(self.pq) if element in self.element_finder: del self.element_finder[element] return element", "def table_top_abs(self): \"\"\"Returns the absolute position of table top\"\"\" table_height = np.array([0, 0, self.table_full_size[2]]) return string_to_array(self.floor.get(\"pos\")) + table_height", "def _top(self): \"\"\" g \"\"\" # Goto top of the list self.top.body.focus_position = 2 if self.compact is False else 0 self.top.keypress(self.size, \"\")", "def heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt"], "rank": 1}
{"nl": "md5 value of a file python", "code": ["def get_file_md5sum(path): \"\"\"Calculate the MD5 hash for a file.\"\"\" with open(path, 'rb') as fh: h = str(hashlib.md5(fh.read()).hexdigest()) return h", "def filehash(path): \"\"\"Make an MD5 hash of a file, ignoring any differences in line ending characters.\"\"\" with open(path, \"rU\") as f: return md5(py3compat.str_to_bytes(f.read())).hexdigest()", "def file_md5sum(filename): \"\"\" :param filename: The filename of the file to process :returns: The MD5 hash of the file \"\"\" hash_md5 = hashlib.md5() with open(filename, 'rb') as f: for chunk in iter(lambda: f.read(1024 * 4), b''): hash_md5.update(chunk) return hash_md5.hexdigest()", "def get_md5_for_file(file): \"\"\"Get the md5 hash for a file. :param file: the file to get the md5 hash for \"\"\" md5 = hashlib.md5() while True: data = file.read(md5.block_size) if not data: break md5.update(data) return md5.hexdigest()", "def md5_hash_file(fh): \"\"\"Return the md5 hash of the given file-object\"\"\" md5 = hashlib.md5() while True: data = fh.read(8192) if not data: break md5.update(data) return md5.hexdigest()"], "rank": 1}
{"nl": "code to create folders in python", "code": ["def create_dir_rec(path: Path): \"\"\" Create a folder recursive. :param path: path :type path: ~pathlib.Path \"\"\" if not path.exists(): Path.mkdir(path, parents=True, exist_ok=True)", "def makedirs(directory): \"\"\" Resursively create a named directory. \"\"\" parent = os.path.dirname(os.path.abspath(directory)) if not os.path.exists(parent): makedirs(parent) os.mkdir(directory)", "def makedirs(path): \"\"\" Create directories if they do not exist, otherwise do nothing. Return path for convenience \"\"\" if not os.path.isdir(path): os.makedirs(path) return path", "def safe_mkdir_for(path, clean=False): \"\"\"Ensure that the parent directory for a file is present. If it's not there, create it. If it is, no-op. \"\"\" safe_mkdir(os.path.dirname(path), clean=clean)", "def mkdir(dir, enter): \"\"\"Create directory with template for topic of the current environment \"\"\" if not os.path.exists(dir): os.makedirs(dir)"], "rank": 5}
{"nl": "python check if certain length of input equals something", "code": ["def check_length(value, length): \"\"\" Checks length of value @param value: value to check @type value: C{str} @param length: length checking for @type length: C{int} @return: None when check successful @raise ValueError: check failed \"\"\" _length = len(value) if _length != length: raise ValueError(\"length must be %d, not %d\" % \\ (length, _length))", "def check_lengths(*arrays): \"\"\" tool to ensure input and output data have the same number of samples Parameters ---------- *arrays : iterable of arrays to be checked Returns ------- None \"\"\" lengths = [len(array) for array in arrays] if len(np.unique(lengths)) > 1: raise ValueError('Inconsistent data lengths: {}'.format(lengths))", "def check_type_and_size_of_param_list(param_list, expected_length): \"\"\" Ensure that param_list is a list with the expected length. Raises a helpful ValueError if this is not the case. \"\"\" try: assert isinstance(param_list, list) assert len(param_list) == expected_length except AssertionError: msg = \"param_list must be a list containing {} elements.\" raise ValueError(msg.format(expected_length)) return None", "def input_validate_str(string, name, max_len=None, exact_len=None): \"\"\" Input validation for strings. \"\"\" if type(string) is not str: raise pyhsm.exception.YHSM_WrongInputType(name, str, type(string)) if max_len != None and len(string) > max_len: raise pyhsm.exception.YHSM_InputTooLong(name, max_len, len(string)) if exact_len != None and len(string) != exact_len: raise pyhsm.exception.YHSM_WrongInputSize(name, exact_len, len(string)) return string", "def check_X_y(X, y): \"\"\" tool to ensure input and output data have the same number of samples Parameters ---------- X : array-like y : array-like Returns ------- None \"\"\" if len(X) != len(y): raise ValueError('Inconsistent input and output data shapes. '\\ 'found X: {} and y: {}'.format(X.shape, y.shape))"], "rank": 2}
{"nl": "python iterate chunks of string", "code": ["def generate_chunks(string, num_chars): \"\"\"Yield num_chars-character chunks from string.\"\"\" for start in range(0, len(string), num_chars): yield string[start:start+num_chars]", "def get_chunks(source, chunk_len): \"\"\" Returns an iterator over 'chunk_len' chunks of 'source' \"\"\" return (source[i: i + chunk_len] for i in range(0, len(source), chunk_len))", "def chunks(iterable, chunk): \"\"\"Yield successive n-sized chunks from an iterable.\"\"\" for i in range(0, len(iterable), chunk): yield iterable[i:i + chunk]", "def iterate_chunks(file, chunk_size): \"\"\" Iterate chunks of size chunk_size from a file-like object \"\"\" chunk = file.read(chunk_size) while chunk: yield chunk chunk = file.read(chunk_size)", "def get_iter_string_reader(stdin): \"\"\" return an iterator that returns a chunk of a string every time it is called. notice that even though bufsize_type might be line buffered, we're not doing any line buffering here. that's because our StreamBufferer handles all buffering. we just need to return a reasonable-sized chunk. \"\"\" bufsize = 1024 iter_str = (stdin[i:i + bufsize] for i in range(0, len(stdin), bufsize)) return get_iter_chunk_reader(iter_str)"], "rank": 1}
{"nl": "how to check datatype in data frame in python", "code": ["def _validate_pos(df): \"\"\"Validates the returned positional object \"\"\" assert isinstance(df, pd.DataFrame) assert [\"seqname\", \"position\", \"strand\"] == df.columns.tolist() assert df.position.dtype == np.dtype(\"int64\") assert df.strand.dtype == np.dtype(\"O\") assert df.seqname.dtype == np.dtype(\"O\") return df", "def _maybe_pandas_data(data, feature_names, feature_types): \"\"\" Extract internal data from pd.DataFrame for DMatrix data \"\"\" if not isinstance(data, DataFrame): return data, feature_names, feature_types data_dtypes = data.dtypes if not all(dtype.name in PANDAS_DTYPE_MAPPER for dtype in data_dtypes): bad_fields = [data.columns[i] for i, dtype in enumerate(data_dtypes) if dtype.name not in PANDAS_DTYPE_MAPPER] msg = \"\"\"DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields \"\"\" raise ValueError(msg + ', '.join(bad_fields)) if feature_names is None: if isinstance(data.columns, MultiIndex): feature_names = [ ' '.join([str(x) for x in i]) for i in data.columns ] else: feature_names = data.columns.format() if feature_types is None: feature_types = [PANDAS_DTYPE_MAPPER[dtype.name] for dtype in data_dtypes] data = data.values.astype('float') return data, feature_names, feature_types", "def is_dataframe(obj): \"\"\" Returns True if the given object is a Pandas Data Frame. Parameters ---------- obj: instance The object to test whether or not is a Pandas DataFrame. \"\"\" try: # This is the best method of type checking from pandas import DataFrame return isinstance(obj, DataFrame) except ImportError: # Pandas is not a dependency, so this is scary return obj.__class__.__name__ == \"DataFrame\"", "def make_kind_check(python_types, numpy_kind): \"\"\" Make a function that checks whether a scalar or array is of a given kind (e.g. float, int, datetime, timedelta). \"\"\" def check(value): if hasattr(value, 'dtype'): return value.dtype.kind == numpy_kind return isinstance(value, python_types) return check", "def _to_corrected_pandas_type(dt): \"\"\" When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong. This method gets the corrected data type for Pandas if that type may be inferred uncorrectly. \"\"\" import numpy as np if type(dt) == ByteType: return np.int8 elif type(dt) == ShortType: return np.int16 elif type(dt) == IntegerType: return np.int32 elif type(dt) == FloatType: return np.float32 else: return None"], "rank": 1}
{"nl": "python get first object in a list", "code": ["def head(self) -> Any: \"\"\"Retrive first element in List.\"\"\" lambda_list = self._get_value() return lambda_list(lambda head, _: head)", "def findfirst(f, coll): \"\"\"Return first occurrence matching f, otherwise None\"\"\" result = list(dropwhile(f, coll)) return result[0] if result else None", "def fetchallfirstvalues(self, sql: str, *args) -> List[Any]: \"\"\"Executes SQL; returns list of first values of each row.\"\"\" rows = self.fetchall(sql, *args) return [row[0] for row in rows]", "def find_first_in_list(txt: str, str_list: [str]) -> int: # type: ignore \"\"\" Returns the index of the earliest occurence of an item from a list in a string Ex: find_first_in_list('foobar', ['bar', 'fin']) -> 3 \"\"\" start = len(txt) + 1 for item in str_list: if start > txt.find(item) > -1: start = txt.find(item) return start if len(txt) + 1 > start > -1 else -1", "def get_object_or_child_by_type(self, *types): \"\"\" Get object if child already been read or get child. Use this method for fast access to objects in case of static configurations. :param types: requested object types. :return: all children of the specified types. \"\"\" objects = self.get_objects_or_children_by_type(*types) return objects[0] if any(objects) else None"], "rank": 2}
{"nl": "python extract all numbers in a string", "code": ["def get_numbers(s): \"\"\"Extracts all integers from a string an return them in a list\"\"\" result = map(int, re.findall(r'[0-9]+', unicode(s))) return result + [1] * (2 - len(result))", "def str2int(string_with_int): \"\"\" Collect digits from a string \"\"\" return int(\"\".join([char for char in string_with_int if char in string.digits]) or 0)", "def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)", "def find_number(regex, s): \"\"\"Find a number using a given regular expression. If the string cannot be found, returns None. The regex should contain one matching group, as only the result of the first group is returned. The group should only contain numeric characters ([0-9]+). s - The string to search. regex - A string containing the regular expression. Returns an integer or None. \"\"\" result = find_string(regex, s) if result is None: return None return int(result)", "def numeric_part(s): \"\"\"Returns the leading numeric part of a string. >>> numeric_part(\"20-alpha\") 20 >>> numeric_part(\"foo\") >>> numeric_part(\"16b\") 16 \"\"\" m = re_numeric_part.match(s) if m: return int(m.group(1)) return None"], "rank": 1}
{"nl": "python unittest assert not raises", "code": ["def assert_is_not(expected, actual, message=None, extra=None): \"\"\"Raises an AssertionError if expected is actual.\"\"\" assert expected is not actual, _assert_fail_message( message, expected, actual, \"is\", extra )", "def __call__(self, actual_value, expect): \"\"\"Main entry point for assertions (called by the wrapper). expect is a function the wrapper class uses to assert a given match. \"\"\" self._expect = expect if self.expected_value is NO_ARG: return self.asserts(actual_value) return self.asserts(actual_value, self.expected_value)", "def raises_regex(self, expected_exception, expected_regexp): \"\"\" Ensures preceding predicates (specifically, :meth:`called_with()`) result in *expected_exception* being raised, and the string representation of *expected_exception* must match regular expression *expected_regexp*. \"\"\" return unittest_case.assertRaisesRegexp(expected_exception, expected_regexp, self._orig_subject, *self._args, **self._kwargs)", "def assert_any_call(self, *args, **kwargs): \"\"\"assert the mock has been called with the specified arguments. The assert passes if the mock has *ever* been called, unlike `assert_called_with` and `assert_called_once_with` that only pass if the call is the most recent one.\"\"\" kall = call(*args, **kwargs) if kall not in self.call_args_list: expected_string = self._format_mock_call_signature(args, kwargs) raise AssertionError( '%s call not found' % expected_string )", "def assert_raises(ex_type, func, *args, **kwargs): r\"\"\" Checks that a function raises an error when given specific arguments. Args: ex_type (Exception): exception type func (callable): live python function CommandLine: python -m utool.util_assert assert_raises --show Example: >>> # ENABLE_DOCTEST >>> from utool.util_assert import * # NOQA >>> import utool as ut >>> ex_type = AssertionError >>> func = len >>> # Check that this raises an error when something else does not >>> assert_raises(ex_type, assert_raises, ex_type, func, []) >>> # Check this does not raise an error when something else does >>> assert_raises(ValueError, [].index, 0) \"\"\" try: func(*args, **kwargs) except Exception as ex: assert isinstance(ex, ex_type), ( 'Raised %r but type should have been %r' % (ex, ex_type)) return True else: raise AssertionError('No error was raised')"], "rank": 1}
{"nl": "python zmq check if connected", "code": ["def start(self, test_connection=True): \"\"\"Starts connection to server if not existent. NO-OP if connection is already established. Makes ping-pong test as well if desired. \"\"\" if self._context is None: self._logger.debug('Starting Client') self._context = zmq.Context() self._poll = zmq.Poller() self._start_socket() if test_connection: self.test_ping()", "def is_connected(self): \"\"\" Return true if the socket managed by this connection is connected :rtype: bool \"\"\" try: return self.socket is not None and self.socket.getsockname()[1] != 0 and BaseTransport.is_connected(self) except socket.error: return False", "def is_port_open(port, host=\"127.0.0.1\"): \"\"\" Check if a port is open :param port: :param host: :return bool: \"\"\" s = socket.socket(socket.AF_INET, socket.SOCK_STREAM) try: s.connect((host, int(port))) s.shutdown(2) return True except Exception as e: return False", "def on_stop(self): \"\"\" stop publisher \"\"\" LOGGER.debug(\"zeromq.Publisher.on_stop\") self.zmqsocket.close() self.zmqcontext.destroy()", "def _connect(self, servers): \"\"\" connect to the given server, e.g.: \\\\connect localhost:4200 \"\"\" self._do_connect(servers.split(' ')) self._verify_connection(verbose=True)"], "rank": 1}
{"nl": "using string to generate datetime date in python", "code": ["def string_to_date(value): \"\"\" Return a Python date that corresponds to the specified string representation. @param value: string representation of a date. @return: an instance ``datetime.datetime`` represented by the string. \"\"\" if isinstance(value, datetime.date): return value return dateutil.parser.parse(value).date()", "def parse_datetime(dt_str): \"\"\"Parse datetime.\"\"\" date_format = \"%Y-%m-%dT%H:%M:%S %z\" dt_str = dt_str.replace(\"Z\", \" +0000\") return datetime.datetime.strptime(dt_str, date_format)", "def convert_date(date): \"\"\"Convert string to datetime object.\"\"\" date = convert_month(date, shorten=False) clean_string = convert_string(date) return datetime.strptime(clean_string, DATE_FMT.replace('-',''))", "def get_date(date): \"\"\" Get the date from a value that could be a date object or a string. :param date: The date object or string. :returns: The date object. \"\"\" if type(date) is str: return datetime.strptime(date, '%Y-%m-%d').date() else: return date", "def _datetime_to_date(arg): \"\"\" convert datetime/str to date :param arg: :return: \"\"\" _arg = parse(arg) if isinstance(_arg, datetime.datetime): _arg = _arg.date() return _arg"], "rank": 12}
{"nl": "python list comprehension flatten", "code": ["def iflatten(L): \"\"\"Iterative flatten.\"\"\" for sublist in L: if hasattr(sublist, '__iter__'): for item in iflatten(sublist): yield item else: yield sublist", "def flatten(nested, containers=(list, tuple)): \"\"\" Flatten a nested list by yielding its scalar items. \"\"\" for item in nested: if hasattr(item, \"next\") or isinstance(item, containers): for subitem in flatten(item): yield subitem else: yield item", "def flatten(l): \"\"\"Flatten a nested list.\"\"\" return sum(map(flatten, l), []) \\ if isinstance(l, list) or isinstance(l, tuple) else [l]", "def flatten_list(l: List[list]) -> list: \"\"\" takes a list of lists, l and returns a flat list \"\"\" return [v for inner_l in l for v in inner_l]", "def flatten(lis): \"\"\"Given a list, possibly nested to any level, return it flattened.\"\"\" new_lis = [] for item in lis: if isinstance(item, collections.Sequence) and not isinstance(item, basestring): new_lis.extend(flatten(item)) else: new_lis.append(item) return new_lis"], "rank": 4}
{"nl": "python code remove duplicate labels", "code": ["def get_labels(labels): \"\"\"Create unique labels.\"\"\" label_u = unique_labels(labels) label_u_line = [i + \"_line\" for i in label_u] return label_u, label_u_line", "def remove_dups(seq): \"\"\"remove duplicates from a sequence, preserving order\"\"\" seen = set() seen_add = seen.add return [x for x in seq if not (x in seen or seen_add(x))]", "def _remove_duplicate_files(xs): \"\"\"Remove files specified multiple times in a list. \"\"\" seen = set([]) out = [] for x in xs: if x[\"path\"] not in seen: out.append(x) seen.add(x[\"path\"]) return out", "def _check_samples_nodups(fnames): \"\"\"Ensure a set of input VCFs do not have duplicate samples. \"\"\" counts = defaultdict(int) for f in fnames: for s in get_samples(f): counts[s] += 1 duplicates = [s for s, c in counts.items() if c > 1] if duplicates: raise ValueError(\"Duplicate samples found in inputs %s: %s\" % (duplicates, fnames))", "def _remove_duplicates(objects): \"\"\"Removes duplicate objects. http://www.peterbe.com/plog/uniqifiers-benchmark. \"\"\" seen, uniq = set(), [] for obj in objects: obj_id = id(obj) if obj_id in seen: continue seen.add(obj_id) uniq.append(obj) return uniq"], "rank": 1}
{"nl": "heatmap python set the axis limits", "code": ["def set_mlimits(self, row, column, min=None, max=None): \"\"\"Set limits for the point meta (colormap). Point meta values outside this range will be clipped. :param min: value for start of the colormap. :param max: value for end of the colormap. \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_mlimits(min, max)", "def _set_axis_limits(self, which, lims, d, scale, reverse=False): \"\"\"Private method for setting axis limits. Sets the axis limits on each axis for an individual plot. Args: which (str): The indicator of which part of the plots to adjust. This currently handles `x` and `y`. lims (len-2 list of floats): The limits for the axis. d (float): Amount to increment by between the limits. scale (str): Scale of the axis. Either `log` or `lin`. reverse (bool, optional): If True, reverse the axis tick marks. Default is False. \"\"\" setattr(self.limits, which + 'lims', lims) setattr(self.limits, 'd' + which, d) setattr(self.limits, which + 'scale', scale) if reverse: setattr(self.limits, 'reverse_' + which + '_axis', True) return", "def set_xlimits_widgets(self, set_min=True, set_max=True): \"\"\"Populate axis limits GUI with current plot values.\"\"\" xmin, xmax = self.tab_plot.ax.get_xlim() if set_min: self.w.x_lo.set_text('{0}'.format(xmin)) if set_max: self.w.x_hi.set_text('{0}'.format(xmax))", "def set_xlimits(self, min=None, max=None): \"\"\"Set limits for the x-axis. :param min: minimum value to be displayed. If None, it will be calculated. :param max: maximum value to be displayed. If None, it will be calculated. \"\"\" self.limits['xmin'] = min self.limits['xmax'] = max", "def set_xlimits(self, row, column, min=None, max=None): \"\"\"Set x-axis limits of a subplot. :param row,column: specify the subplot. :param min: minimal axis value :param max: maximum axis value \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_xlimits(min, max)"], "rank": 1}
{"nl": "rotate between items in a list python", "code": ["def iprotate(l, steps=1): r\"\"\"Like rotate, but modifies `l` in-place. >>> l = [1,2,3] >>> iprotate(l) is l True >>> l [2, 3, 1] >>> iprotate(iprotate(l, 2), -3) [1, 2, 3] \"\"\" if len(l): steps %= len(l) if steps: firstPart = l[:steps] del l[:steps] l.extend(firstPart) return l", "def rotateImage(image, angle): \"\"\" rotates a 2d array to a multiple of 90 deg. 0 = default 1 = 90 deg. cw 2 = 180 deg. 3 = 90 deg. ccw \"\"\" image = [list(row) for row in image] for n in range(angle % 4): image = list(zip(*image[::-1])) return image", "def _rotate(n, x, y, rx, ry): \"\"\"Rotate and flip a quadrant appropriately Based on the implementation here: https://en.wikipedia.org/w/index.php?title=Hilbert_curve&oldid=797332503 \"\"\" if ry == 0: if rx == 1: x = n - 1 - x y = n - 1 - y return y, x return x, y", "def insort_no_dup(lst, item): \"\"\" If item is not in lst, add item to list at its sorted position \"\"\" import bisect ix = bisect.bisect_left(lst, item) if lst[ix] != item: lst[ix:ix] = [item]", "def _reshuffle(mat, shape): \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\" return np.reshape( np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)), (shape[3] * shape[1], shape[0] * shape[2]))"], "rank": 1}
{"nl": "how to exclude item from index python", "code": ["def remove_item(self, item): \"\"\" Remove (and un-index) an object :param item: object to remove :type item: alignak.objects.item.Item :return: None \"\"\" self.unindex_item(item) self.items.pop(item.uuid, None)", "def _remove_from_index(index, obj): \"\"\"Removes object ``obj`` from the ``index``.\"\"\" try: index.value_map[indexed_value(index, obj)].remove(obj.id) except KeyError: pass", "def delete_index(index): \"\"\"Delete index entirely (removes all documents and mapping).\"\"\" logger.info(\"Deleting search index: '%s'\", index) client = get_client() return client.indices.delete(index=index)", "def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def clear_es(): \"\"\"Clear all indexes in the es core\"\"\" # TODO: should receive a catalog slug. ESHypermap.es.indices.delete(ESHypermap.index_name, ignore=[400, 404]) LOGGER.debug('Elasticsearch: Index cleared')"], "rank": 2}
{"nl": "remove punctuation and stop words python nltk", "code": ["def wordify(text): \"\"\"Generate a list of words given text, removing punctuation. Parameters ---------- text : unicode A piece of english text. Returns ------- words : list List of words. \"\"\" stopset = set(nltk.corpus.stopwords.words('english')) tokens = nltk.WordPunctTokenizer().tokenize(text) return [w for w in tokens if w not in stopset]", "def preprocess_french(trans, fr_nlp, remove_brackets_content=True): \"\"\" Takes a list of sentences in french and preprocesses them.\"\"\" if remove_brackets_content: trans = pangloss.remove_content_in_brackets(trans, \"[]\") # Not sure why I have to split and rejoin, but that fixes a Spacy token # error. trans = fr_nlp(\" \".join(trans.split()[:])) #trans = fr_nlp(trans) trans = \" \".join([token.lower_ for token in trans if not token.is_punct]) return trans", "def _clean_str(self, s): \"\"\" Returns a lowercase string with punctuation and bad chars removed :param s: string to clean \"\"\" return s.translate(str.maketrans('', '', punctuation)).replace('\\u200b', \" \").strip().lower()", "def detokenize(s): \"\"\" Detokenize a string by removing spaces before punctuation.\"\"\" print(s) s = re.sub(\"\\s+([;:,\\.\\?!])\", \"\\\\1\", s) s = re.sub(\"\\s+(n't)\", \"\\\\1\", s) return s", "def remove_punctuation(text, exceptions=[]): \"\"\" Return a string with punctuation removed. Parameters: text (str): The text to remove punctuation from. exceptions (list): List of symbols to keep in the given text. Return: str: The input text without the punctuation. \"\"\" all_but = [ r'\\w', r'\\s' ] all_but.extend(exceptions) pattern = '[^{}]'.format(''.join(all_but)) return re.sub(pattern, '', text)"], "rank": 1}
{"nl": "strip spaces from columns in python", "code": ["def strip_columns(tab): \"\"\"Strip whitespace from string columns.\"\"\" for colname in tab.colnames: if tab[colname].dtype.kind in ['S', 'U']: tab[colname] = np.core.defchararray.strip(tab[colname])", "def columnclean(column): \"\"\" Modifies column header format to be importable into a database :param column: raw column header :return: cleanedcolumn: reformatted column header \"\"\" cleanedcolumn = str(column) \\ .replace('%', 'percent') \\ .replace('(', '_') \\ .replace(')', '') \\ .replace('As', 'Adenosines') \\ .replace('Cs', 'Cytosines') \\ .replace('Gs', 'Guanines') \\ .replace('Ts', 'Thymines') \\ .replace('Ns', 'Unknowns') \\ .replace('index', 'adapterIndex') return cleanedcolumn", "def normalize_column_names(df): r\"\"\" Clean up whitespace in column names. See better version at `pugnlp.clean_columns` >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['Hello World', 'not here']) >>> normalize_column_names(df) ['hello_world', 'not_here'] \"\"\" columns = df.columns if hasattr(df, 'columns') else df columns = [c.lower().replace(' ', '_') for c in columns] return columns", "def clean_column_names(df: DataFrame) -> DataFrame: \"\"\" Strip the whitespace from all column names in the given DataFrame and return the result. \"\"\" f = df.copy() f.columns = [col.strip() for col in f.columns] return f", "def format_header_cell(val): \"\"\" Formats given header column. This involves changing '_Px_' to '(', '_xP_' to ')' and all other '_' to spaces. \"\"\" return re.sub('_', ' ', re.sub(r'(_Px_)', '(', re.sub(r'(_xP_)', ')', str(val) )))"], "rank": 1}
{"nl": "python datetime get last month number", "code": ["def get_last_day_of_month(t: datetime) -> int: \"\"\" Returns day number of the last day of the month :param t: datetime :return: int \"\"\" tn = t + timedelta(days=32) tn = datetime(year=tn.year, month=tn.month, day=1) tt = tn - timedelta(hours=1) return tt.day", "def last_day(year=_year, month=_month): \"\"\" get the current month's last day :param year: default to current year :param month: default to current month :return: month's last day \"\"\" last_day = calendar.monthrange(year, month)[1] return datetime.date(year=year, month=month, day=last_day)", "def get_last_weekday_in_month(year, month, weekday): \"\"\"Get the last weekday in a given month. e.g: >>> # the last monday in Jan 2013 >>> Calendar.get_last_weekday_in_month(2013, 1, MON) datetime.date(2013, 1, 28) \"\"\" day = date(year, month, monthrange(year, month)[1]) while True: if day.weekday() == weekday: break day = day - timedelta(days=1) return day", "def last_month(): \"\"\" Return start and end date of this month. \"\"\" since = TODAY + delta(day=1, months=-1) until = since + delta(months=1) return Date(since), Date(until)", "def get_previous_month(self): \"\"\"Returns date range for the previous full month.\"\"\" end = utils.get_month_start() - relativedelta(days=1) end = utils.to_datetime(end) start = utils.get_month_start(end) return start, end"], "rank": 1}
{"nl": "python assertassert no description", "code": ["def assert_is_not(expected, actual, message=None, extra=None): \"\"\"Raises an AssertionError if expected is actual.\"\"\" assert expected is not actual, _assert_fail_message( message, expected, actual, \"is\", extra )", "def assert_error(text, check, n=1): \"\"\"Assert that text has n errors of type check.\"\"\" assert_error.description = \"No {} error for '{}'\".format(check, text) assert(check in [error[0] for error in lint(text)])", "def __call__(self, actual_value, expect): \"\"\"Main entry point for assertions (called by the wrapper). expect is a function the wrapper class uses to assert a given match. \"\"\" self._expect = expect if self.expected_value is NO_ARG: return self.asserts(actual_value) return self.asserts(actual_value, self.expected_value)", "def assert_raises(ex_type, func, *args, **kwargs): r\"\"\" Checks that a function raises an error when given specific arguments. Args: ex_type (Exception): exception type func (callable): live python function CommandLine: python -m utool.util_assert assert_raises --show Example: >>> # ENABLE_DOCTEST >>> from utool.util_assert import * # NOQA >>> import utool as ut >>> ex_type = AssertionError >>> func = len >>> # Check that this raises an error when something else does not >>> assert_raises(ex_type, assert_raises, ex_type, func, []) >>> # Check this does not raise an error when something else does >>> assert_raises(ValueError, [].index, 0) \"\"\" try: func(*args, **kwargs) except Exception as ex: assert isinstance(ex, ex_type), ( 'Raised %r but type should have been %r' % (ex, ex_type)) return True else: raise AssertionError('No error was raised')", "def assert_any_call(self, *args, **kwargs): \"\"\"assert the mock has been called with the specified arguments. The assert passes if the mock has *ever* been called, unlike `assert_called_with` and `assert_called_once_with` that only pass if the call is the most recent one.\"\"\" kall = call(*args, **kwargs) if kall not in self.call_args_list: expected_string = self._format_mock_call_signature(args, kwargs) raise AssertionError( '%s call not found' % expected_string )"], "rank": 1}
{"nl": "python program calculating angle from two points", "code": ["def angle(x0, y0, x1, y1): \"\"\" Returns the angle between two points. \"\"\" return degrees(atan2(y1-y0, x1-x0))", "def angle(x, y): \"\"\"Return the angle between vectors a and b in degrees.\"\"\" return arccos(dot(x, y)/(norm(x)*norm(y)))*180./pi", "def vec_angle(a, b): \"\"\" Calculate angle between two vectors \"\"\" cosang = np.dot(a, b) sinang = fast_norm(np.cross(a, b)) return np.arctan2(sinang, cosang)", "def angle_between_vectors(x, y): \"\"\" Compute the angle between vector x and y \"\"\" dp = dot_product(x, y) if dp == 0: return 0 xm = magnitude(x) ym = magnitude(y) return math.acos(dp / (xm*ym)) * (180. / math.pi)", "def angle_v2_rad(vec_a, vec_b): \"\"\"Returns angle between vec_a and vec_b in range [0, PI]. This does not distinguish if a is left of or right of b. \"\"\" # cos(x) = A * B / |A| * |B| return math.acos(vec_a.dot(vec_b) / (vec_a.length() * vec_b.length()))"], "rank": 4}
{"nl": "python split iterable into batches", "code": ["def ibatch(iterable, size): \"\"\"Yield a series of batches from iterable, each size elements long.\"\"\" source = iter(iterable) while True: batch = itertools.islice(source, size) yield itertools.chain([next(batch)], batch)", "def batch(input_iter, batch_size=32): \"\"\"Batches data from an iterator that returns single items at a time.\"\"\" input_iter = iter(input_iter) next_ = list(itertools.islice(input_iter, batch_size)) while next_: yield next_ next_ = list(itertools.islice(input_iter, batch_size))", "def chunks(iterable, size=1): \"\"\"Splits iterator in chunks.\"\"\" iterator = iter(iterable) for element in iterator: yield chain([element], islice(iterator, size - 1))", "def _split_batches(self, data, batch_size): \"\"\"Yield successive n-sized chunks from l.\"\"\" for i in range(0, len(data), batch_size): yield data[i : i + batch_size]", "def partition(a, sz): \"\"\"splits iterables a in equal parts of size sz\"\"\" return [a[i:i+sz] for i in range(0, len(a), sz)]"], "rank": 1}
{"nl": "easy python decompiler invalid pys file", "code": ["def disassemble_file(filename, outstream=None): \"\"\" disassemble Python byte-code file (.pyc) If given a Python source file (\".py\") file, we'll try to find the corresponding compiled object. \"\"\" filename = check_object_path(filename) (version, timestamp, magic_int, co, is_pypy, source_size) = load_module(filename) if type(co) == list: for con in co: disco(version, con, outstream) else: disco(version, co, outstream, is_pypy=is_pypy) co = None", "def decompress(f): \"\"\"Decompress a Plan 9 image file. Assumes f is already cued past the initial 'compressed\\n' string. \"\"\" r = meta(f.read(60)) return r, decomprest(f, r[4])", "def make_code_from_py(filename): \"\"\"Get source from `filename` and make a code object of it.\"\"\" # Open the source file. try: source_file = open_source(filename) except IOError: raise NoSource(\"No file to run: %r\" % filename) try: source = source_file.read() finally: source_file.close() # We have the source. `compile` still needs the last line to be clean, # so make sure it is, then compile a code object from it. if not source or source[-1] != '\\n': source += '\\n' code = compile(source, filename, \"exec\") return code", "def parse(filename): \"\"\"Parse ASDL from the given file and return a Module node describing it.\"\"\" with open(filename) as f: parser = ASDLParser() return parser.parse(f.read())", "def open(name=None, fileobj=None, closefd=True): \"\"\" Use all decompressor possible to make the stream \"\"\" return Guesser().open(name=name, fileobj=fileobj, closefd=closefd)"], "rank": 1}
{"nl": "close stdin subprocess python", "code": ["def _finish(self): \"\"\" Closes and waits for subprocess to exit. \"\"\" if self._process.returncode is None: self._process.stdin.flush() self._process.stdin.close() self._process.wait() self.closed = True", "def correspond(text): \"\"\"Communicate with the child process without closing stdin.\"\"\" subproc.stdin.write(text) subproc.stdin.flush() return drain()", "def safe_exit(output): \"\"\"exit without breaking pipes.\"\"\" try: sys.stdout.write(output) sys.stdout.flush() except IOError: pass", "def send(self, data): \"\"\" Send data to the child process through. \"\"\" self.stdin.write(data) self.stdin.flush()", "def close(self): \"\"\"Close port.\"\"\" os.close(self.in_d) os.close(self.out_d)"], "rank": 1}
{"nl": "remove a value from all keys in a dictionary python", "code": ["def _delete_keys(dct, keys): \"\"\"Returns a copy of dct without `keys` keys \"\"\" c = deepcopy(dct) assert isinstance(keys, list) for k in keys: c.pop(k) return c", "def rm_keys_from_dict(d, keys): \"\"\" Given a dictionary and a key list, remove any data in the dictionary with the given keys. :param dict d: Metadata :param list keys: Keys to be removed :return dict d: Metadata \"\"\" # Loop for each key given for key in keys: # Is the key in the dictionary? if key in d: try: d.pop(key, None) except KeyError: # Not concerned with an error. Keep going. pass return d", "def filter_dict_by_key(d, keys): \"\"\"Filter the dict *d* to remove keys not in *keys*.\"\"\" return {k: v for k, v in d.items() if k in keys}", "def dictlist_wipe_key(dict_list: Iterable[Dict], key: str) -> None: \"\"\" Process an iterable of dictionaries. For each dictionary ``d``, delete ``d[key]`` if it exists. \"\"\" for d in dict_list: d.pop(key, None)", "def filter_dict(d, keys): \"\"\" Creates a new dict from an existing dict that only has the given keys \"\"\" return {k: v for k, v in d.items() if k in keys}"], "rank": 1}
{"nl": "assertion error python how to solve", "code": ["def process_instance(self, instance): self.log.debug(\"e = mc^2\") self.log.info(\"About to fail..\") self.log.warning(\"Failing.. soooon..\") self.log.critical(\"Ok, you're done.\") assert False, \"\"\"ValidateFailureMock was destined to fail.. Here's some extended information about what went wrong. It has quite the long string associated with it, including a few newlines and a list. - Item 1 - Item 2 \"\"\"", "def assert_error(text, check, n=1): \"\"\"Assert that text has n errors of type check.\"\"\" assert_error.description = \"No {} error for '{}'\".format(check, text) assert(check in [error[0] for error in lint(text)])", "def assert_is_not(expected, actual, message=None, extra=None): \"\"\"Raises an AssertionError if expected is actual.\"\"\" assert expected is not actual, _assert_fail_message( message, expected, actual, \"is\", extra )", "def assert_or_raise(stmt: bool, exception: Exception, *exception_args, **exception_kwargs) -> None: \"\"\" If the statement is false, raise the given exception. \"\"\" if not stmt: raise exception(*exception_args, **exception_kwargs)", "def rex_assert(self, rex, byte=False): \"\"\" If `rex` expression is not found then raise `DataNotFound` exception. \"\"\" self.rex_search(rex, byte=byte)"], "rank": 1}
{"nl": "python remove words from sentences in a list", "code": ["def clean_text_by_sentences(text, language=\"english\", additional_stopwords=None): \"\"\" Tokenizes a given text into sentences, applying filters and lemmatizing them. Returns a SyntacticUnit list. \"\"\" init_textcleanner(language, additional_stopwords) original_sentences = split_sentences(text) filtered_sentences = filter_words(original_sentences) return merge_syntactic_units(original_sentences, filtered_sentences)", "def tokenize_words(self, text): \"\"\"Tokenize an input string into a list of words (with punctuation removed).\"\"\" return [ self.strip_punctuation(word) for word in text.split(' ') if self.strip_punctuation(word) ]", "def wordify(text): \"\"\"Generate a list of words given text, removing punctuation. Parameters ---------- text : unicode A piece of english text. Returns ------- words : list List of words. \"\"\" stopset = set(nltk.corpus.stopwords.words('english')) tokens = nltk.WordPunctTokenizer().tokenize(text) return [w for w in tokens if w not in stopset]", "def _removeStopwords(text_list): \"\"\" Removes stopwords contained in a list of words. :param text_string: A list of strings. :type text_string: list. :returns: The input ``text_list`` with stopwords removed. :rtype: list \"\"\" output_list = [] for word in text_list: if word.lower() not in _stopwords: output_list.append(word) return output_list", "def split_into_words(s): \"\"\"Split a sentence into list of words.\"\"\" s = re.sub(r\"\\W+\", \" \", s) s = re.sub(r\"[_0-9]+\", \" \", s) return s.split()"], "rank": 13}
{"nl": "python iterate over many regex sub", "code": ["def iter_finds(regex_obj, s): \"\"\"Generate all matches found within a string for a regex and yield each match as a string\"\"\" if isinstance(regex_obj, str): for m in re.finditer(regex_obj, s): yield m.group() else: for m in regex_obj.finditer(s): yield m.group()", "def _sub_patterns(patterns, text): \"\"\" Apply re.sub to bunch of (pattern, repl) \"\"\" for pattern, repl in patterns: text = re.sub(pattern, repl, text) return text", "def subn_filter(s, find, replace, count=0): \"\"\"A non-optimal implementation of a regex filter\"\"\" return re.gsub(find, replace, count, s)", "def match_files(files, pattern: Pattern): \"\"\"Yields file name if matches a regular expression pattern.\"\"\" for name in files: if re.match(pattern, name): yield name", "def finditer(self, string, pos=0, endpos=sys.maxint): \"\"\"Return a list of all non-overlapping matches of pattern in string.\"\"\" scanner = self.scanner(string, pos, endpos) return iter(scanner.search, None)"], "rank": 2}
{"nl": "python multiindex get index freeze", "code": ["def validate_multiindex(self, obj): \"\"\"validate that we can store the multi-index; reset and return the new object \"\"\" levels = [l if l is not None else \"level_{0}\".format(i) for i, l in enumerate(obj.index.names)] try: return obj.reset_index(), levels except ValueError: raise ValueError(\"duplicate names/columns in the multi-index when \" \"storing as a table\")", "def keys(self): \"\"\"Return ids of all indexed documents.\"\"\" result = [] if self.fresh_index is not None: result += self.fresh_index.keys() if self.opt_index is not None: result += self.opt_index.keys() return result", "def get_index(self, bucket, index, startkey, endkey=None, return_terms=None, max_results=None, continuation=None, timeout=None, term_regex=None): \"\"\" Performs a secondary index query. \"\"\" raise NotImplementedError", "def to_index(self, index_type, index_name, includes=None): \"\"\" Create an index field from this field \"\"\" return IndexField(self.name, self.data_type, index_type, index_name, includes)", "def _make_index(df, cols=META_IDX): \"\"\"Create an index from the columns of a dataframe\"\"\" return pd.MultiIndex.from_tuples( pd.unique(list(zip(*[df[col] for col in cols]))), names=tuple(cols))"], "rank": 1}
{"nl": "python two range union", "code": ["def merge(self, other): \"\"\" Merge this range object with another (ranges need not overlap or abut). :returns: a new Range object representing the interval containing both ranges. \"\"\" newstart = min(self._start, other.start) newend = max(self._end, other.end) return Range(newstart, newend)", "def __or__(self, other): \"\"\"Return the union of two RangeSets as a new RangeSet. (I.e. all elements that are in either set.) \"\"\" if not isinstance(other, set): return NotImplemented return self.union(other)", "def union_overlapping(intervals): \"\"\"Union any overlapping intervals in the given set.\"\"\" disjoint_intervals = [] for interval in intervals: if disjoint_intervals and disjoint_intervals[-1].overlaps(interval): disjoint_intervals[-1] = disjoint_intervals[-1].union(interval) else: disjoint_intervals.append(interval) return disjoint_intervals", "def overlap(intv1, intv2): \"\"\"Overlaping of two intervals\"\"\" return max(0, min(intv1[1], intv2[1]) - max(intv1[0], intv2[0]))", "def __isub__(self, other): \"\"\"Remove all elements of another set from this RangeSet.\"\"\" self._binary_sanity_check(other) set.difference_update(self, other) return self"], "rank": 2}
{"nl": "decode object to bytes python", "code": ["def _decode(self, obj, context): \"\"\" Get the python representation of the obj \"\"\" return b''.join(map(int2byte, [c + 0x60 for c in bytearray(obj)])).decode(\"utf8\")", "def loadb(b): \"\"\"Deserialize ``b`` (instance of ``bytes``) to a Python object.\"\"\" assert isinstance(b, (bytes, bytearray)) return std_json.loads(b.decode('utf-8'))", "def to_bytes(value): \"\"\" str to bytes (py3k) \"\"\" vtype = type(value) if vtype == bytes or vtype == type(None): return value try: return vtype.encode(value) except UnicodeEncodeError: pass return value", "def decode(self, bytes, raw=False): \"\"\"decode(bytearray, raw=False) -> value Decodes the given bytearray according to this PrimitiveType definition. NOTE: The parameter ``raw`` is present to adhere to the ``decode()`` inteface, but has no effect for PrimitiveType definitions. \"\"\" return struct.unpack(self.format, buffer(bytes))[0]", "def decode_value(stream): \"\"\"Decode the contents of a value from a serialized stream. :param stream: Source data stream :type stream: io.BytesIO :returns: Decoded value :rtype: bytes \"\"\" length = decode_length(stream) (value,) = unpack_value(\">{:d}s\".format(length), stream) return value"], "rank": 1}
{"nl": "limit float decimals in python", "code": ["def format_exp_floats(decimals): \"\"\" sometimes the exp. column can be too large \"\"\" threshold = 10 ** 5 return ( lambda n: \"{:.{prec}e}\".format(n, prec=decimals) if n > threshold else \"{:4.{prec}f}\".format(n, prec=decimals) )", "def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)", "def limitReal(x, max_denominator=1000000): \"\"\"Creates an pysmt Real constant from x. Args: x (number): A number to be cast to a pysmt constant. max_denominator (int, optional): The maximum size of the denominator. Default 1000000. Returns: A Real constant with the given value and the denominator limited. \"\"\" f = Fraction(x).limit_denominator(max_denominator) return Real((f.numerator, f.denominator))", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0", "def truncate(value: Decimal, n_digits: int) -> Decimal: \"\"\"Truncates a value to a number of decimals places\"\"\" return Decimal(math.trunc(value * (10 ** n_digits))) / (10 ** n_digits)"], "rank": 9}
{"nl": "delete an element from set python", "code": ["def discard(self, element): \"\"\"Remove element from the RangeSet if it is a member. If the element is not a member, do nothing. \"\"\" try: i = int(element) set.discard(self, i) except ValueError: pass", "def remove_once(gset, elem): \"\"\"Remove the element from a set, lists or dict. >>> L = [\"Lucy\"]; S = set([\"Sky\"]); D = { \"Diamonds\": True }; >>> remove_once(L, \"Lucy\"); remove_once(S, \"Sky\"); remove_once(D, \"Diamonds\"); >>> print L, S, D [] set([]) {} Returns the element if it was removed. Raises one of the exceptions in :obj:`RemoveError` otherwise. \"\"\" remove = getattr(gset, 'remove', None) if remove is not None: remove(elem) else: del gset[elem] return elem", "def add(self, value): \"\"\"Add the element *value* to the set.\"\"\" if value not in self._set: self._set.add(value) self._list.add(value)", "def __isub__(self, other): \"\"\"Remove all elements of another set from this RangeSet.\"\"\" self._binary_sanity_check(other) set.difference_update(self, other) return self", "def remove(self, entry): \"\"\"Removes an entry\"\"\" try: list = self.cache[entry.key] list.remove(entry) except: pass"], "rank": 2}
{"nl": "object id in python equivalent in golang", "code": ["def generate_id(self, obj): \"\"\"Generate unique document id for ElasticSearch.\"\"\" object_type = type(obj).__name__.lower() return '{}_{}'.format(object_type, self.get_object_id(obj))", "def _id(self): \"\"\"What this object is equal to.\"\"\" return (self.__class__, self.number_of_needles, self.needle_positions, self.left_end_needle)", "def get_obj(ref): \"\"\"Get object from string reference.\"\"\" oid = int(ref) return server.id2ref.get(oid) or server.id2obj[oid]", "def __init__(self): \"\"\"Initializes an attribute container identifier.\"\"\" super(AttributeContainerIdentifier, self).__init__() self._identifier = id(self)", "def _unique_id(self, prefix): \"\"\" Generate a unique (within the graph) identifer internal to graph generation. \"\"\" _id = self._id_gen self._id_gen += 1 return prefix + str(_id)"], "rank": 1}
{"nl": "python create dictionary with keys and no values", "code": ["def nonull_dict(self): \"\"\"Like dict, but does not hold any null values. :return: \"\"\" return {k: v for k, v in six.iteritems(self.dict) if v and k != '_codes'}", "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]: \"\"\" Return a new copied dictionary without the keys with ``None`` values from the given Mapping object. \"\"\" return {k: v for k, v in obj.items() if v is not None}", "def _remove_empty_items(d, required): \"\"\"Return a new dict with any empty items removed. Note that this is not a deep check. If d contains a dictionary which itself contains empty items, those are never checked. This method exists to make to_serializable() functions cleaner. We could revisit this some day, but for now, the serialized objects are stripped of empty values to keep the output YAML more compact. Args: d: a dictionary required: list of required keys (for example, TaskDescriptors always emit the \"task-id\", even if None) Returns: A dictionary with empty items removed. \"\"\" new_dict = {} for k, v in d.items(): if k in required: new_dict[k] = v elif isinstance(v, int) or v: # \"if v\" would suppress emitting int(0) new_dict[k] = v return new_dict", "def make_symmetric(dict): \"\"\"Makes the given dictionary symmetric. Values are assumed to be unique.\"\"\" for key, value in list(dict.items()): dict[value] = key return dict", "def inject_nulls(data: Mapping, field_names) -> dict: \"\"\"Insert None as value for missing fields.\"\"\" record = dict() for field in field_names: record[field] = data.get(field, None) return record"], "rank": 2}
{"nl": "python check if directory is writable", "code": ["def _writable_dir(path): \"\"\"Whether `path` is a directory, to which the user has write access.\"\"\" return os.path.isdir(path) and os.access(path, os.W_OK)", "def is_writable_by_others(filename): \"\"\"Check if file or directory is world writable.\"\"\" mode = os.stat(filename)[stat.ST_MODE] return mode & stat.S_IWOTH", "def is_readable_dir(path): \"\"\"Returns whether a path names an existing directory we can list and read files from.\"\"\" return os.path.isdir(path) and os.access(path, os.R_OK) and os.access(path, os.X_OK)", "def readable(path): \"\"\"Test whether a path exists and is readable. Returns None for broken symbolic links or a failing stat() and False if the file exists but does not have read permission. True is returned if the file is readable.\"\"\" try: st = os.stat(path) return 0 != st.st_mode & READABLE_MASK except os.error: return None return True", "def isdir(s): \"\"\"Return true if the pathname refers to an existing directory.\"\"\" try: st = os.stat(s) except os.error: return False return stat.S_ISDIR(st.st_mode)"], "rank": 1}
{"nl": "python df change type", "code": ["def _to_corrected_pandas_type(dt): \"\"\" When converting Spark SQL records to Pandas DataFrame, the inferred data type may be wrong. This method gets the corrected data type for Pandas if that type may be inferred uncorrectly. \"\"\" import numpy as np if type(dt) == ByteType: return np.int8 elif type(dt) == ShortType: return np.int16 elif type(dt) == IntegerType: return np.int32 elif type(dt) == FloatType: return np.float32 else: return None", "def reverse_transform(self, col): \"\"\"Converts data back into original format. Args: col(pandas.DataFrame): Data to transform. Returns: pandas.DataFrame \"\"\" output = pd.DataFrame() output[self.col_name] = self.get_category(col[self.col_name]) return output", "def _maybe_pandas_data(data, feature_names, feature_types): \"\"\" Extract internal data from pd.DataFrame for DMatrix data \"\"\" if not isinstance(data, DataFrame): return data, feature_names, feature_types data_dtypes = data.dtypes if not all(dtype.name in PANDAS_DTYPE_MAPPER for dtype in data_dtypes): bad_fields = [data.columns[i] for i, dtype in enumerate(data_dtypes) if dtype.name not in PANDAS_DTYPE_MAPPER] msg = \"\"\"DataFrame.dtypes for data must be int, float or bool. Did not expect the data types in fields \"\"\" raise ValueError(msg + ', '.join(bad_fields)) if feature_names is None: if isinstance(data.columns, MultiIndex): feature_names = [ ' '.join([str(x) for x in i]) for i in data.columns ] else: feature_names = data.columns.format() if feature_types is None: feature_types = [PANDAS_DTYPE_MAPPER[dtype.name] for dtype in data_dtypes] data = data.values.astype('float') return data, feature_names, feature_types", "def _possibly_convert_objects(values): \"\"\"Convert arrays of datetime.datetime and datetime.timedelta objects into datetime64 and timedelta64, according to the pandas convention. \"\"\" return np.asarray(pd.Series(values.ravel())).reshape(values.shape)", "def as_dict(df, ix=':'): \"\"\" converts df to dict and adds a datetime field if df is datetime \"\"\" if isinstance(df.index, pd.DatetimeIndex): df['datetime'] = df.index return df.to_dict(orient='records')[ix]"], "rank": 1}
{"nl": "how to put json in file python", "code": ["def _write_json(file, contents): \"\"\"Write a dict to a JSON file.\"\"\" with open(file, 'w') as f: return json.dump(contents, f, indent=2, sort_keys=True)", "def save(self, fname): \"\"\" Saves the dictionary in json format :param fname: file to save to \"\"\" with open(fname, 'wb') as f: json.dump(self, f)", "def store_data(data): \"\"\"Use this function to store data in a JSON file. This function is used for loading up a JSON file and appending additional data to the JSON file. :param data: the data to add to the JSON file. :type data: dict \"\"\" with open(url_json_path) as json_file: try: json_file_data = load(json_file) json_file_data.update(data) except (AttributeError, JSONDecodeError): json_file_data = data with open(url_json_path, 'w') as json_file: dump(json_file_data, json_file, indent=4, sort_keys=True)", "def _write_json(obj, path): # type: (object, str) -> None \"\"\"Writes a serializeable object as a JSON file\"\"\" with open(path, 'w') as f: json.dump(obj, f)", "def _serialize_json(obj, fp): \"\"\" Serialize ``obj`` as a JSON formatted stream to ``fp`` \"\"\" json.dump(obj, fp, indent=4, default=serialize)"], "rank": 2}
{"nl": "set a rect to a variable python", "code": ["def setRect(self, rect): \"\"\" Sets the window bounds from a tuple of (x,y,w,h) \"\"\" self.x, self.y, self.w, self.h = rect", "def move_to(x, y): \"\"\"Moves the brush to a particular position. Arguments: x - a number between -250 and 250. y - a number between -180 and 180. \"\"\" _make_cnc_request(\"coord/{0}/{1}\".format(x, y)) state['turtle'].goto(x, y)", "def adjust_bounding_box(bbox): \"\"\"Adjust the bounding box as specified by user. Returns the adjusted bounding box. - bbox: Bounding box computed from the canvas drawings. It must be a four-tuple of numbers. \"\"\" for i in range(0, 4): if i in bounding_box: bbox[i] = bounding_box[i] else: bbox[i] += delta_bounding_box[i] return bbox", "def swap(self): \"\"\"Return the box (for horizontal graphs)\"\"\" self.xmin, self.ymin = self.ymin, self.xmin self.xmax, self.ymax = self.ymax, self.xmax", "def set_scrollregion(self, event=None): \"\"\" Set the scroll region on the canvas\"\"\" self.canvas.configure(scrollregion=self.canvas.bbox('all'))"], "rank": 1}
{"nl": "python borderless table via format function", "code": ["def paint(self, tbl): \"\"\" Paint the table on terminal Currently only print out basic string format \"\"\" if not isinstance(tbl, Table): logging.error(\"unable to paint table: invalid object\") return False self.term.stream.write(self.term.clear) self.term.stream.write(str(tbl)) return True", "def format_prettytable(table): \"\"\"Converts SoftLayer.CLI.formatting.Table instance to a prettytable.\"\"\" for i, row in enumerate(table.rows): for j, item in enumerate(row): table.rows[i][j] = format_output(item) ptable = table.prettytable() ptable.hrules = prettytable.FRAME ptable.horizontal_char = '.' ptable.vertical_char = ':' ptable.junction_char = ':' return ptable", "def print_table(*args, **kwargs): \"\"\" if csv: import csv t = csv.writer(sys.stdout, delimiter=\";\") t.writerow(header) else: t = PrettyTable(header) t.align = \"r\" t.align[\"details\"] = \"l\" \"\"\" t = format_table(*args, **kwargs) click.echo(t)", "def adapter(data, headers, **kwargs): \"\"\"Wrap vertical table in a function for TabularOutputFormatter.\"\"\" keys = ('sep_title', 'sep_character', 'sep_length') return vertical_table(data, headers, **filter_dict_by_key(kwargs, keys))", "def top(n, width=WIDTH, style=STYLE): \"\"\"Prints the top row of a table\"\"\" return hrule(n, width, linestyle=STYLES[style].top)"], "rank": 4}
{"nl": "how to select region in array python", "code": ["def select_from_array(cls, array, identifier): \"\"\"Return a region from a numpy array. :param array: :class:`numpy.ndarray` :param identifier: value representing the region to select in the array :returns: :class:`jicimagelib.region.Region` \"\"\" base_array = np.zeros(array.shape) array_coords = np.where(array == identifier) base_array[array_coords] = 1 return cls(base_array)", "def index(m, val): \"\"\" Return the indices of all the ``val`` in ``m`` \"\"\" mm = np.array(m) idx_tuple = np.where(mm == val) idx = idx_tuple[0].tolist() return idx", "def find_start_point(self): \"\"\" Find the first location in our array that is not empty \"\"\" for i, row in enumerate(self.data): for j, _ in enumerate(row): if self.data[i, j] != 0: # or not np.isfinite(self.data[i,j]): return i, j", "def feature_subset(self, indices): \"\"\" Returns some subset of the features. Parameters ---------- indices : :obj:`list` of :obj:`int` indices of the features in the list Returns ------- :obj:`list` of :obj:`Feature` \"\"\" if isinstance(indices, np.ndarray): indices = indices.tolist() if not isinstance(indices, list): raise ValueError('Can only index with lists') return [self.features_[i] for i in indices]", "def region_from_segment(image, segment): \"\"\"given a segment (rectangle) and an image, returns it's corresponding subimage\"\"\" x, y, w, h = segment return image[y:y + h, x:x + w]"], "rank": 1}
{"nl": "python is not not none", "code": ["def _not_none(items): \"\"\"Whether the item is a placeholder or contains a placeholder.\"\"\" if not isinstance(items, (tuple, list)): items = (items,) return all(item is not _none for item in items)", "def less_strict_bool(x): \"\"\"Idempotent and None-safe version of strict_bool.\"\"\" if x is None: return False elif x is True or x is False: return x else: return strict_bool(x)", "def is_none(string_, default='raise'): \"\"\" Check if a string is equivalent to None. Parameters ---------- string_ : str default : {'raise', False} Default behaviour if none of the \"None\" strings is detected. Returns ------- is_none : bool Examples -------- >>> is_none('2', default=False) False >>> is_none('undefined', default=False) True \"\"\" none = ['none', 'undefined', 'unknown', 'null', ''] if string_.lower() in none: return True elif not default: return False else: raise ValueError('The value \\'{}\\' cannot be mapped to none.' .format(string_))", "def run(self, value): \"\"\" Determines if value value is empty. Keyword arguments: value str -- the value of the associated field to compare \"\"\" if self.pass_ and not value.strip(): return True if not value: return False return True", "def _not(condition=None, **kwargs): \"\"\" Return the opposite of input condition. :param condition: condition to process. :result: not condition. :rtype: bool \"\"\" result = True if condition is not None: result = not run(condition, **kwargs) return result"], "rank": 2}
{"nl": "python dir is writable", "code": ["def _writable_dir(path): \"\"\"Whether `path` is a directory, to which the user has write access.\"\"\" return os.path.isdir(path) and os.access(path, os.W_OK)", "def is_writable_by_others(filename): \"\"\"Check if file or directory is world writable.\"\"\" mode = os.stat(filename)[stat.ST_MODE] return mode & stat.S_IWOTH", "def is_readable_dir(path): \"\"\"Returns whether a path names an existing directory we can list and read files from.\"\"\" return os.path.isdir(path) and os.access(path, os.R_OK) and os.access(path, os.X_OK)", "def make_writeable(filename): \"\"\" Make sure that the file is writeable. Useful if our source is read-only. \"\"\" if not os.access(filename, os.W_OK): st = os.stat(filename) new_permissions = stat.S_IMODE(st.st_mode) | stat.S_IWUSR os.chmod(filename, new_permissions)", "def makedirs(path, mode=0o777, exist_ok=False): \"\"\"A wrapper of os.makedirs().\"\"\" os.makedirs(path, mode, exist_ok)"], "rank": 2}
{"nl": "behave python element not visible", "code": ["def show(self): \"\"\" Ensure the widget is shown. Calling this method will also set the widget visibility to True. \"\"\" self.visible = True if self.proxy_is_active: self.proxy.ensure_visible()", "def assert_visible(self, locator, msg=None): \"\"\" Hard assert for whether and element is present and visible in the current window/frame :params locator: the locator of the element to search for :params msg: (Optional) msg explaining the difference \"\"\" e = driver.find_elements_by_locator(locator) if len(e) == 0: raise AssertionError(\"Element at %s was not found\" % locator) assert e.is_displayed()", "def is_element_present(driver, selector, by=By.CSS_SELECTOR): \"\"\" Returns whether the specified element selector is present on the page. @Params driver - the webdriver object (required) selector - the locator that is used (required) by - the method to search for the locator (Default: By.CSS_SELECTOR) @Returns Boolean (is element present) \"\"\" try: driver.find_element(by=by, value=selector) return True except Exception: return False", "def hide(self): \"\"\"Hides the main window of the terminal and sets the visible flag to False. \"\"\" if not HidePrevention(self.window).may_hide(): return self.hidden = True self.get_widget('window-root').unstick() self.window.hide()", "def is_element_present(self, strategy, locator): \"\"\"Checks whether an element is present. :param strategy: Location strategy to use. See :py:class:`~selenium.webdriver.common.by.By` or :py:attr:`~pypom.splinter_driver.ALLOWED_STRATEGIES`. :param locator: Location of target element. :type strategy: str :type locator: str :return: ``True`` if element is present, else ``False``. :rtype: bool \"\"\" return self.driver_adapter.is_element_present(strategy, locator, root=self.root)"], "rank": 1}
{"nl": "python detect change of slope", "code": ["def click_estimate_slope(): \"\"\" Takes two clicks and returns the slope. Right-click aborts. \"\"\" c1 = _pylab.ginput() if len(c1)==0: return None c2 = _pylab.ginput() if len(c2)==0: return None return (c1[0][1]-c2[0][1])/(c1[0][0]-c2[0][0])", "def interpolate_logscale_single(start, end, coefficient): \"\"\" Cosine interpolation \"\"\" return np.exp(np.log(start) + (np.log(end) - np.log(start)) * coefficient)", "def _increment(self, *args): \"\"\"Move the slider only by increment given by resolution.\"\"\" value = self._var.get() if self._resolution: value = self._start + int(round((value - self._start) / self._resolution)) * self._resolution self._var.set(value) self.display_value(value)", "def value(self, progress_indicator): \"\"\" Interpolate linearly between start and end \"\"\" return interpolate.interpolate_linear_single(self.initial_value, self.final_value, progress_indicator)", "def update_scale(self, value): \"\"\" updates the scale of all actors in the plotter \"\"\" self.plotter.set_scale(self.x_slider_group.value, self.y_slider_group.value, self.z_slider_group.value)"], "rank": 1}
{"nl": "python tkinter unchecking checkbutton change variable", "code": ["def checkbox_uncheck(self, force_check=False): \"\"\" Wrapper to uncheck a checkbox \"\"\" if self.get_attribute('checked'): self.click(force_click=force_check)", "def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setChecked(Qt.Checked) else: self.setChecked(Qt.Unchecked)", "def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setCheckState(Qt.Checked) else: self.setCheckState(Qt.Unchecked)", "def disable(self): \"\"\" Disable the button, if in non-expert mode; unset its activity flag come-what-may. \"\"\" if not self._expert: self.config(state='disable') self._active = False", "def _on_select(self, *args): \"\"\" Function bound to event of selection in the Combobox, calls callback if callable :param args: Tkinter event \"\"\" if callable(self.__callback): self.__callback(self.selection)"], "rank": 1}
{"nl": "python json dumps numpy key", "code": ["def _convert_dict_to_json(array): \"\"\" Converts array to a json string \"\"\" return json.dumps( array, skipkeys=False, allow_nan=False, indent=None, separators=(\",\", \":\"), sort_keys=True, default=lambda o: o.__dict__, )", "def dump_nparray(self, obj, class_name=numpy_ndarray_class_name): \"\"\" ``numpy.ndarray`` dumper. \"\"\" return {\"$\" + class_name: self._json_convert(obj.tolist())}", "def _ndarray_representer(dumper, data): \"\"\" :param dumper: :param data: :type data: :class:`numpy.ndarray` :return: \"\"\" mapping = [('object', data.tolist()), ('dtype', data.dtype.name)] return dumper.represent_mapping(_NUMPY_ARRAY_TAG, mapping)", "def deserialize_ndarray_npy(d): \"\"\" Deserializes a JSONified :obj:`numpy.ndarray` that was created using numpy's :obj:`save` function. Args: d (:obj:`dict`): A dictionary representation of an :obj:`ndarray` object, created using :obj:`numpy.save`. Returns: An :obj:`ndarray` object. \"\"\" with io.BytesIO() as f: f.write(json.loads(d['npy']).encode('latin-1')) f.seek(0) return np.load(f)", "def jsonify(symbol): \"\"\" returns json format for symbol \"\"\" try: # all symbols have a toJson method, try it return json.dumps(symbol.toJson(), indent=' ') except AttributeError: pass return json.dumps(symbol, indent=' ')"], "rank": 4}
{"nl": "how to change python input to upper case", "code": ["def upcaseTokens(s,l,t): \"\"\"Helper parse action to convert tokens to upper case.\"\"\" return [ tt.upper() for tt in map(_ustr,t) ]", "def clean(some_string, uppercase=False): \"\"\" helper to clean up an input string \"\"\" if uppercase: return some_string.strip().upper() else: return some_string.strip().lower()", "def camel_to_(s): \"\"\" Convert CamelCase to camel_case \"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def _upper(val_list): \"\"\" :param val_list: a list of strings :return: a list of upper-cased strings \"\"\" res = [] for ele in val_list: res.append(ele.upper()) return res", "def camelcase(string): \"\"\" Convert string into camel case. Args: string: String to convert. Returns: string: Camel case string. \"\"\" string = re.sub(r\"^[\\-_\\.]\", '', str(string)) if not string: return string return lowercase(string[0]) + re.sub(r\"[\\-_\\.\\s]([a-z])\", lambda matched: uppercase(matched.group(1)), string[1:])"], "rank": 2}
{"nl": "python minidom html to dict", "code": ["def tag_to_dict(html): \"\"\"Extract tag's attributes into a `dict`.\"\"\" element = document_fromstring(html).xpath(\"//html/body/child::*\")[0] attributes = dict(element.attrib) attributes[\"text\"] = element.text_content() return attributes", "def xml_str_to_dict(s): \"\"\" Transforms an XML string it to python-zimbra dict format For format, see: https://github.com/Zimbra-Community/python-zimbra/blob/master/README.md :param: a string, containing XML :returns: a dict, with python-zimbra format \"\"\" xml = minidom.parseString(s) return pythonzimbra.tools.xmlserializer.dom_to_dict(xml.firstChild)", "def aloha_to_html(html_source): \"\"\"Converts HTML5 from Aloha to a more structured HTML5\"\"\" xml = aloha_to_etree(html_source) return etree.tostring(xml, pretty_print=True)", "def prettify(elem): \"\"\"Return a pretty-printed XML string for the Element. \"\"\" rough_string = ET.tostring(elem, 'utf-8') reparsed = minidom.parseString(rough_string) return reparsed.toprettyxml(indent=\"\\t\")", "def html_to_text(content): \"\"\" Converts html content to plain text \"\"\" text = None h2t = html2text.HTML2Text() h2t.ignore_links = False text = h2t.handle(content) return text"], "rank": 1}
{"nl": "how to remove punctuation and special charachhters in python", "code": ["def _clean_str(self, s): \"\"\" Returns a lowercase string with punctuation and bad chars removed :param s: string to clean \"\"\" return s.translate(str.maketrans('', '', punctuation)).replace('\\u200b', \" \").strip().lower()", "def preprocess(string): \"\"\" Preprocess string to transform all diacritics and remove other special characters than appropriate :param string: :return: \"\"\" string = unicode(string, encoding=\"utf-8\") # convert diacritics to simpler forms string = regex1.sub(lambda x: accents[x.group()], string) # remove all rest of the unwanted characters return regex2.sub('', string).encode('utf-8')", "def remove_punctuation(text, exceptions=[]): \"\"\" Return a string with punctuation removed. Parameters: text (str): The text to remove punctuation from. exceptions (list): List of symbols to keep in the given text. Return: str: The input text without the punctuation. \"\"\" all_but = [ r'\\w', r'\\s' ] all_but.extend(exceptions) pattern = '[^{}]'.format(''.join(all_but)) return re.sub(pattern, '', text)", "def unpunctuate(s, *, char_blacklist=string.punctuation): \"\"\" Remove punctuation from string s. \"\"\" # remove punctuation s = \"\".join(c for c in s if c not in char_blacklist) # remove consecutive spaces return \" \".join(filter(None, s.split(\" \")))", "def strip_accents(text): \"\"\" Strip agents from a string. \"\"\" normalized_str = unicodedata.normalize('NFD', text) return ''.join([ c for c in normalized_str if unicodedata.category(c) != 'Mn'])"], "rank": 24}
{"nl": "update figure in python to show change", "code": ["def OnUpdateFigurePanel(self, event): \"\"\"Redraw event handler for the figure panel\"\"\" if self.updating: return self.updating = True self.figure_panel.update(self.get_figure(self.code)) self.updating = False", "def update(self): \"\"\"Updates image to be displayed with new time frame.\"\"\" if self.single_channel: self.im.set_data(self.data[self.ind, :, :]) else: self.im.set_data(self.data[self.ind, :, :, :]) self.ax.set_ylabel('time frame %s' % self.ind) self.im.axes.figure.canvas.draw()", "def _update_plot(self, _): \"\"\"Callback to redraw the plot to reflect the new parameter values.\"\"\" # Since all sliders call this same callback without saying who they are # I need to update the values for all parameters. This can be # circumvented by creating a seperate callback function for each # parameter. for param in self.model.params: param.value = self._sliders[param].val for indep_var, dep_var in self._projections: self._update_specific_plot(indep_var, dep_var)", "def autozoom(self, n=None): \"\"\" Auto-scales the axes to fit all the data in plot index n. If n == None, auto-scale everyone. \"\"\" if n==None: for p in self.plot_widgets: p.autoRange() else: self.plot_widgets[n].autoRange() return self", "def update_scale(self, value): \"\"\" updates the scale of all actors in the plotter \"\"\" self.plotter.set_scale(self.x_slider_group.value, self.y_slider_group.value, self.z_slider_group.value)"], "rank": 1}
{"nl": "python hashlib of entire file", "code": ["def _hash_the_file(hasher, filename): \"\"\"Helper function for creating hash functions. See implementation of :func:`dtoolcore.filehasher.shasum` for more usage details. \"\"\" BUF_SIZE = 65536 with open(filename, 'rb') as f: buf = f.read(BUF_SIZE) while len(buf) > 0: hasher.update(buf) buf = f.read(BUF_SIZE) return hasher", "def hash_file(fileobj): \"\"\" :param fileobj: a file object :return: a hash of the file content \"\"\" hasher = hashlib.md5() buf = fileobj.read(65536) while len(buf) > 0: hasher.update(buf) buf = fileobj.read(65536) return hasher.hexdigest()", "def update_hash(cls, filelike, digest): \"\"\"Update the digest of a single file in a memory-efficient manner.\"\"\" block_size = digest.block_size * 1024 for chunk in iter(lambda: filelike.read(block_size), b''): digest.update(chunk)", "def generate_hash(filepath): \"\"\"Public function that reads a local file and generates a SHA256 hash digest for it\"\"\" fr = FileReader(filepath) data = fr.read_bin() return _calculate_sha256(data)", "def md5_hash_file(fh): \"\"\"Return the md5 hash of the given file-object\"\"\" md5 = hashlib.md5() while True: data = fh.read(8192) if not data: break md5.update(data) return md5.hexdigest()"], "rank": 1}
{"nl": "get processing power from other devices in python", "code": ["def get_host_power_status(self): \"\"\"Request the power state of the server. :returns: Power State of the server, 'ON' or 'OFF' :raises: IloError, on an error from iLO. \"\"\" sushy_system = self._get_sushy_system(PROLIANT_SYSTEM_ID) return GET_POWER_STATE_MAP.get(sushy_system.power_state)", "def machine_info(): \"\"\"Retrieve core and memory information for the current machine. \"\"\" import psutil BYTES_IN_GIG = 1073741824.0 free_bytes = psutil.virtual_memory().total return [{\"memory\": float(\"%.1f\" % (free_bytes / BYTES_IN_GIG)), \"cores\": multiprocessing.cpu_count(), \"name\": socket.gethostname()}]", "def refresh_core(self): \"\"\"Query device for all attributes that exist regardless of power state. This will force a refresh for all device queries that are valid to request at any time. It's the only safe suite of queries that we can make if we do not know the current state (on or off+standby). This does not return any data, it just issues the queries. \"\"\" self.log.info('Sending out mass query for all attributes') for key in ATTR_CORE: self.query(key)", "def get_power(self): \"\"\"Check if the device is on.\"\"\" power = (yield from self.handle_int(self.API.get('power'))) return bool(power)", "async def sysinfo(dev: Device): \"\"\"Print out system information (version, MAC addrs).\"\"\" click.echo(await dev.get_system_info()) click.echo(await dev.get_interface_information())"], "rank": 4}
{"nl": "python get hash of file filestorage", "code": ["def get_hash(self, handle): \"\"\"Return the hash.\"\"\" fpath = self._fpath_from_handle(handle) return DiskStorageBroker.hasher(fpath)", "def _hash_the_file(hasher, filename): \"\"\"Helper function for creating hash functions. See implementation of :func:`dtoolcore.filehasher.shasum` for more usage details. \"\"\" BUF_SIZE = 65536 with open(filename, 'rb') as f: buf = f.read(BUF_SIZE) while len(buf) > 0: hasher.update(buf) buf = f.read(BUF_SIZE) return hasher", "def hash_file(fileobj): \"\"\" :param fileobj: a file object :return: a hash of the file content \"\"\" hasher = hashlib.md5() buf = fileobj.read(65536) while len(buf) > 0: hasher.update(buf) buf = fileobj.read(65536) return hasher.hexdigest()", "def generate_hash(filepath): \"\"\"Public function that reads a local file and generates a SHA256 hash digest for it\"\"\" fr = FileReader(filepath) data = fr.read_bin() return _calculate_sha256(data)", "def checksum(path): \"\"\"Calculcate checksum for a file.\"\"\" hasher = hashlib.sha1() with open(path, 'rb') as stream: buf = stream.read(BLOCKSIZE) while len(buf) > 0: hasher.update(buf) buf = stream.read(BLOCKSIZE) return hasher.hexdigest()"], "rank": 1}
{"nl": "how to detect number of cpu cores in python", "code": ["def cpu_count() -> int: \"\"\"Returns the number of processors on this machine.\"\"\" if multiprocessing is None: return 1 try: return multiprocessing.cpu_count() except NotImplementedError: pass try: return os.sysconf(\"SC_NPROCESSORS_CONF\") except (AttributeError, ValueError): pass gen_log.error(\"Could not detect number of processors; assuming 1\") return 1", "def _num_cpus_darwin(): \"\"\"Return the number of active CPUs on a Darwin system.\"\"\" p = subprocess.Popen(['sysctl','-n','hw.ncpu'],stdout=subprocess.PIPE) return p.stdout.read()", "def machine_info(): \"\"\"Retrieve core and memory information for the current machine. \"\"\" import psutil BYTES_IN_GIG = 1073741824.0 free_bytes = psutil.virtual_memory().total return [{\"memory\": float(\"%.1f\" % (free_bytes / BYTES_IN_GIG)), \"cores\": multiprocessing.cpu_count(), \"name\": socket.gethostname()}]", "def ncores_reserved(self): \"\"\" Returns the number of cores reserved in this moment. A core is reserved if it's still not running but we have submitted the task to the queue manager. \"\"\" return sum(task.manager.num_cores for task in self if task.status == task.S_SUB)", "def get_system_cpu_times(): \"\"\"Return system CPU times as a namedtuple.\"\"\" user, nice, system, idle = _psutil_osx.get_system_cpu_times() return _cputimes_ntuple(user, nice, system, idle)"], "rank": 1}
{"nl": "how to return the index of a number in a list python", "code": ["def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def get_list_index(lst, index_or_name): \"\"\" Return the index of an element in the list. Args: lst (list): The list. index_or_name (int or str): The value of the reference element, or directly its numeric index. Returns: (int) The index of the element in the list. \"\"\" if isinstance(index_or_name, six.integer_types): return index_or_name return lst.index(index_or_name)", "def binSearch(arr, val): \"\"\" Function for running binary search on a sorted list. :param arr: (list) a sorted list of integers to search :param val: (int) a integer to search for in the sorted array :returns: (int) the index of the element if it is found and -1 otherwise. \"\"\" i = bisect_left(arr, val) if i != len(arr) and arr[i] == val: return i return -1"], "rank": 2}
{"nl": "python list of all objects", "code": ["def fields(self): \"\"\"Returns the list of field names of the model.\"\"\" return (self.attributes.values() + self.lists.values() + self.references.values())", "def values(self): \"\"\"return a list of all state values\"\"\" values = [] for __, data in self.items(): values.append(data) return values", "def __iter__(self): \"\"\" Returns the list of modes. :return: \"\"\" return iter([v for k, v in sorted(self._modes.items())])", "def types(self): \"\"\" Return a list of all the variable types that exist in the Variables object. \"\"\" output = set() for var in self.values(): if var.has_value(): output.update(var.types()) return list(output)", "def __dir__(self): u\"\"\"Returns a list of children and available helper methods.\"\"\" return sorted(self.keys() | {m for m in dir(self.__class__) if m.startswith('to_')})"], "rank": 10}
{"nl": "python iterate regex matches", "code": ["def iter_finds(regex_obj, s): \"\"\"Generate all matches found within a string for a regex and yield each match as a string\"\"\" if isinstance(regex_obj, str): for m in re.finditer(regex_obj, s): yield m.group() else: for m in regex_obj.finditer(s): yield m.group()", "def match_files(files, pattern: Pattern): \"\"\"Yields file name if matches a regular expression pattern.\"\"\" for name in files: if re.match(pattern, name): yield name", "def finditer(self, string, pos=0, endpos=sys.maxint): \"\"\"Return a list of all non-overlapping matches of pattern in string.\"\"\" scanner = self.scanner(string, pos, endpos) return iter(scanner.search, None)", "def _regex_span(_regex, _str, case_insensitive=True): \"\"\"Return all matches in an input string. :rtype : regex.match.span :param _regex: A regular expression pattern. :param _str: Text on which to run the pattern. \"\"\" if case_insensitive: flags = regex.IGNORECASE | regex.FULLCASE | regex.VERSION1 else: flags = regex.VERSION1 comp = regex.compile(_regex, flags=flags) matches = comp.finditer(_str) for match in matches: yield match", "def filter_regex(names, regex): \"\"\" Return a tuple of strings that match the regular expression pattern. \"\"\" return tuple(name for name in names if regex.search(name) is not None)"], "rank": 1}
{"nl": "python get rid of comments in json string", "code": ["def strip_comments(string, comment_symbols=frozenset(('#', '//'))): \"\"\"Strip comments from json string. :param string: A string containing json with comments started by comment_symbols. :param comment_symbols: Iterable of symbols that start a line comment (default # or //). :return: The string with the comments removed. \"\"\" lines = string.splitlines() for k in range(len(lines)): for symbol in comment_symbols: lines[k] = strip_comment_line_with_symbol(lines[k], start=symbol) return '\\n'.join(lines)", "def parse_json(filename): \"\"\" Parse a JSON file First remove comments and then use the json module package Comments look like : // ... or /* ... */ \"\"\" # Regular expression for comments comment_re = re.compile( '(^)?[^\\S\\n]*/(?:\\*(.*?)\\*/[^\\S\\n]*|/[^\\n]*)($)?', re.DOTALL | re.MULTILINE ) with open(filename) as f: content = ''.join(f.readlines()) ## Looking for comments match = comment_re.search(content) while match: # single line comment content = content[:match.start()] + content[match.end():] match = comment_re.search(content) # Return json file return json.loads(content)", "def strip_comment_marker(text): \"\"\" Strip # markers at the front of a block of comment text. \"\"\" lines = [] for line in text.splitlines(): lines.append(line.lstrip('#')) text = textwrap.dedent('\\n'.join(lines)) return text", "def split_comment(cls, code): \"\"\" Removes comments (#...) from python code. \"\"\" if '#' not in code: return code #: Remove comments only (leave quoted strings as they are) subf = lambda m: '' if m.group(0)[0]=='#' else m.group(0) return re.sub(cls.re_pytokens, subf, code)", "def _comment(string): \"\"\"return string as a comment\"\"\" lines = [line.strip() for line in string.splitlines()] return \"# \" + (\"%s# \" % linesep).join(lines)"], "rank": 1}
{"nl": "python code to check if line in file exists", "code": ["def is_line_in_file(filename: str, line: str) -> bool: \"\"\" Detects whether a line is present within a file. Args: filename: file to check line: line to search for (as an exact match) \"\"\" assert \"\\n\" not in line with open(filename, \"r\") as file: for fileline in file: if fileline == line: return True return False", "def file_found(filename,force): \"\"\"Check if a file exists\"\"\" if os.path.exists(filename) and not force: logger.info(\"Found %s; skipping...\"%filename) return True else: return False", "def _file_exists(path, filename): \"\"\"Checks if the filename exists under the path.\"\"\" return os.path.isfile(os.path.join(path, filename))", "def _file_and_exists(val, input_files): \"\"\"Check if an input is a file and exists. Checks both locally (staged) and from input files (re-passed but never localized). \"\"\" return ((os.path.exists(val) and os.path.isfile(val)) or val in input_files)", "def do_files_exist(filenames): \"\"\"Whether any of the filenames exist.\"\"\" preexisting = [tf.io.gfile.exists(f) for f in filenames] return any(preexisting)"], "rank": 1}
{"nl": "python thread spawn async", "code": ["async def async_run(self) -> None: \"\"\" Asynchronously run the worker, does not close connections. Useful when testing. \"\"\" self.main_task = self.loop.create_task(self.main()) await self.main_task", "def asynchronous(function, event): \"\"\" Runs the function asynchronously taking care of exceptions. \"\"\" thread = Thread(target=synchronous, args=(function, event)) thread.daemon = True thread.start()", "def _run_sync(self, method: Callable, *args, **kwargs) -> Any: \"\"\" Utility method to run commands synchronously for testing. \"\"\" if self.loop.is_running(): raise RuntimeError(\"Event loop is already running.\") if not self.is_connected: self.loop.run_until_complete(self.connect()) task = asyncio.Task(method(*args, **kwargs), loop=self.loop) result = self.loop.run_until_complete(task) self.loop.run_until_complete(self.quit()) return result", "def run_task(func): \"\"\" Decorator to wrap an async function in an event loop. Use for main sync interface methods. \"\"\" def _wrapped(*a, **k): loop = asyncio.get_event_loop() return loop.run_until_complete(func(*a, **k)) return _wrapped", "def wait_run_in_executor(func, *args, **kwargs): \"\"\" Run blocking code in a different thread and wait for the result. :param func: Run this function in a different thread :param args: Parameters of the function :param kwargs: Keyword parameters of the function :returns: Return the result of the function \"\"\" loop = asyncio.get_event_loop() future = loop.run_in_executor(None, functools.partial(func, *args, **kwargs)) yield from asyncio.wait([future]) return future.result()"], "rank": 1}
{"nl": "python \"not is none\" \"is not none\"", "code": ["def _not(condition=None, **kwargs): \"\"\" Return the opposite of input condition. :param condition: condition to process. :result: not condition. :rtype: bool \"\"\" result = True if condition is not None: result = not run(condition, **kwargs) return result", "def _not_none(items): \"\"\"Whether the item is a placeholder or contains a placeholder.\"\"\" if not isinstance(items, (tuple, list)): items = (items,) return all(item is not _none for item in items)", "def is_none(string_, default='raise'): \"\"\" Check if a string is equivalent to None. Parameters ---------- string_ : str default : {'raise', False} Default behaviour if none of the \"None\" strings is detected. Returns ------- is_none : bool Examples -------- >>> is_none('2', default=False) False >>> is_none('undefined', default=False) True \"\"\" none = ['none', 'undefined', 'unknown', 'null', ''] if string_.lower() in none: return True elif not default: return False else: raise ValueError('The value \\'{}\\' cannot be mapped to none.' .format(string_))", "def _notnull(expr): \"\"\" Return a sequence or scalar according to the input indicating if the values are not null. :param expr: sequence or scalar :return: sequence or scalar \"\"\" if isinstance(expr, SequenceExpr): return NotNull(_input=expr, _data_type=types.boolean) elif isinstance(expr, Scalar): return NotNull(_input=expr, _value_type=types.boolean)", "def is_all_field_none(self): \"\"\" :rtype: bool \"\"\" if self._type_ is not None: return False if self._value is not None: return False if self._name is not None: return False return True"], "rank": 2}
{"nl": "check if attribute exists python", "code": ["def has_attribute(module_name, attribute_name): \"\"\"Is this attribute present?\"\"\" init_file = '%s/__init__.py' % module_name return any( [attribute_name in init_line for init_line in open(init_file).readlines()] )", "def check_attribute_exists(instance): \"\"\" Additional check for the dimension model, to ensure that attributes given as the key and label attribute on the dimension exist. \"\"\" attributes = instance.get('attributes', {}).keys() if instance.get('key_attribute') not in attributes: return False label_attr = instance.get('label_attribute') if label_attr and label_attr not in attributes: return False return True", "def hasattrs(object, *names): \"\"\" Takes in an object and a variable length amount of named attributes, and checks to see if the object has each property. If any of the attributes are missing, this returns false. :param object: an object that may or may not contain the listed attributes :param names: a variable amount of attribute names to check for :return: True if the object contains each named attribute, false otherwise \"\"\" for name in names: if not hasattr(object, name): return False return True", "def has_field(mc, field_name): \"\"\" detect if a model has a given field has :param field_name: :param mc: :return: \"\"\" try: mc._meta.get_field(field_name) except FieldDoesNotExist: return False return True", "def _validate_key(self, key): \"\"\"Returns a boolean indicating if the attribute name is valid or not\"\"\" return not any([key.startswith(i) for i in self.EXCEPTIONS])"], "rank": 1}
{"nl": "python remove non printing characters", "code": ["def drop_bad_characters(text): \"\"\"Takes a text and drops all non-printable and non-ascii characters and also any whitespace characters that aren't space. :arg str text: the text to fix :returns: text with all bad characters dropped \"\"\" # Strip all non-ascii and non-printable characters text = ''.join([c for c in text if c in ALLOWED_CHARS]) return text", "def clean(ctx, text): \"\"\" Removes all non-printable characters from a text string \"\"\" text = conversions.to_string(text, ctx) return ''.join([c for c in text if ord(c) >= 32])", "def remove_unsafe_chars(text): \"\"\"Remove unsafe unicode characters from a piece of text.\"\"\" if isinstance(text, six.string_types): text = UNSAFE_RE.sub('', text) return text", "def _remove_nonascii(self, df): \"\"\"Make copy and remove non-ascii characters from it.\"\"\" df_copy = df.copy(deep=True) for col in df_copy.columns: if (df_copy[col].dtype == np.dtype('O')): df_copy[col] = df[col].apply( lambda x: re.sub(r'[^\\x00-\\x7f]', r'', x) if isinstance(x, six.string_types) else x) return df_copy", "def _RemoveIllegalXMLCharacters(self, xml_string): \"\"\"Removes illegal characters for XML. If the input is not a string it will be returned unchanged. Args: xml_string (str): XML with possible illegal characters. Returns: str: XML where all illegal characters have been removed. \"\"\" if not isinstance(xml_string, py2to3.STRING_TYPES): return xml_string return self._ILLEGAL_XML_RE.sub('\\ufffd', xml_string)"], "rank": 1}
{"nl": "python pool imap mutltiple argements", "code": ["def imapchain(*a, **kwa): \"\"\" Like map but also chains the results. \"\"\" imap_results = map( *a, **kwa ) return itertools.chain( *imap_results )", "def multiprocess_mapping(func, iterable): \"\"\"Multiprocess mapping the given function on the given iterable. This only works in Linux and Mac systems since Windows has no forking capability. On Windows we fall back on single processing. Also, if we reach memory limits we fall back on single cpu processing. Args: func (func): the function to apply iterable (iterable): the iterable with the elements we want to apply the function on \"\"\" if os.name == 'nt': # In Windows there is no fork. return list(map(func, iterable)) try: p = multiprocessing.Pool() return_data = list(p.imap(func, iterable)) p.close() p.join() return return_data except OSError: return list(map(func, iterable))", "def pool_args(function, sequence, kwargs): \"\"\"Return a single iterator of n elements of lists of length 3, given a sequence of len n.\"\"\" return zip(itertools.repeat(function), sequence, itertools.repeat(kwargs))", "def compute(args): x, y, params = args \"\"\"Callable function for the multiprocessing pool.\"\"\" return x, y, mandelbrot(x, y, params)", "def map(cls, iterable, func, *a, **kw): \"\"\" Iterable-first replacement of Python's built-in `map()` function. \"\"\" return cls(func(x, *a, **kw) for x in iterable)"], "rank": 1}
{"nl": "drop column if all column values are nan in python", "code": ["def clean_df(df, fill_nan=True, drop_empty_columns=True): \"\"\"Clean a pandas dataframe by: 1. Filling empty values with Nan 2. Dropping columns with all empty values Args: df: Pandas DataFrame fill_nan (bool): If any empty values (strings, None, etc) should be replaced with NaN drop_empty_columns (bool): If columns whose values are all empty should be dropped Returns: DataFrame: cleaned DataFrame \"\"\" if fill_nan: df = df.fillna(value=np.nan) if drop_empty_columns: df = df.dropna(axis=1, how='all') return df.sort_index()", "def dropna(self, subset=None): \"\"\"Remove missing values according to Baloo's convention. Parameters ---------- subset : list of str, optional Which columns to check for missing values in. Returns ------- DataFrame DataFrame with no null values in columns. \"\"\" subset = check_and_obtain_subset_columns(subset, self) not_nas = [v.notna() for v in self[subset]._iter()] and_filter = reduce(lambda x, y: x & y, not_nas) return self[and_filter]", "def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df", "def remove_na_arraylike(arr): \"\"\" Return array-like containing only true/non-NaN values, possibly empty. \"\"\" if is_extension_array_dtype(arr): return arr[notna(arr)] else: return arr[notna(lib.values_from_object(arr))]", "def fill_nulls(self, col: str): \"\"\" Fill all null values with NaN values in a column. Null values are ``None`` or en empty string :param col: column name :type col: str :example: ``ds.fill_nulls(\"mycol\")`` \"\"\" n = [None, \"\"] try: self.df[col] = self.df[col].replace(n, nan) except Exception as e: self.err(e)"], "rank": 1}
{"nl": "python check if end of file reached", "code": ["def eof(fd): \"\"\"Determine if end-of-file is reached for file fd.\"\"\" b = fd.read(1) end = len(b) == 0 if not end: curpos = fd.tell() fd.seek(curpos - 1) return end", "def _readuntil(f, end=_TYPE_END): \"\"\"Helper function to read bytes until a certain end byte is hit\"\"\" buf = bytearray() byte = f.read(1) while byte != end: if byte == b'': raise ValueError('File ended unexpectedly. Expected end byte {}.'.format(end)) buf += byte byte = f.read(1) return buf", "def file_empty(fp): \"\"\"Determine if a file is empty or not.\"\"\" # for python 2 we need to use a homemade peek() if six.PY2: contents = fp.read() fp.seek(0) return not bool(contents) else: return not fp.peek()", "def get_known_read_position(fp, buffered=True): \"\"\" Return a position in a file which is known to be read & handled. It assumes a buffered file and streaming processing. \"\"\" buffer_size = io.DEFAULT_BUFFER_SIZE if buffered else 0 return max(fp.tell() - buffer_size, 0)", "def close(self): \"\"\"Flush the file and close it. A closed file cannot be written any more. Calling :meth:`close` more than once is allowed. \"\"\" if not self._closed: self.__flush() object.__setattr__(self, \"_closed\", True)"], "rank": 1}
{"nl": "set table widget cell width python", "code": ["def table_width(self): \"\"\"Return the width of the table including padding and borders.\"\"\" outer_widths = max_dimensions(self.table_data, self.padding_left, self.padding_right)[2] outer_border = 2 if self.outer_border else 0 inner_border = 1 if self.inner_column_border else 0 return table_width(outer_widths, outer_border, inner_border)", "def _column_resized(self, col, old_width, new_width): \"\"\"Update the column width.\"\"\" self.dataTable.setColumnWidth(col, new_width) self._update_layout()", "def append_num_column(self, text: str, index: int): \"\"\" Add value to the output row, width based on index \"\"\" width = self.columns[index][\"width\"] return f\"{text:>{width}}\"", "def _set_tab_width(self, tab_width): \"\"\" Sets the width (in terms of space characters) for tab characters. \"\"\" font_metrics = QtGui.QFontMetrics(self.font) self._control.setTabStopWidth(tab_width * font_metrics.width(' ')) self._tab_width = tab_width", "def pad_cells(table): \"\"\"Pad each cell to the size of the largest cell in its column.\"\"\" col_sizes = [max(map(len, col)) for col in zip(*table)] for row in table: for cell_num, cell in enumerate(row): row[cell_num] = pad_to(cell, col_sizes[cell_num]) return table"], "rank": 1}
{"nl": "from json to obejct python", "code": ["def _unjsonify(x, isattributes=False): \"\"\"Convert JSON string to an ordered defaultdict.\"\"\" if isattributes: obj = json.loads(x) return dict_class(obj) return json.loads(x)", "def from_json(cls, s): \"\"\" Restores the object from the given JSON. :param s: the JSON string to parse :type s: str :return: the \"\"\" d = json.loads(s) return get_dict_handler(d[\"type\"])(d)", "def _construct_from_json(self, rec): \"\"\" Construct this Dagobah instance from a JSON document. \"\"\" self.delete() for required_key in ['dagobah_id', 'created_jobs']: setattr(self, required_key, rec[required_key]) for job_json in rec.get('jobs', []): self._add_job_from_spec(job_json) self.commit(cascade=True)", "def from_json(cls, json_doc): \"\"\"Parse a JSON string and build an entity.\"\"\" try: d = json.load(json_doc) except AttributeError: # catch the read() error d = json.loads(json_doc) return cls.from_dict(d)", "def from_json(cls, json_str): \"\"\"Deserialize the object from a JSON string.\"\"\" d = json.loads(json_str) return cls.from_dict(d)"], "rank": 12}
{"nl": "redis python get list length", "code": ["def llen(self, name): \"\"\" Returns the length of the list. :param name: str the name of the redis key :return: Future() \"\"\" with self.pipe as pipe: return pipe.llen(self.redis_key(name))", "def hstrlen(self, name, key): \"\"\" Return the number of bytes stored in the value of ``key`` within hash ``name`` \"\"\" with self.pipe as pipe: return pipe.hstrlen(self.redis_key(name), key)", "def get(self, key): \"\"\" get a set of keys from redis \"\"\" res = self.connection.get(key) print(res) return res", "def _get_info(self, host, port, unix_socket, auth): \"\"\"Return info dict from specified Redis instance :param str host: redis host :param int port: redis port :rtype: dict \"\"\" client = self._client(host, port, unix_socket, auth) if client is None: return None info = client.info() del client return info", "def values(self): \"\"\" :see::meth:RedisMap.keys \"\"\" for val in self._client.hvals(self.key_prefix): yield self._loads(val)"], "rank": 1}
{"nl": "proxy setup for python", "code": ["def _make_proxy_property(bind_attr, attr_name): def proxy_property(self): \"\"\" proxy \"\"\" bind = getattr(self, bind_attr) return getattr(bind, attr_name) return property(proxy_property)", "def dispatch(self, request, *args, **kwargs): \"\"\"Dispatch all HTTP methods to the proxy.\"\"\" self.request = DownstreamRequest(request) self.args = args self.kwargs = kwargs self._verify_config() self.middleware = MiddlewareSet(self.proxy_middleware) return self.proxy()", "def Proxy(f): \"\"\"A helper to create a proxy method in a class.\"\"\" def Wrapped(self, *args): return getattr(self, f)(*args) return Wrapped", "def set_proxy(proxy_url, transport_proxy=None): \"\"\"Create the proxy to PyPI XML-RPC Server\"\"\" global proxy, PYPI_URL PYPI_URL = proxy_url proxy = xmlrpc.ServerProxy( proxy_url, transport=RequestsTransport(proxy_url.startswith('https://')), allow_none=True)", "def enable_proxy(self, host, port): \"\"\"Enable a default web proxy\"\"\" self.proxy = [host, _number(port)] self.proxy_enabled = True"], "rank": 6}
{"nl": "python check type equals to", "code": ["def istype(obj, check): \"\"\"Like isinstance(obj, check), but strict. This won't catch subclasses. \"\"\" if isinstance(check, tuple): for cls in check: if type(obj) is cls: return True return False else: return type(obj) is check", "def is_type(value): \"\"\"Determine if value is an instance or subclass of the class Type.\"\"\" if isinstance(value, type): return issubclass(value, Type) return isinstance(value, Type)", "def hard_equals(a, b): \"\"\"Implements the '===' operator.\"\"\" if type(a) != type(b): return False return a == b", "def _assert_is_type(name, value, value_type): \"\"\"Assert that a value must be a given type.\"\"\" if not isinstance(value, value_type): if type(value_type) is tuple: types = ', '.join(t.__name__ for t in value_type) raise ValueError('{0} must be one of ({1})'.format(name, types)) else: raise ValueError('{0} must be {1}' .format(name, value_type.__name__))", "def _valid_other_type(x, types): \"\"\" Do all elements of x have a type from types? \"\"\" return all(any(isinstance(el, t) for t in types) for el in np.ravel(x))"], "rank": 7}
{"nl": "python remove none values from a list", "code": ["def clear_list_value(self, value): \"\"\" Clean the argument value to eliminate None or Falsy values if needed. \"\"\" # Don't go any further: this value is empty. if not value: return self.empty_value # Clean empty items if wanted if self.clean_empty: value = [v for v in value if v] return value or self.empty_value", "def filter_none(list_of_points): \"\"\" :param list_of_points: :return: list_of_points with None's removed \"\"\" remove_elementnone = filter(lambda p: p is not None, list_of_points) remove_sublistnone = filter(lambda p: not contains_none(p), remove_elementnone) return list(remove_sublistnone)", "def _remove_none_values(dictionary): \"\"\" Remove dictionary keys whose value is None \"\"\" return list(map(dictionary.pop, [i for i in dictionary if dictionary[i] is None]))", "def _get_non_empty_list(cls, iter): \"\"\"Return a list of the input, excluding all ``None`` values.\"\"\" res = [] for value in iter: if hasattr(value, 'items'): value = cls._get_non_empty_dict(value) or None if value is not None: res.append(value) return res", "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]: \"\"\" Return a new copied dictionary without the keys with ``None`` values from the given Mapping object. \"\"\" return {k: v for k, v in obj.items() if v is not None}"], "rank": 2}
{"nl": "how to see all variables in python", "code": ["def vars_(self): \"\"\" Returns symbol instances corresponding to variables of the current scope. \"\"\" return [x for x in self[self.current_scope].values() if x.class_ == CLASS.var]", "def types(self): \"\"\" Return a list of all the variable types that exist in the Variables object. \"\"\" output = set() for var in self.values(): if var.has_value(): output.update(var.types()) return list(output)", "def functions(self): \"\"\" A list of functions declared or defined in this module. \"\"\" return [v for v in self.globals.values() if isinstance(v, values.Function)]", "def caller_locals(): \"\"\"Get the local variables in the caller's frame.\"\"\" import inspect frame = inspect.currentframe() try: return frame.f_back.f_back.f_locals finally: del frame", "def values(self): \"\"\"return a list of all state values\"\"\" values = [] for __, data in self.items(): values.append(data) return values"], "rank": 4}
{"nl": "python create a filter with cutoff frequency", "code": ["def highpass(cutoff): \"\"\" This strategy uses an exponential approximation for cut-off frequency calculation, found by matching the one-pole Laplace lowpass filter and mirroring the resulting filter to get a highpass. \"\"\" R = thub(exp(cutoff - pi), 2) return (1 - R) / (1 + R * z ** -1)", "def fft_bandpassfilter(data, fs, lowcut, highcut): \"\"\" http://www.swharden.com/blog/2009-01-21-signal-filtering-with-python/#comment-16801 \"\"\" fft = np.fft.fft(data) # n = len(data) # timestep = 1.0 / fs # freq = np.fft.fftfreq(n, d=timestep) bp = fft.copy() # Zero out fft coefficients # bp[10:-10] = 0 # Normalise # bp *= real(fft.dot(fft))/real(bp.dot(bp)) bp *= fft.dot(fft) / bp.dot(bp) # must multipy by 2 to get the correct amplitude ibp = 12 * np.fft.ifft(bp) return ibp", "def range(self, chromosome, start, stop, exact=False): \"\"\" Shortcut to do range filters on genomic datasets. \"\"\" return self._clone( filters=[GenomicFilter(chromosome, start, stop, exact)])", "def filtered_image(self, im): \"\"\"Returns a filtered image after applying the Fourier-space filters\"\"\" q = np.fft.fftn(im) for k,v in self.filters: q[k] -= v return np.real(np.fft.ifftn(q))", "def scaled_fft(fft, scale=1.0): \"\"\" Produces a nicer graph, I'm not sure if this is correct \"\"\" data = np.zeros(len(fft)) for i, v in enumerate(fft): data[i] = scale * (i * v) / NUM_SAMPLES return data"], "rank": 2}
{"nl": "python shuffle columns of a matrix", "code": ["def _reshuffle(mat, shape): \"\"\"Reshuffle the indicies of a bipartite matrix A[ij,kl] -> A[lj,ki].\"\"\" return np.reshape( np.transpose(np.reshape(mat, shape), (3, 1, 2, 0)), (shape[3] * shape[1], shape[0] * shape[2]))", "def _shuffle(data, idx): \"\"\"Shuffle the data.\"\"\" shuffle_data = [] for idx_k, idx_v in data: shuffle_data.append((idx_k, mx.ndarray.array(idx_v.asnumpy()[idx], idx_v.context))) return shuffle_data", "def sort_matrix(a,n=0): \"\"\" This will rearrange the array a[n] from lowest to highest, and rearrange the rest of a[i]'s in the same way. It is dumb and slow. Returns a numpy array. \"\"\" a = _n.array(a) return a[:,a[n,:].argsort()]", "def _swap_rows(self, i, j): \"\"\"Swap i and j rows As the side effect, determinant flips. \"\"\" L = np.eye(3, dtype='intc') L[i, i] = 0 L[j, j] = 0 L[i, j] = 1 L[j, i] = 1 self._L.append(L.copy()) self._A = np.dot(L, self._A)", "def consistent_shuffle(*lists): \"\"\" Shuffle lists consistently. Parameters ---------- *lists Variable length number of lists Returns ------- shuffled_lists : tuple of lists All of the lists are shuffled consistently Examples -------- >>> import mpu, random; random.seed(8) >>> mpu.consistent_shuffle([1,2,3], ['a', 'b', 'c'], ['A', 'B', 'C']) ([3, 2, 1], ['c', 'b', 'a'], ['C', 'B', 'A']) \"\"\" perm = list(range(len(lists[0]))) random.shuffle(perm) lists = tuple([sublist[index] for index in perm] for sublist in lists) return lists"], "rank": 1}
{"nl": "python delete all listswith similar name", "code": ["def without(seq1, seq2): r\"\"\"Return a list with all elements in `seq2` removed from `seq1`, order preserved. Examples: >>> without([1,2,3,1,2], [1]) [2, 3, 2] \"\"\" if isSet(seq2): d2 = seq2 else: d2 = set(seq2) return [elt for elt in seq1 if elt not in d2]", "def distinct(l): \"\"\" Return a list where the duplicates have been removed. Args: l (list): the list to filter. Returns: list: the same list without duplicates. \"\"\" seen = set() seen_add = seen.add return (_ for _ in l if not (_ in seen or seen_add(_)))", "def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def not_matching_list(self): \"\"\" Return a list of string which don't match the given regex. \"\"\" pre_result = comp(self.regex) return [x for x in self.data if not pre_result.search(str(x))]", "def unique(transactions): \"\"\" Remove any duplicate entries. \"\"\" seen = set() # TODO: Handle comments return [x for x in transactions if not (x in seen or seen.add(x))]"], "rank": 2}
{"nl": "round numbers in array to nearest whole python", "code": ["def round_array(array_in): \"\"\" arr_out = round_array(array_in) Rounds an array and recasts it to int. Also works on scalars. \"\"\" if isinstance(array_in, ndarray): return np.round(array_in).astype(int) else: return int(np.round(array_in))", "def proper_round(n): \"\"\" rounds float to closest int :rtype: int :param n: float \"\"\" return int(n) + (n / abs(n)) * int(abs(n - int(n)) >= 0.5) if n != 0 else 0", "def ceil_nearest(x, dx=1): \"\"\" ceil a number to within a given rounding accuracy \"\"\" precision = get_sig_digits(dx) return round(math.ceil(float(x) / dx) * dx, precision)", "def get_rounded(self, digits): \"\"\" Return a vector with the elements rounded to the given number of digits. \"\"\" result = self.copy() result.round(digits) return result", "def index_nearest(value, array): \"\"\" expects a _n.array returns the global minimum of (value-array)^2 \"\"\" a = (array-value)**2 return index(a.min(), a)"], "rank": 1}
{"nl": "change utc time to relative time python", "code": ["def convert_2_utc(self, datetime_, timezone): \"\"\"convert to datetime to UTC offset.\"\"\" datetime_ = self.tz_mapper[timezone].localize(datetime_) return datetime_.astimezone(pytz.UTC)", "def datetime_local_to_utc(local): \"\"\" Simple function to convert naive :std:`datetime.datetime` object containing local time to a naive :std:`datetime.datetime` object with UTC time. \"\"\" timestamp = time.mktime(local.timetuple()) return datetime.datetime.utcfromtimestamp(timestamp)", "def to_utc(self, dt): \"\"\"Convert any timestamp to UTC (with tzinfo).\"\"\" if dt.tzinfo is None: return dt.replace(tzinfo=self.utc) return dt.astimezone(self.utc)", "def normalize_time(timestamp): \"\"\"Normalize time in arbitrary timezone to UTC naive object.\"\"\" offset = timestamp.utcoffset() if offset is None: return timestamp return timestamp.replace(tzinfo=None) - offset", "def timestamp_to_datetime(cls, time_stamp, localized=True): \"\"\" Converts a UTC timestamp to a datetime.datetime.\"\"\" ret = datetime.datetime.utcfromtimestamp(time_stamp) if localized: ret = localize(ret, pytz.utc) return ret"], "rank": 2}
{"nl": "how to check if file is not empty python", "code": ["def file_empty(fp): \"\"\"Determine if a file is empty or not.\"\"\" # for python 2 we need to use a homemade peek() if six.PY2: contents = fp.read() fp.seek(0) return not bool(contents) else: return not fp.peek()", "def file_exists(fname): \"\"\"Check if a file exists and is non-empty. \"\"\" try: return fname and os.path.exists(fname) and os.path.getsize(fname) > 0 except OSError: return False", "def isemptyfile(filepath): \"\"\"Determine if the file both exists and isempty Args: filepath (str, path): file path Returns: bool \"\"\" exists = os.path.exists(safepath(filepath)) if exists: filesize = os.path.getsize(safepath(filepath)) return filesize == 0 else: return False", "def isfile_notempty(inputfile: str) -> bool: \"\"\"Check if the input filename with path is a file and is not empty.\"\"\" try: return isfile(inputfile) and getsize(inputfile) > 0 except TypeError: raise TypeError('inputfile is not a valid type')", "def _cnx_is_empty(in_file): \"\"\"Check if cnr or cns files are empty (only have a header) \"\"\" with open(in_file) as in_handle: for i, line in enumerate(in_handle): if i > 0: return False return True"], "rank": 1}
{"nl": "area of a triangle using python", "code": ["def get_tri_area(pts): \"\"\" Given a list of coords for 3 points, Compute the area of this triangle. Args: pts: [a, b, c] three points \"\"\" a, b, c = pts[0], pts[1], pts[2] v1 = np.array(b) - np.array(a) v2 = np.array(c) - np.array(a) area_tri = abs(sp.linalg.norm(sp.cross(v1, v2)) / 2) return area_tri", "def triangle_area(pt1, pt2, pt3): r\"\"\"Return the area of a triangle. Parameters ---------- pt1: (X,Y) ndarray Starting vertex of a triangle pt2: (X,Y) ndarray Second vertex of a triangle pt3: (X,Y) ndarray Ending vertex of a triangle Returns ------- area: float Area of the given triangle. \"\"\" a = 0.0 a += pt1[0] * pt2[1] - pt2[0] * pt1[1] a += pt2[0] * pt3[1] - pt3[0] * pt2[1] a += pt3[0] * pt1[1] - pt1[0] * pt3[1] return abs(a) / 2", "def area (self): \"\"\"area() -> number Returns the area of this Polygon. \"\"\" area = 0.0 for segment in self.segments(): area += ((segment.p.x * segment.q.y) - (segment.q.x * segment.p.y))/2 return area", "def area(x,y): \"\"\" Calculate the area of a polygon given as x(...),y(...) Implementation of Shoelace formula \"\"\" # http://stackoverflow.com/questions/24467972/calculate-area-of-polygon-given-x-y-coordinates return 0.5 * np.abs(np.dot(x, np.roll(y, 1)) - np.dot(y, np.roll(x, 1)))", "def get_area(self): \"\"\"Calculate area of bounding box.\"\"\" return (self.p2.x-self.p1.x)*(self.p2.y-self.p1.y)"], "rank": 1}
{"nl": "python make a copy not reference", "code": ["def copy(self): \"\"\"Return a shallow copy.\"\"\" return self.__class__(self.operations.copy(), self.collection, self.document)", "def copy(self): \"\"\"Create an identical (deep) copy of this element.\"\"\" result = self.space.element() result.assign(self) return result", "def __copy__(self): \"\"\"A magic method to implement shallow copy behavior.\"\"\" return self.__class__.load(self.dump(), context=self.context)", "def copy(obj): def copy(self): \"\"\" Copy self to a new object. \"\"\" from copy import deepcopy return deepcopy(self) obj.copy = copy return obj", "def copy(self): \"\"\" Creates a copy of model \"\"\" return self.__class__(field_type=self.get_field_type(), data=self.export_data())"], "rank": 4}
{"nl": "python heappush max heap", "code": ["def heappush_max(heap, item): \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\" heap.append(item) _siftdown_max(heap, 0, len(heap) - 1)", "def _heappush_max(heap, item): \"\"\" why is this not in heapq \"\"\" heap.append(item) heapq._siftdown_max(heap, 0, len(heap) - 1)", "def _heapreplace_max(heap, item): \"\"\"Maxheap version of a heappop followed by a heappush.\"\"\" returnitem = heap[0] # raises appropriate IndexError if heap is empty heap[0] = item _siftup_max(heap, 0) return returnitem", "def heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt", "def _heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt"], "rank": 6}
{"nl": "pythonreturn json file from a function", "code": ["def open_json(file_name): \"\"\" returns json contents as string \"\"\" with open(file_name, \"r\") as json_data: data = json.load(json_data) return data", "def json_get_data(filename): \"\"\"Get data from json file \"\"\" with open(filename) as fp: json_data = json.load(fp) return json_data return False", "def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def read_json(location): \"\"\"Open and load JSON from file. location (Path): Path to JSON file. RETURNS (dict): Loaded JSON content. \"\"\" location = ensure_path(location) with location.open('r', encoding='utf8') as f: return ujson.load(f)", "def load_parameters(self, source): \"\"\"For JSON, the source it the file path\"\"\" with open(source) as parameters_source: return json.loads(parameters_source.read())"], "rank": 17}
{"nl": "python check if file can be opened", "code": ["def is_readable(filename): \"\"\"Check if file is a regular file and is readable.\"\"\" return os.path.isfile(filename) and os.access(filename, os.R_OK)", "def is_filelike(ob): \"\"\"Check for filelikeness of an object. Needed to distinguish it from file names. Returns true if it has a read or a write method. \"\"\" if hasattr(ob, 'read') and callable(ob.read): return True if hasattr(ob, 'write') and callable(ob.write): return True return False", "def _is_readable(self, obj): \"\"\"Check if the argument is a readable file-like object.\"\"\" try: read = getattr(obj, 'read') except AttributeError: return False else: return is_method(read, max_arity=1)", "def is_readable(fp, size=1): \"\"\" Check if the file-like object is readable. :param fp: file-like object :param size: byte size :return: bool \"\"\" read_size = len(fp.read(size)) fp.seek(-read_size, 1) return read_size == size", "def readable(path): \"\"\"Test whether a path exists and is readable. Returns None for broken symbolic links or a failing stat() and False if the file exists but does not have read permission. True is returned if the file is readable.\"\"\" try: st = os.stat(path) return 0 != st.st_mode & READABLE_MASK except os.error: return None return True"], "rank": 1}
{"nl": "python lastworking day of month", "code": ["def get_last_day_of_month(t: datetime) -> int: \"\"\" Returns day number of the last day of the month :param t: datetime :return: int \"\"\" tn = t + timedelta(days=32) tn = datetime(year=tn.year, month=tn.month, day=1) tt = tn - timedelta(hours=1) return tt.day", "def get_last_weekday_in_month(year, month, weekday): \"\"\"Get the last weekday in a given month. e.g: >>> # the last monday in Jan 2013 >>> Calendar.get_last_weekday_in_month(2013, 1, MON) datetime.date(2013, 1, 28) \"\"\" day = date(year, month, monthrange(year, month)[1]) while True: if day.weekday() == weekday: break day = day - timedelta(days=1) return day", "def last_day(year=_year, month=_month): \"\"\" get the current month's last day :param year: default to current year :param month: default to current month :return: month's last day \"\"\" last_day = calendar.monthrange(year, month)[1] return datetime.date(year=year, month=month, day=last_day)", "def previous_workday(dt): \"\"\" returns previous weekday used for observances \"\"\" dt -= timedelta(days=1) while dt.weekday() > 4: # Mon-Fri are 0-4 dt -= timedelta(days=1) return dt", "def prevmonday(num): \"\"\" Return unix SECOND timestamp of \"num\" mondays ago \"\"\" today = get_today() lastmonday = today - timedelta(days=today.weekday(), weeks=num) return lastmonday"], "rank": 2}
{"nl": "how to get the maximum cell in one row in python", "code": ["def argmax(self, rows: List[Row], column: ComparableColumn) -> List[Row]: \"\"\" Takes a list of rows and a column name and returns a list containing a single row (dict from columns to cells) that has the maximum numerical value in the given column. We return a list instead of a single dict to be consistent with the return type of ``select`` and ``all_rows``. \"\"\" if not rows: return [] value_row_pairs = [(row.values[column.name], row) for row in rows] if not value_row_pairs: return [] # Returns a list containing the row with the max cell value. return [sorted(value_row_pairs, key=lambda x: x[0], reverse=True)[0][1]]", "def get_last_filled_cell(self, table=None): \"\"\"Returns key for the bottommost rightmost cell with content Parameters ---------- table: Integer, defaults to None \\tLimit search to this table \"\"\" maxrow = 0 maxcol = 0 for row, col, tab in self.dict_grid: if table is None or tab == table: maxrow = max(row, maxrow) maxcol = max(col, maxcol) return maxrow, maxcol, table", "def _longest_val_in_column(self, col): \"\"\" get size of longest value in specific column :param col: str, column name :return int \"\"\" try: # +2 is for implicit separator return max([len(x[col]) for x in self.table if x[col]]) + 2 except KeyError: logger.error(\"there is no column %r\", col) raise", "def index(self, value): \"\"\" Return the smallest index of the row(s) with this column equal to value. \"\"\" for i in xrange(len(self.parentNode)): if getattr(self.parentNode[i], self.Name) == value: return i raise ValueError(value)", "def series_table_row_offset(self, series): \"\"\" Return the number of rows preceding the data table for *series* in the Excel worksheet. \"\"\" title_and_spacer_rows = series.index * 2 data_point_rows = series.data_point_offset return title_and_spacer_rows + data_point_rows"], "rank": 1}
{"nl": "redefine the range in python", "code": ["def add_range(self, sequence, begin, end): \"\"\"Add a read_range primitive\"\"\" sequence.parser_tree = parsing.Range(self.value(begin).strip(\"'\"), self.value(end).strip(\"'\")) return True", "def negate(self): \"\"\"Reverse the range\"\"\" self.from_value, self.to_value = self.to_value, self.from_value self.include_lower, self.include_upper = self.include_upper, self.include_lower", "def empty(self, start=None, stop=None): \"\"\"Empty the range from start to stop. Like delete, but no Error is raised if the entire range isn't mapped. \"\"\" self.set(NOT_SET, start=start, stop=stop)", "def get_range(self, start=None, stop=None): \"\"\"Return a RangeMap for the range start to stop. Returns: A RangeMap \"\"\" return self.from_iterable(self.ranges(start, stop))", "def min_values(args): \"\"\" Return possible range for min function. \"\"\" return Interval(min(x.low for x in args), min(x.high for x in args))"], "rank": 2}
{"nl": "escape percent sign in python", "code": ["def quote(s, unsafe='/'): \"\"\"Pass in a dictionary that has unsafe characters as the keys, and the percent encoded value as the value.\"\"\" res = s.replace('%', '%25') for c in unsafe: res = res.replace(c, '%' + (hex(ord(c)).upper())[2:]) return res", "def escape(s): \"\"\"Escape a URL including any /.\"\"\" if not isinstance(s, bytes): s = s.encode('utf-8') return quote(s, safe='~')", "def url_encode(url): \"\"\" Convert special characters using %xx escape. :param url: str :return: str - encoded url \"\"\" if isinstance(url, text_type): url = url.encode('utf8') return quote(url, ':/%?&=')", "def _escape(self, s): \"\"\"Escape bad characters for regular expressions. Similar to `re.escape` but allows '%' to pass through. \"\"\" for ch, r_ch in self.ESCAPE_SETS: s = s.replace(ch, r_ch) return s", "def do_forceescape(value): \"\"\"Enforce HTML escaping. This will probably double escape variables.\"\"\" if hasattr(value, '__html__'): value = value.__html__() return escape(unicode(value))"], "rank": 1}
{"nl": "plot kde over histogram python", "code": ["def plot_kde(data, ax, title=None, color='r', fill_bt=True): \"\"\" Plot a smoothed (by kernel density estimate) histogram. :type data: numpy array :param data: An array containing the data to be plotted :type ax: matplotlib.Axes :param ax: The Axes object to draw to :type title: str :param title: The plot title :type color: str :param color: The color of the histogram line and fill. Note that the fill will be plotted with an alpha of 0.35. :type fill_bt: bool :param fill_bt: Specify whether to fill the area beneath the histogram line \"\"\" if isinstance(data, list): data = np.asarray(data) e = kde.KDEUnivariate(data.astype(np.float)) e.fit() ax.plot(e.support, e.density, color=color, alpha=0.9, linewidth=2.25) if fill_bt: ax.fill_between(e.support, e.density, alpha=.35, zorder=1, antialiased=True, color=color) if title is not None: t = ax.set_title(title) t.set_y(1.05)", "def plot(self): \"\"\"Plot the empirical histogram versus best-fit distribution's PDF.\"\"\" plt.plot(self.bin_edges, self.hist, self.bin_edges, self.best_pdf)", "def PyplotHistogram(): \"\"\" ============================================================= Demo of the histogram (hist) function with multiple data sets ============================================================= Plot histogram with multiple sample sets and demonstrate: * Use of legend with multiple sample sets * Stacked bars * Step curve with no fill * Data sets of different sample sizes Selecting different bin counts and sizes can significantly affect the shape of a histogram. The Astropy docs have a great section on how to select these parameters: http://docs.astropy.org/en/stable/visualization/histogram.html \"\"\" import numpy as np import matplotlib.pyplot as plt np.random.seed(0) n_bins = 10 x = np.random.randn(1000, 3) fig, axes = plt.subplots(nrows=2, ncols=2) ax0, ax1, ax2, ax3 = axes.flatten() colors = ['red', 'tan', 'lime'] ax0.hist(x, n_bins, normed=1, histtype='bar', color=colors, label=colors) ax0.legend(prop={'size': 10}) ax0.set_title('bars with legend') ax1.hist(x, n_bins, normed=1, histtype='bar', stacked=True) ax1.set_title('stacked bar') ax2.hist(x, n_bins, histtype='step', stacked=True, fill=False) ax2.set_title('stack step (unfilled)') # Make a multiple-histogram of data-sets with different length. x_multi = [np.random.randn(n) for n in [10000, 5000, 2000]] ax3.hist(x_multi, n_bins, histtype='bar') ax3.set_title('different sample sizes') fig.tight_layout() return fig", "def __call__(self, kind: Optional[str] = None, **kwargs): \"\"\"Use the plotter as callable.\"\"\" return plot(self.histogram, kind=kind, **kwargs)", "def _histplot_op(ax, data, **kwargs): \"\"\"Add a histogram for the data to the axes.\"\"\" bins = get_bins(data) ax.hist(data, bins=bins, align=\"left\", density=True, **kwargs) return ax"], "rank": 1}
{"nl": "normalize to 1 in python", "code": ["def normalize(numbers): \"\"\"Multiply each number by a constant such that the sum is 1.0 >>> normalize([1,2,1]) [0.25, 0.5, 0.25] \"\"\" total = float(sum(numbers)) return [n / total for n in numbers]", "def normalize(data): \"\"\"Normalize the data to be in the [0, 1] range. :param data: :return: normalized data \"\"\" out_data = data.copy() for i, sample in enumerate(out_data): out_data[i] /= sum(out_data[i]) return out_data", "def convert_column(self, values): \"\"\"Normalize values.\"\"\" assert all(values >= 0), 'Cannot normalize a column with negatives' total = sum(values) if total > 0: return values / total else: return values", "def normalize(v, axis=None, eps=1e-10): \"\"\"L2 Normalize along specified axes.\"\"\" return v / max(anorm(v, axis=axis, keepdims=True), eps)", "def _normalize(image): \"\"\"Normalize the image to zero mean and unit variance.\"\"\" offset = tf.constant(MEAN_RGB, shape=[1, 1, 3]) image -= offset scale = tf.constant(STDDEV_RGB, shape=[1, 1, 3]) image /= scale return image"], "rank": 1}
{"nl": "python interpolate between different coordinate matrices", "code": ["def _linearInterpolationTransformMatrix(matrix1, matrix2, value): \"\"\" Linear, 'oldstyle' interpolation of the transform matrix.\"\"\" return tuple(_interpolateValue(matrix1[i], matrix2[i], value) for i in range(len(matrix1)))", "def _linear_interpolation(x, X, Y): \"\"\"Given two data points [X,Y], linearly interpolate those at x. \"\"\" return (Y[1] * (x - X[0]) + Y[0] * (X[1] - x)) / (X[1] - X[0])", "def interpolate(f1: float, f2: float, factor: float) -> float: \"\"\" Linearly interpolate between two float values. \"\"\" return f1 + (f2 - f1) * factor", "def lin_interp(x, rangeX, rangeY): \"\"\" Interpolate linearly variable x in rangeX onto rangeY. \"\"\" s = (x - rangeX[0]) / mag(rangeX[1] - rangeX[0]) y = rangeY[0] * (1 - s) + rangeY[1] * s return y", "def interp(x, xp, *args, **kwargs): \"\"\"Wrap interpolate_1d for deprecated interp.\"\"\" return interpolate_1d(x, xp, *args, **kwargs)"], "rank": 1}
{"nl": "see properties of an object python", "code": ["def dict_from_object(obj: object): \"\"\"Convert a object into dictionary with all of its readable attributes.\"\"\" # If object is a dict instance, no need to convert. return (obj if isinstance(obj, dict) else {attr: getattr(obj, attr) for attr in dir(obj) if not attr.startswith('_')})", "def dict_self(self): \"\"\"Return the self object attributes not inherited as dict.\"\"\" return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}", "def as_dict(self): \"\"\"Package up the public attributes as a dict.\"\"\" attrs = vars(self) return {key: attrs[key] for key in attrs if not key.startswith('_')}", "def get_object_attrs(obj): \"\"\" Get the attributes of an object using dir. This filters protected attributes \"\"\" attrs = [k for k in dir(obj) if not k.startswith('__')] if not attrs: attrs = dir(obj) return attrs", "def value(self): \"\"\"Value of property.\"\"\" if self._prop.fget is None: raise AttributeError('Unable to read attribute') return self._prop.fget(self._obj)"], "rank": 8}
{"nl": "how to check if sprites collide in python", "code": ["def check_player_collision(self): \"\"\"Check to see if we are colliding with the player.\"\"\" player_tiles = r.TileMapManager.active_map.grab_collisions(self.char.coords) enemy_tiles = r.TileMapManager.active_map.grab_collisions(self.coords) #Check to see if any of the tiles are the same. If so, there is a collision. for ptile in player_tiles: for etile in enemy_tiles: if r.TileMapManager.active_map.pixels_to_tiles(ptile.coords) == r.TileMapManager.active_map.pixels_to_tiles(etile.coords): return True return False", "def query_collision(collision_object): \"\"\" Check to see if the specified object is colliding with any of the objects currently in the Collision Manager Returns the first object we are colliding with if there was a collision and None if no collisions was found \"\"\" global collidable_objects # Note that we use a Brute Force approach for the time being. # It performs horribly under heavy loads, but it meets # our needs for the time being. for obj in collidable_objects: # Make sure we don't check ourself against ourself. if obj.obj_id is not collision_object.obj_id: if collision_object.is_colliding(obj): # A collision has been detected. Return the object that we are colliding with. return obj # No collision was noticed. Return None. return None", "def boxes_intersect(box1, box2): \"\"\"Determines if two rectangles, each input as a tuple (xmin, xmax, ymin, ymax), intersect.\"\"\" xmin1, xmax1, ymin1, ymax1 = box1 xmin2, xmax2, ymin2, ymax2 = box2 if interval_intersection_width(xmin1, xmax1, xmin2, xmax2) and \\ interval_intersection_width(ymin1, ymax1, ymin2, ymax2): return True else: return False", "def line_line_collide(line1, line2): \"\"\"Determine if two line segments meet. This is a helper for :func:`convex_hull_collide` in the special case that the two convex hulls are actually just line segments. (Even in this case, this is only problematic if both segments are on a single line.) Args: line1 (numpy.ndarray): ``2 x 2`` array of start and end nodes. line2 (numpy.ndarray): ``2 x 2`` array of start and end nodes. Returns: bool: Indicating if the line segments collide. \"\"\" s, t, success = segment_intersection( line1[:, 0], line1[:, 1], line2[:, 0], line2[:, 1] ) if success: return _helpers.in_interval(s, 0.0, 1.0) and _helpers.in_interval( t, 0.0, 1.0 ) else: disjoint, _ = parallel_lines_parameters( line1[:, 0], line1[:, 1], line2[:, 0], line2[:, 1] ) return not disjoint", "def CheckDisjointCalendars(self): \"\"\"Check whether any old service periods intersect with any new ones. This is a rather coarse check based on transitfeed.SevicePeriod.GetDateRange. Returns: True if the calendars are disjoint or False if not. \"\"\" # TODO: Do an exact check here. a_service_periods = self.feed_merger.a_schedule.GetServicePeriodList() b_service_periods = self.feed_merger.b_schedule.GetServicePeriodList() for a_service_period in a_service_periods: a_start, a_end = a_service_period.GetDateRange() for b_service_period in b_service_periods: b_start, b_end = b_service_period.GetDateRange() overlap_start = max(a_start, b_start) overlap_end = min(a_end, b_end) if overlap_end >= overlap_start: return False return True"], "rank": 1}
{"nl": "python function get all objects of certain type", "code": ["def get_object_or_child_by_type(self, *types): \"\"\" Get object if child already been read or get child. Use this method for fast access to objects in case of static configurations. :param types: requested object types. :return: all children of the specified types. \"\"\" objects = self.get_objects_or_children_by_type(*types) return objects[0] if any(objects) else None", "def type(self): \"\"\"Returns type of the data for the given FeatureType.\"\"\" if self is FeatureType.TIMESTAMP: return list if self is FeatureType.BBOX: return BBox return dict", "def types(self): \"\"\" Return a list of all the variable types that exist in the Variables object. \"\"\" output = set() for var in self.values(): if var.has_value(): output.update(var.types()) return list(output)", "def members(self, uid=\"*\", objects=False): \"\"\" members() issues an ldap query for all users, and returns a dict for each matching entry. This can be quite slow, and takes roughly 3s to complete. You may optionally restrict the scope by specifying a uid, which is roughly equivalent to a search(uid='foo') \"\"\" entries = self.search(uid='*') if objects: return self.memberObjects(entries) result = [] for entry in entries: result.append(entry[1]) return result", "def determine_types(self): \"\"\" Determine ES type names from request data. In particular `request.matchdict['collections']` is used to determine types names. Its value is comma-separated sequence of collection names under which views have been registered. \"\"\" from nefertari.elasticsearch import ES collections = self.get_collections() resources = self.get_resources(collections) models = set([res.view.Model for res in resources]) es_models = [mdl for mdl in models if mdl and getattr(mdl, '_index_enabled', False)] types = [ES.src2type(mdl.__name__) for mdl in es_models] return types"], "rank": 1}
{"nl": "how to cast an object as a float python", "code": ["def _tofloat(obj): \"\"\"Convert to float if object is a float string.\"\"\" if \"inf\" in obj.lower().strip(): return obj try: return int(obj) except ValueError: try: return float(obj) except ValueError: return obj", "def parse_float_literal(ast, _variables=None): \"\"\"Parse a float value node in the AST.\"\"\" if isinstance(ast, (FloatValueNode, IntValueNode)): return float(ast.value) return INVALID", "def _force_float(v): \"\"\" Converts given argument to float. On fail logs warning and returns 0.0. Args: v (any): value to convert to float Returns: float: converted v or 0.0 if conversion failed. \"\"\" try: return float(v) except Exception as exc: return float('nan') logger.warning('Failed to convert {} to float with {} error. Using 0 instead.'.format(v, exc))", "def filter_float(n: Node, query: str) -> float: \"\"\" Filter and ensure that the returned value is of type int. \"\"\" return _scalariter2item(n, query, float)", "def get_property_as_float(self, name: str) -> float: \"\"\"Return the value of a float property. :return: The property value (float). Raises exception if property with name doesn't exist. .. versionadded:: 1.0 Scriptable: Yes \"\"\" return float(self.__instrument.get_property(name))"], "rank": 1}
{"nl": "python redis close conn", "code": ["def exit(self): \"\"\" Closes the connection \"\"\" self.pubsub.unsubscribe() self.client.connection_pool.disconnect() logger.info(\"Connection to Redis closed\")", "def disconnect(self): \"\"\"Gracefully close connection to stomp server.\"\"\" if self._connected: self._connected = False self._conn.disconnect()", "def _close(self): \"\"\" Closes the client connection to the database. \"\"\" if self.connection: with self.wrap_database_errors: self.connection.client.close()", "def _ReturnConnection(self): \"\"\" Returns a connection back to the pool @author: Nick Verbeck @since: 9/7/2008 \"\"\" if self.conn is not None: if self.connInfo.commitOnEnd is True or self.commitOnEnd is True: self.conn.Commit() Pool().returnConnection(self.conn) self.conn = None", "def disconnect(self): \"\"\" Closes the connection. \"\"\" self.logger.debug('Close connection...') self.auto_reconnect = False if self.websocket is not None: self.websocket.close()"], "rank": 1}
{"nl": "python dialog box to specify folder", "code": ["def ask_folder(message='Select folder.', default='', title=''): \"\"\" A dialog to get a directory name. Returns the name of a directory, or None if user chose to cancel. If the \"default\" argument specifies a directory name, and that directory exists, then the dialog box will start with that directory. :param message: message to be displayed. :param title: window title :param default: default folder path :rtype: None or string \"\"\" return backend_api.opendialog(\"ask_folder\", dict(message=message, default=default, title=title))", "def ask_dir(self): \"\"\" dialogue box for choosing directory \"\"\" args ['directory'] = askdirectory(**self.dir_opt) self.dir_text.set(args ['directory'])", "def popup(self, title, callfn, initialdir=None): \"\"\"Let user select a directory.\"\"\" super(DirectorySelection, self).popup(title, callfn, initialdir)", "def browse_dialog_dir(): \"\"\" Open up a GUI browse dialog window and let to user pick a target directory. :return str: Target directory path \"\"\" _go_to_package() logger_directory.info(\"enter browse_dialog\") _path_bytes = subprocess.check_output(['python', 'gui_dir_browse.py'], shell=False) _path = _fix_path_bytes(_path_bytes, file=False) if len(_path) >= 1: _path = _path[0] else: _path = \"\" logger_directory.info(\"chosen path: {}\".format(_path)) logger_directory.info(\"exit browse_dialog\") return _path", "def on_source_directory_chooser_clicked(self): \"\"\"Autoconnect slot activated when tbSourceDir is clicked.\"\"\" title = self.tr('Set the source directory for script and scenario') self.choose_directory(self.source_directory, title)"], "rank": 1}
{"nl": "python how to remove docstrings from compiled code", "code": ["def setup(app): \"\"\" Just connects the docstring pre_processor and should_skip functions to be applied on all docstrings. \"\"\" app.connect('autodoc-process-docstring', lambda *args: pre_processor(*args, namer=audiolazy_namer)) app.connect('autodoc-skip-member', should_skip)", "def _replace_docstring_header(paragraph): \"\"\"Process NumPy-like function docstrings.\"\"\" # Replace Markdown headers in docstrings with light headers in bold. paragraph = re.sub(_docstring_header_pattern, r'*\\1*', paragraph, ) paragraph = re.sub(_docstring_parameters_pattern, r'\\n* `\\1` (\\2)\\n', paragraph, ) return paragraph", "def process_docstring(app, what, name, obj, options, lines): \"\"\"Process the docstring for a given python object. Called when autodoc has read and processed a docstring. `lines` is a list of docstring lines that `_process_docstring` modifies in place to change what Sphinx outputs. The following settings in conf.py control what styles of docstrings will be parsed: * ``napoleon_google_docstring`` -- parse Google style docstrings * ``napoleon_numpy_docstring`` -- parse NumPy style docstrings Parameters ---------- app : sphinx.application.Sphinx Application object representing the Sphinx process. what : str A string specifying the type of the object to which the docstring belongs. Valid values: \"module\", \"class\", \"exception\", \"function\", \"method\", \"attribute\". name : str The fully qualified name of the object. obj : module, class, exception, function, method, or attribute The object to which the docstring belongs. options : sphinx.ext.autodoc.Options The options given to the directive: an object with attributes inherited_members, undoc_members, show_inheritance and noindex that are True if the flag option of same name was given to the auto directive. lines : list of str The lines of the docstring, see above. .. note:: `lines` is modified *in place* Notes ----- This function is (to most parts) taken from the :mod:`sphinx.ext.napoleon` module, sphinx version 1.3.1, and adapted to the classes defined here\"\"\" result_lines = lines if app.config.napoleon_numpy_docstring: docstring = ExtendedNumpyDocstring( result_lines, app.config, app, what, name, obj, options) result_lines = docstring.lines() if app.config.napoleon_google_docstring: docstring = ExtendedGoogleDocstring( result_lines, app.config, app, what, name, obj, options) result_lines = docstring.lines() lines[:] = result_lines[:]", "def process_docstring(app, what, name, obj, options, lines): \"\"\"React to a docstring event and append contracts to it.\"\"\" # pylint: disable=unused-argument # pylint: disable=too-many-arguments lines.extend(_format_contracts(what=what, obj=obj))", "def debug_src(src, pm=False, globs=None): \"\"\"Debug a single doctest docstring, in argument `src`'\"\"\" testsrc = script_from_examples(src) debug_script(testsrc, pm, globs)"], "rank": 5}
{"nl": "math normalize a matrix python", "code": ["def normalize_matrix(matrix): \"\"\"Fold all values of the matrix into [0, 1].\"\"\" abs_matrix = np.abs(matrix.copy()) return abs_matrix / abs_matrix.max()", "def normalize(X): \"\"\" equivalent to scipy.preprocessing.normalize on sparse matrices , but lets avoid another depedency just for a small utility function \"\"\" X = coo_matrix(X) X.data = X.data / sqrt(bincount(X.row, X.data ** 2))[X.row] return X", "def cell_normalize(data): \"\"\" Returns the data where the expression is normalized so that the total count per cell is equal. \"\"\" if sparse.issparse(data): data = sparse.csc_matrix(data.astype(float)) # normalize in-place sparse_cell_normalize(data.data, data.indices, data.indptr, data.shape[1], data.shape[0]) return data data_norm = data.astype(float) total_umis = [] for i in range(data.shape[1]): di = data_norm[:,i] total_umis.append(di.sum()) di /= total_umis[i] med = np.median(total_umis) data_norm *= med return data_norm", "def v_normalize(v): \"\"\" Normalizes the given vector. The vector given may have any number of dimensions. \"\"\" vmag = v_magnitude(v) return [ v[i]/vmag for i in range(len(v)) ]", "def _normalize_abmn(abmn): \"\"\"return a normalized version of abmn \"\"\" abmn_2d = np.atleast_2d(abmn) abmn_normalized = np.hstack(( np.sort(abmn_2d[:, 0:2], axis=1), np.sort(abmn_2d[:, 2:4], axis=1), )) return abmn_normalized"], "rank": 4}
{"nl": "passing functions as argumetns python", "code": ["def _correct_args(func, kwargs): \"\"\" Convert a dictionary of arguments including __argv into a list for passing to the function. \"\"\" args = inspect.getargspec(func)[0] return [kwargs[arg] for arg in args] + kwargs['__args']", "def call_with_context(func, context, *args): \"\"\" Check if given function has more arguments than given. Call it with context as last argument or without it. \"\"\" return make_context_aware(func, len(args))(*args + (context,))", "def apply_kwargs(func, **kwargs): \"\"\"Call *func* with kwargs, but only those kwargs that it accepts. \"\"\" new_kwargs = {} params = signature(func).parameters for param_name in params.keys(): if param_name in kwargs: new_kwargs[param_name] = kwargs[param_name] return func(**new_kwargs)", "def _iterable_to_varargs_method(func): \"\"\"decorator to convert a method taking a iterable to a *args one\"\"\" def wrapped(self, *args, **kwargs): return func(self, args, **kwargs) return wrapped", "def pool_args(function, sequence, kwargs): \"\"\"Return a single iterator of n elements of lists of length 3, given a sequence of len n.\"\"\" return zip(itertools.repeat(function), sequence, itertools.repeat(kwargs))"], "rank": 17}
{"nl": "to get the next line in python", "code": ["def readline(self): \"\"\"Get the next line including the newline or '' on EOF.\"\"\" self.lineno += 1 if self._buffer: return self._buffer.pop() else: return self.input.readline()", "def __next__(self): \"\"\" :return: a pair (1-based line number in the input, row) \"\"\" # Retrieve the row, thereby incrementing the line number: row = super(UnicodeReaderWithLineNumber, self).__next__() return self.lineno + 1, row", "def advance_one_line(self): \"\"\"Advances to next line.\"\"\" current_line = self._current_token.line_number while current_line == self._current_token.line_number: self._current_token = ConfigParser.Token(*next(self._token_generator))", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def next(self): \"\"\"Provides hook for Python2 iterator functionality.\"\"\" _LOGGER.debug(\"reading next\") if self.closed: _LOGGER.debug(\"stream is closed\") raise StopIteration() line = self.readline() if not line: _LOGGER.debug(\"nothing more to read\") raise StopIteration() return line"], "rank": 2}
{"nl": "get table column names from database python", "code": ["def get_column_names(engine: Engine, tablename: str) -> List[str]: \"\"\" Get all the database column names for the specified table. \"\"\" return [info.name for info in gen_columns_info(engine, tablename)]", "def get_column_keys_and_names(table): \"\"\" Return a generator of tuples k, c such that k is the name of the python attribute for the column and c is the name of the column in the sql table. \"\"\" ins = inspect(table) return ((k, c.name) for k, c in ins.mapper.c.items())", "def column_names(self, table): \"\"\"An iterable of column names, for a particular table or view.\"\"\" table_info = self.execute( u'PRAGMA table_info(%s)' % quote(table)) return (column['name'] for column in table_info)", "def get_table_columns(dbconn, tablename): \"\"\" Return a list of tuples specifying the column name and type \"\"\" cur = dbconn.cursor() cur.execute(\"PRAGMA table_info('%s');\" % tablename) info = cur.fetchall() cols = [(i[1], i[2]) for i in info] return cols", "def get_table_names_from_metadata(metadata: MetaData) -> List[str]: \"\"\" Returns all database table names found in an SQLAlchemy :class:`MetaData` object. \"\"\" return [table.name for table in metadata.tables.values()]"], "rank": 1}
{"nl": "python sparse matrix features name", "code": ["def build_columns(self, X, verbose=False): \"\"\"construct the model matrix columns for the term Parameters ---------- X : array-like Input dataset with n rows verbose : bool whether to show warnings Returns ------- scipy sparse array with n rows \"\"\" return sp.sparse.csc_matrix(X[:, self.feature][:, np.newaxis])", "def is_sparse_vector(x): \"\"\" x is a 2D sparse matrix with it's first shape equal to 1. \"\"\" return sp.issparse(x) and len(x.shape) == 2 and x.shape[0] == 1", "def _first_and_last_element(arr): \"\"\"Returns first and last element of numpy array or sparse matrix.\"\"\" if isinstance(arr, np.ndarray) or hasattr(arr, 'data'): # numpy array or sparse matrix with .data attribute data = arr.data if sparse.issparse(arr) else arr return data.flat[0], data.flat[-1] else: # Sparse matrices without .data attribute. Only dok_matrix at # the time of writing, in this case indexing is fast return arr[0, 0], arr[-1, -1]", "def get_sparse_matrix_keys(session, key_table): \"\"\"Return a list of keys for the sparse matrix.\"\"\" return session.query(key_table).order_by(key_table.name).all()", "def scipy_sparse_to_spmatrix(A): \"\"\"Efficient conversion from scipy sparse matrix to cvxopt sparse matrix\"\"\" coo = A.tocoo() SP = spmatrix(coo.data.tolist(), coo.row.tolist(), coo.col.tolist(), size=A.shape) return SP"], "rank": 1}
{"nl": "python get objectthat called a function", "code": ["def get_function(function_name): \"\"\" Given a Python function name, return the function it refers to. \"\"\" module, basename = str(function_name).rsplit('.', 1) try: return getattr(__import__(module, fromlist=[basename]), basename) except (ImportError, AttributeError): raise FunctionNotFound(function_name)", "def get_attr(self, method_name): \"\"\"Get attribute from the target object\"\"\" return self.attrs.get(method_name) or self.get_callable_attr(method_name)", "def return_value(self, *args, **kwargs): \"\"\"Extracts the real value to be returned from the wrapping callable. :return: The value the double should return when called. \"\"\" self._called() return self._return_value(*args, **kwargs)", "def __getattr__(self, item: str) -> Callable: \"\"\"Get a callable that sends the actual API request internally.\"\"\" return functools.partial(self.call_action, item)", "def method_caller(method_name, *args, **kwargs): \"\"\" Return a function that will call a named method on the target object with optional positional and keyword arguments. >>> lower = method_caller('lower') >>> lower('MyString') 'mystring' \"\"\" def call_method(target): func = getattr(target, method_name) return func(*args, **kwargs) return call_method"], "rank": 6}
{"nl": "python async function update state", "code": ["def async_update(self, event): \"\"\"New event for light. Check that state is part of event. Signal that light has updated state. \"\"\" self.update_attr(event.get('state', {})) super().async_update(event)", "def apply(self, func, args=(), kwds=dict()): \"\"\"Equivalent of the apply() builtin function. It blocks till the result is ready.\"\"\" return self.apply_async(func, args, kwds).get()", "def run_task(func): \"\"\" Decorator to wrap an async function in an event loop. Use for main sync interface methods. \"\"\" def _wrapped(*a, **k): loop = asyncio.get_event_loop() return loop.run_until_complete(func(*a, **k)) return _wrapped", "async def _thread_coro(self, *args): \"\"\" Coroutine called by MapAsync. It's wrapping the call of run_in_executor to run the synchronous function as thread \"\"\" return await self._loop.run_in_executor( self._executor, self._function, *args)", "def runcoro(async_function): \"\"\" Runs an asynchronous function without needing to use await - useful for lambda Args: async_function (Coroutine): The asynchronous function to run \"\"\" future = _asyncio.run_coroutine_threadsafe(async_function, client.loop) result = future.result() return result"], "rank": 1}
{"nl": "python check if variable is float, int, boolean", "code": ["def numberp(v): \"\"\"Return true iff 'v' is a number.\"\"\" return (not(isinstance(v, bool)) and (isinstance(v, int) or isinstance(v, float)))", "def is_numeric(value): \"\"\"Test if a value is numeric. \"\"\" return type(value) in [ int, float, np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64, np.float128 ]", "def isreal(obj): \"\"\" Test if the argument is a real number (float or integer). :param obj: Object :type obj: any :rtype: boolean \"\"\" return ( (obj is not None) and (not isinstance(obj, bool)) and isinstance(obj, (int, float)) )", "def is_float(value): \"\"\"must be a float\"\"\" return isinstance(value, float) or isinstance(value, int) or isinstance(value, np.float64), float(value)", "def isnumber(*args): \"\"\"Checks if value is an integer, long integer or float. NOTE: Treats booleans as numbers, where True=1 and False=0. \"\"\" return all(map(lambda c: isinstance(c, int) or isinstance(c, float), args))"], "rank": 4}
{"nl": "maximum list depth python", "code": ["def maxlevel(lst): \"\"\"Return maximum nesting depth\"\"\" maxlev = 0 def f(lst, level): nonlocal maxlev if isinstance(lst, list): level += 1 maxlev = max(level, maxlev) for item in lst: f(item, level) f(lst, 0) return maxlev", "def list_depth(list_, func=max, _depth=0): \"\"\" Returns the deepest level of nesting within a list of lists Args: list_ : a nested listlike object func : depth aggregation strategy (defaults to max) _depth : internal var Example: >>> # ENABLE_DOCTEST >>> from utool.util_list import * # NOQA >>> list_ = [[[[[1]]], [3]], [[1], [3]], [[1], [3]]] >>> result = (list_depth(list_, _depth=0)) >>> print(result) \"\"\" depth_list = [list_depth(item, func=func, _depth=_depth + 1) for item in list_ if util_type.is_listlike(item)] if len(depth_list) > 0: return func(depth_list) else: return _depth", "def maxDepth(self, currentDepth=0): \"\"\"Compute the depth of the longest branch of the tree\"\"\" if not any((self.left, self.right)): return currentDepth result = 0 for child in (self.left, self.right): if child: result = max(result, child.maxDepth(currentDepth + 1)) return result", "def _heapify_max(x): \"\"\"Transform list into a maxheap, in-place, in O(len(x)) time.\"\"\" n = len(x) for i in reversed(range(n//2)): _siftup_max(x, i)", "def min_depth(self, root): \"\"\" :type root: TreeNode :rtype: int \"\"\" if root is None: return 0 if root.left is not None or root.right is not None: return max(self.minDepth(root.left), self.minDepth(root.right))+1 return min(self.minDepth(root.left), self.minDepth(root.right)) + 1"], "rank": 1}
{"nl": "ssis check if python is still running in vackground", "code": ["def service_available(service_name): \"\"\"Determine whether a system service is available\"\"\" try: subprocess.check_output( ['service', service_name, 'status'], stderr=subprocess.STDOUT).decode('UTF-8') except subprocess.CalledProcessError as e: return b'unrecognized service' not in e.output else: return True", "def is_standalone(self): \"\"\"Return True if Glances is running in standalone mode.\"\"\" return (not self.args.client and not self.args.browser and not self.args.server and not self.args.webserver)", "async def vc_check(ctx: commands.Context): # pylint: disable=unused-argument \"\"\" Check for whether VC is available in this bot. \"\"\" if not discord.voice_client.has_nacl: raise commands.CheckFailure(\"voice cannot be used because PyNaCl is not loaded\") if not discord.opus.is_loaded(): raise commands.CheckFailure(\"voice cannot be used because libopus is not loaded\") return True", "def is_alive(self): \"\"\" Will test whether the ACS service is up and alive. \"\"\" response = self.get_monitoring_heartbeat() if response.status_code == 200 and response.content == 'alive': return True return False", "def has_virtualenv(self): \"\"\" Returns true if the virtualenv tool is installed. \"\"\" with self.settings(warn_only=True): ret = self.run_or_local('which virtualenv').strip() return bool(ret)"], "rank": 9}
{"nl": "python max length of one line", "code": ["def _multiline_width(multiline_s, line_width_fn=len): \"\"\"Visible width of a potentially multiline content.\"\"\" return max(map(line_width_fn, re.split(\"[\\r\\n]\", multiline_s)))", "def get_longest_line_length(text): \"\"\"Get the length longest line in a paragraph\"\"\" lines = text.split(\"\\n\") length = 0 for i in range(len(lines)): if len(lines[i]) > length: length = len(lines[i]) return length", "def indentsize(line): \"\"\"Return the indent size, in spaces, at the start of a line of text.\"\"\" expline = string.expandtabs(line) return len(expline) - len(string.lstrip(expline))", "def truncate_string(value, max_width=None): \"\"\"Truncate string values.\"\"\" if isinstance(value, text_type) and max_width is not None and len(value) > max_width: return value[:max_width] return value", "def wrap(string, length, indent): \"\"\" Wrap a string at a line length \"\"\" newline = \"\\n\" + \" \" * indent return newline.join((string[i : i + length] for i in range(0, len(string), length)))"], "rank": 1}
{"nl": "how to get thousands of http requests asynchronously python", "code": ["def _async_requests(urls): \"\"\" Sends multiple non-blocking requests. Returns a list of responses. :param urls: List of urls \"\"\" session = FuturesSession(max_workers=30) futures = [ session.get(url) for url in urls ] return [ future.result() for future in futures ]", "def fetch_event(urls): \"\"\" This parallel fetcher uses gevent one uses gevent \"\"\" rs = (grequests.get(u) for u in urls) return [content.json() for content in grequests.map(rs)]", "def parallel(processes, threads): \"\"\" execute jobs in processes using N threads \"\"\" pool = multithread(threads) pool.map(run_process, processes) pool.close() pool.join()", "async def result_processor(tasks): \"\"\"An async result aggregator that combines all the results This gets executed in unsync.loop and unsync.thread\"\"\" output = {} for task in tasks: num, res = await task output[num] = res return output", "async def parallel_results(future_map: Sequence[Tuple]) -> Dict: \"\"\" Run parallel execution of futures and return mapping of their results to the provided keys. Just a neat shortcut around ``asyncio.gather()`` :param future_map: Keys to futures mapping, e.g.: ( ('nav', get_nav()), ('content, get_content()) ) :return: Dict with futures results mapped to keys {'nav': {1:2}, 'content': 'xyz'} \"\"\" ctx_methods = OrderedDict(future_map) fs = list(ctx_methods.values()) results = await asyncio.gather(*fs) results = { key: results[idx] for idx, key in enumerate(ctx_methods.keys()) } return results"], "rank": 1}
{"nl": "python range function stack overflow", "code": ["def min_values(args): \"\"\" Return possible range for min function. \"\"\" return Interval(min(x.low for x in args), min(x.high for x in args))", "def negate(self): \"\"\"Reverse the range\"\"\" self.from_value, self.to_value = self.to_value, self.from_value self.include_lower, self.include_upper = self.include_upper, self.include_lower", "def make_bound(lower, upper, lineno): \"\"\" Wrapper: Creates an array bound \"\"\" return symbols.BOUND.make_node(lower, upper, lineno)", "def range(*args, interval=0): \"\"\"Generate a given range of numbers. It supports the same arguments as the builtin function. An optional interval can be given to space the values out. \"\"\" agen = from_iterable.raw(builtins.range(*args)) return time.spaceout.raw(agen, interval) if interval else agen", "def empty(self, start=None, stop=None): \"\"\"Empty the range from start to stop. Like delete, but no Error is raised if the entire range isn't mapped. \"\"\" self.set(NOT_SET, start=start, stop=stop)"], "rank": 10}
{"nl": "python get last record in file", "code": ["def last(self): \"\"\"Get the last object in file.\"\"\" # End of file self.__file.seek(0, 2) # Get the last struct data = self.get(self.length - 1) return data", "def get_last_id(self, cur, table='reaction'): \"\"\" Get the id of the last written row in table Parameters ---------- cur: database connection().cursor() object table: str 'reaction', 'publication', 'publication_system', 'reaction_system' Returns: id \"\"\" cur.execute(\"SELECT seq FROM sqlite_sequence WHERE name='{0}'\" .format(table)) result = cur.fetchone() if result is not None: id = result[0] else: id = 0 return id", "def get_last(self, table=None): \"\"\"Just the last entry.\"\"\" if table is None: table = self.main_table query = 'SELECT * FROM \"%s\" ORDER BY ROWID DESC LIMIT 1;' % table return self.own_cursor.execute(query).fetchone()", "def _get_current_label(self): \"\"\"Get the label from the last line read\"\"\" if len(self._last) == 0: raise StopIteration return self._last[:self._last.find(\":\")]", "def last(self): \"\"\"Last time step available. Example: >>> sdat = StagyyData('path/to/run') >>> assert(sdat.steps.last is sdat.steps[-1]) \"\"\" if self._last is UNDETERMINED: # not necessarily the last one... self._last = self.sdat.tseries.index[-1] return self[self._last]"], "rank": 1}
{"nl": "python network activity log on and log off", "code": ["def _log_disconnect(self): \"\"\" Decrement connection count \"\"\" if self.logged: self.server.stats.connectionClosed() self.logged = False", "def log_leave(event, nick, channel): \"\"\" Log a quit or part event. \"\"\" if channel not in pmxbot.config.log_channels: return ParticipantLogger.store.log(nick, channel, event.type)", "def _print(self, msg, flush=False, end=\"\\n\"): \"\"\"Helper function to print connection status messages when in verbose mode.\"\"\" if self._verbose: print2(msg, end=end, flush=flush)", "def _debug_log(self, msg): \"\"\"Debug log messages if debug=True\"\"\" if not self.debug: return sys.stderr.write('{}\\n'.format(msg))", "def stoplog(self): \"\"\" Stop logging. @return: 1 on success and 0 on error @rtype: integer \"\"\" if self._file_logger: self.logger.removeHandler(_file_logger) self._file_logger = None return 1"], "rank": 17}
{"nl": "python list get index with default", "code": ["def list_get(l, idx, default=None): \"\"\" Get from a list with an optional default value. \"\"\" try: if l[idx]: return l[idx] else: return default except IndexError: return default", "def _get_or_default(mylist, i, default=None): \"\"\"return list item number, or default if don't exist\"\"\" if i >= len(mylist): return default else : return mylist[i]", "def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def get_list_index(lst, index_or_name): \"\"\" Return the index of an element in the list. Args: lst (list): The list. index_or_name (int or str): The value of the reference element, or directly its numeric index. Returns: (int) The index of the element in the list. \"\"\" if isinstance(index_or_name, six.integer_types): return index_or_name return lst.index(index_or_name)", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i"], "rank": 1}
{"nl": "in a random generate sequence in python how do you retain a function", "code": ["def rand_elem(seq, n=None): \"\"\"returns a random element from seq n times. If n is None, it continues indefinitly\"\"\" return map(random.choice, repeat(seq, n) if n is not None else repeat(seq))", "def generate_seed(seed): \"\"\"Generate seed for random number generator\"\"\" if seed is None: random.seed() seed = random.randint(0, sys.maxsize) random.seed(a=seed) return seed", "def sometimesish(fn): \"\"\" Has a 50/50 chance of calling a function \"\"\" def wrapped(*args, **kwargs): if random.randint(1, 2) == 1: return fn(*args, **kwargs) return wrapped", "def uniqueID(size=6, chars=string.ascii_uppercase + string.digits): \"\"\"A quick and dirty way to get a unique string\"\"\" return ''.join(random.choice(chars) for x in xrange(size))", "def endless_permutations(N, random_state=None): \"\"\" Generate an endless sequence of random integers from permutations of the set [0, ..., N). If we call this N times, we will sweep through the entire set without replacement, on the (N+1)th call a new permutation will be created, etc. Parameters ---------- N: int the length of the set random_state: int or RandomState, optional random seed Yields ------ int: a random int from the set [0, ..., N) \"\"\" generator = check_random_state(random_state) while True: batch_inds = generator.permutation(N) for b in batch_inds: yield b"], "rank": 3}
{"nl": "python strictredis redis password", "code": ["def get_connection(self, host, port, db): \"\"\" Returns a ``StrictRedis`` connection instance. \"\"\" return redis.StrictRedis( host=host, port=port, db=db, decode_responses=True )", "def __connect(): \"\"\" Connect to a redis instance. \"\"\" global redis_instance if use_tcp_socket: redis_instance = redis.StrictRedis(host=hostname, port=port) else: redis_instance = redis.StrictRedis(unix_socket_path=unix_socket)", "def _get_info(self, host, port, unix_socket, auth): \"\"\"Return info dict from specified Redis instance :param str host: redis host :param int port: redis port :rtype: dict \"\"\" client = self._client(host, port, unix_socket, auth) if client is None: return None info = client.info() del client return info", "def connect(self): \"\"\" Connects to publisher \"\"\" self.client = redis.Redis( host=self.host, port=self.port, password=self.password)", "def from_url(url, db=None, **kwargs): \"\"\" Returns an active Redis client generated from the given database URL. Will attempt to extract the database id from the path url fragment, if none is provided. \"\"\" from redis.client import Redis return Redis.from_url(url, db, **kwargs)"], "rank": 1}
{"nl": "python dict to key string and value stirng", "code": ["def stringify_dict_contents(dct): \"\"\"Turn dict keys and values into native strings.\"\"\" return { str_if_nested_or_str(k): str_if_nested_or_str(v) for k, v in dct.items() }", "def str_dict(some_dict): \"\"\"Convert dict of ascii str/unicode to dict of str, if necessary\"\"\" return {str(k): str(v) for k, v in some_dict.items()}", "def clean_dict_keys(d): \"\"\"Convert all keys of the dict 'd' to (ascii-)strings. :Raises: UnicodeEncodeError \"\"\" new_d = {} for (k, v) in d.iteritems(): new_d[str(k)] = v return new_d", "def encode_list(key, list_): # type: (str, Iterable) -> Dict[str, str] \"\"\" Converts a list into a space-separated string and puts it in a dictionary :param key: Dictionary key to store the list :param list_: A list of objects :return: A dictionary key->string or an empty dictionary \"\"\" if not list_: return {} return {key: \" \".join(str(i) for i in list_)}", "def dict_to_querystring(dictionary): \"\"\"Converts a dict to a querystring suitable to be appended to a URL.\"\"\" s = u\"\" for d in dictionary.keys(): s = unicode.format(u\"{0}{1}={2}&\", s, d, dictionary[d]) return s[:-1]"], "rank": 1}
{"nl": "python update minify js file", "code": ["def minify_js(input_files, output_file): \"\"\" Minifies the input javascript files to the output file. Output file may be same as input to minify in place. In debug mode this function just concatenates the files without minifying. \"\"\" from .modules import minify, utils if not isinstance(input_files, (list, tuple)): raise RuntimeError('JS minifier takes a list of input files.') return { 'dependencies_fn': utils.no_dependencies, 'compiler_fn': minify.minify_js, 'input': input_files, 'output': output_file, 'kwargs': {}, }", "def minify(path): \"\"\" Load a javascript file and minify. Parameters ------------ path: str, path of resource \"\"\" if 'http' in path: data = requests.get(path).content.decode( 'ascii', errors='ignore') else: with open(path, 'rb') as f: # some of these assholes use unicode spaces -_- data = f.read().decode('ascii', errors='ignore') # don't re- minify if '.min.' in path: return data try: return jsmin.jsmin(data) except BaseException: return data", "def static_urls_js(): \"\"\" Add global variables to JavaScript about the location and latest version of transpiled files. Usage:: {% static_urls_js %} \"\"\" if apps.is_installed('django.contrib.staticfiles'): from django.contrib.staticfiles.storage import staticfiles_storage static_base_url = staticfiles_storage.base_url else: static_base_url = PrefixNode.handle_simple(\"STATIC_URL\") transpile_base_url = urljoin(static_base_url, 'js/transpile/') return { 'static_base_url': static_base_url, 'transpile_base_url': transpile_base_url, 'version': LAST_RUN['version'] }", "def typescript_compile(source): \"\"\"Compiles the given ``source`` from TypeScript to ES5 using TypescriptServices.js\"\"\" with open(TS_COMPILER, 'r') as tsservices_js: return evaljs( (tsservices_js.read(), 'ts.transpile(dukpy.tscode, {options});'.format(options=TSC_OPTIONS)), tscode=source )", "def import_js(path, lib_name, globals): \"\"\"Imports from javascript source file. globals is your globals()\"\"\" with codecs.open(path_as_local(path), \"r\", \"utf-8\") as f: js = f.read() e = EvalJs() e.execute(js) var = e.context['var'] globals[lib_name] = var.to_python()"], "rank": 2}
{"nl": "how to deifne a rotation in python", "code": ["def _rotate(n, x, y, rx, ry): \"\"\"Rotate and flip a quadrant appropriately Based on the implementation here: https://en.wikipedia.org/w/index.php?title=Hilbert_curve&oldid=797332503 \"\"\" if ry == 0: if rx == 1: x = n - 1 - x y = n - 1 - y return y, x return x, y", "def create_rot2d(angle): \"\"\"Create 2D rotation matrix\"\"\" ca = math.cos(angle) sa = math.sin(angle) return np.array([[ca, -sa], [sa, ca]])", "def rotateImage(image, angle): \"\"\" rotates a 2d array to a multiple of 90 deg. 0 = default 1 = 90 deg. cw 2 = 180 deg. 3 = 90 deg. ccw \"\"\" image = [list(row) for row in image] for n in range(angle % 4): image = list(zip(*image[::-1])) return image", "def earth_orientation(date): \"\"\"Earth orientation as a rotating matrix \"\"\" x_p, y_p, s_prime = np.deg2rad(_earth_orientation(date)) return rot3(-s_prime) @ rot2(x_p) @ rot1(y_p)", "def iprotate(l, steps=1): r\"\"\"Like rotate, but modifies `l` in-place. >>> l = [1,2,3] >>> iprotate(l) is l True >>> l [2, 3, 1] >>> iprotate(iprotate(l, 2), -3) [1, 2, 3] \"\"\" if len(l): steps %= len(l) if steps: firstPart = l[:steps] del l[:steps] l.extend(firstPart) return l"], "rank": 4}
{"nl": "python get average volume of audio", "code": ["def calc_volume(self, sample: np.ndarray): \"\"\"Find the RMS of the audio\"\"\" return sqrt(np.mean(np.square(sample)))", "def synthesize(self, duration): \"\"\" Synthesize white noise Args: duration (numpy.timedelta64): The duration of the synthesized sound \"\"\" sr = self.samplerate.samples_per_second seconds = duration / Seconds(1) samples = np.random.uniform(low=-1., high=1., size=int(sr * seconds)) return AudioSamples(samples, self.samplerate)", "def bitdepth(self): \"\"\"The number of bits per sample in the audio encoding (an int). Only available for certain file formats (zero where unavailable). \"\"\" if hasattr(self.mgfile.info, 'bits_per_sample'): return self.mgfile.info.bits_per_sample return 0", "def enbw(wnd): \"\"\" Equivalent Noise Bandwidth in bins (Processing Gain reciprocal). \"\"\" return sum(el ** 2 for el in wnd) / sum(wnd) ** 2 * len(wnd)", "def setVolume(self, volume): \"\"\"Changes volume\"\"\" val = float(val) cmd = \"volume %s\" % val self._execute(cmd)"], "rank": 1}
{"nl": "python split words in to list", "code": ["def split_into_words(s): \"\"\"Split a sentence into list of words.\"\"\" s = re.sub(r\"\\W+\", \" \", s) s = re.sub(r\"[_0-9]+\", \" \", s) return s.split()", "def tokenize_list(self, text): \"\"\" Split a text into separate words. \"\"\" return [self.get_record_token(record) for record in self.analyze(text)]", "def tokenize_words(self, text): \"\"\"Tokenize an input string into a list of words (with punctuation removed).\"\"\" return [ self.strip_punctuation(word) for word in text.split(' ') if self.strip_punctuation(word) ]", "def split_into_sentences(s): \"\"\"Split text into list of sentences.\"\"\" s = re.sub(r\"\\s+\", \" \", s) s = re.sub(r\"[\\\\.\\\\?\\\\!]\", \"\\n\", s) return s.split(\"\\n\")", "def split(text: str) -> List[str]: \"\"\"Split a text into a list of tokens. :param text: the text to split :return: tokens \"\"\" return [word for word in SEPARATOR.split(text) if word.strip(' \\t')]"], "rank": 2}
{"nl": "python how check int or float", "code": ["def is_integer(value: Any) -> bool: \"\"\"Return true if a value is an integer number.\"\"\" return (isinstance(value, int) and not isinstance(value, bool)) or ( isinstance(value, float) and isfinite(value) and int(value) == value )", "def is_float(value): \"\"\"must be a float\"\"\" return isinstance(value, float) or isinstance(value, int) or isinstance(value, np.float64), float(value)", "def test_value(self, value): \"\"\"Test if value is an instance of float.\"\"\" if not isinstance(value, float): raise ValueError('expected float value: ' + str(type(value)))", "def is_numeric(value): \"\"\"Test if a value is numeric. \"\"\" return type(value) in [ int, float, np.int8, np.int16, np.int32, np.int64, np.float16, np.float32, np.float64, np.float128 ]", "def _is_numeric(self, values): \"\"\"Check to be sure values are numbers before doing numerical operations.\"\"\" if len(values) > 0: assert isinstance(values[0], (float, int)), \\ \"values must be numbers to perform math operations. Got {}\".format( type(values[0])) return True"], "rank": 2}
{"nl": "write file python change text color", "code": ["def _write_color_ansi (fp, text, color): \"\"\"Colorize text with given color.\"\"\" fp.write(esc_ansicolor(color)) fp.write(text) fp.write(AnsiReset)", "def _write_color_colorama (fp, text, color): \"\"\"Colorize text with given color.\"\"\" foreground, background, style = get_win_color(color) colorama.set_console(foreground=foreground, background=background, style=style) fp.write(text) colorama.reset_console()", "def colorize(txt, fg=None, bg=None): \"\"\" Print escape codes to set the terminal color. fg and bg are indices into the color palette for the foreground and background colors. \"\"\" setting = '' setting += _SET_FG.format(fg) if fg else '' setting += _SET_BG.format(bg) if bg else '' return setting + str(txt) + _STYLE_RESET", "def _update_fontcolor(self, fontcolor): \"\"\"Updates text font color button Parameters ---------- fontcolor: Integer \\tText color in integer RGB format \"\"\" textcolor = wx.SystemSettings_GetColour(wx.SYS_COLOUR_WINDOWTEXT) textcolor.SetRGB(fontcolor) self.textcolor_choice.SetColour(textcolor)", "def write_text(filename: str, text: str) -> None: \"\"\" Writes text to a file. \"\"\" with open(filename, 'w') as f: # type: TextIO print(text, file=f)"], "rank": 2}
{"nl": "python get location of min/max", "code": ["def values(self): \"\"\"Gets the user enter max and min values of where the raster points should appear on the y-axis :returns: (float, float) -- (min, max) y-values to bound the raster plot by \"\"\" lower = float(self.lowerSpnbx.value()) upper = float(self.upperSpnbx.value()) return (lower, upper)", "def calculate_bounding_box(data): \"\"\" Returns a 2 x m array indicating the min and max along each dimension. \"\"\" mins = data.min(0) maxes = data.max(0) return mins, maxes", "def get_bound(pts): \"\"\"Compute a minimal rectangle that covers all the points.\"\"\" (x0, y0, x1, y1) = (INF, INF, -INF, -INF) for (x, y) in pts: x0 = min(x0, x) y0 = min(y0, y) x1 = max(x1, x) y1 = max(y1, y) return (x0, y0, x1, y1)", "def min_values(args): \"\"\" Return possible range for min function. \"\"\" return Interval(min(x.low for x in args), min(x.high for x in args))", "def get_idx_rect(index_list): \"\"\"Extract the boundaries from a list of indexes\"\"\" rows, cols = list(zip(*[(i.row(), i.column()) for i in index_list])) return ( min(rows), max(rows), min(cols), max(cols) )"], "rank": 1}
{"nl": "extract words from documents python", "code": ["def contains_extractor(document): \"\"\"A basic document feature extractor that returns a dict of words that the document contains.\"\"\" tokens = _get_document_tokens(document) features = dict((u'contains({0})'.format(w), True) for w in tokens) return features", "def extract_words(lines): \"\"\" Extract from the given iterable of lines the list of words. :param lines: an iterable of lines; :return: a generator of words of lines. \"\"\" for line in lines: for word in re.findall(r\"\\w+\", line): yield word", "def tokenize_list(self, text): \"\"\" Split a text into separate words. \"\"\" return [self.get_record_token(record) for record in self.analyze(text)]", "def clean_text_by_sentences(text, language=\"english\", additional_stopwords=None): \"\"\" Tokenizes a given text into sentences, applying filters and lemmatizing them. Returns a SyntacticUnit list. \"\"\" init_textcleanner(language, additional_stopwords) original_sentences = split_sentences(text) filtered_sentences = filter_words(original_sentences) return merge_syntactic_units(original_sentences, filtered_sentences)", "def extract_keywords_from_text(self, text): \"\"\"Method to extract keywords from the text provided. :param text: Text to extract keywords from, provided as a string. \"\"\" sentences = nltk.tokenize.sent_tokenize(text) self.extract_keywords_from_sentences(sentences)"], "rank": 1}
{"nl": "python \"binary string\" to int", "code": ["def bin_to_int(string): \"\"\"Convert a one element byte string to signed int for python 2 support.\"\"\" if isinstance(string, str): return struct.unpack(\"b\", string)[0] else: return struct.unpack(\"b\", bytes([string]))[0]", "def string_to_int( s ): \"\"\"Convert a string of bytes into an integer, as per X9.62.\"\"\" result = 0 for c in s: if not isinstance(c, int): c = ord( c ) result = 256 * result + c return result", "def _from_bytes(bytes, byteorder=\"big\", signed=False): \"\"\"This is the same functionality as ``int.from_bytes`` in python 3\"\"\" return int.from_bytes(bytes, byteorder=byteorder, signed=signed)", "def intToBin(i): \"\"\" Integer to two bytes \"\"\" # divide in two parts (bytes) i1 = i % 256 i2 = int(i / 256) # make string (little endian) return i.to_bytes(2, byteorder='little')", "def s2b(s): \"\"\" String to binary. \"\"\" ret = [] for c in s: ret.append(bin(ord(c))[2:].zfill(8)) return \"\".join(ret)"], "rank": 3}
{"nl": "python return index of object in a list", "code": ["def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def find_geom(geom, geoms): \"\"\" Returns the index of a geometry in a list of geometries avoiding expensive equality checks of `in` operator. \"\"\" for i, g in enumerate(geoms): if g is geom: return i", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def get_index_nested(x, i): \"\"\" Description: Returns the first index of the array (vector) x containing the value i. Parameters: x: one-dimensional array i: search value \"\"\" for ind in range(len(x)): if i == x[ind]: return ind return -1"], "rank": 7}
{"nl": "l2 norm for array python", "code": ["def l2_norm(arr): \"\"\" The l2 norm of an array is is defined as: sqrt(||x||), where ||x|| is the dot product of the vector. \"\"\" arr = np.asarray(arr) return np.sqrt(np.dot(arr.ravel().squeeze(), arr.ravel().squeeze()))", "def l2_norm(params): \"\"\"Computes l2 norm of params by flattening them into a vector.\"\"\" flattened, _ = flatten(params) return np.dot(flattened, flattened)", "def normalize(v, axis=None, eps=1e-10): \"\"\"L2 Normalize along specified axes.\"\"\" return v / max(anorm(v, axis=axis, keepdims=True), eps)", "def ln_norm(x, mu, sigma=1.0): \"\"\" Natural log of scipy norm function truncated at zero \"\"\" return np.log(stats.norm(loc=mu, scale=sigma).pdf(x))", "def _norm(self, x): \"\"\"Compute the safe norm.\"\"\" return tf.sqrt(tf.reduce_sum(tf.square(x), keepdims=True, axis=-1) + 1e-7)"], "rank": 1}
{"nl": "how to fill linterrepter python", "code": ["def register(linter): \"\"\"Register the reporter classes with the linter.\"\"\" linter.register_reporter(TextReporter) linter.register_reporter(ParseableTextReporter) linter.register_reporter(VSTextReporter) linter.register_reporter(ColorizedTextReporter)", "def lint(ctx: click.Context, amend: bool = False, stage: bool = False): \"\"\" Runs all linters Args: ctx: click context amend: whether or not to commit results stage: whether or not to stage changes \"\"\" _lint(ctx, amend, stage)", "def main(output=None, error=None, verbose=False): \"\"\" The main (cli) interface for the pylint runner. \"\"\" runner = Runner(args=[\"--verbose\"] if verbose is not False else None) runner.run(output, error)", "def lint(fmt='colorized'): \"\"\"Run verbose PyLint on source. Optionally specify fmt=html for HTML output.\"\"\" if fmt == 'html': outfile = 'pylint_report.html' local('pylint -f %s davies > %s || true' % (fmt, outfile)) local('open %s' % outfile) else: local('pylint -f %s davies || true' % fmt)", "def format_pylint_disables(error_names, tag=True): \"\"\" Format a list of error_names into a 'pylint: disable=' line. \"\"\" tag_str = \"lint-amnesty, \" if tag else \"\" if error_names: return u\" # {tag}pylint: disable={disabled}\".format( disabled=\", \".join(sorted(error_names)), tag=tag_str, ) else: return \"\""], "rank": 1}
{"nl": "how to check datatype of column in python", "code": ["def is_sqlatype_numeric(coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type one that inherits from :class:`Numeric`, such as :class:`Float`, :class:`Decimal`? \"\"\" coltype = _coltype_to_typeengine(coltype) return isinstance(coltype, sqltypes.Numeric)", "def is_sqlatype_string(coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type a string type? \"\"\" coltype = _coltype_to_typeengine(coltype) return isinstance(coltype, sqltypes.String)", "def is_sqlatype_integer(coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type an integer type? \"\"\" coltype = _coltype_to_typeengine(coltype) return isinstance(coltype, sqltypes.Integer)", "def is_sqlatype_text_over_one_char( coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type a string type that's more than one character long? \"\"\" coltype = _coltype_to_typeengine(coltype) return is_sqlatype_text_of_length_at_least(coltype, 2)", "def get_datatype(self, table: str, column: str) -> str: \"\"\"Returns database SQL datatype for a column: e.g. VARCHAR.\"\"\" return self.flavour.get_datatype(self, table, column).upper()"], "rank": 1}
{"nl": "python interactive window python is not defined", "code": ["def is_interactive(self): \"\"\" Determine if the user requested interactive mode. \"\"\" # The Python interpreter sets sys.flags correctly, so use them! if sys.flags.interactive: return True # IPython does not set sys.flags when -i is specified, so first # check it if it is already imported. if '__IPYTHON__' not in dir(six.moves.builtins): return False # Then we check the application singleton and determine based on # a variable it sets. try: from IPython.config.application import Application as App return App.initialized() and App.instance().interact except (ImportError, AttributeError): return False", "def determine_interactive(self): \"\"\"Determine whether we're in an interactive shell. Sets interactivity off if appropriate. cf http://stackoverflow.com/questions/24861351/how-to-detect-if-python-script-is-being-run-as-a-background-process \"\"\" try: if not sys.stdout.isatty() or os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno()): self.interactive = 0 return False except Exception: self.interactive = 0 return False if self.interactive == 0: return False return True", "def isInteractive(): \"\"\" A basic check of if the program is running in interactive mode \"\"\" if sys.stdout.isatty() and os.name != 'nt': #Hopefully everything but ms supports '\\r' try: import threading except ImportError: return False else: return True else: return False", "def _in_qtconsole() -> bool: \"\"\" A small utility function which determines if we're running in QTConsole's context. \"\"\" try: from IPython import get_ipython try: from ipykernel.zmqshell import ZMQInteractiveShell shell_object = ZMQInteractiveShell except ImportError: from IPython.kernel.zmq import zmqshell shell_object = zmqshell.ZMQInteractiveShell return isinstance(get_ipython(), shell_object) except Exception: return False", "def page_guiref(arg_s=None): \"\"\"Show a basic reference about the GUI Console.\"\"\" from IPython.core import page page.page(gui_reference, auto_html=True)"], "rank": 3}
{"nl": "datetime add a month to a date python", "code": ["def start_of_month(val): \"\"\" Return a new datetime.datetime object with values that represent a start of a month. :param val: Date to ... :type val: datetime.datetime | datetime.date :rtype: datetime.datetime \"\"\" if type(val) == date: val = datetime.fromordinal(val.toordinal()) return start_of_day(val).replace(day=1)", "def months_ago(date, nb_months=1): \"\"\" Return the given `date` with `nb_months` substracted from it. \"\"\" nb_years = nb_months // 12 nb_months = nb_months % 12 month_diff = date.month - nb_months if month_diff > 0: new_month = month_diff else: new_month = 12 + month_diff nb_years += 1 return date.replace(day=1, month=new_month, year=date.year - nb_years)", "def replace_month_abbr_with_num(date_str, lang=DEFAULT_DATE_LANG): \"\"\"Replace month strings occurrences with month number.\"\"\" num, abbr = get_month_from_date_str(date_str, lang) return re.sub(abbr, str(num), date_str, flags=re.IGNORECASE)", "def day_to_month(timeperiod): \"\"\":param timeperiod: as string in YYYYMMDD00 format :return string in YYYYMM0000 format\"\"\" t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN) return t.strftime(SYNERGY_MONTHLY_PATTERN)", "def convert_date(date): \"\"\"Convert string to datetime object.\"\"\" date = convert_month(date, shorten=False) clean_string = convert_string(date) return datetime.strptime(clean_string, DATE_FMT.replace('-',''))"], "rank": 1}
{"nl": "python correlation pearson coefficient", "code": ["def cor(y_true, y_pred): \"\"\"Compute Pearson correlation coefficient. \"\"\" y_true, y_pred = _mask_nan(y_true, y_pred) return np.corrcoef(y_true, y_pred)[0, 1]", "def autocorr_coeff(x, t, tau1, tau2): \"\"\"Calculate the autocorrelation coefficient.\"\"\" return corr_coeff(x, x, t, tau1, tau2)", "def mcc(y, z): \"\"\"Matthews correlation coefficient \"\"\" tp, tn, fp, fn = contingency_table(y, z) return (tp * tn - fp * fn) / K.sqrt((tp + fp) * (tp + fn) * (tn + fp) * (tn + fn))", "def mcc(y_true, y_pred, round=True): \"\"\"Matthews correlation coefficient \"\"\" y_true, y_pred = _mask_value_nan(y_true, y_pred) if round: y_true = np.round(y_true) y_pred = np.round(y_pred) return skm.matthews_corrcoef(y_true, y_pred)", "def correlation(df, rowvar=False): \"\"\" Calculate column-wise Pearson correlations using ``numpy.ma.corrcoef`` Input data is masked to ignore NaNs when calculating correlations. Data is returned as a Pandas ``DataFrame`` of column_n x column_n dimensions, with column index copied to both axes. :param df: Pandas DataFrame :return: Pandas DataFrame (n_columns x n_columns) of column-wise correlations \"\"\" # Create a correlation matrix for all correlations # of the columns (filled with na for all values) df = df.copy() maskv = np.ma.masked_where(np.isnan(df.values), df.values) cdf = np.ma.corrcoef(maskv, rowvar=False) cdf = pd.DataFrame(np.array(cdf)) cdf.columns = df.columns cdf.index = df.columns cdf = cdf.sort_index(level=0, axis=1) cdf = cdf.sort_index(level=0) return cdf"], "rank": 1}
{"nl": "how do i mae the cursor on python skinny again", "code": ["def disable_busy_cursor(): \"\"\"Disable the hourglass cursor and listen for layer changes.\"\"\" while QgsApplication.instance().overrideCursor() is not None and \\ QgsApplication.instance().overrideCursor().shape() == \\ QtCore.Qt.WaitCursor: QgsApplication.instance().restoreOverrideCursor()", "def set_cursor(self, x, y): \"\"\" Sets the cursor to the desired position. :param x: X position :param y: Y position \"\"\" curses.curs_set(1) self.screen.move(y, x)", "def get_cursor(self): \"\"\"Return the virtual cursor position. The cursor can be moved with the :any:`move` method. Returns: Tuple[int, int]: The (x, y) coordinate of where :any:`print_str` will continue from. .. seealso:: :any:move` \"\"\" x, y = self._cursor width, height = self.parent.get_size() while x >= width: x -= width y += 1 if y >= height and self.scrollMode == 'scroll': y = height - 1 return x, y", "def hidden_cursor(self): \"\"\"Return a context manager that hides the cursor while inside it and makes it visible on leaving.\"\"\" self.stream.write(self.hide_cursor) try: yield finally: self.stream.write(self.normal_cursor)", "def ensure_hbounds(self): \"\"\"Ensure the cursor is within horizontal screen bounds.\"\"\" self.cursor.x = min(max(0, self.cursor.x), self.columns - 1)"], "rank": 4}
{"nl": "test the number of characters in python list", "code": ["def token_list_len(tokenlist): \"\"\" Return the amount of characters in this token list. :param tokenlist: List of (token, text) or (token, text, mouse_handler) tuples. \"\"\" ZeroWidthEscape = Token.ZeroWidthEscape return sum(len(item[1]) for item in tokenlist if item[0] != ZeroWidthEscape)", "def display_len(text): \"\"\" Get the display length of a string. This can differ from the character length if the string contains wide characters. \"\"\" text = unicodedata.normalize('NFD', text) return sum(char_width(char) for char in text)", "def _check_list_len(row, length): \"\"\" Sanity check for csv parser :param row :param length :return:None \"\"\" if len(row) != length: raise Exception( \"row length does not match expected length of \" + str(length) + \"\\nrow: \" + str(row))", "def check_type_and_size_of_param_list(param_list, expected_length): \"\"\" Ensure that param_list is a list with the expected length. Raises a helpful ValueError if this is not the case. \"\"\" try: assert isinstance(param_list, list) assert len(param_list) == expected_length except AssertionError: msg = \"param_list must be a list containing {} elements.\" raise ValueError(msg.format(expected_length)) return None", "def get_list_dimensions(_list): \"\"\" Takes a nested list and returns the size of each dimension followed by the element type in the list \"\"\" if isinstance(_list, list) or isinstance(_list, tuple): return [len(_list)] + get_list_dimensions(_list[0]) return []"], "rank": 1}
{"nl": "sent urlencoded payload in python", "code": ["def urlencoded(body, charset='ascii', **kwargs): \"\"\"Converts query strings into native Python objects\"\"\" return parse_query_string(text(body, charset=charset), False)", "def get_dict_to_encoded_url(data): \"\"\" Converts a dict to an encoded URL. Example: given data = {'a': 1, 'b': 2}, it returns 'a=1&b=2' \"\"\" unicode_data = dict([(k, smart_str(v)) for k, v in data.items()]) encoded = urllib.urlencode(unicode_data) return encoded", "def _request(self, data): \"\"\"Moved out to make testing easier.\"\"\" return requests.post(self.endpoint, data=data.encode(\"ascii\")).content", "def set_json_item(key, value): \"\"\" manipulate json data on the fly \"\"\" data = get_json() data[key] = value request = get_request() request[\"BODY\"] = json.dumps(data)", "def create_search_url(self): \"\"\" Generates (urlencoded) query string from stored key-values tuples :returns: A string containing all arguments in a url-encoded format \"\"\" url = '?' for key, value in self.arguments.items(): url += '%s=%s&' % (quote_plus(key), quote_plus(value)) self.url = url[:-1] return self.url"], "rank": 1}
{"nl": "python setuptools command not found", "code": ["def main(argv, version=DEFAULT_VERSION): \"\"\"Install or upgrade setuptools and EasyInstall\"\"\" tarball = download_setuptools() _install(tarball, _build_install_args(argv))", "def _extension(modpath: str) -> setuptools.Extension: \"\"\"Make setuptools.Extension.\"\"\" return setuptools.Extension(modpath, [modpath.replace(\".\", \"/\") + \".py\"])", "def stub_main(): \"\"\"setuptools blah: it still can't run a module as a script entry_point\"\"\" from google.apputils import run_script_module import butcher.main run_script_module.RunScriptModule(butcher.main)", "def yum_install(self, packages, ignore_error=False): \"\"\"Install some packages on the remote host. :param packages: ist of packages to install. \"\"\" return self.run('yum install -y --quiet ' + ' '.join(packages), ignore_error=ignore_error, retry=5)", "def check_dependencies_remote(args): \"\"\" Invoke this command on a remote Python. \"\"\" cmd = [args.python, '-m', 'depends', args.requirement] env = dict(PYTHONPATH=os.path.dirname(__file__)) return subprocess.check_call(cmd, env=env)"], "rank": 1}
{"nl": "python ctypes load dll dependancies", "code": ["def _windowsLdmodTargets(target, source, env, for_signature): \"\"\"Get targets for loadable modules.\"\"\" return _dllTargets(target, source, env, for_signature, 'LDMODULE')", "def load(self, name): \"\"\"Loads and returns foreign library.\"\"\" name = ctypes.util.find_library(name) return ctypes.cdll.LoadLibrary(name)", "def get_winfunc(libname, funcname, restype=None, argtypes=(), _libcache={}): \"\"\"Retrieve a function from a library/DLL, and set the data types.\"\"\" if libname not in _libcache: _libcache[libname] = windll.LoadLibrary(libname) func = getattr(_libcache[libname], funcname) func.argtypes = argtypes func.restype = restype return func", "def libpath(self): \"\"\"Returns the full path to the shared *wrapper* library created for the module. \"\"\" from os import path return path.join(self.dirpath, self.libname)", "def GetPythonLibraryDirectoryPath(): \"\"\"Retrieves the Python library directory path.\"\"\" path = sysconfig.get_python_lib(True) _, _, path = path.rpartition(sysconfig.PREFIX) if path.startswith(os.sep): path = path[1:] return path"], "rank": 1}
{"nl": "python create dictionray from a list of keys", "code": ["def _generate_key_map(entity_list, key, entity_class): \"\"\" Helper method to generate map from key to entity object for given list of dicts. Args: entity_list: List consisting of dict. key: Key in each dict which will be key in the map. entity_class: Class representing the entity. Returns: Map mapping key to entity object. \"\"\" key_map = {} for obj in entity_list: key_map[obj[key]] = entity_class(**obj) return key_map", "def encode_list(key, list_): # type: (str, Iterable) -> Dict[str, str] \"\"\" Converts a list into a space-separated string and puts it in a dictionary :param key: Dictionary key to store the list :param list_: A list of objects :return: A dictionary key->string or an empty dictionary \"\"\" if not list_: return {} return {key: \" \".join(str(i) for i in list_)}", "def flattened_nested_key_indices(nested_dict): \"\"\" Combine the outer and inner keys of nested dictionaries into a single ordering. \"\"\" outer_keys, inner_keys = collect_nested_keys(nested_dict) combined_keys = list(sorted(set(outer_keys + inner_keys))) return {k: i for (i, k) in enumerate(combined_keys)}", "def _kw(keywords): \"\"\"Turn list of keywords into dictionary.\"\"\" r = {} for k, v in keywords: r[k] = v return r", "def list_string_to_dict(string): \"\"\"Inputs ``['a', 'b', 'c']``, returns ``{'a': 0, 'b': 1, 'c': 2}``.\"\"\" dictionary = {} for idx, c in enumerate(string): dictionary.update({c: idx}) return dictionary"], "rank": 2}
{"nl": "python function to reduce image size", "code": ["def getDimensionForImage(filename, maxsize): \"\"\"Return scaled image size in (width, height) format. The scaling preserves the aspect ratio. If PIL is not found returns None.\"\"\" try: from PIL import Image except ImportError: return None img = Image.open(filename) width, height = img.size if width > maxsize[0] or height > maxsize[1]: img.thumbnail(maxsize) out.info(\"Downscaled display size from %s to %s\" % ((width, height), img.size)) return img.size", "def resize(self): \"\"\" Get target size for a cropped image and do the resizing if we got anything usable. \"\"\" resized_size = self.get_resized_size() if not resized_size: return self.image = self.image.resize(resized_size, Image.ANTIALIAS)", "def resize_by_area(img, size): \"\"\"image resize function used by quite a few image problems.\"\"\" return tf.to_int64( tf.image.resize_images(img, [size, size], tf.image.ResizeMethod.AREA))", "def resize_image(self, data, size): \"\"\" Resizes the given image to fit inside a box of the given size. \"\"\" from machina.core.compat import PILImage as Image image = Image.open(BytesIO(data)) # Resize! image.thumbnail(size, Image.ANTIALIAS) string = BytesIO() image.save(string, format='PNG') return string.getvalue()", "def scale_image(image, new_width): \"\"\"Resizes an image preserving the aspect ratio. \"\"\" (original_width, original_height) = image.size aspect_ratio = original_height/float(original_width) new_height = int(aspect_ratio * new_width) # This scales it wider than tall, since characters are biased new_image = image.resize((new_width*2, new_height)) return new_image"], "rank": 3}
{"nl": "python filling missing values with fillna", "code": ["def fillna(series_or_arr, missing_value=0.0): \"\"\"Fill missing values in pandas objects and numpy arrays. Arguments --------- series_or_arr : pandas.Series, numpy.ndarray The numpy array or pandas series for which the missing values need to be replaced. missing_value : float, int, str The value to replace the missing value with. Default 0.0. Returns ------- pandas.Series, numpy.ndarray The numpy array or pandas series with the missing values filled. \"\"\" if pandas.notnull(missing_value): if isinstance(series_or_arr, (numpy.ndarray)): series_or_arr[numpy.isnan(series_or_arr)] = missing_value else: series_or_arr.fillna(missing_value, inplace=True) return series_or_arr", "def _maybe_fill(arr, fill_value=np.nan): \"\"\" if we have a compatible fill_value and arr dtype, then fill \"\"\" if _isna_compat(arr, fill_value): arr.fill(fill_value) return arr", "def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df", "def fill_nulls(self, col: str): \"\"\" Fill all null values with NaN values in a column. Null values are ``None`` or en empty string :param col: column name :type col: str :example: ``ds.fill_nulls(\"mycol\")`` \"\"\" n = [None, \"\"] try: self.df[col] = self.df[col].replace(n, nan) except Exception as e: self.err(e)", "def clean_df(df, fill_nan=True, drop_empty_columns=True): \"\"\"Clean a pandas dataframe by: 1. Filling empty values with Nan 2. Dropping columns with all empty values Args: df: Pandas DataFrame fill_nan (bool): If any empty values (strings, None, etc) should be replaced with NaN drop_empty_columns (bool): If columns whose values are all empty should be dropped Returns: DataFrame: cleaned DataFrame \"\"\" if fill_nan: df = df.fillna(value=np.nan) if drop_empty_columns: df = df.dropna(axis=1, how='all') return df.sort_index()"], "rank": 1}
{"nl": "how to count the number of objects in python", "code": ["def get_size(objects): \"\"\"Compute the total size of all elements in objects.\"\"\" res = 0 for o in objects: try: res += _getsizeof(o) except AttributeError: print(\"IGNORING: type=%s; o=%s\" % (str(type(o)), str(o))) return res", "def objectcount(data, key): \"\"\"return the count of objects of key\"\"\" objkey = key.upper() return len(data.dt[objkey])", "def __len__(self): \"\"\" This will equal 124 for the V1 database. \"\"\" length = 0 for typ, siz, _ in self.format: length += siz return length", "def __len__(self): \"\"\"Get a list of the public data attributes.\"\"\" return len([i for i in (set(dir(self)) - self._STANDARD_ATTRS) if i[0] != '_'])", "def count_rows(self, table_name): \"\"\"Return the number of entries in a table by counting them.\"\"\" self.table_must_exist(table_name) query = \"SELECT COUNT (*) FROM `%s`\" % table_name.lower() self.own_cursor.execute(query) return int(self.own_cursor.fetchone()[0])"], "rank": 1}
{"nl": "python cmd get dynamically added do methods to show up in help", "code": ["def do_help(self, arg): \"\"\" Show help on all commands. \"\"\" print(self.response_prompt, file=self.stdout) return cmd.Cmd.do_help(self, arg)", "def help(self, level=0): \"\"\"return the usage string for available options \"\"\" self.cmdline_parser.formatter.output_level = level with _patch_optparse(): return self.cmdline_parser.format_help()", "def help_for_command(command): \"\"\"Get the help text (signature + docstring) for a command (function).\"\"\" help_text = pydoc.text.document(command) # remove backspaces return re.subn('.\\\\x08', '', help_text)[0]", "def find(command, on): \"\"\"Find the command usage.\"\"\" output_lines = parse_man_page(command, on) click.echo(''.join(output_lines))", "def _help(): \"\"\" Display both SQLAlchemy and Python help statements \"\"\" statement = '%s%s' % (shelp, phelp % ', '.join(cntx_.keys())) print statement.strip()"], "rank": 1}
{"nl": "how to change a dictionary to a numy array in python", "code": ["def dict_to_numpy_array(d): \"\"\" Convert a dict of 1d array to a numpy recarray \"\"\" return fromarrays(d.values(), np.dtype([(str(k), v.dtype) for k, v in d.items()]))", "def C_dict2array(C): \"\"\"Convert an OrderedDict containing C values to a 1D array.\"\"\" return np.hstack([np.asarray(C[k]).ravel() for k in C_keys])", "def setdict(self, D): \"\"\"Set dictionary array.\"\"\" self.D = np.asarray(D, dtype=self.dtype)", "def to_dicts(recarray): \"\"\"convert record array to a dictionaries\"\"\" for rec in recarray: yield dict(zip(recarray.dtype.names, rec.tolist()))", "def ensure_us_time_resolution(val): \"\"\"Convert val out of numpy time, for use in to_dict. Needed because of numpy bug GH#7619\"\"\" if np.issubdtype(val.dtype, np.datetime64): val = val.astype('datetime64[us]') elif np.issubdtype(val.dtype, np.timedelta64): val = val.astype('timedelta64[us]') return val"], "rank": 2}
