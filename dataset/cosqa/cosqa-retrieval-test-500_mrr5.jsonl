{"nl": "sort by a token in string python", "code": ["def _process_and_sort(s, force_ascii, full_process=True): \"\"\"Return a cleaned string with token sorted.\"\"\" # pull tokens ts = utils.full_process(s, force_ascii=force_ascii) if full_process else s tokens = ts.split() # sort tokens and join sorted_string = u\" \".join(sorted(tokens)) return sorted_string.strip()", "def transcript_sort_key(transcript): \"\"\" Key function used to sort transcripts. Taking the negative of protein sequence length and nucleotide sequence length so that the transcripts with longest sequences come first in the list. This couldn't be accomplished with `reverse=True` since we're also sorting by transcript name (which places TP53-001 before TP53-002). \"\"\" return ( -len(transcript.protein_sequence), -len(transcript.sequence), transcript.name )", "def naturalsortkey(s): \"\"\"Natural sort order\"\"\" return [int(part) if part.isdigit() else part for part in re.split('([0-9]+)', s)]", "def natural_sort(list, key=lambda s:s): \"\"\" Sort the list into natural alphanumeric order. \"\"\" def get_alphanum_key_func(key): convert = lambda text: int(text) if text.isdigit() else text return lambda s: [convert(c) for c in re.split('([0-9]+)', key(s))] sort_key = get_alphanum_key_func(key) list.sort(key=sort_key)", "def sort_key(x): \"\"\" >>> sort_key(('name', ('ROUTE', 'URL'))) -3 \"\"\" name, (r, u) = x return - len(u) + u.count('}') * 100"], "rank": 1}
{"nl": "python check file is readonly", "code": ["def make_file_read_only(file_path): \"\"\" Removes the write permissions for the given file for owner, groups and others. :param file_path: The file whose privileges are revoked. :raise FileNotFoundError: If the given file does not exist. \"\"\" old_permissions = os.stat(file_path).st_mode os.chmod(file_path, old_permissions & ~WRITE_PERMISSIONS)", "def make_writeable(filename): \"\"\" Make sure that the file is writeable. Useful if our source is read-only. \"\"\" if not os.access(filename, os.W_OK): st = os.stat(filename) new_permissions = stat.S_IMODE(st.st_mode) | stat.S_IWUSR os.chmod(filename, new_permissions)", "def readable(path): \"\"\"Test whether a path exists and is readable. Returns None for broken symbolic links or a failing stat() and False if the file exists but does not have read permission. True is returned if the file is readable.\"\"\" try: st = os.stat(path) return 0 != st.st_mode & READABLE_MASK except os.error: return None return True", "def make_file_readable (filename): \"\"\"Make file user readable if it is not a link.\"\"\" if not os.path.islink(filename): util.set_mode(filename, stat.S_IRUSR)", "def is_readable(filename): \"\"\"Check if file is a regular file and is readable.\"\"\" return os.path.isfile(filename) and os.access(filename, os.R_OK)"], "rank": 5}
{"nl": "declaring empty numpy array in python", "code": ["def empty(self, name, **kwargs): \"\"\"Create an array. Keyword arguments as per :func:`zarr.creation.empty`.\"\"\" return self._write_op(self._empty_nosync, name, **kwargs)", "def _create_empty_array(self, frames, always_2d, dtype): \"\"\"Create an empty array with appropriate shape.\"\"\" import numpy as np if always_2d or self.channels > 1: shape = frames, self.channels else: shape = frames, return np.empty(shape, dtype, order='C')", "def zeros(self, name, **kwargs): \"\"\"Create an array. Keyword arguments as per :func:`zarr.creation.zeros`.\"\"\" return self._write_op(self._zeros_nosync, name, **kwargs)", "def to_0d_array(value: Any) -> np.ndarray: \"\"\"Given a value, wrap it in a 0-D numpy.ndarray. \"\"\" if np.isscalar(value) or (isinstance(value, np.ndarray) and value.ndim == 0): return np.array(value) else: return to_0d_object_array(value)", "def remove_na_arraylike(arr): \"\"\" Return array-like containing only true/non-NaN values, possibly empty. \"\"\" if is_extension_array_dtype(arr): return arr[notna(arr)] else: return arr[notna(lib.values_from_object(arr))]"], "rank": 1}
{"nl": "test for iterable is string in python", "code": ["def is_seq(obj): \"\"\" Returns True if object is not a string but is iterable \"\"\" if not hasattr(obj, '__iter__'): return False if isinstance(obj, basestring): return False return True", "def is_iterable_but_not_string(obj): \"\"\" Determine whether or not obj is iterable but not a string (eg, a list, set, tuple etc). \"\"\" return hasattr(obj, '__iter__') and not isinstance(obj, str) and not isinstance(obj, bytes)", "def _is_iterable(item): \"\"\" Checks if an item is iterable (list, tuple, generator), but not string \"\"\" return isinstance(item, collections.Iterable) and not isinstance(item, six.string_types)", "def is_iterable(obj): \"\"\" Are we being asked to look up a list of things, instead of a single thing? We check for the `__iter__` attribute so that this can cover types that don't have to be known by this module, such as NumPy arrays. Strings, however, should be considered as atomic values to look up, not iterables. The same goes for tuples, since they are immutable and therefore valid entries. We don't need to check for the Python 2 `unicode` type, because it doesn't have an `__iter__` attribute anyway. \"\"\" return ( hasattr(obj, \"__iter__\") and not isinstance(obj, str) and not isinstance(obj, tuple) )", "def listlike(obj): \"\"\"Is an object iterable like a list (and not a string)?\"\"\" return hasattr(obj, \"__iter__\") \\ and not issubclass(type(obj), str)\\ and not issubclass(type(obj), unicode)"], "rank": 2}
{"nl": "python print results of query loop", "code": ["def print_runs(query): \"\"\" Print all rows in this result query. \"\"\" if query is None: return for tup in query: print((\"{0} @ {1} - {2} id: {3} group: {4}\".format( tup.end, tup.experiment_name, tup.project_name, tup.experiment_group, tup.run_group)))", "def stdout_display(): \"\"\" Print results straight to stdout \"\"\" if sys.version_info[0] == 2: yield SmartBuffer(sys.stdout) else: yield SmartBuffer(sys.stdout.buffer)", "def print_statements(self): \"\"\"Print all INDRA Statements collected by the processors.\"\"\" for i, stmt in enumerate(self.statements): print(\"%s: %s\" % (i, stmt))", "def stats(self): \"\"\" shotcut to pull out useful info for interactive use \"\"\" printDebug(\"Classes.....: %d\" % len(self.all_classes)) printDebug(\"Properties..: %d\" % len(self.all_properties))", "async def fetchall(self) -> Iterable[sqlite3.Row]: \"\"\"Fetch all remaining rows.\"\"\" return await self._execute(self._cursor.fetchall)"], "rank": 1}
{"nl": "how to save header of fits file to export python", "code": ["def write_fits(data, header, file_name): \"\"\" Combine data and a fits header to write a fits file. Parameters ---------- data : numpy.ndarray The data to be written. header : astropy.io.fits.hduheader The header for the fits file. file_name : string The file to write Returns ------- None \"\"\" hdu = fits.PrimaryHDU(data) hdu.header = header hdulist = fits.HDUList([hdu]) hdulist.writeto(file_name, overwrite=True) logging.info(\"Wrote {0}\".format(file_name)) return", "def write_fits(self, fitsfile): \"\"\"Write the ROI model to a FITS file.\"\"\" tab = self.create_table() hdu_data = fits.table_to_hdu(tab) hdus = [fits.PrimaryHDU(), hdu_data] fits_utils.write_hdus(hdus, fitsfile)", "def create_table_from_fits(fitsfile, hduname, colnames=None): \"\"\"Memory efficient function for loading a table from a FITS file.\"\"\" if colnames is None: return Table.read(fitsfile, hduname) cols = [] with fits.open(fitsfile, memmap=True) as h: for k in colnames: data = h[hduname].data.field(k) cols += [Column(name=k, data=data)] return Table(cols)", "def generate_header(headerfields, oldheader, group_by_field): \"\"\"Returns a header as a list, ready to write to TSV file\"\"\" fieldtypes = ['peptidefdr', 'peptidepep', 'nopsms', 'proteindata', 'precursorquant', 'isoquant'] return generate_general_header(headerfields, fieldtypes, peptabledata.HEADER_PEPTIDE, oldheader, group_by_field)", "def format_op_hdr(): \"\"\" Build the header \"\"\" txt = 'Base Filename'.ljust(36) + ' ' txt += 'Lines'.rjust(7) + ' ' txt += 'Words'.rjust(7) + ' ' txt += 'Unique'.ljust(8) + '' return txt"], "rank": 2}
{"nl": "python calc page align", "code": ["def page_align_content_length(length): # type: (int) -> int \"\"\"Compute page boundary alignment :param int length: content length :rtype: int :return: aligned byte boundary \"\"\" mod = length % _PAGEBLOB_BOUNDARY if mod != 0: return length + (_PAGEBLOB_BOUNDARY - mod) return length", "def text_alignment(x, y): \"\"\" Align text labels based on the x- and y-axis coordinate values. This function is used for computing the appropriate alignment of the text label. For example, if the text is on the \"right\" side of the plot, we want it to be left-aligned. If the text is on the \"top\" side of the plot, we want it to be bottom-aligned. :param x, y: (`int` or `float`) x- and y-axis coordinate respectively. :returns: A 2-tuple of strings, the horizontal and vertical alignments respectively. \"\"\" if x == 0: ha = \"center\" elif x > 0: ha = \"left\" else: ha = \"right\" if y == 0: va = \"center\" elif y > 0: va = \"bottom\" else: va = \"top\" return ha, va", "def align_to_mmap(num, round_up): \"\"\" Align the given integer number to the closest page offset, which usually is 4096 bytes. :param round_up: if True, the next higher multiple of page size is used, otherwise the lower page_size will be used (i.e. if True, 1 becomes 4096, otherwise it becomes 0) :return: num rounded to closest page\"\"\" res = (num // ALLOCATIONGRANULARITY) * ALLOCATIONGRANULARITY if round_up and (res != num): res += ALLOCATIONGRANULARITY # END handle size return res", "def _set_lastpage(self): \"\"\"Calculate value of class attribute ``last_page``.\"\"\" self.last_page = (len(self._page_data) - 1) // self.screen.page_size", "def move_page_bottom(self): \"\"\" Move the cursor to the last item on the page. \"\"\" self.nav.page_index = self.content.range[1] self.nav.cursor_index = 0 self.nav.inverted = True"], "rank": 1}
{"nl": "python numpy array as float", "code": ["def is_float_array(val): \"\"\" Checks whether a variable is a numpy float array. Parameters ---------- val The variable to check. Returns ------- bool True if the variable is a numpy float array. Otherwise False. \"\"\" return is_np_array(val) and issubclass(val.dtype.type, np.floating)", "def as_float_array(a): \"\"\"View the quaternion array as an array of floats This function is fast (of order 1 microsecond) because no data is copied; the returned quantity is just a \"view\" of the original. The output view has one more dimension (of size 4) than the input array, but is otherwise the same shape. \"\"\" return np.asarray(a, dtype=np.quaternion).view((np.double, 4))", "def cfloat32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_float)): return np.fromiter(cptr, dtype=np.float32, count=length) else: raise RuntimeError('Expected float pointer')", "def ensure_dtype_float(x, default=np.float64): r\"\"\"Makes sure that x is type of float \"\"\" if isinstance(x, np.ndarray): if x.dtype.kind == 'f': return x elif x.dtype.kind == 'i': return x.astype(default) else: raise TypeError('x is of type '+str(x.dtype)+' that cannot be converted to float') else: raise TypeError('x is not an array')", "def is_float_array(l): r\"\"\"Checks if l is a numpy array of floats (any dimension \"\"\" if isinstance(l, np.ndarray): if l.dtype.kind == 'f': return True return False"], "rank": 2}
{"nl": "input string that replaces occurences python", "code": ["def replace(self, text): \"\"\"Do j/v replacement\"\"\" for (pattern, repl) in self.patterns: text = re.subn(pattern, repl, text)[0] return text", "def __replace_all(repls: dict, input: str) -> str: \"\"\" Replaces from a string **input** all the occurrences of some symbols according to mapping **repls**. :param dict repls: where #key is the old character and #value is the one to substitute with; :param str input: original string where to apply the replacements; :return: *(str)* the string with the desired characters replaced \"\"\" return re.sub('|'.join(re.escape(key) for key in repls.keys()), lambda k: repls[k.group(0)], input)", "def fmt_subst(regex, subst): \"\"\"Replace regex with string.\"\"\" return lambda text: re.sub(regex, subst, text) if text else text", "def myreplace(astr, thefind, thereplace): \"\"\"in string astr replace all occurences of thefind with thereplace\"\"\" alist = astr.split(thefind) new_s = alist.split(thereplace) return new_s", "def subn_filter(s, find, replace, count=0): \"\"\"A non-optimal implementation of a regex filter\"\"\" return re.gsub(find, replace, count, s)"], "rank": 2}
{"nl": "python check all items in list are ints", "code": ["def is_iterable_of_int(l): r\"\"\" Checks if l is iterable and contains only integral types \"\"\" if not is_iterable(l): return False return all(is_int(value) for value in l)", "def is_int_vector(l): r\"\"\"Checks if l is a numpy array of integers \"\"\" if isinstance(l, np.ndarray): if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'): return True return False", "def test_value(self, value): \"\"\"Test if value is an instance of int.\"\"\" if not isinstance(value, int): raise ValueError('expected int value: ' + str(type(value)))", "def check_if_numbers_are_consecutive(list_): \"\"\" Returns True if numbers in the list are consecutive :param list_: list of integers :return: Boolean \"\"\" return all((True if second - first == 1 else False for first, second in zip(list_[:-1], list_[1:])))", "def toListInt(value): \"\"\" Convert a value to list of ints, if possible. \"\"\" if TypeConverters._can_convert_to_list(value): value = TypeConverters.toList(value) if all(map(lambda v: TypeConverters._is_integer(v), value)): return [int(v) for v in value] raise TypeError(\"Could not convert %s to list of ints\" % value)"], "rank": 1}
{"nl": "how to save variable to text file python", "code": ["def save(variable, filename): \"\"\"Save variable on given path using Pickle Args: variable: what to save path (str): path of the output \"\"\" fileObj = open(filename, 'wb') pickle.dump(variable, fileObj) fileObj.close()", "def save(self, fname: str): \"\"\" Saves this training state to fname. \"\"\" with open(fname, \"wb\") as fp: pickle.dump(self, fp)", "def append_text(self, txt): \"\"\" adds a line of text to a file \"\"\" with open(self.fullname, \"a\") as myfile: myfile.write(txt)", "def write_text(filename: str, text: str) -> None: \"\"\" Writes text to a file. \"\"\" with open(filename, 'w') as f: # type: TextIO print(text, file=f)", "def pickle_save(thing,fname): \"\"\"save something to a pickle file\"\"\" pickle.dump(thing, open(fname,\"wb\"),pickle.HIGHEST_PROTOCOL) return thing"], "rank": 1}
{"nl": "how to skip an index in a for loop python", "code": ["def _skip_frame(self): \"\"\"Skip a single frame from the trajectory\"\"\" size = self.read_size() for i in range(size+1): line = self._f.readline() if len(line) == 0: raise StopIteration", "def _skip_frame(self): \"\"\"Skip the next time frame\"\"\" for line in self._f: if line == 'ITEM: ATOMS\\n': break for i in range(self.num_atoms): next(self._f)", "def _skip_section(self): \"\"\"Skip a section\"\"\" self._last = self._f.readline() while len(self._last) > 0 and len(self._last[0].strip()) == 0: self._last = self._f.readline()", "def stop_at(iterable, idx): \"\"\"Stops iterating before yielding the specified idx.\"\"\" for i, item in enumerate(iterable): if i == idx: return yield item", "def skip(self, n): \"\"\"Skip the specified number of elements in the list. If the number skipped is greater than the number of elements in the list, hasNext() becomes false and available() returns zero as there are no more elements to retrieve. arg: n (cardinal): the number of elements to skip *compliance: mandatory -- This method must be implemented.* \"\"\" try: self._iter_object.skip(n) except AttributeError: for i in range(0, n): self.next()"], "rank": 4}
{"nl": "how to create a tokenization code in python", "code": ["def listified_tokenizer(source): \"\"\"Tokenizes *source* and returns the tokens as a list of lists.\"\"\" io_obj = io.StringIO(source) return [list(a) for a in tokenize.generate_tokens(io_obj.readline)]", "def token(name): \"\"\"Marker for a token :param str name: Name of tokenizer \"\"\" def wrap(f): tokenizers.append((name, f)) return f return wrap", "def tree(string, token=[WORD, POS, CHUNK, PNP, REL, ANCHOR, LEMMA]): \"\"\" Transforms the output of parse() into a Text object. The token parameter lists the order of tags in each token in the input string. \"\"\" return Text(string, token)", "def tokenize(self, s): \"\"\"Return a list of token strings from the given sentence. :param string s: The sentence string to tokenize. :rtype: iter(str) \"\"\" return [s[start:end] for start, end in self.span_tokenize(s)]", "def lemmatize(self, text, best_guess=True, return_frequencies=False): \"\"\"Lemmatize all tokens in a string or a list. A string is first tokenized using punkt. Throw a type error if the input is neither a string nor a list. \"\"\" if isinstance(text, str): tokens = wordpunct_tokenize(text) elif isinstance(text, list): tokens= text else: raise TypeError(\"lemmatize only works with strings or lists of string tokens.\") return [self._lemmatize_token(token, best_guess, return_frequencies) for token in tokens]"], "rank": 2}
{"nl": "python raise without parentheses", "code": ["def raise_(exception=ABSENT, *args, **kwargs): \"\"\"Raise (or re-raises) an exception. :param exception: Exception object to raise, or an exception class. In the latter case, remaining arguments are passed to the exception's constructor. If omitted, the currently handled exception is re-raised. \"\"\" if exception is ABSENT: raise else: if inspect.isclass(exception): raise exception(*args, **kwargs) else: if args or kwargs: raise TypeError(\"can't pass arguments along with \" \"exception object to raise_()\") raise exception", "def assert_or_raise(stmt: bool, exception: Exception, *exception_args, **exception_kwargs) -> None: \"\"\" If the statement is false, raise the given exception. \"\"\" if not stmt: raise exception(*exception_args, **exception_kwargs)", "def _re_raise_as(NewExc, *args, **kw): \"\"\"Raise a new exception using the preserved traceback of the last one.\"\"\" etype, val, tb = sys.exc_info() raise NewExc(*args, **kw), None, tb", "def reraise(error): \"\"\"Re-raises the error that was processed by prepare_for_reraise earlier.\"\"\" if hasattr(error, \"_type_\"): six.reraise(type(error), error, error._traceback) raise error", "def re_raise(self): \"\"\" Raise this exception with the original traceback \"\"\" if self.exc_info is not None: six.reraise(type(self), self, self.exc_info[2]) else: raise self"], "rank": 2}
{"nl": "how to seperate list with commas python", "code": ["def list_to_str(lst): \"\"\" Turn a list into a comma- and/or and-separated string. Parameters ---------- lst : :obj:`list` A list of strings to join into a single string. Returns ------- str_ : :obj:`str` A string with commas and/or ands separating th elements from ``lst``. \"\"\" if len(lst) == 1: str_ = lst[0] elif len(lst) == 2: str_ = ' and '.join(lst) elif len(lst) > 2: str_ = ', '.join(lst[:-1]) str_ += ', and {0}'.format(lst[-1]) else: raise ValueError('List of length 0 provided.') return str_", "def split_elements(value): \"\"\"Split a string with comma or space-separated elements into a list.\"\"\" l = [v.strip() for v in value.split(',')] if len(l) == 1: l = value.split() return l", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids", "def vectorize(values): \"\"\" Takes a value or list of values and returns a single result, joined by \",\" if necessary. \"\"\" if isinstance(values, list): return ','.join(str(v) for v in values) return values", "def list_to_csv(value): \"\"\" Converts list to string with comma separated values. For string is no-op. \"\"\" if isinstance(value, (list, tuple, set)): value = \",\".join(value) return value"], "rank": 3}
{"nl": "python asynchronous function call return", "code": ["def apply(self, func, args=(), kwds=dict()): \"\"\"Equivalent of the apply() builtin function. It blocks till the result is ready.\"\"\" return self.apply_async(func, args, kwds).get()", "def runcoro(async_function): \"\"\" Runs an asynchronous function without needing to use await - useful for lambda Args: async_function (Coroutine): The asynchronous function to run \"\"\" future = _asyncio.run_coroutine_threadsafe(async_function, client.loop) result = future.result() return result", "def delegate(self, fn, *args, **kwargs): \"\"\"Return the given operation as an asyncio future.\"\"\" callback = functools.partial(fn, *args, **kwargs) coro = self.loop.run_in_executor(self.subexecutor, callback) return asyncio.ensure_future(coro)", "def asynchronous(function, event): \"\"\" Runs the function asynchronously taking care of exceptions. \"\"\" thread = Thread(target=synchronous, args=(function, event)) thread.daemon = True thread.start()", "def return_future(fn): \"\"\"Decorator that turns a synchronous function into one returning a future. This should only be applied to non-blocking functions. Will do set_result() with the return value, or set_exc_info() if an exception is raised. \"\"\" @wraps(fn) def decorated(*args, **kwargs): return gen.maybe_future(fn(*args, **kwargs)) return decorated"], "rank": 4}
{"nl": "how to make a seconds to time in python", "code": ["def seconds_to_time(x): \"\"\"Convert a number of second into a time\"\"\" t = int(x * 10**6) ms = t % 10**6 t = t // 10**6 s = t % 60 t = t // 60 m = t % 60 t = t // 60 h = t return time(h, m, s, ms)", "def time2seconds(t): \"\"\"Returns seconds since 0h00.\"\"\" return t.hour * 3600 + t.minute * 60 + t.second + float(t.microsecond) / 1e6", "def RoundToSeconds(cls, timestamp): \"\"\"Takes a timestamp value and rounds it to a second precision.\"\"\" leftovers = timestamp % definitions.MICROSECONDS_PER_SECOND scrubbed = timestamp - leftovers rounded = round(float(leftovers) / definitions.MICROSECONDS_PER_SECOND) return int(scrubbed + rounded * definitions.MICROSECONDS_PER_SECOND)", "def datetime_to_ms(dt): \"\"\" Converts a datetime to a millisecond accuracy timestamp \"\"\" seconds = calendar.timegm(dt.utctimetuple()) return seconds * 1000 + int(dt.microsecond / 1000)", "def seconds_to_hms(input_seconds): \"\"\"Convert seconds to human-readable time.\"\"\" minutes, seconds = divmod(input_seconds, 60) hours, minutes = divmod(minutes, 60) hours = int(hours) minutes = int(minutes) seconds = str(int(seconds)).zfill(2) return hours, minutes, seconds"], "rank": 2}
{"nl": "python cast true or false as numbers", "code": ["def to_bool(value): # type: (Any) -> bool \"\"\" Convert a value into a bool but handle \"truthy\" strings eg, yes, true, ok, y \"\"\" if isinstance(value, _compat.string_types): return value.upper() in ('Y', 'YES', 'T', 'TRUE', '1', 'OK') return bool(value)", "def to_bool(value: Any) -> bool: \"\"\"Convert string or other Python object to boolean. **Rationalle** Passing flags is one of the most common cases of using environment vars and as values are strings we need to have an easy way to convert them to boolean Python value. Without this function int or float string values can be converted as false positives, e.g. ``bool('0') => True``, but using this function ensure that digit flag be properly converted to boolean value. :param value: String or other value. \"\"\" return bool(strtobool(value) if isinstance(value, str) else value)", "def _check_and_convert_bools(self): \"\"\"Replace boolean variables by the characters 'F'/'T' \"\"\" replacements = { True: 'T', False: 'F', } for key in self.bools: if isinstance(self[key], bool): self[key] = replacements[self[key]]", "def _cast_boolean(value): \"\"\" Helper to convert config values to boolean as ConfigParser do. \"\"\" _BOOLEANS = {'1': True, 'yes': True, 'true': True, 'on': True, '0': False, 'no': False, 'false': False, 'off': False, '': False} value = str(value) if value.lower() not in _BOOLEANS: raise ValueError('Not a boolean: %s' % value) return _BOOLEANS[value.lower()]", "def process_bool_arg(arg): \"\"\" Determine True/False from argument \"\"\" if isinstance(arg, bool): return arg elif isinstance(arg, basestring): if arg.lower() in [\"true\", \"1\"]: return True elif arg.lower() in [\"false\", \"0\"]: return False"], "rank": 27}
{"nl": "add milliseconds to datetime python", "code": ["def ms_to_datetime(ms): \"\"\" Converts a millisecond accuracy timestamp to a datetime \"\"\" dt = datetime.datetime.utcfromtimestamp(ms / 1000) return dt.replace(microsecond=(ms % 1000) * 1000).replace(tzinfo=pytz.utc)", "def datetime_to_ms(dt): \"\"\" Converts a datetime to a millisecond accuracy timestamp \"\"\" seconds = calendar.timegm(dt.utctimetuple()) return seconds * 1000 + int(dt.microsecond / 1000)", "def datetime_delta_to_ms(delta): \"\"\" Given a datetime.timedelta object, return the delta in milliseconds \"\"\" delta_ms = delta.days * 24 * 60 * 60 * 1000 delta_ms += delta.seconds * 1000 delta_ms += delta.microseconds / 1000 delta_ms = int(delta_ms) return delta_ms", "def convertDatetime(t): \"\"\" Converts the specified datetime object into its appropriate protocol value. This is the number of milliseconds from the epoch. \"\"\" epoch = datetime.datetime.utcfromtimestamp(0) delta = t - epoch millis = delta.total_seconds() * 1000 return int(millis)", "def ToDatetime(self): \"\"\"Converts Timestamp to datetime.\"\"\" return datetime.utcfromtimestamp( self.seconds + self.nanos / float(_NANOS_PER_SECOND))"], "rank": 2}
{"nl": "how to read the last n lines of a program on python", "code": ["def get_readline_tail(self, n=10): \"\"\"Get the last n items in readline history.\"\"\" end = self.shell.readline.get_current_history_length() + 1 start = max(end-n, 1) ghi = self.shell.readline.get_history_item return [ghi(x) for x in range(start, end)]", "def head(filename, n=10): \"\"\" prints the top `n` lines of a file \"\"\" with freader(filename) as fr: for _ in range(n): print(fr.readline().strip())", "def clear_last_lines(self, n): \"\"\"Clear last N lines of terminal output. \"\"\" self.term.stream.write( self.term.move_up * n + self.term.clear_eos) self.term.stream.flush()", "def tail(filename, number_of_bytes): \"\"\"Returns the last number_of_bytes of filename\"\"\" with open(filename, \"rb\") as f: if os.stat(filename).st_size > number_of_bytes: f.seek(-number_of_bytes, 2) return f.read()", "def _read_section(self): \"\"\"Read and return an entire section\"\"\" lines = [self._last[self._last.find(\":\")+1:]] self._last = self._f.readline() while len(self._last) > 0 and len(self._last[0].strip()) == 0: lines.append(self._last) self._last = self._f.readline() return lines"], "rank": 1}
{"nl": "python mysql get list of table columns", "code": ["def get_column_names(engine: Engine, tablename: str) -> List[str]: \"\"\" Get all the database column names for the specified table. \"\"\" return [info.name for info in gen_columns_info(engine, tablename)]", "def get_table_columns(dbconn, tablename): \"\"\" Return a list of tuples specifying the column name and type \"\"\" cur = dbconn.cursor() cur.execute(\"PRAGMA table_info('%s');\" % tablename) info = cur.fetchall() cols = [(i[1], i[2]) for i in info] return cols", "def column_names(self, table): \"\"\"An iterable of column names, for a particular table or view.\"\"\" table_info = self.execute( u'PRAGMA table_info(%s)' % quote(table)) return (column['name'] for column in table_info)", "def get_table_names_from_metadata(metadata: MetaData) -> List[str]: \"\"\" Returns all database table names found in an SQLAlchemy :class:`MetaData` object. \"\"\" return [table.name for table in metadata.tables.values()]", "def _columns_for_table(table_name): \"\"\" Return all of the columns registered for a given table. Parameters ---------- table_name : str Returns ------- columns : dict of column wrappers Keys will be column names. \"\"\" return {cname: col for (tname, cname), col in _COLUMNS.items() if tname == table_name}"], "rank": 2}
{"nl": "how to get domain part of a url in python", "code": ["def parse_domain(url): \"\"\" parse the domain from the url \"\"\" domain_match = lib.DOMAIN_REGEX.match(url) if domain_match: return domain_match.group()", "def get_domain(url): \"\"\" Get domain part of an url. For example: https://www.python.org/doc/ -> https://www.python.org \"\"\" parse_result = urlparse(url) domain = \"{schema}://{netloc}\".format( schema=parse_result.scheme, netloc=parse_result.netloc) return domain", "def top_level(url, fix_protocol=True): \"\"\"Extract the top level domain from an URL.\"\"\" ext = tld.get_tld(url, fix_protocol=fix_protocol) toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split( ext)[0] + ext return toplevel", "def url_host(url: str) -> str: \"\"\" Parses hostname from URL. :param url: URL :return: hostname \"\"\" from urllib.parse import urlparse res = urlparse(url) return res.netloc.split(':')[0] if res.netloc else ''", "def get_site_name(request): \"\"\"Return the domain:port part of the URL without scheme. Eg: facebook.com, 127.0.0.1:8080, etc. \"\"\" urlparts = request.urlparts return ':'.join([urlparts.hostname, str(urlparts.port)])"], "rank": 4}
{"nl": "python flatten a nested dictionaary", "code": ["def flatten(nested, containers=(list, tuple)): \"\"\" Flatten a nested list by yielding its scalar items. \"\"\" for item in nested: if hasattr(item, \"next\") or isinstance(item, containers): for subitem in flatten(item): yield subitem else: yield item", "def flatten_multidict(multidict): \"\"\"Return flattened dictionary from ``MultiDict``.\"\"\" return dict([(key, value if len(value) > 1 else value[0]) for (key, value) in multidict.iterlists()])", "def flatten(nested): \"\"\" Return a flatten version of the nested argument \"\"\" flat_return = list() def __inner_flat(nested,flat): for i in nested: __inner_flat(i, flat) if isinstance(i, list) else flat.append(i) return flat __inner_flat(nested,flat_return) return flat_return", "def flatten(lis): \"\"\"Given a list, possibly nested to any level, return it flattened.\"\"\" new_lis = [] for item in lis: if isinstance(item, collections.Sequence) and not isinstance(item, basestring): new_lis.extend(flatten(item)) else: new_lis.append(item) return new_lis", "def _iterate_flattened_values(value): \"\"\"Provides an iterator over all values in a nested structure.\"\"\" if isinstance(value, six.string_types): yield value return if isinstance(value, collections.Mapping): value = collections.ValuesView(value) if isinstance(value, collections.Iterable): for nested_value in value: for nested_nested_value in _iterate_flattened_values(nested_value): yield nested_nested_value yield value"], "rank": 2}
{"nl": "python session set get", "code": ["def _session_set(self, key, value): \"\"\" Saves a value to session. \"\"\" self.session[self._session_key(key)] = value", "def _get_data(self): \"\"\" Extracts the session data from cookie. \"\"\" cookie = self.adapter.cookies.get(self.name) return self._deserialize(cookie) if cookie else {}", "def save(self): \"\"\"save the current session override, if session was saved earlier\"\"\" if self.path: self._saveState(self.path) else: self.saveAs()", "def save_config_value(request, response, key, value): \"\"\"Sets value of key `key` to `value` in both session and cookies.\"\"\" request.session[key] = value response.set_cookie(key, value, expires=one_year_from_now()) return response", "def save_session_to_file(self, sessionfile): \"\"\"Not meant to be used directly, use :meth:`Instaloader.save_session_to_file`.\"\"\" pickle.dump(requests.utils.dict_from_cookiejar(self._session.cookies), sessionfile)"], "rank": 1}
{"nl": "python save graph into file", "code": ["def draw(graph, fname): \"\"\"Draw a graph and save it into a file\"\"\" ag = networkx.nx_agraph.to_agraph(graph) ag.draw(fname, prog='dot')", "def to_dotfile(G: nx.DiGraph, filename: str): \"\"\" Output a networkx graph to a DOT file. \"\"\" A = to_agraph(G) A.write(filename)", "def export_to_dot(self, filename: str = 'output') -> None: \"\"\" Export the graph to the dot file \"filename.dot\". \"\"\" with open(filename + '.dot', 'w') as output: output.write(self.as_dot())", "def save_dot(self, fd): \"\"\" Saves a representation of the case in the Graphviz DOT language. \"\"\" from pylon.io import DotWriter DotWriter(self).write(fd)", "def draw_graph(G: nx.DiGraph, filename: str): \"\"\" Draw a networkx graph with Pygraphviz. \"\"\" A = to_agraph(G) A.graph_attr[\"rankdir\"] = \"LR\" A.draw(filename, prog=\"dot\")"], "rank": 2}
{"nl": "chmod python windows to remove file", "code": ["def rmfile(path): \"\"\"Ensure file deleted also on *Windows* where read-only files need special treatment.\"\"\" if osp.isfile(path): if is_win: os.chmod(path, 0o777) os.remove(path)", "def chmod_plus_w(path): \"\"\"Equivalent of unix `chmod +w path`\"\"\" path_mode = os.stat(path).st_mode path_mode &= int('777', 8) path_mode |= stat.S_IWRITE os.chmod(path, path_mode)", "def chmod(f): \"\"\" change mod to writeable \"\"\" try: os.chmod(f, S_IWRITE) # windows (cover all) except Exception as e: pass try: os.chmod(f, 0o777) # *nix except Exception as e: pass", "def make_file_read_only(file_path): \"\"\" Removes the write permissions for the given file for owner, groups and others. :param file_path: The file whose privileges are revoked. :raise FileNotFoundError: If the given file does not exist. \"\"\" old_permissions = os.stat(file_path).st_mode os.chmod(file_path, old_permissions & ~WRITE_PERMISSIONS)", "def add_exec_permission_to(target_file): \"\"\"Add executable permissions to the file :param target_file: the target file whose permission to be changed \"\"\" mode = os.stat(target_file).st_mode os.chmod(target_file, mode | stat.S_IXUSR)"], "rank": 1}
{"nl": "add suffixes on concat python", "code": ["def add_suffix(fullname, suffix): \"\"\" Add suffix to a full file name\"\"\" name, ext = os.path.splitext(fullname) return name + '_' + suffix + ext", "def filename_addstring(filename, text): \"\"\" Add `text` to filename, keeping the extension in place For example when adding a timestamp to the filename \"\"\" fn, ext = os.path.splitext(filename) return fn + text + ext", "def _increment_numeric_suffix(s): \"\"\"Increment (or add) numeric suffix to identifier.\"\"\" if re.match(r\".*\\d+$\", s): return re.sub(r\"\\d+$\", lambda n: str(int(n.group(0)) + 1), s) return s + \"_2\"", "def get_abi3_suffix(): \"\"\"Return the file extension for an abi3-compliant Extension()\"\"\" for suffix, _, _ in (s for s in imp.get_suffixes() if s[2] == imp.C_EXTENSION): if '.abi3' in suffix: # Unix return suffix elif suffix == '.pyd': # Windows return suffix", "def drop_post(self): \"\"\"Remove .postXXXX postfix from version\"\"\" post_index = self.version.find('.post') if post_index >= 0: self.version = self.version[:post_index]"], "rank": 1}
{"nl": "how to check python object iterable", "code": ["def is_iterable(obj): \"\"\" Are we being asked to look up a list of things, instead of a single thing? We check for the `__iter__` attribute so that this can cover types that don't have to be known by this module, such as NumPy arrays. Strings, however, should be considered as atomic values to look up, not iterables. The same goes for tuples, since they are immutable and therefore valid entries. We don't need to check for the Python 2 `unicode` type, because it doesn't have an `__iter__` attribute anyway. \"\"\" return ( hasattr(obj, \"__iter__\") and not isinstance(obj, str) and not isinstance(obj, tuple) )", "def is_lazy_iterable(obj): \"\"\" Returns whether *obj* is iterable lazily, such as generators, range objects, etc. \"\"\" return isinstance(obj, (types.GeneratorType, collections.MappingView, six.moves.range, enumerate))", "def listlike(obj): \"\"\"Is an object iterable like a list (and not a string)?\"\"\" return hasattr(obj, \"__iter__\") \\ and not issubclass(type(obj), str)\\ and not issubclass(type(obj), unicode)", "def is_seq(obj): \"\"\" Returns True if object is not a string but is iterable \"\"\" if not hasattr(obj, '__iter__'): return False if isinstance(obj, basestring): return False return True", "def _is_iterable(item): \"\"\" Checks if an item is iterable (list, tuple, generator), but not string \"\"\" return isinstance(item, collections.Iterable) and not isinstance(item, six.string_types)"], "rank": 7}
{"nl": "python 3 tkinter open file dialog", "code": ["def askopenfilename(**kwargs): \"\"\"Return file name(s) from Tkinter's file open dialog.\"\"\" try: from Tkinter import Tk import tkFileDialog as filedialog except ImportError: from tkinter import Tk, filedialog root = Tk() root.withdraw() root.update() filenames = filedialog.askopenfilename(**kwargs) root.destroy() return filenames", "def fileopenbox(msg=None, title=None, argInitialFile=None): \"\"\"Original doc: A dialog to get a file name. Returns the name of a file, or None if user chose to cancel. if argInitialFile contains a valid filename, the dialog will be positioned at that file when it appears. \"\"\" return psidialogs.ask_file(message=msg, title=title, default=argInitialFile)", "def filesavebox(msg=None, title=None, argInitialFile=None): \"\"\"Original doc: A file to get the name of a file to save. Returns the name of a file, or None if user chose to cancel. if argInitialFile contains a valid filename, the dialog will be positioned at that file when it appears. \"\"\" return psidialogs.ask_file(message=msg, title=title, default=argInitialFile, save=True)", "def popup(self, title, callfn, initialdir=None): \"\"\"Let user select a directory.\"\"\" super(DirectorySelection, self).popup(title, callfn, initialdir)", "def ask_folder(message='Select folder.', default='', title=''): \"\"\" A dialog to get a directory name. Returns the name of a directory, or None if user chose to cancel. If the \"default\" argument specifies a directory name, and that directory exists, then the dialog box will start with that directory. :param message: message to be displayed. :param title: window title :param default: default folder path :rtype: None or string \"\"\" return backend_api.opendialog(\"ask_folder\", dict(message=message, default=default, title=title))"], "rank": 1}
{"nl": "how to indent self python", "code": ["def _pad(self): \"\"\"Pads the output with an amount of indentation appropriate for the number of open element. This method does nothing if the indent value passed to the constructor is falsy. \"\"\" if self._indent: self.whitespace(self._indent * len(self._open_elements))", "def indent(self, message): \"\"\" Sets the indent for standardized output :param message: (str) :return: (str) \"\"\" indent = self.indent_char * self.indent_size return indent + message", "def dumped(text, level, indent=2): \"\"\"Put curly brackets round an indented text\"\"\" return indented(\"{\\n%s\\n}\" % indented(text, level + 1, indent) or \"None\", level, indent) + \"\\n\"", "def PrintIndented(self, file, ident, code): \"\"\"Takes an array, add indentation to each entry and prints it.\"\"\" for entry in code: print >>file, '%s%s' % (ident, entry)", "def indent(text: str, num: int = 2) -> str: \"\"\"Indent a piece of text.\"\"\" lines = text.splitlines() return \"\\n\".join(indent_iterable(lines, num=num))"], "rank": 1}
{"nl": "tracking centroid of an object python", "code": ["def compute_centroid(points): \"\"\" Computes the centroid of set of points Args: points (:obj:`list` of :obj:`Point`) Returns: :obj:`Point` \"\"\" lats = [p[1] for p in points] lons = [p[0] for p in points] return Point(np.mean(lats), np.mean(lons), None)", "def find_dist_to_centroid(cvects, idx_list, weights=None): \"\"\" Find the centroid for a set of vectors Parameters ---------- cvects : ~numpy.ndarray(3,nsrc) with directional cosine (i.e., x,y,z component) values idx_list : [int,...] list of the source indices in the cluster weights : ~numpy.ndarray(nsrc) with the weights to use. None for equal weighting returns (np.ndarray(nsrc)) distances to the centroid (in degrees) \"\"\" centroid = find_centroid(cvects, idx_list, weights) dist_vals = np.degrees(np.arccos((centroid * cvects.T[idx_list]).sum(1))) return dist_vals, centroid", "def find_centroid(region): \"\"\" Finds an approximate centroid for a region that is within the region. Parameters ---------- region : np.ndarray(shape=(m, n), dtype='bool') mask of the region. Returns ------- i, j : tuple(int, int) 2d index within the region nearest the center of mass. \"\"\" x, y = center_of_mass(region) w = np.argwhere(region) i, j = w[np.argmin(np.linalg.norm(w - (x, y), axis=1))] return i, j", "def _cal_dist2center(X, center): \"\"\" Calculate the SSE to the cluster center \"\"\" dmemb2cen = scipy.spatial.distance.cdist(X, center.reshape(1,X.shape[1]), metric='seuclidean') return(np.sum(dmemb2cen))", "def _centroids(n_clusters: int, points: List[List[float]]) -> List[List[float]]: \"\"\" Return n_clusters centroids of points \"\"\" k_means = KMeans(n_clusters=n_clusters) k_means.fit(points) closest, _ = pairwise_distances_argmin_min(k_means.cluster_centers_, points) return list(map(list, np.array(points)[closest.tolist()]))"], "rank": 1}
{"nl": "python3 ctypes return float array", "code": ["def cfloat32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_float)): return np.fromiter(cptr, dtype=np.float32, count=length) else: raise RuntimeError('Expected float pointer')", "def cfloat64_array_to_numpy(cptr, length): \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_double)): return np.fromiter(cptr, dtype=np.float64, count=length) else: raise RuntimeError('Expected double pointer')", "def cint32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)): return np.fromiter(cptr, dtype=np.int32, count=length) else: raise RuntimeError('Expected int pointer')", "def cint8_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)): return np.fromiter(cptr, dtype=np.int8, count=length) else: raise RuntimeError('Expected int pointer')", "def c_array(ctype, values): \"\"\"Convert a python string to c array.\"\"\" if isinstance(values, np.ndarray) and values.dtype.itemsize == ctypes.sizeof(ctype): return (ctype * len(values)).from_buffer_copy(values) return (ctype * len(values))(*values)"], "rank": 1}
{"nl": "how to make paragraphs in python", "code": ["def paragraph(separator='\\n\\n', wrap_start='', wrap_end='', html=False, sentences_quantity=3): \"\"\"Return a random paragraph.\"\"\" return paragraphs(quantity=1, separator=separator, wrap_start=wrap_start, wrap_end=wrap_end, html=html, sentences_quantity=sentences_quantity)", "def indent(block, spaces): \"\"\" indents paragraphs of text for rst formatting \"\"\" new_block = '' for line in block.split('\\n'): new_block += spaces + line + '\\n' return new_block", "def wrap(text, width=70, **kwargs): \"\"\"Wrap a single paragraph of text, returning a list of wrapped lines. Reformat the single paragraph in 'text' so it fits in lines of no more than 'width' columns, and return a list of wrapped lines. By default, tabs in 'text' are expanded with string.expandtabs(), and all other whitespace characters (including newline) are converted to space. See TextWrapper class for available keyword args to customize wrapping behaviour. \"\"\" w = TextWrapper(width=width, **kwargs) return w.wrap(text)", "def layout(self, indent=' '): \"\"\"This will indent each new tag in the body by given number of spaces.\"\"\" self.__indent(self.head, indent) self.__indent(self.meta, indent) self.__indent(self.stylesheet, indent) self.__indent(self.header, indent) self.__indent(self.body, indent, initial=3) self.__indent(self.footer, indent) self.__indent(self.body_pre_docinfo, indent, initial=3) self.__indent(self.docinfo, indent)", "def fill_document(doc): \"\"\"Add a section, a subsection and some text to the document. :param doc: the document :type doc: :class:`pylatex.document.Document` instance \"\"\" with doc.create(Section('A section')): doc.append('Some regular text and some ') doc.append(italic('italic text. ')) with doc.create(Subsection('A subsection')): doc.append('Also some crazy characters: $&#{}')"], "rank": 1}
{"nl": "python rest requests delete", "code": ["def delete(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP DELETE operation to API endpoint.\"\"\" return self._request('DELETE', endpoint, **kwargs)", "def put(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP PUT operation to API endpoint.\"\"\" return self._request('PUT', endpoint, **kwargs)", "def deleted(self, instance): \"\"\" Convenience method for deleting a model (automatically commits the delete to the database and returns with an HTTP 204 status code) \"\"\" self.session_manager.delete(instance, commit=True) return '', HTTPStatus.NO_CONTENT", "def post(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP POST operation to API endpoint.\"\"\" return self._request('POST', endpoint, **kwargs)", "def POST(self, *args, **kwargs): \"\"\" POST request \"\"\" return self._handle_api(self.API_POST, args, kwargs)"], "rank": 1}
{"nl": "python replace month number", "code": ["def replace_month_abbr_with_num(date_str, lang=DEFAULT_DATE_LANG): \"\"\"Replace month strings occurrences with month number.\"\"\" num, abbr = get_month_from_date_str(date_str, lang) return re.sub(abbr, str(num), date_str, flags=re.IGNORECASE)", "def pmon(month): \"\"\" P the month >>> print(pmon('2012-08')) August, 2012 \"\"\" year, month = month.split('-') return '{month_name}, {year}'.format( month_name=calendar.month_name[int(month)], year=year, )", "def calculate_month(birth_date): \"\"\" Calculates and returns a month number basing on PESEL standard. \"\"\" year = int(birth_date.strftime('%Y')) month = int(birth_date.strftime('%m')) + ((int(year / 100) - 14) % 5) * 20 return month", "def day_to_month(timeperiod): \"\"\":param timeperiod: as string in YYYYMMDD00 format :return string in YYYYMM0000 format\"\"\" t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN) return t.strftime(SYNERGY_MONTHLY_PATTERN)", "def start_of_month(val): \"\"\" Return a new datetime.datetime object with values that represent a start of a month. :param val: Date to ... :type val: datetime.datetime | datetime.date :rtype: datetime.datetime \"\"\" if type(val) == date: val = datetime.fromordinal(val.toordinal()) return start_of_day(val).replace(day=1)"], "rank": 1}
{"nl": "how to tell what type of data object is in python", "code": ["def type(self): \"\"\"Returns type of the data for the given FeatureType.\"\"\" if self is FeatureType.TIMESTAMP: return list if self is FeatureType.BBOX: return BBox return dict", "def _get_type(self, value): \"\"\"Get the data type for *value*.\"\"\" if value is None: return type(None) elif type(value) in int_types: return int elif type(value) in float_types: return float elif isinstance(value, binary_type): return binary_type else: return text_type", "def maybe_infer_dtype_type(element): \"\"\"Try to infer an object's dtype, for use in arithmetic ops Uses `element.dtype` if that's available. Objects implementing the iterator protocol are cast to a NumPy array, and from there the array's type is used. Parameters ---------- element : object Possibly has a `.dtype` attribute, and possibly the iterator protocol. Returns ------- tipo : type Examples -------- >>> from collections import namedtuple >>> Foo = namedtuple(\"Foo\", \"dtype\") >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\"))) numpy.int64 \"\"\" tipo = None if hasattr(element, 'dtype'): tipo = element.dtype elif is_list_like(element): element = np.asarray(element) tipo = element.dtype return tipo", "def is_integer(obj): \"\"\"Is this an integer. :param object obj: :return: \"\"\" if PYTHON3: return isinstance(obj, int) return isinstance(obj, (int, long))", "def _api_type(self, value): \"\"\" Returns the API type of the given value based on its python type. \"\"\" if isinstance(value, six.string_types): return 'string' elif isinstance(value, six.integer_types): return 'integer' elif type(value) is datetime.datetime: return 'date'"], "rank": 4}
{"nl": "python jsonschema validate schema file", "code": ["def validate(payload, schema): \"\"\"Validate `payload` against `schema`, returning an error list. jsonschema provides lots of information in it's errors, but it can be a bit of work to extract all the information. \"\"\" v = jsonschema.Draft4Validator( schema, format_checker=jsonschema.FormatChecker()) error_list = [] for error in v.iter_errors(payload): message = error.message location = '/' + '/'.join([str(c) for c in error.absolute_path]) error_list.append(message + ' at ' + location) return error_list", "def _validate(data, schema, ac_schema_safe=True, **options): \"\"\" See the descritpion of :func:`validate` for more details of parameters and return value. Validate target object 'data' with given schema object. \"\"\" try: jsonschema.validate(data, schema, **options) except (jsonschema.ValidationError, jsonschema.SchemaError, Exception) as exc: if ac_schema_safe: return (False, str(exc)) # Validation was failed. raise return (True, '')", "def validate(raw_schema, target=None, **kwargs): \"\"\" Given the python representation of a JSONschema as defined in the swagger spec, validate that the schema complies to spec. If `target` is provided, that target will be validated against the provided schema. \"\"\" schema = schema_validator(raw_schema, **kwargs) if target is not None: validate_object(target, schema=schema, **kwargs)", "def validate(request: Union[Dict, List], schema: dict) -> Union[Dict, List]: \"\"\" Wraps jsonschema.validate, returning the same object passed in. Args: request: The deserialized-from-json request. schema: The jsonschema schema to validate against. Raises: jsonschema.ValidationError \"\"\" jsonschema_validate(request, schema) return request", "def load_schema(schema_path): \"\"\"Prepare the api specification for request and response validation. :returns: a mapping from :class:`RequestMatcher` to :class:`ValidatorMap` for every operation in the api specification. :rtype: dict \"\"\" with open(schema_path, 'r') as schema_file: schema = simplejson.load(schema_file) resolver = RefResolver('', '', schema.get('models', {})) return build_request_to_validator_map(schema, resolver)"], "rank": 2}
{"nl": "get wechat access token python", "code": ["def access_token(self): \"\"\" WeChat access token \"\"\" access_token = self.session.get(self.access_token_key) if access_token: if not self.expires_at: # user provided access_token, just return it return access_token timestamp = time.time() if self.expires_at - timestamp > 60: return access_token self.fetch_access_token() return self.session.get(self.access_token_key)", "def fetch_token(self, **kwargs): \"\"\"Exchange a code (and 'state' token) for a bearer token\"\"\" return super(AsanaOAuth2Session, self).fetch_token(self.token_url, client_secret=self.client_secret, **kwargs)", "def get_oauth_token(): \"\"\"Retrieve a simple OAuth Token for use with the local http client.\"\"\" url = \"{0}/token\".format(DEFAULT_ORIGIN[\"Origin\"]) r = s.get(url=url) return r.json()[\"t\"]", "def get_tweepy_auth(twitter_api_key, twitter_api_secret, twitter_access_token, twitter_access_token_secret): \"\"\"Make a tweepy auth object\"\"\" auth = tweepy.OAuthHandler(twitter_api_key, twitter_api_secret) auth.set_access_token(twitter_access_token, twitter_access_token_secret) return auth", "def _get_token(self, oauth_request, token_type='access'): \"\"\"Try to find the token for the provided request token key.\"\"\" token_field = oauth_request.get_parameter('oauth_token') token = self.data_store.lookup_token(token_type, token_field) if not token: raise OAuthError('Invalid %s token: %s' % (token_type, token_field)) return token"], "rank": 1}
{"nl": "python change str value to int", "code": ["def try_cast_int(s): \"\"\"(str) -> int All the digits in a given string are concatenated and converted into a single number. \"\"\" try: temp = re.findall('\\d', str(s)) temp = ''.join(temp) return int(temp) except: return s", "def str2int(num, radix=10, alphabet=BASE85): \"\"\"helper function for quick base conversions from strings to integers\"\"\" return NumConv(radix, alphabet).str2int(num)", "def str_to_num(str_value): \"\"\"Convert str_value to an int or a float, depending on the numeric value represented by str_value. \"\"\" str_value = str(str_value) try: return int(str_value) except ValueError: return float(str_value)", "def cast_int(x): \"\"\" Cast unknown type into integer :param any x: :return int: \"\"\" try: x = int(x) except ValueError: try: x = x.strip() except AttributeError as e: logger_misc.warn(\"parse_str: AttributeError: String not number or word, {}, {}\".format(x, e)) return x", "def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)"], "rank": 2}
{"nl": "implementing drag and drop python", "code": ["def drag_and_drop(self, droppable): \"\"\" Performs drag a element to another elmenet. Currently works only on Chrome driver. \"\"\" self.scroll_to() ActionChains(self.parent.driver).drag_and_drop(self._element, droppable._element).perform()", "def mouseMoveEvent(self, event): \"\"\" Handle the mouse move event for a drag operation. \"\"\" self.declaration.mouse_move_event(event) super(QtGraphicsView, self).mouseMoveEvent(event)", "def _on_release(self, event): \"\"\"Stop dragging.\"\"\" if self._drag_cols or self._drag_rows: self._visual_drag.place_forget() self._dragged_col = None self._dragged_row = None", "def __init__(self, pos, cell, motion, cellmotion): self.pos = pos \"\"\"(x, y) position of the mouse on the screen. type: (int, int)\"\"\" self.cell = cell \"\"\"(x, y) position of the mouse snapped to a cell on the root console. type: (int, int)\"\"\" self.motion = motion \"\"\"(x, y) motion of the mouse on the screen. type: (int, int)\"\"\" self.cellmotion = cellmotion \"\"\"(x, y) mostion of the mouse moving over cells on the root console. type: (int, int)\"\"\"", "def onLeftDown(self, event=None): \"\"\" left button down: report x,y coords, start zooming mode\"\"\" if event is None: return self.cursor_mode_action('leftdown', event=event) self.ForwardEvent(event=event.guiEvent)"], "rank": 1}
{"nl": "get unique list from two lists python", "code": ["def uniquify_list(L): \"\"\"Same order unique list using only a list compression.\"\"\" return [e for i, e in enumerate(L) if L.index(e) == i]", "def unique(input_list): \"\"\" Return a list of unique items (similar to set functionality). Parameters ---------- input_list : list A list containg some items that can occur more than once. Returns ------- list A list with only unique occurances of an item. \"\"\" output = [] for item in input_list: if item not in output: output.append(item) return output", "def _uniquify(_list): \"\"\"Remove duplicates in a list.\"\"\" seen = set() result = [] for x in _list: if x not in seen: result.append(x) seen.add(x) return result", "def unique_list(lst): \"\"\"Make a list unique, retaining order of initial appearance.\"\"\" uniq = [] for item in lst: if item not in uniq: uniq.append(item) return uniq", "def unique(list): \"\"\" Returns a copy of the list without duplicates. \"\"\" unique = []; [unique.append(x) for x in list if x not in unique] return unique"], "rank": 4}
{"nl": "python 3 not runnning in git bash", "code": ["def check_git(): \"\"\"Check if git command is available.\"\"\" try: with open(os.devnull, \"wb\") as devnull: subprocess.check_call([\"git\", \"--version\"], stdout=devnull, stderr=devnull) except: raise RuntimeError(\"Please make sure git is installed and on your path.\")", "def is_git_repo(): \"\"\"Check whether the current folder is a Git repo.\"\"\" cmd = \"git\", \"rev-parse\", \"--git-dir\" try: subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True) return True except subprocess.CalledProcessError: return False", "def get_git_branch(git_path='git'): \"\"\"Returns the name of the current git branch \"\"\" branch_match = call((git_path, 'rev-parse', '--symbolic-full-name', 'HEAD')) if branch_match == \"HEAD\": return None else: return os.path.basename(branch_match)", "def get_current_branch(): \"\"\" Return the current branch \"\"\" cmd = [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"] output = subprocess.check_output(cmd, stderr=subprocess.STDOUT) return output.strip().decode(\"utf-8\")", "def branches(self): \"\"\"All branches in a list\"\"\" result = self.git(self.default + ['branch', '-a', '--no-color']) return [l.strip(' *\\n') for l in result.split('\\n') if l.strip(' *\\n')]"], "rank": 1}
{"nl": "create unknown number of names to print in python", "code": ["def _prtstr(self, obj, dashes): \"\"\"Print object information using a namedtuple and a format pattern.\"\"\" self.prt.write('{DASHES:{N}}'.format( DASHES=self.fmt_dashes.format(DASHES=dashes, ID=obj.item_id), N=self.dash_len)) self.prt.write(\"{INFO}\\n\".format(INFO=str(obj)))", "def _prtfmt(self, item_id, dashes): \"\"\"Print object information using a namedtuple and a format pattern.\"\"\" ntprt = self.id2nt[item_id] dct = ntprt._asdict() self.prt.write('{DASHES:{N}}'.format( DASHES=self.fmt_dashes.format(DASHES=dashes, ID=self.nm2prtfmt['ID'].format(**dct)), N=self.dash_len)) self.prt.write(\"{INFO}\\n\".format(INFO=self.nm2prtfmt['ITEM'].format(**dct)))", "def prin(*args, **kwargs): r\"\"\"Like ``print``, but a function. I.e. prints out all arguments as ``print`` would do. Specify output stream like this:: print('ERROR', `out=\"sys.stderr\"``). \"\"\" print >> kwargs.get('out',None), \" \".join([str(arg) for arg in args])", "def display_list_by_prefix(names_list, starting_spaces=0): \"\"\"Creates a help string for names_list grouped by prefix.\"\"\" cur_prefix, result_lines = None, [] space = \" \" * starting_spaces for name in sorted(names_list): split = name.split(\"_\", 1) prefix = split[0] if cur_prefix != prefix: result_lines.append(space + prefix + \":\") cur_prefix = prefix result_lines.append(space + \" * \" + name) return \"\\n\".join(result_lines)", "def safe_repr(obj): \"\"\" Try to get ``__name__`` first, ``__class__.__name__`` second and finally, if we can't get anything acceptable, fallback to user a ``repr()`` call. \"\"\" name = getattr(obj, '__name__', getattr(obj.__class__, '__name__')) if name == 'ndict': name = 'dict' return name or repr(obj)"], "rank": 63}
{"nl": "python json if element exists", "code": ["def task_property_present_predicate(service, task, prop): \"\"\" True if the json_element passed is present for the task specified. \"\"\" try: response = get_service_task(service, task) except Exception as e: pass return (response is not None) and (prop in response)", "def json_get_data(filename): \"\"\"Get data from json file \"\"\" with open(filename) as fp: json_data = json.load(fp) return json_data return False", "def is_json_file(filename, show_warnings = False): \"\"\"Check configuration file type is JSON Return a boolean indicating wheather the file is JSON format or not \"\"\" try: config_dict = load_config(filename, file_type = \"json\") is_json = True except: is_json = False return(is_json)", "def device_state(device_id): \"\"\" Get device state via HTTP GET. \"\"\" if device_id not in devices: return jsonify(success=False) return jsonify(state=devices[device_id].state)", "def exists(self): \"\"\" Checks if the item exists. \"\"\" try: return self.metadata is not None except datalab.utils.RequestException: return False except Exception as e: raise e"], "rank": 1}
{"nl": "number of unique values in list python", "code": ["def count_list(the_list): \"\"\" Generates a count of the number of times each unique item appears in a list \"\"\" count = the_list.count result = [(item, count(item)) for item in set(the_list)] result.sort() return result", "def unique(input_list): \"\"\" Return a list of unique items (similar to set functionality). Parameters ---------- input_list : list A list containg some items that can occur more than once. Returns ------- list A list with only unique occurances of an item. \"\"\" output = [] for item in input_list: if item not in output: output.append(item) return output", "def unique_element(ll): \"\"\" returns unique elements from a list preserving the original order \"\"\" seen = {} result = [] for item in ll: if item in seen: continue seen[item] = 1 result.append(item) return result", "def uniqued(iterable): \"\"\"Return unique list of ``iterable`` items preserving order. >>> uniqued('spameggs') ['s', 'p', 'a', 'm', 'e', 'g'] \"\"\" seen = set() return [item for item in iterable if item not in seen and not seen.add(item)]", "def uniquify_list(L): \"\"\"Same order unique list using only a list compression.\"\"\" return [e for i, e in enumerate(L) if L.index(e) == i]"], "rank": 1}
{"nl": "how to randomize items in a list in python", "code": ["def rand_elem(seq, n=None): \"\"\"returns a random element from seq n times. If n is None, it continues indefinitly\"\"\" return map(random.choice, repeat(seq, n) if n is not None else repeat(seq))", "def get_randomized_guid_sample(self, item_count): \"\"\" Fetch a subset of randomzied GUIDs from the whitelist \"\"\" dataset = self.get_whitelist() random.shuffle(dataset) return dataset[:item_count]", "def consistent_shuffle(*lists): \"\"\" Shuffle lists consistently. Parameters ---------- *lists Variable length number of lists Returns ------- shuffled_lists : tuple of lists All of the lists are shuffled consistently Examples -------- >>> import mpu, random; random.seed(8) >>> mpu.consistent_shuffle([1,2,3], ['a', 'b', 'c'], ['A', 'B', 'C']) ([3, 2, 1], ['c', 'b', 'a'], ['C', 'B', 'A']) \"\"\" perm = list(range(len(lists[0]))) random.shuffle(perm) lists = tuple([sublist[index] for index in perm] for sublist in lists) return lists", "def RandomShuffle(a, seed): \"\"\" Random uniform op. \"\"\" if seed: np.random.seed(seed) r = a.copy() np.random.shuffle(r) return r,", "def endless_permutations(N, random_state=None): \"\"\" Generate an endless sequence of random integers from permutations of the set [0, ..., N). If we call this N times, we will sweep through the entire set without replacement, on the (N+1)th call a new permutation will be created, etc. Parameters ---------- N: int the length of the set random_state: int or RandomState, optional random seed Yields ------ int: a random int from the set [0, ..., N) \"\"\" generator = check_random_state(random_state) while True: batch_inds = generator.permutation(N) for b in batch_inds: yield b"], "rank": 2}
{"nl": "python windows check for keypress", "code": ["def _kbhit_unix() -> bool: \"\"\" Under UNIX: is a keystroke available? \"\"\" dr, dw, de = select.select([sys.stdin], [], [], 0) return dr != []", "def read_key(suppress=False): \"\"\" Blocks until a keyboard event happens, then returns that event's name or, if missing, its scan code. \"\"\" event = read_event(suppress) return event.name or event.scan_code", "def on_key_press(self, symbol, modifiers): \"\"\" Pyglet specific key press callback. Forwards and translates the events to :py:func:`keyboard_event` \"\"\" self.keyboard_event(symbol, self.keys.ACTION_PRESS, modifiers)", "def keyPressEvent(self, event): \"\"\" Pyqt specific key press callback function. Translates and forwards events to :py:func:`keyboard_event`. \"\"\" self.keyboard_event(event.key(), self.keys.ACTION_PRESS, 0)", "def on_press_key(key, callback, suppress=False): \"\"\" Invokes `callback` for KEY_DOWN event related to the given key. For details see `hook`. \"\"\" return hook_key(key, lambda e: e.event_type == KEY_UP or callback(e), suppress=suppress)"], "rank": 1}
{"nl": "python bind scrollbar to canvas", "code": ["def set_scrollregion(self, event=None): \"\"\" Set the scroll region on the canvas\"\"\" self.canvas.configure(scrollregion=self.canvas.bbox('all'))", "def _set_scroll_v(self, *args): \"\"\"Scroll both categories Canvas and scrolling container\"\"\" self._canvas_categories.yview(*args) self._canvas_scroll.yview(*args)", "def __grid_widgets(self): \"\"\"Places all the child widgets in the appropriate positions.\"\"\" scrollbar_column = 0 if self.__compound is tk.LEFT else 2 self._canvas.grid(row=0, column=1, sticky=\"nswe\") self._scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")", "def restore_scrollbar_position(self): \"\"\"Restoring scrollbar position after main window is visible\"\"\" scrollbar_pos = self.get_option('scrollbar_position', None) if scrollbar_pos is not None: self.explorer.treewidget.set_scrollbar_position(scrollbar_pos)", "def set_scrollbars_cb(self, w, tf): \"\"\"This callback is invoked when the user checks the 'Use Scrollbars' box in the preferences pane.\"\"\" scrollbars = 'on' if tf else 'off' self.t_.set(scrollbars=scrollbars)"], "rank": 1}
{"nl": "python setlevel how to only record error", "code": ["def set_verbosity(verbosity): \"\"\"Banana banana \"\"\" Logger._verbosity = min(max(0, WARNING - verbosity), 2) debug(\"Verbosity set to %d\" % (WARNING - Logger._verbosity), 'logging')", "def logger(message, level=10): \"\"\"Handle logging.\"\"\" logging.getLogger(__name__).log(level, str(message))", "def set_log_level(logger_name: str, log_level: str, propagate: bool = False): \"\"\"Set the log level of the specified logger.\"\"\" log = logging.getLogger(logger_name) log.propagate = propagate log.setLevel(log_level)", "def log(self, level, msg=None, *args, **kwargs): \"\"\"Writes log out at any arbitray level.\"\"\" return self._log(level, msg, args, kwargs)", "def log(logger, level, message): \"\"\"Logs message to stderr if logging isn't initialized.\"\"\" if logger.parent.name != 'root': logger.log(level, message) else: print(message, file=sys.stderr)"], "rank": 12}
{"nl": "format string with *args python", "code": ["def safe_format(s, **kwargs): \"\"\" :type s str \"\"\" return string.Formatter().vformat(s, (), defaultdict(str, **kwargs))", "def _replace_variables(data, variables): \"\"\"Replace the format variables in all items of data.\"\"\" formatter = string.Formatter() return [formatter.vformat(item, [], variables) for item in data]", "def PythonPercentFormat(format_str): \"\"\"Use Python % format strings as template format specifiers.\"\"\" if format_str.startswith('printf '): fmt = format_str[len('printf '):] return lambda value: fmt % value else: return None", "def reprkwargs(kwargs, sep=', ', fmt=\"{0!s}={1!r}\"): \"\"\"Display kwargs.\"\"\" return sep.join(fmt.format(k, v) for k, v in kwargs.iteritems())", "def parse(self): \"\"\" Parses format string looking for substitutions This method is responsible for returning a list of fields (as strings) to include in all log messages. \"\"\" standard_formatters = re.compile(r'\\((.+?)\\)', re.IGNORECASE) return standard_formatters.findall(self._fmt)"], "rank": 1}
{"nl": "python get hostip from url", "code": ["def url_host(url: str) -> str: \"\"\" Parses hostname from URL. :param url: URL :return: hostname \"\"\" from urllib.parse import urlparse res = urlparse(url) return res.netloc.split(':')[0] if res.netloc else ''", "def get_own_ip(): \"\"\"Get the host's ip number. \"\"\" sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) try: sock.connect((\"8.8.8.8\", 80)) except socket.gaierror: ip_ = \"127.0.0.1\" else: ip_ = sock.getsockname()[0] finally: sock.close() return ip_", "def _get_local_ip(self): \"\"\"Try to determine the local IP address of the machine.\"\"\" try: sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM) # Use Google Public DNS server to determine own IP sock.connect(('8.8.8.8', 80)) return sock.getsockname()[0] except socket.error: try: return socket.gethostbyname(socket.gethostname()) except socket.gaierror: return '127.0.0.1' finally: sock.close()", "def get_site_name(request): \"\"\"Return the domain:port part of the URL without scheme. Eg: facebook.com, 127.0.0.1:8080, etc. \"\"\" urlparts = request.urlparts return ':'.join([urlparts.hostname, str(urlparts.port)])", "def _get_local_ip(): \"\"\" Get the local ip of this device :return: Ip of this computer :rtype: str \"\"\" return set([x[4][0] for x in socket.getaddrinfo( socket.gethostname(), 80, socket.AF_INET )]).pop()"], "rank": 1}
{"nl": "python numpy masked vailding", "code": ["def ma(self): \"\"\"Represent data as a masked array. The array is returned with column-first indexing, i.e. for a data file with columns X Y1 Y2 Y3 ... the array a will be a[0] = X, a[1] = Y1, ... . inf and nan are filtered via :func:`numpy.isfinite`. \"\"\" a = self.array return numpy.ma.MaskedArray(a, mask=numpy.logical_not(numpy.isfinite(a)))", "def asMaskedArray(self): \"\"\" Creates converts to a masked array \"\"\" return ma.masked_array(data=self.data, mask=self.mask, fill_value=self.fill_value)", "def mask_and_flatten(self): \"\"\"Return a vector of the masked data. Returns ------- np.ndarray, tuple of indices (np.ndarray), tuple of the mask shape \"\"\" self._check_for_mask() return self.get_data(smoothed=True, masked=True, safe_copy=False)[self.get_mask_indices()],\\ self.get_mask_indices(), self.mask.shape", "def maskIndex(self): \"\"\" Returns a boolean index with True if the value is masked. Always has the same shape as the maksedArray.data, event if the mask is a single boolan. \"\"\" if isinstance(self.mask, bool): return np.full(self.data.shape, self.mask, dtype=np.bool) else: return self.mask", "def _join_masks_from_masked_array(data): \"\"\"Union of masks.\"\"\" if not isinstance(data.mask, np.ndarray): # workaround to handle mask compressed to single value mask = np.empty(data.data.shape, dtype=np.bool) mask.fill(data.mask) return mask mask = data.mask[0].copy() for i in range(1, len(data.mask)): mask = np.logical_or(mask, data.mask[i]) return mask[np.newaxis, :, :]"], "rank": 2}
{"nl": "python child widget close signal", "code": ["def closeEvent(self, e): \"\"\"Qt slot when the window is closed.\"\"\" self.emit('close_widget') super(DockWidget, self).closeEvent(e)", "def closeEvent(self, event): \"\"\" Called when closing this window. \"\"\" logger.debug(\"closeEvent\") self.argosApplication.saveSettingsIfNeeded() self.finalize() self.argosApplication.removeMainWindow(self) event.accept() logger.debug(\"closeEvent accepted\")", "def closeEvent(self, e): \"\"\"Qt slot when the window is closed.\"\"\" if self._closed: return res = self.emit('close') # Discard the close event if False is returned by one of the callback # functions. if False in res: # pragma: no cover e.ignore() return super(GUI, self).closeEvent(e) self._closed = True", "def on_close(self, evt): \"\"\" Pop-up menu and wx.EVT_CLOSE closing event \"\"\" self.stop() # DoseWatcher if evt.EventObject is not self: # Avoid deadlocks self.Close() # wx.Frame evt.Skip()", "def closeEvent(self, event): \"\"\"closeEvent reimplementation\"\"\" if self.closing(True): event.accept() else: event.ignore()"], "rank": 1}
{"nl": "python read from csv into numpy array", "code": ["def csv_to_numpy(string_like, dtype=None): # type: (str) -> np.array \"\"\"Convert a CSV object to a numpy array. Args: string_like (str): CSV string. dtype (dtype, optional): Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually. This argument can only be used to 'upcast' the array. For downcasting, use the .astype(t) method. Returns: (np.array): numpy array \"\"\" stream = StringIO(string_like) return np.genfromtxt(stream, dtype=dtype, delimiter=',')", "def load_data(filename): \"\"\" :rtype : numpy matrix \"\"\" data = pandas.read_csv(filename, header=None, delimiter='\\t', skiprows=9) return data.as_matrix()", "def csv_to_dicts(file, header=None): \"\"\"Reads a csv and returns a List of Dicts with keys given by header row.\"\"\" with open(file) as csvfile: return [row for row in csv.DictReader(csvfile, fieldnames=header)]", "def _openResources(self): \"\"\" Uses numpy.load to open the underlying file \"\"\" arr = np.load(self._fileName, allow_pickle=ALLOW_PICKLE) check_is_an_array(arr) self._array = arr", "def convert_array(array): \"\"\" Converts an ARRAY string stored in the database back into a Numpy array. Parameters ---------- array: ARRAY The array object to be converted back into a Numpy array. Returns ------- array The converted Numpy array. \"\"\" out = io.BytesIO(array) out.seek(0) return np.load(out)"], "rank": 1}
{"nl": "cast string to bytearray python", "code": ["def to_bytes(data: Any) -> bytearray: \"\"\" Convert anything to a ``bytearray``. See - http://stackoverflow.com/questions/7585435/best-way-to-convert-string-to-bytes-in-python-3 - http://stackoverflow.com/questions/10459067/how-to-convert-my-bytearrayb-x9e-x18k-x9a-to-something-like-this-x9e-x1 \"\"\" # noqa if isinstance(data, int): return bytearray([data]) return bytearray(data, encoding='latin-1')", "def strtobytes(input, encoding): \"\"\"Take a str and transform it into a byte array.\"\"\" py_version = sys.version_info[0] if py_version >= 3: return _strtobytes_py3(input, encoding) return _strtobytes_py2(input, encoding)", "def str2bytes(x): \"\"\"Convert input argument to bytes\"\"\" if type(x) is bytes: return x elif type(x) is str: return bytes([ ord(i) for i in x ]) else: return str2bytes(str(x))", "def u2b(string): \"\"\" unicode to bytes\"\"\" if ((PY2 and isinstance(string, unicode)) or ((not PY2) and isinstance(string, str))): return string.encode('utf-8') return string", "def s2b(s): \"\"\" String to binary. \"\"\" ret = [] for c in s: ret.append(bin(ord(c))[2:].zfill(8)) return \"\".join(ret)"], "rank": 1}
{"nl": "load str into python object", "code": ["def loads(s, model=None, parser=None): \"\"\"Deserialize s (a str) to a Python object.\"\"\" with StringIO(s) as f: return load(f, model=model, parser=parser)", "def from_json_str(cls, json_str): \"\"\"Convert json string representation into class instance. Args: json_str: json representation as string. Returns: New instance of the class with data loaded from json string. \"\"\" return cls.from_json(json.loads(json_str, cls=JsonDecoder))", "def loads(cls, s): \"\"\" Load an instance of this class from YAML. \"\"\" with closing(StringIO(s)) as fileobj: return cls.load(fileobj)", "def from_json(cls, json_str): \"\"\"Deserialize the object from a JSON string.\"\"\" d = json.loads(json_str) return cls.from_dict(d)", "def loads(string): \"\"\" Deserializes Java objects and primitive data serialized by ObjectOutputStream from a string. \"\"\" f = StringIO.StringIO(string) marshaller = JavaObjectUnmarshaller(f) marshaller.add_transformer(DefaultObjectTransformer()) return marshaller.readObject()"], "rank": 1}
{"nl": "python ldap get all groups a user belongs to", "code": ["def get_groups(self, username): \"\"\"Get all groups of a user\"\"\" username = ldap.filter.escape_filter_chars(self._byte_p2(username)) userdn = self._get_user(username, NO_ATTR) searchfilter = self.group_filter_tmpl % { 'userdn': userdn, 'username': username } groups = self._search(searchfilter, NO_ATTR, self.groupdn) ret = [] for entry in groups: ret.append(self._uni(entry[0])) return ret", "def members(self, uid=\"*\", objects=False): \"\"\" members() issues an ldap query for all users, and returns a dict for each matching entry. This can be quite slow, and takes roughly 3s to complete. You may optionally restrict the scope by specifying a uid, which is roughly equivalent to a search(uid='foo') \"\"\" entries = self.search(uid='*') if objects: return self.memberObjects(entries) result = [] for entry in entries: result.append(entry[1]) return result", "def search(self, filterstr, attrlist): \"\"\"Query the configured LDAP server.\"\"\" return self._paged_search_ext_s(self.settings.BASE, ldap.SCOPE_SUBTREE, filterstr=filterstr, attrlist=attrlist, page_size=self.settings.PAGE_SIZE)", "def query(self, base, filterstr, attrlist=None): \"\"\" wrapper to search_s \"\"\" return self.conn.search_s(base, ldap.SCOPE_SUBTREE, filterstr, attrlist)", "def _Members(self, group): \"\"\"Unify members of a group and accounts with the group as primary gid.\"\"\" group.members = set(group.members).union(self.gids.get(group.gid, [])) return group"], "rank": 1}
{"nl": "python check type if string", "code": ["def is_unicode(string): \"\"\"Validates that the object itself is some kinda string\"\"\" str_type = str(type(string)) if str_type.find('str') > 0 or str_type.find('unicode') > 0: return True return False", "def is_string(val): \"\"\"Determines whether the passed value is a string, safe for 2/3.\"\"\" try: basestring except NameError: return isinstance(val, str) return isinstance(val, basestring)", "def isString(s): \"\"\"Convenience method that works with all 2.x versions of Python to determine whether or not something is stringlike.\"\"\" try: return isinstance(s, unicode) or isinstance(s, basestring) except NameError: return isinstance(s, str)", "def is_string(obj): \"\"\"Is this a string. :param object obj: :rtype: bool \"\"\" if PYTHON3: str_type = (bytes, str) else: str_type = (bytes, str, unicode) return isinstance(obj, str_type)", "def isstring(value): \"\"\"Report whether the given value is a byte or unicode string.\"\"\" classes = (str, bytes) if pyutils.PY3 else basestring # noqa: F821 return isinstance(value, classes)"], "rank": 1}
{"nl": "python get current globals", "code": ["def caller_locals(): \"\"\"Get the local variables in the caller's frame.\"\"\" import inspect frame = inspect.currentframe() try: return frame.f_back.f_back.f_locals finally: del frame", "def get_frame_locals(stepback=0): \"\"\"Returns locals dictionary from a given frame. :param int stepback: :rtype: dict \"\"\" with Frame(stepback=stepback) as frame: locals_dict = frame.f_locals return locals_dict", "def getvariable(name): \"\"\"Get the value of a local variable somewhere in the call stack.\"\"\" import inspect fr = inspect.currentframe() try: while fr: fr = fr.f_back vars = fr.f_locals if name in vars: return vars[name] except: pass return None", "def functions(self): \"\"\" A list of functions declared or defined in this module. \"\"\" return [v for v in self.globals.values() if isinstance(v, values.Function)]", "def exec_function(ast, globals_map): \"\"\"Execute a python code object in the given environment. Args: globals_map: Dictionary to use as the globals context. Returns: locals_map: Dictionary of locals from the environment after execution. \"\"\" locals_map = globals_map exec ast in globals_map, locals_map return locals_map"], "rank": 1}
{"nl": "get child loggers python", "code": ["def _get_loggers(): \"\"\"Return list of Logger classes.\"\"\" from .. import loader modules = loader.get_package_modules('logger') return list(loader.get_plugins(modules, [_Logger]))", "def __getLogger(cls): \"\"\" Get the logger for this object. :returns: (Logger) A Logger object. \"\"\" if cls.__logger is None: cls.__logger = opf_utils.initLogger(cls) return cls.__logger", "def find_console_handler(logger): \"\"\"Return a stream handler, if it exists.\"\"\" for handler in logger.handlers: if (isinstance(handler, logging.StreamHandler) and handler.stream == sys.stderr): return handler", "def register_logging_factories(loader): \"\"\" Registers default factories for logging standard package. :param loader: Loader where you want register default logging factories \"\"\" loader.register_factory(logging.Logger, LoggerFactory) loader.register_factory(logging.Handler, LoggingHandlerFactory)", "def calling_logger(height=1): \"\"\" Obtain a logger for the calling module. Uses the inspect module to find the name of the calling function and its position in the module hierarchy. With the optional height argument, logs for caller's caller, and so forth. see: http://stackoverflow.com/a/900404/48251 \"\"\" stack = inspect.stack() height = min(len(stack) - 1, height) caller = stack[height] scope = caller[0].f_globals path = scope['__name__'] if path == '__main__': path = scope['__package__'] or os.path.basename(sys.argv[0]) return logging.getLogger(path)"], "rank": 1}
{"nl": "how to know if a text file is empty in python", "code": ["def file_empty(fp): \"\"\"Determine if a file is empty or not.\"\"\" # for python 2 we need to use a homemade peek() if six.PY2: contents = fp.read() fp.seek(0) return not bool(contents) else: return not fp.peek()", "def _cnx_is_empty(in_file): \"\"\"Check if cnr or cns files are empty (only have a header) \"\"\" with open(in_file) as in_handle: for i, line in enumerate(in_handle): if i > 0: return False return True", "def isemptyfile(filepath): \"\"\"Determine if the file both exists and isempty Args: filepath (str, path): file path Returns: bool \"\"\" exists = os.path.exists(safepath(filepath)) if exists: filesize = os.path.getsize(safepath(filepath)) return filesize == 0 else: return False", "def isfile_notempty(inputfile: str) -> bool: \"\"\"Check if the input filename with path is a file and is not empty.\"\"\" try: return isfile(inputfile) and getsize(inputfile) > 0 except TypeError: raise TypeError('inputfile is not a valid type')", "def file_exists(fname): \"\"\"Check if a file exists and is non-empty. \"\"\" try: return fname and os.path.exists(fname) and os.path.getsize(fname) > 0 except OSError: return False"], "rank": 2}
{"nl": "python normal distribution p values", "code": ["def pdf(x, mu, std): \"\"\"Probability density function (normal distribution)\"\"\" return (1.0 / (std * sqrt(2 * pi))) * np.exp(-(x - mu) ** 2 / (2 * std ** 2))", "def EvalGaussianPdf(x, mu, sigma): \"\"\"Computes the unnormalized PDF of the normal distribution. x: value mu: mean sigma: standard deviation returns: float probability density \"\"\" return scipy.stats.norm.pdf(x, mu, sigma)", "def norm(x, mu, sigma=1.0): \"\"\" Scipy norm function \"\"\" return stats.norm(loc=mu, scale=sigma).pdf(x)", "def lognorm(x, mu, sigma=1.0): \"\"\" Log-normal function from scipy \"\"\" return stats.lognorm(sigma, scale=mu).pdf(x)", "def normal_log_q(self,z): \"\"\" The unnormalized log posterior components for mean-field normal family (the quantity we want to approximate) RAO-BLACKWELLIZED! \"\"\" means, scale = self.get_means_and_scales() return ss.norm.logpdf(z,loc=means,scale=scale)"], "rank": 2}
{"nl": "python ctypes array of arrays", "code": ["def cfloat64_array_to_numpy(cptr, length): \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_double)): return np.fromiter(cptr, dtype=np.float64, count=length) else: raise RuntimeError('Expected double pointer')", "def c_array(ctype, values): \"\"\"Convert a python string to c array.\"\"\" if isinstance(values, np.ndarray) and values.dtype.itemsize == ctypes.sizeof(ctype): return (ctype * len(values)).from_buffer_copy(values) return (ctype * len(values))(*values)", "def cint32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)): return np.fromiter(cptr, dtype=np.int32, count=length) else: raise RuntimeError('Expected int pointer')", "def cint8_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)): return np.fromiter(cptr, dtype=np.int8, count=length) else: raise RuntimeError('Expected int pointer')", "def cfloat32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_float)): return np.fromiter(cptr, dtype=np.float32, count=length) else: raise RuntimeError('Expected float pointer')"], "rank": 10}
{"nl": "scale 1d array python to between 0 and 1", "code": ["def _rescale_array(self, array, scale, zero): \"\"\" Scale the input array \"\"\" if scale != 1.0: sval = numpy.array(scale, dtype=array.dtype) array *= sval if zero != 0.0: zval = numpy.array(zero, dtype=array.dtype) array += zval", "def _normalize(mat: np.ndarray): \"\"\"rescales a numpy array, so that min is 0 and max is 255\"\"\" return ((mat - mat.min()) * (255 / mat.max())).astype(np.uint8)", "def set_scale(self, scale, no_reset=False): \"\"\"Scale the image in a channel. Also see :meth:`zoom_to`. Parameters ---------- scale : tuple of float Scaling factors for the image in the X and Y axes. no_reset : bool Do not reset ``autozoom`` setting. \"\"\" return self.scale_to(*scale[:2], no_reset=no_reset)", "def scale_dtype(arr, dtype): \"\"\"Convert an array from 0..1 to dtype, scaling up linearly \"\"\" max_int = np.iinfo(dtype).max return (arr * max_int).astype(dtype)", "def _scale_shape(dshape, scale = (1,1,1)): \"\"\"returns the shape after scaling (should be the same as ndimage.zoom\"\"\" nshape = np.round(np.array(dshape) * np.array(scale)) return tuple(nshape.astype(np.int))"], "rank": 1}
{"nl": "python upper case lower case converter", "code": ["def camel_to_(s): \"\"\" Convert CamelCase to camel_case \"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', s) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def to_snake_case(name): \"\"\" Given a name in camelCase return in snake_case \"\"\" s1 = FIRST_CAP_REGEX.sub(r'\\1_\\2', name) return ALL_CAP_REGEX.sub(r'\\1_\\2', s1).lower()", "def upcaseTokens(s,l,t): \"\"\"Helper parse action to convert tokens to upper case.\"\"\" return [ tt.upper() for tt in map(_ustr,t) ]", "def convert_camel_case_to_snake_case(name): \"\"\"Convert CamelCase to snake_case.\"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def camel_to_snake_case(name): \"\"\"Takes a camelCased string and converts to snake_case.\"\"\" pattern = r'[A-Z][a-z]+|[A-Z]+(?![a-z])' return '_'.join(map(str.lower, re.findall(pattern, name)))"], "rank": 3}
{"nl": "ply python expression evaluator", "code": ["def upoint2exprpoint(upoint): \"\"\"Convert an untyped point into an Expression point. .. seealso:: For definitions of points and untyped points, see the :mod:`pyeda.boolalg.boolfunc` module. \"\"\" point = dict() for uniqid in upoint[0]: point[_LITS[uniqid]] = 0 for uniqid in upoint[1]: point[_LITS[uniqid]] = 1 return point", "def get_value(self, context): \"\"\"Run python eval on the input string.\"\"\" if self.value: return expressions.eval_string(self.value, context) else: # Empty input raises cryptic EOF syntax err, this more human # friendly raise ValueError('!py string expression is empty. It must be a ' 'valid python expression instead.')", "def p_postfix_expr(self, p): \"\"\"postfix_expr : left_hand_side_expr | left_hand_side_expr PLUSPLUS | left_hand_side_expr MINUSMINUS \"\"\" if len(p) == 2: p[0] = p[1] else: p[0] = ast.UnaryOp(op=p[2], value=p[1], postfix=True)", "def print_latex(o): \"\"\"A function to generate the latex representation of sympy expressions.\"\"\" if can_print_latex(o): s = latex(o, mode='plain') s = s.replace('\\\\dag','\\\\dagger') s = s.strip('$') return '$$%s$$' % s # Fallback to the string printer return None", "def xpathEvalExpression(self, str): \"\"\"Evaluate the XPath expression in the given context. \"\"\" ret = libxml2mod.xmlXPathEvalExpression(str, self._o) if ret is None:raise xpathError('xmlXPathEvalExpression() failed') return xpathObjectRet(ret)"], "rank": 34}
{"nl": "python hashlib calc sha1 of file", "code": ["def _sha1_for_file(filename): \"\"\"Return sha1 for contents of filename.\"\"\" with open(filename, \"rb\") as fileobj: contents = fileobj.read() return hashlib.sha1(contents).hexdigest()", "def generate_hash(filepath): \"\"\"Public function that reads a local file and generates a SHA256 hash digest for it\"\"\" fr = FileReader(filepath) data = fr.read_bin() return _calculate_sha256(data)", "def _get_file_sha1(file): \"\"\"Return the SHA1 hash of the given a file-like object as ``file``. This will seek the file back to 0 when it's finished. \"\"\" bits = file.read() file.seek(0) h = hashlib.new('sha1', bits).hexdigest() return h", "def checksum(path): \"\"\"Calculcate checksum for a file.\"\"\" hasher = hashlib.sha1() with open(path, 'rb') as stream: buf = stream.read(BLOCKSIZE) while len(buf) > 0: hasher.update(buf) buf = stream.read(BLOCKSIZE) return hasher.hexdigest()", "def sha1(s): \"\"\" Returns a sha1 of the given string \"\"\" h = hashlib.new('sha1') h.update(s) return h.hexdigest()"], "rank": 3}
{"nl": "python pdb no capture std output", "code": ["def set_trace(): \"\"\"Start a Pdb instance at the calling frame, with stdout routed to sys.__stdout__.\"\"\" # https://github.com/nose-devs/nose/blob/master/nose/tools/nontrivial.py pdb.Pdb(stdout=sys.__stdout__).set_trace(sys._getframe().f_back)", "def debug_on_error(type, value, tb): \"\"\"Code due to Thomas Heller - published in Python Cookbook (O'Reilley)\"\"\" traceback.print_exc(type, value, tb) print() pdb.pm()", "def suppress_stdout(): \"\"\" Context manager that suppresses stdout. Examples: >>> with suppress_stdout(): ... print('Test print') >>> print('test') test \"\"\" save_stdout = sys.stdout sys.stdout = DevNull() yield sys.stdout = save_stdout", "def disable_stdout_buffering(): \"\"\"This turns off stdout buffering so that outputs are immediately materialized and log messages show up before the program exits\"\"\" stdout_orig = sys.stdout sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0) # NOTE(brandyn): This removes the original stdout return stdout_orig", "def stdout_display(): \"\"\" Print results straight to stdout \"\"\" if sys.version_info[0] == 2: yield SmartBuffer(sys.stdout) else: yield SmartBuffer(sys.stdout.buffer)"], "rank": 1}
{"nl": "python how to write a factorial", "code": ["def factorial(n, mod=None): \"\"\"Calculates factorial iteratively. If mod is not None, then return (n! % mod) Time Complexity - O(n)\"\"\" if not (isinstance(n, int) and n >= 0): raise ValueError(\"'n' must be a non-negative integer.\") if mod is not None and not (isinstance(mod, int) and mod > 0): raise ValueError(\"'mod' must be a positive integer\") result = 1 if n == 0: return 1 for i in range(2, n+1): result *= i if mod: result %= mod return result", "def computeFactorial(n): \"\"\" computes factorial of n \"\"\" sleep_walk(10) ret = 1 for i in range(n): ret = ret * (i + 1) return ret", "def factors(n): \"\"\" Computes all the integer factors of the number `n` Example: >>> # ENABLE_DOCTEST >>> from utool.util_alg import * # NOQA >>> import utool as ut >>> result = sorted(ut.factors(10)) >>> print(result) [1, 2, 5, 10] References: http://stackoverflow.com/questions/6800193/finding-all-the-factors \"\"\" return set(reduce(list.__add__, ([i, n // i] for i in range(1, int(n ** 0.5) + 1) if n % i == 0)))", "def _factor_generator(n): \"\"\" From a given natural integer, returns the prime factors and their multiplicity :param n: Natural integer :return: \"\"\" p = prime_factors(n) factors = {} for p1 in p: try: factors[p1] += 1 except KeyError: factors[p1] = 1 return factors", "def Exponential(x, a, tau, y0): \"\"\"Exponential function Inputs: ------- ``x``: independent variable ``a``: scaling factor ``tau``: time constant ``y0``: additive constant Formula: -------- ``a*exp(x/tau)+y0`` \"\"\" return np.exp(x / tau) * a + y0"], "rank": 1}
{"nl": "python get the last column", "code": ["def get_last(self, table=None): \"\"\"Just the last entry.\"\"\" if table is None: table = self.main_table query = 'SELECT * FROM \"%s\" ORDER BY ROWID DESC LIMIT 1;' % table return self.own_cursor.execute(query).fetchone()", "def get_last_row(dbconn, tablename, n=1, uuid=None): \"\"\" Returns the last `n` rows in the table \"\"\" return fetch(dbconn, tablename, n, uuid, end=True)", "def other_ind(self): \"\"\"last row or column of square A\"\"\" return np.full(self.n_min, self.size - 1, dtype=np.int)", "def get_last_id(self, cur, table='reaction'): \"\"\" Get the id of the last written row in table Parameters ---------- cur: database connection().cursor() object table: str 'reaction', 'publication', 'publication_system', 'reaction_system' Returns: id \"\"\" cur.execute(\"SELECT seq FROM sqlite_sequence WHERE name='{0}'\" .format(table)) result = cur.fetchone() if result is not None: id = result[0] else: id = 0 return id", "def _last_index(x, default_dim): \"\"\"Returns the last dimension's index or default_dim if x has no shape.\"\"\" if x.get_shape().ndims is not None: return len(x.get_shape()) - 1 else: return default_dim"], "rank": 2}
{"nl": "python how to change file extension", "code": ["def lower_ext(abspath): \"\"\"Convert file extension to lowercase. \"\"\" fname, ext = os.path.splitext(abspath) return fname + ext.lower()", "def remove_ext(fname): \"\"\"Removes the extension from a filename \"\"\" bn = os.path.basename(fname) return os.path.splitext(bn)[0]", "def add_suffix(fullname, suffix): \"\"\" Add suffix to a full file name\"\"\" name, ext = os.path.splitext(fullname) return name + '_' + suffix + ext", "def infer_format(filename:str) -> str: \"\"\"Return extension identifying format of given filename\"\"\" _, ext = os.path.splitext(filename) return ext", "def filename_addstring(filename, text): \"\"\" Add `text` to filename, keeping the extension in place For example when adding a timestamp to the filename \"\"\" fn, ext = os.path.splitext(filename) return fn + text + ext"], "rank": 1}
{"nl": "python get process memory info", "code": ["def machine_info(): \"\"\"Retrieve core and memory information for the current machine. \"\"\" import psutil BYTES_IN_GIG = 1073741824.0 free_bytes = psutil.virtual_memory().total return [{\"memory\": float(\"%.1f\" % (free_bytes / BYTES_IN_GIG)), \"cores\": multiprocessing.cpu_count(), \"name\": socket.gethostname()}]", "def get_memory_usage(): \"\"\"Gets RAM memory usage :return: MB of memory used by this process \"\"\" process = psutil.Process(os.getpid()) mem = process.memory_info().rss return mem / (1024 * 1024)", "def memory(): \"\"\"Determine memory specifications of the machine. Returns ------- mem_info : dictonary Holds the current values for the total, free and used memory of the system. \"\"\" mem_info = dict() for k, v in psutil.virtual_memory()._asdict().items(): mem_info[k] = int(v) return mem_info", "def current_memory_usage(): \"\"\" Returns this programs current memory usage in bytes \"\"\" import psutil proc = psutil.Process(os.getpid()) #meminfo = proc.get_memory_info() meminfo = proc.memory_info() rss = meminfo[0] # Resident Set Size / Mem Usage vms = meminfo[1] # Virtual Memory Size / VM Size # NOQA return rss", "def memory_usage(): \"\"\"return memory usage of python process in MB from http://fa.bianp.net/blog/2013/different-ways-to-get-memory-consumption-or-lessons-learned-from-memory_profiler/ psutil is quicker >>> isinstance(memory_usage(),float) True \"\"\" try: import psutil import os except ImportError: return _memory_usage_ps() process = psutil.Process(os.getpid()) mem = process.memory_info()[0] / float(2 ** 20) return mem"], "rank": 1}
{"nl": "check if 2 string are equal python", "code": ["def eqstr(a, b): \"\"\" Determine whether two strings are equivalent. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/eqstr_c.html :param a: Arbitrary character string. :type a: str :param b: Arbitrary character string. :type b: str :return: True if A and B are equivalent. :rtype: bool \"\"\" return bool(libspice.eqstr_c(stypes.stringToCharP(a), stypes.stringToCharP(b)))", "def compare(string1, string2): \"\"\"Compare two strings while protecting against timing attacks :param str string1: the first string :param str string2: the second string :returns: True if the strings are equal, False if not :rtype: :obj:`bool` \"\"\" if len(string1) != len(string2): return False result = True for c1, c2 in izip(string1, string2): result &= c1 == c2 return result", "def indexes_equal(a: Index, b: Index) -> bool: \"\"\" Are two indexes equal? Checks by comparing ``str()`` versions of them. (AM UNSURE IF THIS IS ENOUGH.) \"\"\" return str(a) == str(b)", "def compare(self, first, second): \"\"\" Case in-sensitive comparison of two strings. Required arguments: * first - The first string to compare. * second - The second string to compare. \"\"\" if first.lower() == second.lower(): return True else: return False", "def is_equal_strings_ignore_case(first, second): \"\"\"The function compares strings ignoring case\"\"\" if first and second: return first.upper() == second.upper() else: return not (first or second)"], "rank": 3}
{"nl": "flask python create one table sqlalchemy", "code": ["def create_db(app, appbuilder): \"\"\" Create all your database objects (SQLAlchemy specific). \"\"\" from flask_appbuilder.models.sqla import Base _appbuilder = import_application(app, appbuilder) engine = _appbuilder.get_session.get_bind(mapper=None, clause=None) Base.metadata.create_all(engine) click.echo(click.style(\"DB objects created\", fg=\"green\"))", "def createdb(): \"\"\"Create database tables from sqlalchemy models\"\"\" manager.db.engine.echo = True manager.db.create_all() set_alembic_revision()", "def create_all(self, check_first: bool = True): \"\"\"Create the empty database (tables). :param bool check_first: Defaults to True, don't issue CREATEs for tables already present in the target database. Defers to :meth:`sqlalchemy.sql.schema.MetaData.create_all` \"\"\" self._metadata.create_all(self.engine, checkfirst=check_first)", "def init_db(): \"\"\" Drops and re-creates the SQL schema \"\"\" db.drop_all() db.configure_mappers() db.create_all() db.session.commit()", "def _store_helper(model: Action, session: Optional[Session] = None) -> None: \"\"\"Help store an action.\"\"\" if session is None: session = _make_session() session.add(model) session.commit() session.close()"], "rank": 1}
{"nl": "clean output folder in python", "code": ["def cleanup(): \"\"\"Cleanup the output directory\"\"\" if _output_dir and os.path.exists(_output_dir): log.msg_warn(\"Cleaning up output directory at '{output_dir}' ...\" .format(output_dir=_output_dir)) if not _dry_run: shutil.rmtree(_output_dir)", "def cli(ctx, project_dir): \"\"\"Clean the previous generated files.\"\"\" exit_code = SCons(project_dir).clean() ctx.exit(exit_code)", "def delete(build_folder): \"\"\"Delete build directory and all its contents. \"\"\" if _meta_.del_build in [\"on\", \"ON\"] and os.path.exists(build_folder): shutil.rmtree(build_folder)", "def _cleanup(path: str) -> None: \"\"\"Cleanup temporary directory.\"\"\" if os.path.isdir(path): shutil.rmtree(path)", "def remove_examples_all(): \"\"\"remove arduino/examples/all directory. :rtype: None \"\"\" d = examples_all_dir() if d.exists(): log.debug('remove %s', d) d.rmtree() else: log.debug('nothing to remove: %s', d)"], "rank": 1}
{"nl": "python print nodes binary tree", "code": ["def print_bintree(tree, indent=' '): \"\"\"print a binary tree\"\"\" for n in sorted(tree.keys()): print \"%s%s\" % (indent * depth(n,tree), n)", "def debugTreePrint(node,pfx=\"->\"): \"\"\"Purely a debugging aid: Ascii-art picture of a tree descended from node\"\"\" print pfx,node.item for c in node.children: debugTreePrint(c,\" \"+pfx)", "def print_tree(self, indent=2): \"\"\" print_tree: prints out structure of tree Args: indent (int): What level of indentation at which to start printing Returns: None \"\"\" config.LOGGER.info(\"{indent}{data}\".format(indent=\" \" * indent, data=str(self))) for child in self.children: child.print_tree(indent + 1)", "def make_bintree(levels): \"\"\"Make a symmetrical binary tree with @levels\"\"\" G = nx.DiGraph() root = '0' G.add_node(root) add_children(G, root, levels, 2) return G", "def pprint(self, ind): \"\"\"pretty prints the tree with indentation\"\"\" pp = pprint.PrettyPrinter(indent=ind) pp.pprint(self.tree)"], "rank": 2}
{"nl": "python query string parsing", "code": ["def parse_query_string(query): \"\"\" parse_query_string: very simplistic. won't do the right thing with list values \"\"\" result = {} qparts = query.split('&') for item in qparts: key, value = item.split('=') key = key.strip() value = value.strip() result[key] = unquote_plus(value) return result", "def urlencoded(body, charset='ascii', **kwargs): \"\"\"Converts query strings into native Python objects\"\"\" return parse_query_string(text(body, charset=charset), False)", "def get_querystring(uri): \"\"\"Get Querystring information from uri. :param uri: uri :return: querystring info or {} \"\"\" parts = urlparse.urlsplit(uri) return urlparse.parse_qs(parts.query)", "def filter_query_string(query): \"\"\" Return a version of the query string with the _e, _k and _s values removed. \"\"\" return '&'.join([q for q in query.split('&') if not (q.startswith('_k=') or q.startswith('_e=') or q.startswith('_s'))])", "def parse_querystring(self, req, name, field): \"\"\"Pull a querystring value from the request.\"\"\" return core.get_value(req.args, name, field)"], "rank": 2}
{"nl": "cycle through a folder of images python", "code": ["def each_img(dir_path): \"\"\" Iterates through each image in the given directory. (not recursive) :param dir_path: Directory path where images files are present :return: Iterator to iterate through image files \"\"\" for fname in os.listdir(dir_path): if fname.endswith('.jpg') or fname.endswith('.png') or fname.endswith('.bmp'): yield fname", "def each_img(img_dir): \"\"\" Reads and iterates through each image file in the given directory \"\"\" for fname in utils.each_img(img_dir): fname = os.path.join(img_dir, fname) yield cv.imread(fname), fname", "def get_all_files(folder): \"\"\" Generator that loops through all absolute paths of the files within folder Parameters ---------- folder: str Root folder start point for recursive search. Yields ------ fpath: str Absolute path of one file in the folders \"\"\" for path, dirlist, filelist in os.walk(folder): for fn in filelist: yield op.join(path, fn)", "def listfolderpath(p): \"\"\" generator of list folder in the path. folders only \"\"\" for entry in scandir.scandir(p): if entry.is_dir(): yield entry.path", "def get_files(dir_name): \"\"\"Simple directory walker\"\"\" return [(os.path.join('.', d), [os.path.join(d, f) for f in files]) for d, _, files in os.walk(dir_name)]"], "rank": 1}
{"nl": "check if a date is valid python", "code": ["def valid_date(x: str) -> bool: \"\"\" Retrun ``True`` if ``x`` is a valid YYYYMMDD date; otherwise return ``False``. \"\"\" try: if x != dt.datetime.strptime(x, DATE_FORMAT).strftime(DATE_FORMAT): raise ValueError return True except ValueError: return False", "def is_date(thing): \"\"\"Checks if the given thing represents a date :param thing: The object to check if it is a date :type thing: arbitrary object :returns: True if we have a date object :rtype: bool \"\"\" # known date types date_types = (datetime.datetime, datetime.date, DateTime) return isinstance(thing, date_types)", "def datetime_is_iso(date_str): \"\"\"Attempts to parse a date formatted in ISO 8601 format\"\"\" try: if len(date_str) > 10: dt = isodate.parse_datetime(date_str) else: dt = isodate.parse_date(date_str) return True, [] except: # Any error qualifies as not ISO format return False, ['Datetime provided is not in a valid ISO 8601 format']", "def _validate_date_str(str_): \"\"\"Validate str as a date and return string version of date\"\"\" if not str_: return None # Convert to datetime so we can validate it's a real date that exists then # convert it back to the string. try: date = datetime.strptime(str_, DATE_FMT) except ValueError: msg = 'Invalid date format, should be YYYY-MM-DD' raise argparse.ArgumentTypeError(msg) return date.strftime(DATE_FMT)", "def is_date_type(cls): \"\"\"Return True if the class is a date type.\"\"\" if not isinstance(cls, type): return False return issubclass(cls, date) and not issubclass(cls, datetime)"], "rank": 1}
{"nl": "python spherical bessel functions", "code": ["def sbessely(x, N): \"\"\"Returns a vector of spherical bessel functions yn: x: The argument. N: values of n will run from 0 to N-1. \"\"\" out = np.zeros(N, dtype=np.float64) out[0] = -np.cos(x) / x out[1] = -np.cos(x) / (x ** 2) - np.sin(x) / x for n in xrange(2, N): out[n] = ((2.0 * n - 1.0) / x) * out[n - 1] - out[n - 2] return out", "def Fsphere(q, R): \"\"\"Scattering form-factor amplitude of a sphere normalized to F(q=0)=V Inputs: ------- ``q``: independent variable ``R``: sphere radius Formula: -------- ``4*pi/q^3 * (sin(qR) - qR*cos(qR))`` \"\"\" return 4 * np.pi / q ** 3 * (np.sin(q * R) - q * R * np.cos(q * R))", "def algo_exp(x, m, t, b): \"\"\"mono-exponential curve.\"\"\" return m*np.exp(-t*x)+b", "def _hess_two_param(self, funct, p0, p1, dl=2e-5, rts=False, **kwargs): \"\"\" Hessian of `func` wrt two parameters `p0` and `p1`. (see _graddoc) \"\"\" vals0 = self.get_values(p0) vals1 = self.get_values(p1) f00 = funct(**kwargs) self.update(p0, vals0+dl) f10 = funct(**kwargs) self.update(p1, vals1+dl) f11 = funct(**kwargs) self.update(p0, vals0) f01 = funct(**kwargs) if rts: self.update(p0, vals0) self.update(p1, vals1) return (f11 - f10 - f01 + f00) / (dl**2)", "def asin(x): \"\"\" Inverse sine \"\"\" if isinstance(x, UncertainFunction): mcpts = np.arcsin(x._mcpts) return UncertainFunction(mcpts) else: return np.arcsin(x)"], "rank": 1}
{"nl": "how to check a file is empty in python", "code": ["def file_empty(fp): \"\"\"Determine if a file is empty or not.\"\"\" # for python 2 we need to use a homemade peek() if six.PY2: contents = fp.read() fp.seek(0) return not bool(contents) else: return not fp.peek()", "def isemptyfile(filepath): \"\"\"Determine if the file both exists and isempty Args: filepath (str, path): file path Returns: bool \"\"\" exists = os.path.exists(safepath(filepath)) if exists: filesize = os.path.getsize(safepath(filepath)) return filesize == 0 else: return False", "def isfile_notempty(inputfile: str) -> bool: \"\"\"Check if the input filename with path is a file and is not empty.\"\"\" try: return isfile(inputfile) and getsize(inputfile) > 0 except TypeError: raise TypeError('inputfile is not a valid type')", "def file_exists(fname): \"\"\"Check if a file exists and is non-empty. \"\"\" try: return fname and os.path.exists(fname) and os.path.getsize(fname) > 0 except OSError: return False", "def _cnx_is_empty(in_file): \"\"\"Check if cnr or cns files are empty (only have a header) \"\"\" with open(in_file) as in_handle: for i, line in enumerate(in_handle): if i > 0: return False return True"], "rank": 5}
{"nl": "python stop process multiprocessing", "code": ["def stop(self, timeout=None): \"\"\" Initiates a graceful stop of the processes \"\"\" self.stopping = True for process in list(self.processes): self.stop_process(process, timeout=timeout)", "def kill_mprocess(process): \"\"\"kill process Args: process - Popen object for process \"\"\" if process and proc_alive(process): process.terminate() process.communicate() return not proc_alive(process)", "def stop(self): \"\"\" Stop this server so that the calling process can exit \"\"\" # unsetup_fuse() self.fuse_process.teardown() for uuid in self.processes: self.processes[uuid].terminate()", "def stop_process(self): \"\"\" Stops the child process. \"\"\" self._process.terminate() if not self._process.waitForFinished(100): self._process.kill()", "def terminate(self): \"\"\"Terminate all workers and threads.\"\"\" for t in self._threads: t.quit() self._thread = [] self._workers = []"], "rank": 1}
{"nl": "python make sure all words are separated by a single space", "code": ["def split_into_words(s): \"\"\"Split a sentence into list of words.\"\"\" s = re.sub(r\"\\W+\", \" \", s) s = re.sub(r\"[_0-9]+\", \" \", s) return s.split()", "def detokenize(s): \"\"\" Detokenize a string by removing spaces before punctuation.\"\"\" print(s) s = re.sub(\"\\s+([;:,\\.\\?!])\", \"\\\\1\", s) s = re.sub(\"\\s+(n't)\", \"\\\\1\", s) return s", "def tokenize_words(self, text): \"\"\"Tokenize an input string into a list of words (with punctuation removed).\"\"\" return [ self.strip_punctuation(word) for word in text.split(' ') if self.strip_punctuation(word) ]", "def unpunctuate(s, *, char_blacklist=string.punctuation): \"\"\" Remove punctuation from string s. \"\"\" # remove punctuation s = \"\".join(c for c in s if c not in char_blacklist) # remove consecutive spaces return \" \".join(filter(None, s.split(\" \")))", "def strip_spaces(value, sep=None, join=True): \"\"\"Cleans trailing whitespaces and replaces also multiple whitespaces with a single space.\"\"\" value = value.strip() value = [v.strip() for v in value.split(sep)] join_sep = sep or ' ' return join_sep.join(value) if join else value"], "rank": 30}
{"nl": "making a multidimensional array of only 1 in python", "code": ["def pack_triples_numpy(triples): \"\"\"Packs a list of triple indexes into a 2D numpy array.\"\"\" if len(triples) == 0: return np.array([], dtype=np.int64) return np.stack(list(map(_transform_triple_numpy, triples)), axis=0)", "def make_2d(ary): \"\"\"Convert any array into a 2d numpy array. In case the array is already more than 2 dimensional, will ravel the dimensions after the first. \"\"\" dim_0, *_ = np.atleast_1d(ary).shape return ary.reshape(dim_0, -1, order=\"F\")", "def _unique_rows_numpy(a): \"\"\"return unique rows\"\"\" a = np.ascontiguousarray(a) unique_a = np.unique(a.view([('', a.dtype)] * a.shape[1])) return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))", "def length(self): \"\"\"Array of vector lengths\"\"\" return np.sqrt(np.sum(self**2, axis=1)).view(np.ndarray)", "def _transform_triple_numpy(x): \"\"\"Transform triple index into a 1-D numpy array.\"\"\" return np.array([x.head, x.relation, x.tail], dtype=np.int64)"], "rank": 21}
{"nl": "python initialize variable of an object", "code": ["def __init__(self): \"\"\"Initialize the state of the object\"\"\" self.state = self.STATE_INITIALIZING self.state_start = time.time()", "def create_object(cls, members): \"\"\"Promise an object of class `cls` with content `members`.\"\"\" obj = cls.__new__(cls) obj.__dict__ = members return obj", "def assign_to(self, obj): \"\"\"Assign `x` and `y` to an object that has properties `x` and `y`.\"\"\" obj.x = self.x obj.y = self.y", "def __init__(self): \"\"\"Initializes an attribute container identifier.\"\"\" super(AttributeContainerIdentifier, self).__init__() self._identifier = id(self)", "def __init__(self, interval, key): \"\"\"Constructor. See class docstring for parameter details.\"\"\" self.interval = interval self.key = key"], "rank": 1}
{"nl": "python elasticsearch limit results", "code": ["def scan(client, query=None, scroll='5m', raise_on_error=True, preserve_order=False, size=1000, **kwargs): \"\"\" Simple abstraction on top of the :meth:`~elasticsearch.Elasticsearch.scroll` api - a simple iterator that yields all hits as returned by underlining scroll requests. By default scan does not return results in any pre-determined order. To have a standard order in the returned documents (either by score or explicit sort definition) when scrolling, use ``preserve_order=True``. This may be an expensive operation and will negate the performance benefits of using ``scan``. :arg client: instance of :class:`~elasticsearch.Elasticsearch` to use :arg query: body for the :meth:`~elasticsearch.Elasticsearch.search` api :arg scroll: Specify how long a consistent view of the index should be maintained for scrolled search :arg raise_on_error: raises an exception (``ScanError``) if an error is encountered (some shards fail to execute). By default we raise. :arg preserve_order: don't set the ``search_type`` to ``scan`` - this will cause the scroll to paginate with preserving the order. Note that this can be an extremely expensive operation and can easily lead to unpredictable results, use with caution. :arg size: size (per shard) of the batch send at each iteration. Any additional keyword arguments will be passed to the initial :meth:`~elasticsearch.Elasticsearch.search` call:: scan(es, query={\"query\": {\"match\": {\"title\": \"python\"}}}, index=\"orders-*\", doc_type=\"books\" ) \"\"\" if not preserve_order: kwargs['search_type'] = 'scan' # initial search resp = client.search(body=query, scroll=scroll, size=size, **kwargs) scroll_id = resp.get('_scroll_id') if scroll_id is None: return first_run = True while True: # if we didn't set search_type to scan initial search contains data if preserve_order and first_run: first_run = False else: resp = client.scroll(scroll_id, scroll=scroll) for hit in resp['hits']['hits']: yield hit # check if we have any errrors if resp[\"_shards\"][\"failed\"]: logger.warning( 'Scroll request has failed on %d shards out of %d.', resp['_shards']['failed'], resp['_shards']['total'] ) if raise_on_error: raise ScanError( 'Scroll request has failed on %d shards out of %d.' % (resp['_shards']['failed'], resp['_shards']['total']) ) scroll_id = resp.get('_scroll_id') # end of scroll if scroll_id is None or not resp['hits']['hits']: break", "def raw(self): \"\"\" Build query and passes to `Elasticsearch`, then returns the raw format returned. \"\"\" es = self.get_es() params = dict(self.query_params) mlt_fields = self.mlt_fields or params.pop('mlt_fields', []) body = self.s.build_search() if self.s else '' hits = es.mlt( index=self.index, doc_type=self.doctype, id=self.id, mlt_fields=mlt_fields, body=body, **params) log.debug(hits) return hits", "def all_documents(index=INDEX_NAME): \"\"\" Get all documents from the given index. Returns full Elasticsearch objects so you can get metadata too. \"\"\" query = { 'query': { 'match_all': {} } } for result in raw_query(query, index=index): yield result", "def paginate(self, request, offset=0, limit=None): \"\"\"Paginate queryset.\"\"\" return self.collection.offset(offset).limit(limit), self.collection.count()", "def can_elasticsearch(record): \"\"\"Check if a given record is indexed. :param record: A record object. :returns: If the record is indexed returns `True`, otherwise `False`. \"\"\" search = request._methodview.search_class() search = search.get_record(str(record.id)) return search.count() == 1"], "rank": 1}
{"nl": "how to force exit python without raise", "code": ["def fast_exit(code): \"\"\"Exit without garbage collection, this speeds up exit by about 10ms for things like bash completion. \"\"\" sys.stdout.flush() sys.stderr.flush() os._exit(code)", "def fail(message=None, exit_status=None): \"\"\"Prints the specified message and exits the program with the specified exit status. \"\"\" print('Error:', message, file=sys.stderr) sys.exit(exit_status or 1)", "def exit_and_fail(self, msg=None, out=None): \"\"\"Exits the runtime with a nonzero exit code, indicating failure. :param msg: A string message to print to stderr or another custom file desciptor before exiting. (Optional) :param out: The file descriptor to emit `msg` to. (Optional) \"\"\" self.exit(result=PANTS_FAILED_EXIT_CODE, msg=msg, out=out)", "def Exit(msg, code=1): \"\"\"Exit execution with return code and message :param msg: Message displayed prior to exit :param code: code returned upon exiting \"\"\" print >> sys.stderr, msg sys.exit(code)", "def safe_exit(output): \"\"\"exit without breaking pipes.\"\"\" try: sys.stdout.write(output) sys.stdout.flush() except IOError: pass"], "rank": 6}
{"nl": "python weak reference to bound method", "code": ["def __call__(self, obj, *arg, **kw): \"\"\" Call the unbound method. We essentially build a bound method and call that. This ensures that the code for managing observers is invoked in the same was as it would be for a bound method. \"\"\" bound_method = self._manager.__get__(obj, obj.__class__) return bound_method(*arg, **kw)", "def Proxy(f): \"\"\"A helper to create a proxy method in a class.\"\"\" def Wrapped(self, *args): return getattr(self, f)(*args) return Wrapped", "def __is_bound_method(method): \"\"\"Return ``True`` if the `method` is a bound method (attached to an class instance. Args: method: A method or function type object. \"\"\" if not(hasattr(method, \"__func__\") and hasattr(method, \"__self__\")): return False # Bound methods have a __self__ attribute pointing to the owner instance return six.get_method_self(method) is not None", "def __set__(self, instance, value): \"\"\" Set a related object for an instance. \"\"\" self.map[id(instance)] = (weakref.ref(instance), value)", "def RemoveMethod(self, function): \"\"\" Removes the specified function's MethodWrapper from the added_methods list, so we don't re-bind it when making a clone. \"\"\" self.added_methods = [dm for dm in self.added_methods if not dm.method is function]"], "rank": 6}
{"nl": "calling index iterable python", "code": ["def stop_at(iterable, idx): \"\"\"Stops iterating before yielding the specified idx.\"\"\" for i, item in enumerate(iterable): if i == idx: return yield item", "def uniform_iterator(sequence): \"\"\"Uniform (key, value) iteration on a `dict`, or (idx, value) on a `list`.\"\"\" if isinstance(sequence, abc.Mapping): return six.iteritems(sequence) else: return enumerate(sequence)", "def find_all(self, string, callback): \"\"\" Wrapper on iter method, callback gets an iterator result \"\"\" for index, output in self.iter(string): callback(index, output)", "def fromiterable(cls, itr): \"\"\"Initialize from iterable\"\"\" x, y, z = itr return cls(x, y, z)", "def items(iterable): \"\"\" Iterates over the items of a sequence. If the sequence supports the dictionary protocol (iteritems/items) then we use that. Otherwise we use the enumerate built-in function. \"\"\" if hasattr(iterable, 'iteritems'): return (p for p in iterable.iteritems()) elif hasattr(iterable, 'items'): return (p for p in iterable.items()) else: return (p for p in enumerate(iterable))"], "rank": 1}
{"nl": "python datetime maybe undefined", "code": ["def date_to_datetime(x): \"\"\"Convert a date into a datetime\"\"\" if not isinstance(x, datetime) and isinstance(x, date): return datetime.combine(x, time()) return x", "def date_to_datetime(d): \"\"\" >>> date_to_datetime(date(2000, 1, 2)) datetime.datetime(2000, 1, 2, 0, 0) >>> date_to_datetime(datetime(2000, 1, 2, 3, 4, 5)) datetime.datetime(2000, 1, 2, 3, 4, 5) \"\"\" if not isinstance(d, datetime): d = datetime.combine(d, datetime.min.time()) return d", "def to_datetime(value): \"\"\"Converts a string to a datetime.\"\"\" if value is None: return None if isinstance(value, six.integer_types): return parser.parse(value) return parser.isoparse(value)", "def is_datetime_like(dtype): \"\"\"Check if a dtype is a subclass of the numpy datetime types \"\"\" return (np.issubdtype(dtype, np.datetime64) or np.issubdtype(dtype, np.timedelta64))", "def is_date_type(cls): \"\"\"Return True if the class is a date type.\"\"\" if not isinstance(cls, type): return False return issubclass(cls, date) and not issubclass(cls, datetime)"], "rank": 1}
{"nl": "python go to next page", "code": ["def accel_next(self, *args): \"\"\"Callback to go to the next tab. Called by the accel key. \"\"\" if self.get_notebook().get_current_page() + 1 == self.get_notebook().get_n_pages(): self.get_notebook().set_current_page(0) else: self.get_notebook().next_page() return True", "def next(self): \"\"\"Get the next value in the page.\"\"\" item = six.next(self._item_iter) result = self._item_to_value(self._parent, item) # Since we've successfully got the next value from the # iterator, we update the number of remaining. self._remaining -= 1 return result", "def update_redirect(self): \"\"\" Call it on your own endpoint's to update the back history navigation. If you bypass it, the next submit or back will go over it. \"\"\" page_history = Stack(session.get(\"page_history\", [])) page_history.push(request.url) session[\"page_history\"] = page_history.to_json()", "def do_next(self, args): \"\"\"Step over the next statement \"\"\" self._do_print_from_last_cmd = True self._interp.step_over() return True", "def accel_prev(self, *args): \"\"\"Callback to go to the previous tab. Called by the accel key. \"\"\" if self.get_notebook().get_current_page() == 0: self.get_notebook().set_current_page(self.get_notebook().get_n_pages() - 1) else: self.get_notebook().prev_page() return True"], "rank": 1}
{"nl": "filling null value sin data frame in python", "code": ["def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df", "def fill_nulls(self, col: str): \"\"\" Fill all null values with NaN values in a column. Null values are ``None`` or en empty string :param col: column name :type col: str :example: ``ds.fill_nulls(\"mycol\")`` \"\"\" n = [None, \"\"] try: self.df[col] = self.df[col].replace(n, nan) except Exception as e: self.err(e)", "def fillna(series_or_arr, missing_value=0.0): \"\"\"Fill missing values in pandas objects and numpy arrays. Arguments --------- series_or_arr : pandas.Series, numpy.ndarray The numpy array or pandas series for which the missing values need to be replaced. missing_value : float, int, str The value to replace the missing value with. Default 0.0. Returns ------- pandas.Series, numpy.ndarray The numpy array or pandas series with the missing values filled. \"\"\" if pandas.notnull(missing_value): if isinstance(series_or_arr, (numpy.ndarray)): series_or_arr[numpy.isnan(series_or_arr)] = missing_value else: series_or_arr.fillna(missing_value, inplace=True) return series_or_arr", "def _maybe_fill(arr, fill_value=np.nan): \"\"\" if we have a compatible fill_value and arr dtype, then fill \"\"\" if _isna_compat(arr, fill_value): arr.fill(fill_value) return arr", "def __convert_none_to_zero(self, ts): \"\"\" Convert None values to 0 so the data works with Matplotlib :param ts: :return: a list with 0s where Nones existed \"\"\" if not ts: return ts ts_clean = [val if val else 0 for val in ts] return ts_clean"], "rank": 1}
{"nl": "python run external command and get output", "code": ["def check_output(args): \"\"\"Runs command and returns the output as string.\"\"\" log.debug('run: %s', args) out = subprocess.check_output(args=args).decode('utf-8') log.debug('out: %r', out) return out", "def check_output(args, env=None, sp=subprocess): \"\"\"Call an external binary and return its stdout.\"\"\" log.debug('calling %s with env %s', args, env) output = sp.check_output(args=args, env=env) log.debug('output: %r', output) return output", "def call_out(command): \"\"\" Run the given command (with shell=False) and return a tuple of (int returncode, str output). Strip the output of enclosing whitespace. \"\"\" # start external command process p = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # get outputs out, _ = p.communicate() return p.returncode, out.strip()", "def _run_cmd_get_output(cmd): \"\"\"Runs a shell command, returns console output. Mimics python3's subprocess.getoutput \"\"\" process = subprocess.Popen(cmd.split(), stdout=subprocess.PIPE) out, err = process.communicate() return out or err", "def run_command(cmd, *args): \"\"\" Runs command on the system with given ``args``. \"\"\" command = ' '.join((cmd, args)) p = Popen(command, shell=True, stdout=PIPE, stderr=PIPE) stdout, stderr = p.communicate() return p.retcode, stdout, stderr"], "rank": 2}
{"nl": "how to compile python program to use in c++", "code": ["def cpp_prog_builder(build_context, target): \"\"\"Build a C++ binary executable\"\"\" yprint(build_context.conf, 'Build CppProg', target) workspace_dir = build_context.get_workspace('CppProg', target.name) build_cpp(build_context, target, target.compiler_config, workspace_dir)", "def cpp_checker(code, working_directory): \"\"\"Return checker.\"\"\" return gcc_checker(code, '.cpp', [os.getenv('CXX', 'g++'), '-std=c++0x'] + INCLUDE_FLAGS, working_directory=working_directory)", "def code(self): \"\"\"Returns the code object for this BUILD file.\"\"\" return compile(self.source(), self.full_path, 'exec', flags=0, dont_inherit=True)", "def generate(env): \"\"\"Add Builders and construction variables for SGI MIPS C++ to an Environment.\"\"\" cplusplus.generate(env) env['CXX'] = 'CC' env['CXXFLAGS'] = SCons.Util.CLVar('-LANG:std') env['SHCXX'] = '$CXX' env['SHOBJSUFFIX'] = '.o' env['STATIC_AND_SHARED_OBJECTS_ARE_THE_SAME'] = 1", "def get_code(module): \"\"\" Compile and return a Module's code object. \"\"\" fp = open(module.path) try: return compile(fp.read(), str(module.name), 'exec') finally: fp.close()"], "rank": 1}
{"nl": "python img to bytearray", "code": ["def to_bytes(self): \"\"\"Convert the entire image to bytes. :rtype: bytes \"\"\" chunks = [PNG_SIGN] chunks.extend(c[1] for c in self.chunks) return b\"\".join(chunks)", "def from_bytes(cls, b): \"\"\"Create :class:`PNG` from raw bytes. :arg bytes b: The raw bytes of the PNG file. :rtype: :class:`PNG` \"\"\" im = cls() im.chunks = list(parse_chunks(b)) im.init() return im", "def get_buffer(self, data_np, header, format, output=None): \"\"\"Get image as a buffer in (format). Format should be 'jpeg', 'png', etc. \"\"\" if not have_pil: raise Exception(\"Install PIL to use this method\") image = PILimage.fromarray(data_np) buf = output if buf is None: buf = BytesIO() image.save(buf, format) return buf", "def barray(iterlines): \"\"\" Array of bytes \"\"\" lst = [line.encode('utf-8') for line in iterlines] arr = numpy.array(lst) return arr", "def be_array_from_bytes(fmt, data): \"\"\" Reads an array from bytestring with big-endian data. \"\"\" arr = array.array(str(fmt), data) return fix_byteorder(arr)"], "rank": 1}
{"nl": "how to use access token oauth python", "code": ["def get_oauth_token(): \"\"\"Retrieve a simple OAuth Token for use with the local http client.\"\"\" url = \"{0}/token\".format(DEFAULT_ORIGIN[\"Origin\"]) r = s.get(url=url) return r.json()[\"t\"]", "def fetch_token(self, **kwargs): \"\"\"Exchange a code (and 'state' token) for a bearer token\"\"\" return super(AsanaOAuth2Session, self).fetch_token(self.token_url, client_secret=self.client_secret, **kwargs)", "def _get_token(self, oauth_request, token_type='access'): \"\"\"Try to find the token for the provided request token key.\"\"\" token_field = oauth_request.get_parameter('oauth_token') token = self.data_store.lookup_token(token_type, token_field) if not token: raise OAuthError('Invalid %s token: %s' % (token_type, token_field)) return token", "def get_tweepy_auth(twitter_api_key, twitter_api_secret, twitter_access_token, twitter_access_token_secret): \"\"\"Make a tweepy auth object\"\"\" auth = tweepy.OAuthHandler(twitter_api_key, twitter_api_secret) auth.set_access_token(twitter_access_token, twitter_access_token_secret) return auth", "def access_token(self): \"\"\" WeChat access token \"\"\" access_token = self.session.get(self.access_token_key) if access_token: if not self.expires_at: # user provided access_token, just return it return access_token timestamp = time.time() if self.expires_at - timestamp > 60: return access_token self.fetch_access_token() return self.session.get(self.access_token_key)"], "rank": 2}
{"nl": "have python line continue on to next line", "code": ["def advance_one_line(self): \"\"\"Advances to next line.\"\"\" current_line = self._current_token.line_number while current_line == self._current_token.line_number: self._current_token = ConfigParser.Token(*next(self._token_generator))", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def _skip_newlines(self): \"\"\"Increment over newlines.\"\"\" while self._cur_token['type'] is TT.lbreak and not self._finished: self._increment()", "def readline(self): \"\"\"Get the next line including the newline or '' on EOF.\"\"\" self.lineno += 1 if self._buffer: return self._buffer.pop() else: return self.input.readline()", "def __next__(self): \"\"\" :return: a pair (1-based line number in the input, row) \"\"\" # Retrieve the row, thereby incrementing the line number: row = super(UnicodeReaderWithLineNumber, self).__next__() return self.lineno + 1, row"], "rank": 1}
{"nl": "precision of ints in python", "code": ["def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)", "def round_to_int(number, precision): \"\"\"Round a number to a precision\"\"\" precision = int(precision) rounded = (int(number) + precision / 2) // precision * precision return rounded", "def round_to_float(number, precision): \"\"\"Round a float to a precision\"\"\" rounded = Decimal(str(floor((number + precision / 2) // precision)) ) * Decimal(str(precision)) return float(rounded)", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0", "def _saferound(value, decimal_places): \"\"\" Rounds a float value off to the desired precision \"\"\" try: f = float(value) except ValueError: return '' format = '%%.%df' % decimal_places return format % f"], "rank": 2}
{"nl": "modify the dice roll program to call a function for the die roll s python", "code": ["def roll_dice(): \"\"\" Roll a die. :return: None \"\"\" sums = 0 # will return the sum of the roll calls. while True: roll = random.randint(1, 6) sums += roll if(input(\"Enter y or n to continue: \").upper()) == 'N': print(sums) # prints the sum of the roll calls break", "def rollapply(data, window, fn): \"\"\" Apply a function fn over a rolling window of size window. Args: * data (Series or DataFrame): Series or DataFrame * window (int): Window size * fn (function): Function to apply over the rolling window. For a series, the return value is expected to be a single number. For a DataFrame, it shuold return a new row. Returns: * Object of same dimensions as data \"\"\" res = data.copy() res[:] = np.nan n = len(data) if window > n: return res for i in range(window - 1, n): res.iloc[i] = fn(data.iloc[i - window + 1:i + 1]) return res", "def trigger(self, target: str, trigger: str, parameters: Dict[str, Any]={}): \"\"\"Calls the specified Trigger of another Area with the optionally given parameters. Args: target: The name of the target Area. trigger: The name of the Trigger. parameters: The parameters of the function call. \"\"\" pass", "def __call__(self, args): \"\"\"Execute the user function.\"\"\" window, ij = args return self.user_func(srcs, window, ij, global_args), window", "def __call__(self, func, *args, **kwargs): \"\"\"Shorcut for self.run.\"\"\" return self.run(func, *args, **kwargs)"], "rank": 1}
{"nl": "python 'namespace' object is not iterable", "code": ["def __add_namespaceinfo(self, ni): \"\"\"Internal method to directly add a _NamespaceInfo object to this set. No sanity checks are done (e.g. checking for prefix conflicts), so be sure to do it yourself before calling this.\"\"\" self.__ns_uri_map[ni.uri] = ni for prefix in ni.prefixes: self.__prefix_map[prefix] = ni", "def is_lazy_iterable(obj): \"\"\" Returns whether *obj* is iterable lazily, such as generators, range objects, etc. \"\"\" return isinstance(obj, (types.GeneratorType, collections.MappingView, six.moves.range, enumerate))", "def functions(self): \"\"\" A list of functions declared or defined in this module. \"\"\" return [v for v in self.globals.values() if isinstance(v, values.Function)]", "def ensure_iterable(inst): \"\"\" Wraps scalars or string types as a list, or returns the iterable instance. \"\"\" if isinstance(inst, string_types): return [inst] elif not isinstance(inst, collections.Iterable): return [inst] else: return inst", "def is_iterable(etype) -> bool: \"\"\" Determine whether etype is a List or other iterable \"\"\" return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)"], "rank": 1}
{"nl": "how to specify seed for python random", "code": ["def generate_seed(seed): \"\"\"Generate seed for random number generator\"\"\" if seed is None: random.seed() seed = random.randint(0, sys.maxsize) random.seed(a=seed) return seed", "def reseed_random(seed): \"\"\"Reseed factory.fuzzy's random generator.\"\"\" r = random.Random(seed) random_internal_state = r.getstate() set_random_state(random_internal_state)", "def new_random_state(seed=None, fully_random=False): \"\"\" Returns a new random state. Parameters ---------- seed : None or int, optional Optional seed value to use. The same datatypes are allowed as for ``numpy.random.RandomState(seed)``. fully_random : bool, optional Whether to use numpy's random initialization for the RandomState (used if set to True). If False, a seed is sampled from the global random state, which is a bit faster and hence the default. Returns ------- numpy.random.RandomState The new random state. \"\"\" if seed is None: if not fully_random: # sample manually a seed instead of just RandomState(), # because the latter one # is way slower. seed = CURRENT_RANDOM_STATE.randint(SEED_MIN_VALUE, SEED_MAX_VALUE, 1)[0] return np.random.RandomState(seed)", "def const_rand(size, seed=23980): \"\"\" Generate a random array with a fixed seed. \"\"\" old_seed = np.random.seed() np.random.seed(seed) out = np.random.rand(size) np.random.seed(old_seed) return out", "def block(seed): \"\"\" Return block of normal random numbers Parameters ---------- seed : {None, int} The seed to generate the noise.sd Returns -------- noise : numpy.ndarray Array of random numbers \"\"\" num = SAMPLE_RATE * BLOCK_SIZE rng = RandomState(seed % 2**32) variance = SAMPLE_RATE / 2 return rng.normal(size=num, scale=variance**0.5)"], "rank": 1}
{"nl": "python use default arg", "code": ["def arg_default(*args, **kwargs): \"\"\"Return default argument value as given by argparse's add_argument(). The argument is passed through a mocked-up argument parser. This way, we get default parameters even if the feature is called directly and not through the CLI. \"\"\" parser = argparse.ArgumentParser() parser.add_argument(*args, **kwargs) args = vars(parser.parse_args([])) _, default = args.popitem() return default", "def set_default(self, section, option, default): \"\"\"If the option did not exist, create a default value.\"\"\" if not self.parser.has_option(section, option): self.parser.set(section, option, default)", "def check_for_positional_argument(kwargs, name, default=False): \"\"\" @type kwargs: dict @type name: str @type default: bool, int, str @return: bool, int \"\"\" if name in kwargs: if str(kwargs[name]) == \"True\": return True elif str(kwargs[name]) == \"False\": return False else: return kwargs[name] return default", "def set_default(self_,param_name,value): \"\"\" Set the default value of param_name. Equivalent to setting param_name on the class. \"\"\" cls = self_.cls setattr(cls,param_name,value)", "def set_default(self, key, value): \"\"\"Set the default value for this key. Default only used when no value is provided by the user via arg, config or env. \"\"\" k = self._real_key(key.lower()) self._defaults[k] = value"], "rank": 1}
{"nl": "python change array dtype to int", "code": ["def to_int64(a): \"\"\"Return view of the recarray with all int32 cast to int64.\"\"\" # build new dtype and replace i4 --> i8 def promote_i4(typestr): if typestr[1:] == 'i4': typestr = typestr[0]+'i8' return typestr dtype = [(name, promote_i4(typestr)) for name,typestr in a.dtype.descr] return a.astype(dtype)", "def scale_dtype(arr, dtype): \"\"\"Convert an array from 0..1 to dtype, scaling up linearly \"\"\" max_int = np.iinfo(dtype).max return (arr * max_int).astype(dtype)", "def _convert_to_array(array_like, dtype): \"\"\" Convert Matrix attributes which are array-like or buffer to array. \"\"\" if isinstance(array_like, bytes): return np.frombuffer(array_like, dtype=dtype) return np.asarray(array_like, dtype=dtype)", "def dict_to_numpy_array(d): \"\"\" Convert a dict of 1d array to a numpy recarray \"\"\" return fromarrays(d.values(), np.dtype([(str(k), v.dtype) for k, v in d.items()]))", "def astype(array, y): \"\"\"A functional form of the `astype` method. Args: array: The array or number to cast. y: An array or number, as the input, whose type should be that of array. Returns: An array or number with the same dtype as `y`. \"\"\" if isinstance(y, autograd.core.Node): return array.astype(numpy.array(y.value).dtype) return array.astype(numpy.array(y).dtype)"], "rank": 1}
{"nl": "python forcible close socket before opening", "code": ["def _close_socket(self): \"\"\"Shutdown and close the Socket. :return: \"\"\" try: self.socket.shutdown(socket.SHUT_RDWR) except (OSError, socket.error): pass self.socket.close()", "def shutdown(self): \"\"\"close socket, immediately.\"\"\" if self.sock: self.sock.close() self.sock = None self.connected = False", "def socket_close(self): \"\"\"Close our socket.\"\"\" if self.sock != NC.INVALID_SOCKET: self.sock.close() self.sock = NC.INVALID_SOCKET", "def close_stream(self): \"\"\" Closes the stream. Performs cleanup. \"\"\" self.keep_listening = False self.stream.stop() self.stream.close()", "def close(self): \"\"\"Close port.\"\"\" os.close(self.in_d) os.close(self.out_d)"], "rank": 6}
{"nl": "python series'value non zero index", "code": ["def reduce_fn(x): \"\"\" Aggregation function to get the first non-zero value. \"\"\" values = x.values if pd and isinstance(x, pd.Series) else x for v in values: if not is_nan(v): return v return np.NaN", "def __convert_none_to_zero(self, ts): \"\"\" Convert None values to 0 so the data works with Matplotlib :param ts: :return: a list with 0s where Nones existed \"\"\" if not ts: return ts ts_clean = [val if val else 0 for val in ts] return ts_clean", "def get_X0(X): \"\"\" Return zero-th element of a one-element data container. \"\"\" if pandas_available and isinstance(X, pd.DataFrame): assert len(X) == 1 x = np.array(X.iloc[0]) else: x, = X return x", "def series_index(self, series): \"\"\" Return the integer index of *series* in this sequence. \"\"\" for idx, s in enumerate(self): if series is s: return idx raise ValueError('series not in chart data object')", "def yvals(self): \"\"\"All y values\"\"\" return [ val[1] for serie in self.series for val in serie.values if val[1] is not None ]"], "rank": 1}
{"nl": "how to add a number to certain elements of an array numpy python", "code": ["def _increment(arr, indices): \"\"\"Increment some indices in a 1D vector of non-negative integers. Repeated indices are taken into account.\"\"\" arr = _as_array(arr) indices = _as_array(indices) bbins = np.bincount(indices) arr[:len(bbins)] += bbins return arr", "def layer_with(self, sample: np.ndarray, value: int) -> np.ndarray: \"\"\"Create an identical 2d array where the second row is filled with value\"\"\" b = np.full((2, len(sample)), value, dtype=float) b[0] = sample return b", "def Sum(a, axis, keep_dims): \"\"\" Sum reduction op. \"\"\" return np.sum(a, axis=axis if not isinstance(axis, np.ndarray) else tuple(axis), keepdims=keep_dims),", "def generic_add(a, b): \"\"\"Simple function to add two numbers\"\"\" logger.debug('Called generic_add({}, {})'.format(a, b)) return a + b", "def generic_add(a, b): print \"\"\"Simple function to add two numbers\"\"\" logger.info('Called generic_add({}, {})'.format(a, b)) return a + b"], "rank": 1}
{"nl": "python unit test and coverage at same time", "code": ["def cover(session): \"\"\"Run the final coverage report. This outputs the coverage report aggregating coverage from the unit test runs (not system test runs), and then erases coverage data. \"\"\" session.interpreter = 'python3.6' session.install('coverage', 'pytest-cov') session.run('coverage', 'report', '--show-missing', '--fail-under=100') session.run('coverage', 'erase')", "def coverage(ctx, opts=\"\"): \"\"\" Execute all tests (normal and slow) with coverage enabled. \"\"\" return test(ctx, coverage=True, include_slow=True, opts=opts)", "def coverage(): \"\"\"Run coverage tests.\"\"\" # Note: coverage options are controlled by .coveragerc file install() test_setup() sh(\"%s -m coverage run %s\" % (PYTHON, TEST_SCRIPT)) sh(\"%s -m coverage report\" % PYTHON) sh(\"%s -m coverage html\" % PYTHON) sh(\"%s -m webbrowser -t htmlcov/index.html\" % PYTHON)", "def get_cov(config): \"\"\"Returns the coverage object of pytest-cov.\"\"\" # Check with hasplugin to avoid getplugin exception in older pytest. if config.pluginmanager.hasplugin('_cov'): plugin = config.pluginmanager.getplugin('_cov') if plugin.cov_controller: return plugin.cov_controller.cov return None", "def test(): \"\"\" Run all Tests [nose] \"\"\" command = 'nosetests --with-coverage --cover-package=pwnurl' status = subprocess.call(shlex.split(command)) sys.exit(status)"], "rank": 2}
{"nl": "python only list files with specific extension", "code": ["def glob_by_extensions(directory, extensions): \"\"\" Returns files matched by all extensions in the extensions list \"\"\" directorycheck(directory) files = [] xt = files.extend for ex in extensions: xt(glob.glob('{0}/*.{1}'.format(directory, ex))) return files", "def match_files(files, pattern: Pattern): \"\"\"Yields file name if matches a regular expression pattern.\"\"\" for name in files: if re.match(pattern, name): yield name", "def watched_extension(extension): \"\"\"Return True if the given extension is one of the watched extensions\"\"\" for ext in hamlpy.VALID_EXTENSIONS: if extension.endswith('.' + ext): return True return False", "def list_files(directory): \"\"\"Returns all files in a given directory \"\"\" return [f for f in pathlib.Path(directory).iterdir() if f.is_file() and not f.name.startswith('.')]", "def get_file_extension_type(filename): \"\"\" Return the group associated to the file :param filename: :return: str \"\"\" ext = get_file_extension(filename) if ext: for name, group in EXTENSIONS.items(): if ext in group: return name return \"OTHER\""], "rank": 1}
{"nl": "python global type hinting", "code": ["def is_builtin_type(tp): \"\"\"Checks if the given type is a builtin one. \"\"\" return hasattr(__builtins__, tp.__name__) and tp is getattr(__builtins__, tp.__name__)", "def parse_parameter(value): \"\"\" @return: The best approximation of a type of the given value. \"\"\" if any((isinstance(value, float), isinstance(value, int), isinstance(value, bool))): return value try: return int(value) except ValueError: try: return float(value) except ValueError: if value in string_aliases.true_boolean_aliases: return True elif value in string_aliases.false_boolean_aliases: return False else: return str(value)", "def istype(obj, check): \"\"\"Like isinstance(obj, check), but strict. This won't catch subclasses. \"\"\" if isinstance(check, tuple): for cls in check: if type(obj) is cls: return True return False else: return type(obj) is check", "def return_type(type_name, formatter=None): \"\"\"Specify that this function returns a typed value. Args: type_name (str): A type name known to the global typedargs type system formatter (str): An optional name of a formatting function specified for the type given in type_name. \"\"\" def _returns(func): annotated(func) func.metadata.typed_returnvalue(type_name, formatter) return func return _returns", "def get_function_class(function_name): \"\"\" Return the type for the requested function :param function_name: the function to return :return: the type for that function (i.e., this is a class, not an instance) \"\"\" if function_name in _known_functions: return _known_functions[function_name] else: raise UnknownFunction(\"Function %s is not known. Known functions are: %s\" % (function_name, \",\".join(_known_functions.keys())))"], "rank": 1}
{"nl": "how to make a function in python to take the average of list numbers", "code": ["def calc_list_average(l): \"\"\" Calculates the average value of a list of numbers Returns a float \"\"\" total = 0.0 for value in l: total += value return total / len(l)", "def mean(inlist): \"\"\" Returns the arithematic mean of the values in the passed list. Assumes a '1D' list, but will function on the 1st dim of an array(!). Usage: lmean(inlist) \"\"\" sum = 0 for item in inlist: sum = sum + item return sum / float(len(inlist))", "def average(arr): \"\"\"average of the values, must have more than 0 entries. :param arr: list of numbers :type arr: number[] a number array :return: average :rtype: float \"\"\" if len(arr) == 0: sys.stderr.write(\"ERROR: no content in array to take average\\n\") sys.exit() if len(arr) == 1: return arr[0] return float(sum(arr))/float(len(arr))", "def average(iterator): \"\"\"Iterative mean.\"\"\" count = 0 total = 0 for num in iterator: count += 1 total += num return float(total)/count", "def _aggr_mean(inList): \"\"\" Returns mean of non-None elements of the list \"\"\" aggrSum = 0 nonNone = 0 for elem in inList: if elem != SENTINEL_VALUE_FOR_MISSING_DATA: aggrSum += elem nonNone += 1 if nonNone != 0: return aggrSum / nonNone else: return None"], "rank": 1}
{"nl": "check if two arrays are equal python", "code": ["def numpy_aware_eq(a, b): \"\"\"Return whether two objects are equal via recursion, using :func:`numpy.array_equal` for comparing numpy arays. \"\"\" if isinstance(a, np.ndarray) or isinstance(b, np.ndarray): return np.array_equal(a, b) if ((isinstance(a, Iterable) and isinstance(b, Iterable)) and not isinstance(a, str) and not isinstance(b, str)): if len(a) != len(b): return False return all(numpy_aware_eq(x, y) for x, y in zip(a, b)) return a == b", "def allclose(a, b): \"\"\" Test that a and b are close and match in shape. Parameters ---------- a : ndarray First array to check b : ndarray First array to check \"\"\" from numpy import allclose return (a.shape == b.shape) and allclose(a, b)", "def all_equal(arg1,arg2): \"\"\" Return a single boolean for arg1==arg2, even for numpy arrays using element-wise comparison. Uses all(arg1==arg2) for sequences, and arg1==arg2 otherwise. If both objects have an '_infinitely_iterable' attribute, they are not be zipped together and are compared directly instead. \"\"\" if all(hasattr(el, '_infinitely_iterable') for el in [arg1,arg2]): return arg1==arg2 try: return all(a1 == a2 for a1, a2 in zip(arg1, arg2)) except TypeError: return arg1==arg2", "def check_consistent_length(*arrays): \"\"\"Check that all arrays have consistent first dimensions. Checks whether all objects in arrays have the same shape or length. Parameters ---------- arrays : list or tuple of input objects. Objects that will be checked for consistent length. \"\"\" uniques = np.unique([_num_samples(X) for X in arrays if X is not None]) if len(uniques) > 1: raise ValueError(\"Found arrays with inconsistent numbers of samples: %s\" % str(uniques))", "def compare(a, b): \"\"\" Compare items in 2 arrays. Returns sum(abs(a(i)-b(i))) \"\"\" s=0 for i in range(len(a)): s=s+abs(a[i]-b[i]) return s"], "rank": 1}
{"nl": "python round down numpy", "code": ["def round_array(array_in): \"\"\" arr_out = round_array(array_in) Rounds an array and recasts it to int. Also works on scalars. \"\"\" if isinstance(array_in, ndarray): return np.round(array_in).astype(int) else: return int(np.round(array_in))", "def get_rounded(self, digits): \"\"\" Return a vector with the elements rounded to the given number of digits. \"\"\" result = self.copy() result.round(digits) return result", "def _rescale_array(self, array, scale, zero): \"\"\" Scale the input array \"\"\" if scale != 1.0: sval = numpy.array(scale, dtype=array.dtype) array *= sval if zero != 0.0: zval = numpy.array(zero, dtype=array.dtype) array += zval", "def round_data(filter_data): \"\"\" round the data\"\"\" for index, _ in enumerate(filter_data): filter_data[index][0] = round(filter_data[index][0] / 100.0) * 100.0 return filter_data", "def ceil_nearest(x, dx=1): \"\"\" ceil a number to within a given rounding accuracy \"\"\" precision = get_sig_digits(dx) return round(math.ceil(float(x) / dx) * dx, precision)"], "rank": 1}
{"nl": "python pywin32 screenshoot refresh", "code": ["def win32_refresh_window(cls): \"\"\" Call win32 API to refresh the whole Window. This is sometimes necessary when the application paints background for completion menus. When the menu disappears, it leaves traces due to a bug in the Windows Console. Sending a repaint request solves it. \"\"\" # Get console handle handle = windll.kernel32.GetConsoleWindow() RDW_INVALIDATE = 0x0001 windll.user32.RedrawWindow(handle, None, None, c_uint(RDW_INVALIDATE))", "def update_screen(self): \"\"\"Refresh the screen. You don't need to override this except to update only small portins of the screen.\"\"\" self.clock.tick(self.FPS) pygame.display.update()", "def display(self): \"\"\" Get screen width and height \"\"\" w, h = self.session.window_size() return Display(w*self.scale, h*self.scale)", "def clear(): \"\"\"Clears the console.\"\"\" if sys.platform.startswith(\"win\"): call(\"cls\", shell=True) else: call(\"clear\", shell=True)", "def SwitchToThisWindow(handle: int) -> None: \"\"\" SwitchToThisWindow from Win32. handle: int, the handle of a native window. \"\"\" ctypes.windll.user32.SwitchToThisWindow(ctypes.c_void_p(handle), 1)"], "rank": 1}
{"nl": "get largest date from a list python", "code": ["def _latest_date(self, query, datetime_field_name): \"\"\"Given a QuerySet and the name of field containing datetimes, return the latest (most recent) date. Return None if QuerySet is empty. \"\"\" return list( query.aggregate(django.db.models.Max(datetime_field_name)).values() )[0]", "def newest_file(file_iterable): \"\"\" Returns the name of the newest file given an iterable of file names. \"\"\" return max(file_iterable, key=lambda fname: os.path.getmtime(fname))", "def get_longest_orf(orfs): \"\"\"Find longest ORF from the given list of ORFs.\"\"\" sorted_orf = sorted(orfs, key=lambda x: len(x['sequence']), reverse=True)[0] return sorted_orf", "def mostCommonItem(lst): \"\"\"Choose the most common item from the list, or the first item if all items are unique.\"\"\" # This elegant solution from: http://stackoverflow.com/a/1518632/1760218 lst = [l for l in lst if l] if lst: return max(set(lst), key=lst.count) else: return None", "def get_default_bucket_key(buckets: List[Tuple[int, int]]) -> Tuple[int, int]: \"\"\" Returns the default bucket from a list of buckets, i.e. the largest bucket. :param buckets: List of buckets. :return: The largest bucket in the list. \"\"\" return max(buckets)"], "rank": 1}
{"nl": "python lock no blocking", "code": ["def lock(self, block=True): \"\"\" Lock connection from being used else where \"\"\" self._locked = True return self._lock.acquire(block)", "async def acquire_async(self): \"\"\"Acquire the :attr:`lock` asynchronously \"\"\" r = self.acquire(blocking=False) while not r: await asyncio.sleep(.01) r = self.acquire(blocking=False)", "def lock_file(f, block=False): \"\"\" If block=False (the default), die hard and fast if another process has already grabbed the lock for this file. If block=True, wait for the lock to be released, then continue. \"\"\" try: flags = fcntl.LOCK_EX if not block: flags |= fcntl.LOCK_NB fcntl.flock(f.fileno(), flags) except IOError as e: if e.errno in (errno.EACCES, errno.EAGAIN): raise SystemExit(\"ERROR: %s is locked by another process.\" % f.name) raise", "def writer_acquire(self): \"\"\"Acquire the lock to write\"\"\" self._order_mutex.acquire() self._access_mutex.acquire() self._order_mutex.release()", "def acquire_nix(lock_file): # pragma: no cover \"\"\"Acquire a lock file on linux or osx.\"\"\" fd = os.open(lock_file, OPEN_MODE) try: fcntl.flock(fd, fcntl.LOCK_EX | fcntl.LOCK_NB) except (IOError, OSError): os.close(fd) else: return fd"], "rank": 1}
{"nl": "delete item from a set python", "code": ["def remove_once(gset, elem): \"\"\"Remove the element from a set, lists or dict. >>> L = [\"Lucy\"]; S = set([\"Sky\"]); D = { \"Diamonds\": True }; >>> remove_once(L, \"Lucy\"); remove_once(S, \"Sky\"); remove_once(D, \"Diamonds\"); >>> print L, S, D [] set([]) {} Returns the element if it was removed. Raises one of the exceptions in :obj:`RemoveError` otherwise. \"\"\" remove = getattr(gset, 'remove', None) if remove is not None: remove(elem) else: del gset[elem] return elem", "def discard(self, element): \"\"\"Remove element from the RangeSet if it is a member. If the element is not a member, do nothing. \"\"\" try: i = int(element) set.discard(self, i) except ValueError: pass", "def remove(self, entry): \"\"\"Removes an entry\"\"\" try: list = self.cache[entry.key] list.remove(entry) except: pass", "def add(self, value): \"\"\"Add the element *value* to the set.\"\"\" if value not in self._set: self._set.add(value) self._list.add(value)", "def __isub__(self, other): \"\"\"Remove all elements of another set from this RangeSet.\"\"\" self._binary_sanity_check(other) set.difference_update(self, other) return self"], "rank": 1}
{"nl": "how to make a sentence into underscores with python", "code": ["def underscore(text): \"\"\"Converts text that may be camelcased into an underscored format\"\"\" return UNDERSCORE[1].sub(r'\\1_\\2', UNDERSCORE[0].sub(r'\\1_\\2', text)).lower()", "def camelcase_underscore(name): \"\"\" Convert camelcase names to underscore \"\"\" s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "def normalise_string(string): \"\"\" Strips trailing whitespace from string, lowercases it and replaces spaces with underscores \"\"\" string = (string.strip()).lower() return re.sub(r'\\W+', '_', string)", "def camel_to_underscore(string): \"\"\"Convert camelcase to lowercase and underscore. Recipe from http://stackoverflow.com/a/1176023 Args: string (str): The string to convert. Returns: str: The converted string. \"\"\" string = FIRST_CAP_RE.sub(r'\\1_\\2', string) return ALL_CAP_RE.sub(r'\\1_\\2', string).lower()", "def camel_case_from_underscores(string): \"\"\"generate a CamelCase string from an underscore_string.\"\"\" components = string.split('_') string = '' for component in components: string += component[0].upper() + component[1:] return string"], "rank": 7}
{"nl": "python timestamp remove timezone", "code": ["def normalize_time(timestamp): \"\"\"Normalize time in arbitrary timezone to UTC naive object.\"\"\" offset = timestamp.utcoffset() if offset is None: return timestamp return timestamp.replace(tzinfo=None) - offset", "def datetime_from_timestamp(timestamp, content): \"\"\" Helper function to add timezone information to datetime, so that datetime is comparable to other datetime objects in recent versions that now also have timezone information. \"\"\" return set_date_tzinfo( datetime.fromtimestamp(timestamp), tz_name=content.settings.get('TIMEZONE', None))", "def fromtimestamp(cls, timestamp): \"\"\"Returns a datetime object of a given timestamp (in local tz).\"\"\" d = cls.utcfromtimestamp(timestamp) return d.astimezone(localtz())", "def normalize(self, dt, is_dst=False): \"\"\"Correct the timezone information on the given datetime\"\"\" if dt.tzinfo is self: return dt if dt.tzinfo is None: raise ValueError('Naive time - no tzinfo set') return dt.astimezone(self)", "def timestamp_to_datetime(cls, time_stamp, localized=True): \"\"\" Converts a UTC timestamp to a datetime.datetime.\"\"\" ret = datetime.datetime.utcfromtimestamp(time_stamp) if localized: ret = localize(ret, pytz.utc) return ret"], "rank": 9}
{"nl": "python sanic change all object id to string", "code": ["def generate_id(self, obj): \"\"\"Generate unique document id for ElasticSearch.\"\"\" object_type = type(obj).__name__.lower() return '{}_{}'.format(object_type, self.get_object_id(obj))", "def pythonise(id, encoding='ascii'): \"\"\"Return a Python-friendly id\"\"\" replace = {'-': '_', ':': '_', '/': '_'} func = lambda id, pair: id.replace(pair[0], pair[1]) id = reduce(func, replace.iteritems(), id) id = '_%s' % id if id[0] in string.digits else id return id.encode(encoding)", "def __init__(self): \"\"\"Initializes an attribute container identifier.\"\"\" super(AttributeContainerIdentifier, self).__init__() self._identifier = id(self)", "def _unique_id(self, prefix): \"\"\" Generate a unique (within the graph) identifer internal to graph generation. \"\"\" _id = self._id_gen self._id_gen += 1 return prefix + str(_id)", "def generate_id(self): \"\"\"Generate a fresh id\"\"\" if self.use_repeatable_ids: self.repeatable_id_counter += 1 return 'autobaked-{}'.format(self.repeatable_id_counter) else: return str(uuid4())"], "rank": 1}
{"nl": "python mock mark a test as expected failure", "code": ["def process_instance(self, instance): self.log.debug(\"e = mc^2\") self.log.info(\"About to fail..\") self.log.warning(\"Failing.. soooon..\") self.log.critical(\"Ok, you're done.\") assert False, \"\"\"ValidateFailureMock was destined to fail.. Here's some extended information about what went wrong. It has quite the long string associated with it, including a few newlines and a list. - Item 1 - Item 2 \"\"\"", "def assert_called(_mock_self): \"\"\"assert that the mock was called at least once \"\"\" self = _mock_self if self.call_count == 0: msg = (\"Expected '%s' to have been called.\" % self._mock_name or 'mock') raise AssertionError(msg)", "def test_kwargs_are_optional(self): \"\"\"kwarg values always have defaults\"\"\" with patch(\"sys.exit\") as mock_exit: cli = MicroCLITestCase.T(\"script_name f3\".split()).run() # kwargs are optional mock_exit.assert_called_with(4)", "def assert_any_call(self, *args, **kwargs): \"\"\"assert the mock has been called with the specified arguments. The assert passes if the mock has *ever* been called, unlike `assert_called_with` and `assert_called_once_with` that only pass if the call is the most recent one.\"\"\" kall = call(*args, **kwargs) if kall not in self.call_args_list: expected_string = self._format_mock_call_signature(args, kwargs) raise AssertionError( '%s call not found' % expected_string )", "def assert_called_once(_mock_self): \"\"\"assert that the mock was called only once. \"\"\" self = _mock_self if not self.call_count == 1: msg = (\"Expected '%s' to have been called once. Called %s times.\" % (self._mock_name or 'mock', self.call_count)) raise AssertionError(msg)"], "rank": 2}
{"nl": "python get current users desktop", "code": ["def get_current_desktop(self): \"\"\" Get the current desktop. Uses ``_NET_CURRENT_DESKTOP`` of the EWMH spec. \"\"\" desktop = ctypes.c_long(0) _libxdo.xdo_get_current_desktop(self._xdo, ctypes.byref(desktop)) return desktop.value", "def get_screen_resolution(self): \"\"\"Return the screen resolution of the primary screen.\"\"\" widget = QDesktopWidget() geometry = widget.availableGeometry(widget.primaryScreen()) return geometry.width(), geometry.height()", "def get_user_name(): \"\"\"Get user name provide by operating system \"\"\" if sys.platform == 'win32': #user = os.getenv('USERPROFILE') user = os.getenv('USERNAME') else: user = os.getenv('LOGNAME') return user", "def _is_root(): \"\"\"Checks if the user is rooted.\"\"\" import os import ctypes try: return os.geteuid() == 0 except AttributeError: return ctypes.windll.shell32.IsUserAnAdmin() != 0 return False", "def get_active_window(): \"\"\"Get the currently focused window \"\"\" active_win = None default = wnck.screen_get_default() while gtk.events_pending(): gtk.main_iteration(False) window_list = default.get_windows() if len(window_list) == 0: print \"No Windows Found\" for win in window_list: if win.is_active(): active_win = win.get_name() return active_win"], "rank": 1}
{"nl": "python connect to redis in other docker container", "code": ["def __connect(): \"\"\" Connect to a redis instance. \"\"\" global redis_instance if use_tcp_socket: redis_instance = redis.StrictRedis(host=hostname, port=port) else: redis_instance = redis.StrictRedis(unix_socket_path=unix_socket)", "def connect(self): \"\"\" Connects to publisher \"\"\" self.client = redis.Redis( host=self.host, port=self.port, password=self.password)", "def from_url(url, db=None, **kwargs): \"\"\" Returns an active Redis client generated from the given database URL. Will attempt to extract the database id from the path url fragment, if none is provided. \"\"\" from redis.client import Redis return Redis.from_url(url, db, **kwargs)", "def get_connection(self, host, port, db): \"\"\" Returns a ``StrictRedis`` connection instance. \"\"\" return redis.StrictRedis( host=host, port=port, db=db, decode_responses=True )", "def _get_info(self, host, port, unix_socket, auth): \"\"\"Return info dict from specified Redis instance :param str host: redis host :param int port: redis port :rtype: dict \"\"\" client = self._client(host, port, unix_socket, auth) if client is None: return None info = client.info() del client return info"], "rank": 1}
{"nl": "python subplot second y axis", "code": ["def show_yticklabels(self, row, column): \"\"\"Show the y-axis tick labels for a subplot. :param row,column: specify the subplot. \"\"\" subplot = self.get_subplot_at(row, column) subplot.show_yticklabels()", "def set_ylimits(self, row, column, min=None, max=None): \"\"\"Set y-axis limits of a subplot. :param row,column: specify the subplot. :param min: minimal axis value :param max: maximum axis value \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_ylimits(min, max)", "def set_axis_options(self, row, column, text): \"\"\"Set additionnal options as plain text.\"\"\" subplot = self.get_subplot_at(row, column) subplot.set_axis_options(text)", "def activate_subplot(numPlot): \"\"\"Make subplot *numPlot* active on the canvas. Use this if a simple ``subplot(numRows, numCols, numPlot)`` overwrites the subplot instead of activating it. \"\"\" # see http://www.mail-archive.com/matplotlib-users@lists.sourceforge.net/msg07156.html from pylab import gcf, axes numPlot -= 1 # index is 0-based, plots are 1-based return axes(gcf().get_axes()[numPlot])", "def show_xticklabels(self, row, column): \"\"\"Show the x-axis tick labels for a subplot. :param row,column: specify the subplot. \"\"\" subplot = self.get_subplot_at(row, column) subplot.show_xticklabels()"], "rank": 1}
{"nl": "python function to detect first element of list", "code": ["def head(self) -> Any: \"\"\"Retrive first element in List.\"\"\" lambda_list = self._get_value() return lambda_list(lambda head, _: head)", "def findfirst(f, coll): \"\"\"Return first occurrence matching f, otherwise None\"\"\" result = list(dropwhile(f, coll)) return result[0] if result else None", "def find_first_in_list(txt: str, str_list: [str]) -> int: # type: ignore \"\"\" Returns the index of the earliest occurence of an item from a list in a string Ex: find_first_in_list('foobar', ['bar', 'fin']) -> 3 \"\"\" start = len(txt) + 1 for item in str_list: if start > txt.find(item) > -1: start = txt.find(item) return start if len(txt) + 1 > start > -1 else -1", "def _find_first_of(line, substrings): \"\"\"Find earliest occurrence of one of substrings in line. Returns pair of index and found substring, or (-1, None) if no occurrences of any of substrings were found in line. \"\"\" starts = ((line.find(i), i) for i in substrings) found = [(i, sub) for i, sub in starts if i != -1] if found: return min(found) else: return -1, None", "def getfirstline(file, default): \"\"\" Returns the first line of a file. \"\"\" with open(file, 'rb') as fh: content = fh.readlines() if len(content) == 1: return content[0].decode('utf-8').strip('\\n') return default"], "rank": 3}
{"nl": "how to flip a matrix in python", "code": ["def imflip(img, direction='horizontal'): \"\"\"Flip an image horizontally or vertically. Args: img (ndarray): Image to be flipped. direction (str): The flip direction, either \"horizontal\" or \"vertical\". Returns: ndarray: The flipped image. \"\"\" assert direction in ['horizontal', 'vertical'] if direction == 'horizontal': return np.flip(img, axis=1) else: return np.flip(img, axis=0)", "def hflip(img): \"\"\"Horizontally flip the given PIL Image. Args: img (PIL Image): Image to be flipped. Returns: PIL Image: Horizontall flipped image. \"\"\" if not _is_pil_image(img): raise TypeError('img should be PIL Image. Got {}'.format(type(img))) return img.transpose(Image.FLIP_LEFT_RIGHT)", "def transpose(table): \"\"\" transpose matrix \"\"\" t = [] for i in range(0, len(table[0])): t.append([row[i] for row in table]) return t", "def _swap_rows(self, i, j): \"\"\"Swap i and j rows As the side effect, determinant flips. \"\"\" L = np.eye(3, dtype='intc') L[i, i] = 0 L[j, j] = 0 L[i, j] = 1 L[j, i] = 1 self._L.append(L.copy()) self._A = np.dot(L, self._A)", "def similarity_transformation(rot, mat): \"\"\" R x M x R^-1 \"\"\" return np.dot(rot, np.dot(mat, np.linalg.inv(rot)))"], "rank": 1}
{"nl": "python full name of object from global", "code": ["def get_qualified_name(_object): \"\"\"Return the Fully Qualified Name from an instance or class.\"\"\" module = _object.__module__ if hasattr(_object, '__name__'): _class = _object.__name__ else: _class = _object.__class__.__name__ return module + '.' + _class", "def class_name(obj): \"\"\" Get the name of an object, including the module name if available. \"\"\" name = obj.__name__ module = getattr(obj, '__module__') if module: name = f'{module}.{name}' return name", "def load_object_by_name(object_name): \"\"\"Load an object from a module by name\"\"\" mod_name, attr = object_name.rsplit('.', 1) mod = import_module(mod_name) return getattr(mod, attr)", "def get_member(thing_obj, member_string): \"\"\"Get a member from an object by (string) name\"\"\" mems = {x[0]: x[1] for x in inspect.getmembers(thing_obj)} if member_string in mems: return mems[member_string]", "def _fullname(o): \"\"\"Return the fully-qualified name of a function.\"\"\" return o.__module__ + \".\" + o.__name__ if o.__module__ else o.__name__"], "rank": 5}
{"nl": "check if input is an integer or boolean python", "code": ["def is_int(value): \"\"\"Return `True` if ``value`` is an integer.\"\"\" if isinstance(value, bool): return False try: int(value) return True except (ValueError, TypeError): return False", "def is_integer(value: Any) -> bool: \"\"\"Return true if a value is an integer number.\"\"\" return (isinstance(value, int) and not isinstance(value, bool)) or ( isinstance(value, float) and isfinite(value) and int(value) == value )", "def isnumber(*args): \"\"\"Checks if value is an integer, long integer or float. NOTE: Treats booleans as numbers, where True=1 and False=0. \"\"\" return all(map(lambda c: isinstance(c, int) or isinstance(c, float), args))", "def numberp(v): \"\"\"Return true iff 'v' is a number.\"\"\" return (not(isinstance(v, bool)) and (isinstance(v, int) or isinstance(v, float)))", "def is_int_type(val): \"\"\"Return True if `val` is of integer type.\"\"\" try: # Python 2 return isinstance(val, (int, long)) except NameError: # Python 3 return isinstance(val, int)"], "rank": 2}
{"nl": "storing columns as array python", "code": ["def to_array(self): \"\"\"Convert the table to a structured NumPy array.\"\"\" dt = np.dtype(list(zip(self.labels, (c.dtype for c in self.columns)))) arr = np.empty_like(self.columns[0], dt) for label in self.labels: arr[label] = self[label] return arr", "def from_array(cls, arr): \"\"\"Convert a structured NumPy array into a Table.\"\"\" return cls().with_columns([(f, arr[f]) for f in arr.dtype.names])", "def extract(self): \"\"\" Creates a copy of this tabarray in the form of a numpy ndarray. Useful if you want to do math on array elements, e.g. if you have a subset of the columns that are all numerical, you can construct a numerical matrix and do matrix operations. \"\"\" return np.vstack([self[r] for r in self.dtype.names]).T.squeeze()", "def adapt_array(arr): \"\"\" http://stackoverflow.com/a/31312102/190597 (SoulNibbler) \"\"\" out = io.BytesIO() np.save(out, arr) out.seek(0) return sqlite3.Binary(out.read())", "def _transform_triple_numpy(x): \"\"\"Transform triple index into a 1-D numpy array.\"\"\" return np.array([x.head, x.relation, x.tail], dtype=np.int64)"], "rank": 1}
{"nl": "create column in python by joining columns", "code": ["def join_cols(cols): \"\"\"Join list of columns into a string for a SQL query\"\"\" return \", \".join([i for i in cols]) if isinstance(cols, (list, tuple, set)) else cols", "def get_join_cols(by_entry): \"\"\" helper function used for joins builds left and right join list for join function \"\"\" left_cols = [] right_cols = [] for col in by_entry: if isinstance(col, str): left_cols.append(col) right_cols.append(col) else: left_cols.append(col[0]) right_cols.append(col[1]) return left_cols, right_cols", "def _join(verb): \"\"\" Join helper \"\"\" data = pd.merge(verb.x, verb.y, **verb.kwargs) # Preserve x groups if isinstance(verb.x, GroupedDataFrame): data.plydata_groups = list(verb.x.plydata_groups) return data", "def add_column(connection, column): \"\"\" Add a column to the current table. \"\"\" stmt = alembic.ddl.base.AddColumn(_State.table.name, column) connection.execute(stmt) _State.reflect_metadata()", "def merge(left, right, how='inner', key=None, left_key=None, right_key=None, left_as='left', right_as='right'): \"\"\" Performs a join using the union join function. \"\"\" return join(left, right, how, key, left_key, right_key, join_fn=make_union_join(left_as, right_as))"], "rank": 1}
{"nl": "how to remove blank lines in python file", "code": ["def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line", "def get_stripped_file_lines(filename): \"\"\" Return lines of a file with whitespace removed \"\"\" try: lines = open(filename).readlines() except FileNotFoundError: fatal(\"Could not open file: {!r}\".format(filename)) return [line.strip() for line in lines]", "def lines(input): \"\"\"Remove comments and empty lines\"\"\" for raw_line in input: line = raw_line.strip() if line and not line.startswith('#'): yield strip_comments(line)", "def lint_file(in_file, out_file=None): \"\"\"Helps remove extraneous whitespace from the lines of a file :param file in_file: A readable file or file-like :param file out_file: A writable file or file-like \"\"\" for line in in_file: print(line.strip(), file=out_file)", "def filter_useless_pass(source): \"\"\"Yield code with useless \"pass\" lines removed.\"\"\" try: marked_lines = frozenset(useless_pass_line_numbers(source)) except (SyntaxError, tokenize.TokenError): marked_lines = frozenset() sio = io.StringIO(source) for line_number, line in enumerate(sio.readlines(), start=1): if line_number not in marked_lines: yield line"], "rank": 3}
{"nl": "how to know size of queue in python", "code": ["def qsize(self): \"\"\"Return the approximate size of the queue (not reliable!).\"\"\" self.mutex.acquire() n = self._qsize() self.mutex.release() return n", "def count(self, elem): \"\"\" Return the number of elements equal to elem present in the queue >>> pdeque([1, 2, 1]).count(1) 2 \"\"\" return self._left_list.count(elem) + self._right_list.count(elem)", "def full(self): \"\"\"Return True if the queue is full\"\"\" if not self.size: return False return len(self.pq) == (self.size + self.removed_count)", "def full(self): \"\"\"Return ``True`` if the queue is full, ``False`` otherwise (not reliable!). Only applicable if :attr:`maxsize` is set. \"\"\" return self.maxsize and len(self.list) >= self.maxsize or False", "def ncores_reserved(self): \"\"\" Returns the number of cores reserved in this moment. A core is reserved if it's still not running but we have submitted the task to the queue manager. \"\"\" return sum(task.manager.num_cores for task in self if task.status == task.S_SUB)"], "rank": 1}
{"nl": "python substring index of", "code": ["def _find(string, sub_string, start_index): \"\"\"Return index of sub_string in string. Raise TokenError if sub_string is not found. \"\"\" result = string.find(sub_string, start_index) if result == -1: raise TokenError(\"expected '{0}'\".format(sub_string)) return result", "def fsliceafter(astr, sub): \"\"\"Return the slice after at sub in string astr\"\"\" findex = astr.find(sub) return astr[findex + len(sub):]", "def get_substring_idxs(substr, string): \"\"\" Return a list of indexes of substr. If substr not found, list is empty. Arguments: substr (str): Substring to match. string (str): String to match in. Returns: list of int: Start indices of substr. \"\"\" return [match.start() for match in re.finditer(substr, string)]", "def subat(orig, index, replace): \"\"\"Substitutes the replacement string/character at the given index in the given string, returns the modified string. **Examples**: :: auxly.stringy.subat(\"bit\", 2, \"n\") \"\"\" return \"\".join([(orig[x] if x != index else replace) for x in range(len(orig))])", "def _mid(string, start, end=None): \"\"\" Returns a substring delimited by start and end position. \"\"\" if end is None: end = len(string) return string[start:start + end]"], "rank": 3}
{"nl": "comparing int to none python", "code": ["def strictly_positive_int_or_none(val): \"\"\"Parse `val` into either `None` or a strictly positive integer.\"\"\" val = positive_int_or_none(val) if val is None or val > 0: return val raise ValueError('\"{}\" must be strictly positive'.format(val))", "def is_natural(x): \"\"\"A non-negative integer.\"\"\" try: is_integer = int(x) == x except (TypeError, ValueError): return False return is_integer and x >= 0", "def min_or_none(val1, val2): \"\"\"Returns min(val1, val2) returning None only if both values are None\"\"\" return min(val1, val2, key=lambda x: sys.maxint if x is None else x)", "def int32_to_negative(int32): \"\"\"Checks if a suspicious number (e.g. ligand position) is in fact a negative number represented as a 32 bit integer and returns the actual number. \"\"\" dct = {} if int32 == 4294967295: # Special case in some structures (note, this is just a workaround) return -1 for i in range(-1000, -1): dct[np.uint32(i)] = i if int32 in dct: return dct[int32] else: return int32", "def is_none(string_, default='raise'): \"\"\" Check if a string is equivalent to None. Parameters ---------- string_ : str default : {'raise', False} Default behaviour if none of the \"None\" strings is detected. Returns ------- is_none : bool Examples -------- >>> is_none('2', default=False) False >>> is_none('undefined', default=False) True \"\"\" none = ['none', 'undefined', 'unknown', 'null', ''] if string_.lower() in none: return True elif not default: return False else: raise ValueError('The value \\'{}\\' cannot be mapped to none.' .format(string_))"], "rank": 2}
{"nl": "json to protobuf python", "code": ["def toJson(protoObject, indent=None): \"\"\" Serialises a protobuf object as json \"\"\" # Using the internal method because this way we can reformat the JSON js = json_format.MessageToDict(protoObject, False) return json.dumps(js, indent=indent)", "def _dict_to_proto(py_dict, proto): \"\"\" Converts a python dictionary to the proto supplied :param py_dict: The dictionary to convert :type py_dict: dict :param proto: The proto object to merge with dictionary :type proto: protobuf :return: A parsed python dictionary in provided proto format :raises: ParseError: On JSON parsing problems. \"\"\" dict_json_str = json.dumps(py_dict) return json_format.Parse(dict_json_str, proto)", "def MessageToDict(message, including_default_value_fields=False, preserving_proto_field_name=False): \"\"\"Converts protobuf message to a JSON dictionary. Args: message: The protocol buffers message instance to serialize. including_default_value_fields: If True, singular primitive fields, repeated fields, and map fields will always be serialized. If False, only serialize non-empty fields. Singular message fields and oneof fields are not affected by this option. preserving_proto_field_name: If True, use the original proto field names as defined in the .proto file. If False, convert the field names to lowerCamelCase. Returns: A dict representation of the JSON formatted protocol buffer message. \"\"\" printer = _Printer(including_default_value_fields, preserving_proto_field_name) # pylint: disable=protected-access return printer._MessageToJsonObject(message)", "def AsPrimitiveProto(self): \"\"\"Return an old style protocol buffer object.\"\"\" if self.protobuf: result = self.protobuf() result.ParseFromString(self.SerializeToString()) return result", "def from_pb(cls, pb): \"\"\"Instantiate the object from a protocol buffer. Args: pb (protobuf) Save a reference to the protocol buffer on the object. \"\"\" obj = cls._from_pb(pb) obj._pb = pb return obj"], "rank": 1}
{"nl": "how to join 2 data frames in python", "code": ["def cross_join(df1, df2): \"\"\" Return a dataframe that is a cross between dataframes df1 and df2 ref: https://github.com/pydata/pandas/issues/5401 \"\"\" if len(df1) == 0: return df2 if len(df2) == 0: return df1 # Add as lists so that the new index keeps the items in # the order that they are added together all_columns = pd.Index(list(df1.columns) + list(df2.columns)) df1['key'] = 1 df2['key'] = 1 return pd.merge(df1, df2, on='key').loc[:, all_columns]", "def _join(verb): \"\"\" Join helper \"\"\" data = pd.merge(verb.x, verb.y, **verb.kwargs) # Preserve x groups if isinstance(verb.x, GroupedDataFrame): data.plydata_groups = list(verb.x.plydata_groups) return data", "def intersect(self, other): \"\"\" Return a new :class:`DataFrame` containing rows only in both this frame and another frame. This is equivalent to `INTERSECT` in SQL. \"\"\" return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)", "def merge(left, right, how='inner', key=None, left_key=None, right_key=None, left_as='left', right_as='right'): \"\"\" Performs a join using the union join function. \"\"\" return join(left, right, how, key, left_key, right_key, join_fn=make_union_join(left_as, right_as))", "def get_join_cols(by_entry): \"\"\" helper function used for joins builds left and right join list for join function \"\"\" left_cols = [] right_cols = [] for col in by_entry: if isinstance(col, str): left_cols.append(col) right_cols.append(col) else: left_cols.append(col[0]) right_cols.append(col[1]) return left_cols, right_cols"], "rank": 1}
{"nl": "how to product of a list in python", "code": ["def _cumprod(l): \"\"\"Cumulative product of a list. Args: l: a list of integers Returns: a list with one more element (starting with 1) \"\"\" ret = [1] for item in l: ret.append(ret[-1] * item) return ret", "def combinations(l): \"\"\"Pure-Python implementation of itertools.combinations(l, 2).\"\"\" result = [] for x in xrange(len(l) - 1): ls = l[x + 1:] for y in ls: result.append((l[x], y)) return result", "def multiply(self, number): \"\"\"Return a Vector as the product of the vector and a real number.\"\"\" return self.from_list([x * number for x in self.to_list()])", "def sorted_product_set(array_a, array_b): \"\"\"Compute the product set of array_a and array_b and sort it.\"\"\" return np.sort( np.concatenate( [array_a[i] * array_b for i in xrange(len(array_a))], axis=0) )[::-1]", "def dotproduct(X, Y): \"\"\"Return the sum of the element-wise product of vectors x and y. >>> dotproduct([1, 2, 3], [1000, 100, 10]) 1230 \"\"\" return sum([x * y for x, y in zip(X, Y)])"], "rank": 5}
{"nl": "get all dates between range datetime python", "code": ["def dates_in_range(start_date, end_date): \"\"\"Returns all dates between two dates. Inclusive of the start date but not the end date. Args: start_date (datetime.date) end_date (datetime.date) Returns: (list) of datetime.date objects \"\"\" return [ start_date + timedelta(n) for n in range(int((end_date - start_date).days)) ]", "def daterange(start_date, end_date): \"\"\" Yield one date per day from starting date to ending date. Args: start_date (date): starting date. end_date (date): ending date. Yields: date: a date for each day within the range. \"\"\" for n in range(int((end_date - start_date).days)): yield start_date + timedelta(n)", "def daterange(start, end, delta=timedelta(days=1), lower=Interval.CLOSED, upper=Interval.OPEN): \"\"\"Returns a generator which creates the next value in the range on demand\"\"\" date_interval = Interval(lower=lower, lower_value=start, upper_value=end, upper=upper) current = start if start in date_interval else start + delta while current in date_interval: yield current current = current + delta", "def from_years_range(start_year, end_year): \"\"\"Transform a range of years (two ints) to a DateRange object.\"\"\" start = datetime.date(start_year, 1 , 1) end = datetime.date(end_year, 12 , 31) return DateRange(start, end)", "def last_month(): \"\"\" Return start and end date of this month. \"\"\" since = TODAY + delta(day=1, months=-1) until = since + delta(months=1) return Date(since), Date(until)"], "rank": 1}
{"nl": "python access file on remote", "code": ["def get_remote_content(filepath): \"\"\" A handy wrapper to get a remote file content \"\"\" with hide('running'): temp = BytesIO() get(filepath, temp) content = temp.getvalue().decode('utf-8') return content.strip()", "def s3_get(url: str, temp_file: IO) -> None: \"\"\"Pull a file directly from S3.\"\"\" s3_resource = boto3.resource(\"s3\") bucket_name, s3_path = split_s3_path(url) s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)", "def send_file(self, local_path, remote_path, user='root', unix_mode=None): \"\"\"Upload a local file on the remote host. \"\"\" self.enable_user(user) return self.ssh_pool.send_file(user, local_path, remote_path, unix_mode=unix_mode)", "def download_file(save_path, file_url): \"\"\" Download file from http url link \"\"\" r = requests.get(file_url) # create HTTP response object with open(save_path, 'wb') as f: f.write(r.content) return save_path", "def _download(url): \"\"\"Downloads an URL and returns a file-like object open for reading, compatible with zipping.ZipFile (it has a seek() method). \"\"\" fh = StringIO() for line in get(url): fh.write(line) fh.seek(0) return fh"], "rank": 1}
{"nl": "how to download txt file from internet in python", "code": ["def wget(url): \"\"\" Download the page into a string \"\"\" import urllib.parse request = urllib.request.urlopen(url) filestring = request.read() return filestring", "def url_read_text(url, verbose=True): r\"\"\" Directly reads text data from url \"\"\" data = url_read(url, verbose) text = data.decode('utf8') return text", "def download_file(save_path, file_url): \"\"\" Download file from http url link \"\"\" r = requests.get(file_url) # create HTTP response object with open(save_path, 'wb') as f: f.write(r.content) return save_path", "def get_dates_link(url): \"\"\" download the dates file from the internet and parse it as a dates file\"\"\" urllib.request.urlretrieve(url, \"temp.txt\") dates = get_dates_file(\"temp.txt\") os.remove(\"temp.txt\") return dates", "def get_remote_content(filepath): \"\"\" A handy wrapper to get a remote file content \"\"\" with hide('running'): temp = BytesIO() get(filepath, temp) content = temp.getvalue().decode('utf-8') return content.strip()"], "rank": 4}
{"nl": "stopwords list remove python", "code": ["def _removeStopwords(text_list): \"\"\" Removes stopwords contained in a list of words. :param text_string: A list of strings. :type text_string: list. :returns: The input ``text_list`` with stopwords removed. :rtype: list \"\"\" output_list = [] for word in text_list: if word.lower() not in _stopwords: output_list.append(word) return output_list", "def wordify(text): \"\"\"Generate a list of words given text, removing punctuation. Parameters ---------- text : unicode A piece of english text. Returns ------- words : list List of words. \"\"\" stopset = set(nltk.corpus.stopwords.words('english')) tokens = nltk.WordPunctTokenizer().tokenize(text) return [w for w in tokens if w not in stopset]", "def get_stoplist(language): \"\"\"Returns an built-in stop-list for the language as a set of words.\"\"\" file_path = os.path.join(\"stoplists\", \"%s.txt\" % language) try: stopwords = pkgutil.get_data(\"justext\", file_path) except IOError: raise ValueError( \"Stoplist for language '%s' is missing. \" \"Please use function 'get_stoplists' for complete list of stoplists \" \"and feel free to contribute by your own stoplist.\" % language ) return frozenset(w.decode(\"utf8\").lower() for w in stopwords.splitlines())", "def clean_text_by_sentences(text, language=\"english\", additional_stopwords=None): \"\"\" Tokenizes a given text into sentences, applying filters and lemmatizing them. Returns a SyntacticUnit list. \"\"\" init_textcleanner(language, additional_stopwords) original_sentences = split_sentences(text) filtered_sentences = filter_words(original_sentences) return merge_syntactic_units(original_sentences, filtered_sentences)", "def tokenize_words(self, text): \"\"\"Tokenize an input string into a list of words (with punctuation removed).\"\"\" return [ self.strip_punctuation(word) for word in text.split(' ') if self.strip_punctuation(word) ]"], "rank": 1}
{"nl": "limit on open file handles in python", "code": ["def _increase_file_handle_limit(): \"\"\"Raise the open file handles permitted by the Dusty daemon process and its child processes. The number we choose here needs to be within the OS X default kernel hard limit, which is 10240.\"\"\" logging.info('Increasing file handle limit to {}'.format(constants.FILE_HANDLE_LIMIT)) resource.setrlimit(resource.RLIMIT_NOFILE, (constants.FILE_HANDLE_LIMIT, resource.RLIM_INFINITY))", "def get_known_read_position(fp, buffered=True): \"\"\" Return a position in a file which is known to be read & handled. It assumes a buffered file and streaming processing. \"\"\" buffer_size = io.DEFAULT_BUFFER_SIZE if buffered else 0 return max(fp.tell() - buffer_size, 0)", "def fopen(name, mode='r', buffering=-1): \"\"\"Similar to Python's built-in `open()` function.\"\"\" f = _fopen(name, mode, buffering) return _FileObjectThreadWithContext(f, mode, buffering)", "def is_readable(fp, size=1): \"\"\" Check if the file-like object is readable. :param fp: file-like object :param size: byte size :return: bool \"\"\" read_size = len(fp.read(size)) fp.seek(-read_size, 1) return read_size == size", "def open_file(file, mode): \"\"\"Open a file. :arg file: file-like or path-like object. :arg str mode: ``mode`` argument for :func:`open`. \"\"\" if hasattr(file, \"read\"): return file if hasattr(file, \"open\"): return file.open(mode) return open(file, mode)"], "rank": 1}
{"nl": "determine the longest sentence in corpus in nlp python ocde", "code": ["def get_longest_orf(orfs): \"\"\"Find longest ORF from the given list of ORFs.\"\"\" sorted_orf = sorted(orfs, key=lambda x: len(x['sequence']), reverse=True)[0] return sorted_orf", "def _rank(self, ranking, n): \"\"\" return the first n sentences with highest ranking \"\"\" return nlargest(n, ranking, key=ranking.get)", "def get_longest_line_length(text): \"\"\"Get the length longest line in a paragraph\"\"\" lines = text.split(\"\\n\") length = 0 for i in range(len(lines)): if len(lines[i]) > length: length = len(lines[i]) return length", "def enumerate_chunks (phrase, spacy_nlp): \"\"\" iterate through the noun phrases \"\"\" if (len(phrase) > 1): found = False text = \" \".join([rl.text for rl in phrase]) doc = spacy_nlp(text.strip(), parse=True) for np in doc.noun_chunks: if np.text != text: found = True yield np.text, find_chunk(phrase, np.text.split(\" \")) if not found and all([rl.pos[0] != \"v\" for rl in phrase]): yield text, phrase", "def get_least_distinct_words(vocab, topic_word_distrib, doc_topic_distrib, doc_lengths, n=None): \"\"\" Order the words from `vocab` by \"distinctiveness score\" (Chuang et al. 2012) from least to most distinctive. Optionally only return the `n` least distinctive words. J. Chuang, C. Manning, J. Heer 2012: \"Termite: Visualization Techniques for Assessing Textual Topic Models\" \"\"\" return _words_by_distinctiveness_score(vocab, topic_word_distrib, doc_topic_distrib, doc_lengths, n, least_to_most=True)"], "rank": 1}
{"nl": "python deterministic dictionary printing", "code": ["def pretty_dict(d): \"\"\"Return dictionary d's repr but with the items sorted. >>> pretty_dict({'m': 'M', 'a': 'A', 'r': 'R', 'k': 'K'}) \"{'a': 'A', 'k': 'K', 'm': 'M', 'r': 'R'}\" >>> pretty_dict({z: C, y: B, x: A}) '{x: A, y: B, z: C}' \"\"\" return '{%s}' % ', '.join('%r: %r' % (k, v) for k, v in sorted(d.items(), key=repr))", "def printdict(adict): \"\"\"printdict\"\"\" dlist = list(adict.keys()) dlist.sort() for i in range(0, len(dlist)): print(dlist[i], adict[dlist[i]])", "def prettyprint(d): \"\"\"Print dicttree in Json-like format. keys are sorted \"\"\" print(json.dumps(d, sort_keys=True, indent=4, separators=(\",\" , \": \")))", "def pretty_dict_str(d, indent=2): \"\"\"shows JSON indented representation of d\"\"\" b = StringIO() write_pretty_dict_str(b, d, indent=indent) return b.getvalue()", "def pretty_dict_string(d, indent=0): \"\"\"Pretty output of nested dictionaries. \"\"\" s = '' for key, value in sorted(d.items()): s += ' ' * indent + str(key) if isinstance(value, dict): s += '\\n' + pretty_dict_string(value, indent+1) else: s += '=' + str(value) + '\\n' return s"], "rank": 6}
{"nl": "remove namespace from xml tag python", "code": ["def _strip_namespace(self, xml): \"\"\"strips any namespaces from an xml string\"\"\" p = re.compile(b\"xmlns=*[\\\"\\\"][^\\\"\\\"]*[\\\"\\\"]\") allmatches = p.finditer(xml) for match in allmatches: xml = xml.replace(match.group(), b\"\") return xml", "def strip_xml_namespace(root): \"\"\"Strip out namespace data from an ElementTree. This function is recursive and will traverse all subnodes to the root element @param root: the root element @return: the same root element, minus namespace \"\"\" try: root.tag = root.tag.split('}')[1] except IndexError: pass for element in root.getchildren(): strip_xml_namespace(element)", "def remove_namespaces(root): \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\" for elem in root.getiterator(): if not hasattr(elem.tag, 'find'): continue i = elem.tag.find('}') if i >= 0: elem.tag = elem.tag[i + 1:] objectify.deannotate(root, cleanup_namespaces=True)", "def recClearTag(element): \"\"\"Applies maspy.xml.clearTag() to the tag attribute of the \"element\" and recursively to all child elements. :param element: an :instance:`xml.etree.Element` \"\"\" children = element.getchildren() if len(children) > 0: for child in children: recClearTag(child) element.tag = clearTag(element.tag)", "def GetAttributeNs(self, localName, namespaceURI): \"\"\"Provides the value of the specified attribute \"\"\" ret = libxml2mod.xmlTextReaderGetAttributeNs(self._o, localName, namespaceURI) return ret"], "rank": 2}
{"nl": "fill is null with other columns python", "code": ["def fill_nulls(self, col: str): \"\"\" Fill all null values with NaN values in a column. Null values are ``None`` or en empty string :param col: column name :type col: str :example: ``ds.fill_nulls(\"mycol\")`` \"\"\" n = [None, \"\"] try: self.df[col] = self.df[col].replace(n, nan) except Exception as e: self.err(e)", "def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df", "def inject_nulls(data: Mapping, field_names) -> dict: \"\"\"Insert None as value for missing fields.\"\"\" record = dict() for field in field_names: record[field] = data.get(field, None) return record", "def _maybe_fill(arr, fill_value=np.nan): \"\"\" if we have a compatible fill_value and arr dtype, then fill \"\"\" if _isna_compat(arr, fill_value): arr.fill(fill_value) return arr", "def _none_value(self): \"\"\"Get an appropriate \"null\" value for this field's type. This is used internally when setting the field to None. \"\"\" if self.out_type == int: return 0 elif self.out_type == float: return 0.0 elif self.out_type == bool: return False elif self.out_type == six.text_type: return u''"], "rank": 2}
{"nl": "python async input from gui", "code": ["async def async_input(prompt): \"\"\" Python's ``input()`` is blocking, which means the event loop we set above can't be running while we're blocking there. This method will let the loop run while we wait for input. \"\"\" print(prompt, end='', flush=True) return (await loop.run_in_executor(None, sys.stdin.readline)).rstrip()", "def read_stdin(): \"\"\" Read text from stdin, and print a helpful message for ttys. \"\"\" if sys.stdin.isatty() and sys.stdout.isatty(): print('\\nReading from stdin until end of file (Ctrl + D)...') return sys.stdin.read()", "def _read_stdin(): \"\"\" Generator for reading from standard input in nonblocking mode. Other ways of reading from ``stdin`` in python waits, until the buffer is big enough, or until EOF character is sent. This functions yields immediately after each line. \"\"\" line = sys.stdin.readline() while line: yield line line = sys.stdin.readline()", "def standard_input(): \"\"\"Generator that yields lines from standard input.\"\"\" with click.get_text_stream(\"stdin\") as stdin: while stdin.readable(): line = stdin.readline() if line: yield line.strip().encode(\"utf-8\")", "def enable_gtk3(self, app=None): \"\"\"Enable event loop integration with Gtk3 (gir bindings). Parameters ---------- app : ignored Ignored, it's only a placeholder to keep the call signature of all gui activation methods consistent, which simplifies the logic of supporting magics. Notes ----- This methods sets the PyOS_InputHook for Gtk3, which allows the Gtk3 to integrate with terminal based applications like IPython. \"\"\" from pydev_ipython.inputhookgtk3 import create_inputhook_gtk3 self.set_inputhook(create_inputhook_gtk3(self._stdin_file)) self._current_gui = GUI_GTK"], "rank": 1}
{"nl": "how to identify the index of an element of a set in python", "code": ["def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def find_geom(geom, geoms): \"\"\" Returns the index of a geometry in a list of geometries avoiding expensive equality checks of `in` operator. \"\"\" for i, g in enumerate(geoms): if g is geom: return i", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def elem_find(self, field, value): \"\"\" Return the indices of elements whose field first satisfies the given values ``value`` should be unique in self.field. This function does not check the uniqueness. :param field: name of the supplied field :param value: value of field of the elemtn to find :return: idx of the elements :rtype: list, int, float, str \"\"\" if isinstance(value, (int, float, str)): value = [value] f = list(self.__dict__[field]) uid = np.vectorize(f.index)(value) return self.get_idx(uid)"], "rank": 3}
{"nl": "python 3 print object string", "code": ["def _get_pretty_string(obj): \"\"\"Return a prettier version of obj Parameters ---------- obj : object Object to pretty print Returns ------- s : str Pretty print object repr \"\"\" sio = StringIO() pprint.pprint(obj, stream=sio) return sio.getvalue()", "def pretty(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Pretty print the object's representation. \"\"\" stream = StringIO() printer = RepresentationPrinter(stream, verbose, max_width, newline) printer.pretty(obj) printer.flush() return stream.getvalue()", "def __str__(self): \"\"\"Returns a pretty-printed string for this object.\"\"\" return 'Output name: \"%s\" watts: %d type: \"%s\" id: %d' % ( self._name, self._watts, self._output_type, self._integration_id)", "def pprint(self, stream=None, indent=1, width=80, depth=None): \"\"\" Pretty print the underlying literal Python object \"\"\" pp.pprint(to_literal(self), stream, indent, width, depth)", "def pprint(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Like `pretty` but print to stdout. \"\"\" printer = RepresentationPrinter(sys.stdout, verbose, max_width, newline) printer.pretty(obj) printer.flush() sys.stdout.write(newline) sys.stdout.flush()"], "rank": 1}
{"nl": "python get most recent file containing string", "code": ["def get_last_modified_timestamp(self): \"\"\" Looks at the files in a git root directory and grabs the last modified timestamp \"\"\" cmd = \"find . -print0 | xargs -0 stat -f '%T@ %p' | sort -n | tail -1 | cut -f2- -d' '\" ps = subprocess.Popen(cmd,shell=True,stdout=subprocess.PIPE,stderr=subprocess.STDOUT) output = ps.communicate()[0] print output", "def get_single_file_info(self, rel_path): \"\"\" Gets last change time for a single file \"\"\" f_path = self.get_full_file_path(rel_path) return get_single_file_info(f_path, rel_path)", "def get_last_commit_line(git_path=None): \"\"\" Get one-line description of HEAD commit for repository in current dir. \"\"\" if git_path is None: git_path = GIT_PATH output = check_output([git_path, \"log\", \"--pretty=format:'%ad %h %s'\", \"--date=short\", \"-n1\"]) return output.strip()[1:-1]", "def newest_file(file_iterable): \"\"\" Returns the name of the newest file given an iterable of file names. \"\"\" return max(file_iterable, key=lambda fname: os.path.getmtime(fname))", "def get_last_commit(git_path=None): \"\"\" Get the HEAD commit SHA1 of repository in current dir. \"\"\" if git_path is None: git_path = GIT_PATH line = get_last_commit_line(git_path) revision_id = line.split()[1] return revision_id"], "rank": 4}
{"nl": "python sqlite3 delete doesn't delete records", "code": ["def wipe(self): \"\"\" Wipe the store \"\"\" query = \"DELETE FROM {}\".format(self.__tablename__) connection = sqlite3.connect(self.sqlite_file) cursor = connection.cursor() cursor.execute(query) connection.commit()", "def truncate_table(self, tablename): \"\"\" SQLite3 doesn't support direct truncate, so we just use delete here \"\"\" self.get(tablename).remove() self.db.commit()", "def wipe_table(self, table: str) -> int: \"\"\"Delete all records from a table. Use caution!\"\"\" sql = \"DELETE FROM \" + self.delimit(table) return self.db_exec(sql)", "def clear_table(dbconn, table_name): \"\"\" Delete all rows from a table :param dbconn: data base connection :param table_name: name of the table :return: \"\"\" cur = dbconn.cursor() cur.execute(\"DELETE FROM '{name}'\".format(name=table_name)) dbconn.commit()", "def __del__(self): \"\"\"Deletes the database file.\"\"\" if self._delete_file: try: os.remove(self.name) except (OSError, IOError): pass"], "rank": 2}
{"nl": "python apply function to iterator", "code": ["def map(cls, iterable, func, *a, **kw): \"\"\" Iterable-first replacement of Python's built-in `map()` function. \"\"\" return cls(func(x, *a, **kw) for x in iterable)", "def find_all(self, string, callback): \"\"\" Wrapper on iter method, callback gets an iterator result \"\"\" for index, output in self.iter(string): callback(index, output)", "def __init__(self, function): \"\"\"function: to be called with each stream element as its only argument \"\"\" super(filter, self).__init__() self.function = function", "def fromiterable(cls, itr): \"\"\"Initialize from iterable\"\"\" x, y, z = itr return cls(x, y, z)", "def _izip(*iterables): \"\"\" Iterate through multiple lists or arrays of equal size \"\"\" # This izip routine is from itertools # izip('ABCD', 'xy') --> Ax By iterators = map(iter, iterables) while iterators: yield tuple(map(next, iterators))"], "rank": 2}
{"nl": "use python next to iterate through", "code": ["def __next__(self): \"\"\" :return: int \"\"\" self.current += 1 if self.current > self.total: raise StopIteration else: return self.iterable[self.current - 1]", "def __next__(self, reward, ask_id, lbl): \"\"\"For Python3 compatibility of generator.\"\"\" return self.next(reward, ask_id, lbl)", "def next(self): \"\"\"Get the next value in the page.\"\"\" item = six.next(self._item_iter) result = self._item_to_value(self._parent, item) # Since we've successfully got the next value from the # iterator, we update the number of remaining. self._remaining -= 1 return result", "def next(self): \"\"\"Provides hook for Python2 iterator functionality.\"\"\" _LOGGER.debug(\"reading next\") if self.closed: _LOGGER.debug(\"stream is closed\") raise StopIteration() line = self.readline() if not line: _LOGGER.debug(\"nothing more to read\") raise StopIteration() return line", "def next(self): \"\"\"Retrieve the next row.\"\"\" # I'm pretty sure this is the completely wrong way to go about this, but # oh well, this works. if not hasattr(self, '_iter'): self._iter = self.readrow_as_dict() return self._iter.next()"], "rank": 2}
{"nl": "how to print all the variables in an object python", "code": ["def print_param_values(self_): \"\"\"Print the values of all this object's Parameters.\"\"\" self = self_.self for name,val in self.param.get_param_values(): print('%s.%s = %s' % (self.name,name,val))", "def _repr(obj): \"\"\"Show the received object as precise as possible.\"\"\" vals = \", \".join(\"{}={!r}\".format( name, getattr(obj, name)) for name in obj._attribs) if vals: t = \"{}(name={}, {})\".format(obj.__class__.__name__, obj.name, vals) else: t = \"{}(name={})\".format(obj.__class__.__name__, obj.name) return t", "def var_dump(*obs): \"\"\" shows structured information of a object, list, tuple etc \"\"\" i = 0 for x in obs: str = var_dump_output(x, 0, ' ', '\\n', True) print (str.strip()) #dump(x, 0, i, '', object) i += 1", "def __repr__(self): \"\"\"Return string representation of object.\"\"\" return str(self.__class__) + '(' + ', '.join([list.__repr__(d) for d in self.data]) + ')'", "def prnt(self): \"\"\" Prints DB data representation of the object. \"\"\" print(\"= = = =\\n\\n%s object key: \\033[32m%s\\033[0m\" % (self.__class__.__name__, self.key)) pprnt(self._data or self.clean_value())"], "rank": 3}
{"nl": "how to determine the index interval for given range of array python", "code": ["def _index_range(self, version, symbol, from_version=None, **kwargs): \"\"\" Tuple describing range to read from the ndarray - closed:open \"\"\" from_index = None if from_version: from_index = from_version['up_to'] return from_index, None", "def _interval_to_bound_points(array): \"\"\" Helper function which returns an array with the Intervals' boundaries. \"\"\" array_boundaries = np.array([x.left for x in array]) array_boundaries = np.concatenate( (array_boundaries, np.array([array[-1].right]))) return array_boundaries", "def get_idx_rect(index_list): \"\"\"Extract the boundaries from a list of indexes\"\"\" rows, cols = list(zip(*[(i.row(), i.column()) for i in index_list])) return ( min(rows), max(rows), min(cols), max(cols) )", "def _infer_interval_breaks(coord): \"\"\" >>> _infer_interval_breaks(np.arange(5)) array([-0.5, 0.5, 1.5, 2.5, 3.5, 4.5]) Taken from xarray.plotting.plot module \"\"\" coord = np.asarray(coord) deltas = 0.5 * (coord[1:] - coord[:-1]) first = coord[0] - deltas[0] last = coord[-1] + deltas[-1] return np.r_[[first], coord[:-1] + deltas, [last]]", "def _indexes(arr): \"\"\" Returns the list of all indexes of the given array. Currently works for one and two-dimensional arrays \"\"\" myarr = np.array(arr) if myarr.ndim == 1: return list(range(len(myarr))) elif myarr.ndim == 2: return tuple(itertools.product(list(range(arr.shape[0])), list(range(arr.shape[1])))) else: raise NotImplementedError('Only supporting arrays of dimension 1 and 2 as yet.')"], "rank": 4}
{"nl": "python print numpy array with string", "code": ["def array2string(arr: numpy.ndarray) -> str: \"\"\"Format numpy array as a string.\"\"\" shape = str(arr.shape)[1:-1] if shape.endswith(\",\"): shape = shape[:-1] return numpy.array2string(arr, threshold=11) + \"%s[%s]\" % (arr.dtype, shape)", "def toStringArray(name, a, width = 0): \"\"\" Returns an array (any sequence of floats, really) as a string. \"\"\" string = name + \": \" cnt = 0 for i in a: string += \"%4.2f \" % i if width > 0 and (cnt + 1) % width == 0: string += '\\n' cnt += 1 return string", "def bitsToString(arr): \"\"\"Returns a string representing a numpy array of 0's and 1's\"\"\" s = array('c','.'*len(arr)) for i in xrange(len(arr)): if arr[i] == 1: s[i]='*' return s", "def array_to_npy(array_like): # type: (np.array or Iterable or int or float) -> object \"\"\"Convert an array like object to the NPY format. To understand better what an array like object is see: https://docs.scipy.org/doc/numpy/user/basics.creation.html#converting-python-array-like-objects-to-numpy-arrays Args: array_like (np.array or Iterable or int or float): array like object to be converted to NPY. Returns: (obj): NPY array. \"\"\" buffer = BytesIO() np.save(buffer, array_like) return buffer.getvalue()", "def ndarr2str(arr, encoding='ascii'): \"\"\" This is used to ensure that the return value of arr.tostring() is actually a string. This will prevent lots of if-checks in calling code. As of numpy v1.6.1 (in Python 3.2.3), the tostring() function still returns type 'bytes', not 'str' as it advertises. \"\"\" # be fast, don't check - just assume 'arr' is a numpy array - the tostring # call will fail anyway if not retval = arr.tostring() # would rather check \"if isinstance(retval, bytes)\", but support 2.5. # could rm the if PY3K check, but it makes this faster on 2.x. if PY3K and not isinstance(retval, str): return retval.decode(encoding) else: # is str return retval"], "rank": 1}
{"nl": "python assert value is of type", "code": ["def _assert_is_type(name, value, value_type): \"\"\"Assert that a value must be a given type.\"\"\" if not isinstance(value, value_type): if type(value_type) is tuple: types = ', '.join(t.__name__ for t in value_type) raise ValueError('{0} must be one of ({1})'.format(name, types)) else: raise ValueError('{0} must be {1}' .format(name, value_type.__name__))", "def assert_is_instance(value, types, message=None, extra=None): \"\"\"Raises an AssertionError if value is not an instance of type(s).\"\"\" assert isinstance(value, types), _assert_fail_message( message, value, types, \"is not an instance of\", extra )", "def assert_list(self, putative_list, expected_type=string_types, key_arg=None): \"\"\" :API: public \"\"\" return assert_list(putative_list, expected_type, key_arg=key_arg, raise_type=lambda msg: TargetDefinitionException(self, msg))", "def assert_is_not(expected, actual, message=None, extra=None): \"\"\"Raises an AssertionError if expected is actual.\"\"\" assert expected is not actual, _assert_fail_message( message, expected, actual, \"is\", extra )", "def __call__(self, actual_value, expect): \"\"\"Main entry point for assertions (called by the wrapper). expect is a function the wrapper class uses to assert a given match. \"\"\" self._expect = expect if self.expected_value is NO_ARG: return self.asserts(actual_value) return self.asserts(actual_value, self.expected_value)"], "rank": 1}
{"nl": "python multiproccessing map with multiple inputs", "code": ["def multiprocess_mapping(func, iterable): \"\"\"Multiprocess mapping the given function on the given iterable. This only works in Linux and Mac systems since Windows has no forking capability. On Windows we fall back on single processing. Also, if we reach memory limits we fall back on single cpu processing. Args: func (func): the function to apply iterable (iterable): the iterable with the elements we want to apply the function on \"\"\" if os.name == 'nt': # In Windows there is no fork. return list(map(func, iterable)) try: p = multiprocessing.Pool() return_data = list(p.imap(func, iterable)) p.close() p.join() return return_data except OSError: return list(map(func, iterable))", "def imapchain(*a, **kwa): \"\"\" Like map but also chains the results. \"\"\" imap_results = map( *a, **kwa ) return itertools.chain( *imap_results )", "def mmap(func, iterable): \"\"\"Wrapper to make map() behave the same on Py2 and Py3.\"\"\" if sys.version_info[0] > 2: return [i for i in map(func, iterable)] else: return map(func, iterable)", "def dmap(fn, record): \"\"\"map for a directory\"\"\" values = (fn(v) for k, v in record.items()) return dict(itertools.izip(record, values))", "def map(cls, iterable, func, *a, **kw): \"\"\" Iterable-first replacement of Python's built-in `map()` function. \"\"\" return cls(func(x, *a, **kw) for x in iterable)"], "rank": 2}
{"nl": "python add suffix to filename", "code": ["def add_suffix(fullname, suffix): \"\"\" Add suffix to a full file name\"\"\" name, ext = os.path.splitext(fullname) return name + '_' + suffix + ext", "def filename_addstring(filename, text): \"\"\" Add `text` to filename, keeping the extension in place For example when adding a timestamp to the filename \"\"\" fn, ext = os.path.splitext(filename) return fn + text + ext", "def remove_ext(fname): \"\"\"Removes the extension from a filename \"\"\" bn = os.path.basename(fname) return os.path.splitext(bn)[0]", "def slugify_filename(filename): \"\"\" Slugify filename \"\"\" name, ext = os.path.splitext(filename) slugified = get_slugified_name(name) return slugified + ext", "def lower_ext(abspath): \"\"\"Convert file extension to lowercase. \"\"\" fname, ext = os.path.splitext(abspath) return fname + ext.lower()"], "rank": 1}
{"nl": "best way to deal with pagination in python", "code": ["def items(self, limit=0): \"\"\"Return iterator for items in each page\"\"\" i = ItemIterator(self.iterator) i.limit = limit return i", "def autopage(self): \"\"\"Iterate through results from all pages. :return: all results :rtype: generator \"\"\" while self.items: yield from self.items self.items = self.fetch_next()", "def paginate(self, request, offset=0, limit=None): \"\"\"Paginate queryset.\"\"\" return self.collection.offset(offset).limit(limit), self.collection.count()", "def end_index(self): \"\"\" Returns the 1-based index of the last object on this page, relative to total objects found (hits). \"\"\" return ((self.number - 1) * self.paginator.per_page + len(self.object_list))", "def get_bucket_page(page): \"\"\" Returns all the keys in a s3 bucket paginator page. \"\"\" key_list = page.get('Contents', []) logger.debug(\"Retrieving page with {} keys\".format( len(key_list), )) return dict((k.get('Key'), k) for k in key_list)"], "rank": 3}
{"nl": "finding factors in python and return list", "code": ["def factors(n): \"\"\" Computes all the integer factors of the number `n` Example: >>> # ENABLE_DOCTEST >>> from utool.util_alg import * # NOQA >>> import utool as ut >>> result = sorted(ut.factors(10)) >>> print(result) [1, 2, 5, 10] References: http://stackoverflow.com/questions/6800193/finding-all-the-factors \"\"\" return set(reduce(list.__add__, ([i, n // i] for i in range(1, int(n ** 0.5) + 1) if n % i == 0)))", "def _factor_generator(n): \"\"\" From a given natural integer, returns the prime factors and their multiplicity :param n: Natural integer :return: \"\"\" p = prime_factors(n) factors = {} for p1 in p: try: factors[p1] += 1 except KeyError: factors[p1] = 1 return factors", "def getPrimeFactors(n): \"\"\" Get all the prime factor of given integer @param n integer @return list [1, ..., n] \"\"\" lo = [1] n2 = n // 2 k = 2 for k in range(2, n2 + 1): if (n // k)*k == n: lo.append(k) return lo + [n, ]", "def factorial(n, mod=None): \"\"\"Calculates factorial iteratively. If mod is not None, then return (n! % mod) Time Complexity - O(n)\"\"\" if not (isinstance(n, int) and n >= 0): raise ValueError(\"'n' must be a non-negative integer.\") if mod is not None and not (isinstance(mod, int) and mod > 0): raise ValueError(\"'mod' must be a positive integer\") result = 1 if n == 0: return 1 for i in range(2, n+1): result *= i if mod: result %= mod return result", "def computeFactorial(n): \"\"\" computes factorial of n \"\"\" sleep_walk(10) ret = 1 for i in range(n): ret = ret * (i + 1) return ret"], "rank": 1}
{"nl": "best way to read xml in python", "code": ["def xmltreefromfile(filename): \"\"\"Internal function to read an XML file\"\"\" try: return ElementTree.parse(filename, ElementTree.XMLParser(collect_ids=False)) except TypeError: return ElementTree.parse(filename, ElementTree.XMLParser())", "def xml(cls, res, *args, **kwargs): \"\"\"Parses XML from a response.\"\"\" return parse_xml(res.text, *args, **kwargs)", "def schemaParse(self): \"\"\"parse a schema definition resource and build an internal XML Shema struture which can be used to validate instances. \"\"\" ret = libxml2mod.xmlSchemaParse(self._o) if ret is None:raise parserError('xmlSchemaParse() failed') __tmp = Schema(_obj=ret) return __tmp", "def from_file(cls, file_path, validate=True): \"\"\" Creates a Python object from a XML file :param file_path: Path to the XML file :param validate: XML should be validated against the embedded XSD definition :type validate: Boolean :returns: the Python object \"\"\" return xmlmap.load_xmlobject_from_file(file_path, xmlclass=cls, validate=validate)", "def __get_xml_text(root): \"\"\" Return the text for the given root node (xml.dom.minidom). \"\"\" txt = \"\" for e in root.childNodes: if (e.nodeType == e.TEXT_NODE): txt += e.data return txt"], "rank": 1}
{"nl": "how to put a string in a yaml file python", "code": ["def generate_yaml_file(filename, contents): \"\"\"Creates a yaml file with the given content.\"\"\" with open(filename, 'w') as file: file.write(yaml.dump(contents, default_flow_style=False))", "def generate_write_yaml_to_file(file_name): \"\"\" generate a method to write the configuration in yaml to the method desired \"\"\" def write_yaml(config): with open(file_name, 'w+') as fh: fh.write(yaml.dump(config)) return write_yaml", "def serialize_yaml_tofile(filename, resource): \"\"\" Serializes a K8S resource to YAML-formatted file. \"\"\" stream = file(filename, \"w\") yaml.dump(resource, stream, default_flow_style=False)", "def loads(cls, s): \"\"\" Load an instance of this class from YAML. \"\"\" with closing(StringIO(s)) as fileobj: return cls.load(fileobj)", "def load_yaml_file(file_path: str): \"\"\"Load a YAML file from path\"\"\" with codecs.open(file_path, 'r') as f: return yaml.safe_load(f)"], "rank": 1}
{"nl": "python move cursor to secific line", "code": ["def go_to_line(self, line): \"\"\" Moves the text cursor to given line. :param line: Line to go to. :type line: int :return: Method success. :rtype: bool \"\"\" cursor = self.textCursor() cursor.setPosition(self.document().findBlockByNumber(line - 1).position()) self.setTextCursor(cursor) return True", "def _go_to_line(editor, line): \"\"\" Move cursor to this line in the current buffer. \"\"\" b = editor.application.current_buffer b.cursor_position = b.document.translate_row_col_to_index(max(0, int(line) - 1), 0)", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def backspace(self): \"\"\" Moves the cursor one place to the left, erasing the character at the current position. Cannot move beyond column zero, nor onto the previous line. \"\"\" if self._cx + self._cw >= 0: self.erase() self._cx -= self._cw self.flush()", "def move_up(lines=1, file=sys.stdout): \"\"\" Move the cursor up a number of lines. Esc[ValueA: Moves the cursor up by the specified number of lines without changing columns. If the cursor is already on the top line, ANSI.SYS ignores this sequence. \"\"\" move.up(lines).write(file=file)"], "rank": 2}
{"nl": "token to id python", "code": ["def strids2ids(tokens: Iterable[str]) -> List[int]: \"\"\" Returns sequence of integer ids given a sequence of string ids. :param tokens: List of integer tokens. :return: List of word ids. \"\"\" return list(map(int, tokens))", "def word_to_id(self, word): \"\"\"Returns the integer word id of a word string.\"\"\" if word in self.vocab: return self.vocab[word] else: return self.unk_id", "def generate_user_token(self, user, salt=None): \"\"\"Generates a unique token associated to the user \"\"\" return self.token_serializer.dumps(str(user.id), salt=salt)", "def aws_to_unix_id(aws_key_id): \"\"\"Converts a AWS Key ID into a UID\"\"\" uid_bytes = hashlib.sha256(aws_key_id.encode()).digest()[-2:] if USING_PYTHON2: return 2000 + int(from_bytes(uid_bytes) // 2) else: return 2000 + (int.from_bytes(uid_bytes, byteorder=sys.byteorder) // 2)", "def normalize_job_id(job_id): \"\"\" Convert a value to a job id. :param job_id: Value to convert. :type job_id: int, str :return: The job id. :rtype: :py:class:`uuid.UUID` \"\"\" if not isinstance(job_id, uuid.UUID): job_id = uuid.UUID(job_id) return job_id"], "rank": 1}
{"nl": "python urlparse get domain", "code": ["def get_domain(url): \"\"\" Get domain part of an url. For example: https://www.python.org/doc/ -> https://www.python.org \"\"\" parse_result = urlparse(url) domain = \"{schema}://{netloc}\".format( schema=parse_result.scheme, netloc=parse_result.netloc) return domain", "def parse_domain(url): \"\"\" parse the domain from the url \"\"\" domain_match = lib.DOMAIN_REGEX.match(url) if domain_match: return domain_match.group()", "def url_host(url: str) -> str: \"\"\" Parses hostname from URL. :param url: URL :return: hostname \"\"\" from urllib.parse import urlparse res = urlparse(url) return res.netloc.split(':')[0] if res.netloc else ''", "def get_site_name(request): \"\"\"Return the domain:port part of the URL without scheme. Eg: facebook.com, 127.0.0.1:8080, etc. \"\"\" urlparts = request.urlparts return ':'.join([urlparts.hostname, str(urlparts.port)])", "def top_level(url, fix_protocol=True): \"\"\"Extract the top level domain from an URL.\"\"\" ext = tld.get_tld(url, fix_protocol=fix_protocol) toplevel = '.'.join(urlparse(url).netloc.split('.')[-2:]).split( ext)[0] + ext return toplevel"], "rank": 1}
{"nl": "python turn a string into a number", "code": ["def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)", "def try_cast_int(s): \"\"\"(str) -> int All the digits in a given string are concatenated and converted into a single number. \"\"\" try: temp = re.findall('\\d', str(s)) temp = ''.join(temp) return int(temp) except: return s", "def str_to_num(str_value): \"\"\"Convert str_value to an int or a float, depending on the numeric value represented by str_value. \"\"\" str_value = str(str_value) try: return int(str_value) except ValueError: return float(str_value)", "def covstr(s): \"\"\" convert string to int or float. \"\"\" try: ret = int(s) except ValueError: ret = float(s) return ret", "def str2int(num, radix=10, alphabet=BASE85): \"\"\"helper function for quick base conversions from strings to integers\"\"\" return NumConv(radix, alphabet).str2int(num)"], "rank": 1}
{"nl": "python check if all are type string in a column", "code": ["def is_sqlatype_string(coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type a string type? \"\"\" coltype = _coltype_to_typeengine(coltype) return isinstance(coltype, sqltypes.String)", "def is_sqlatype_text_over_one_char( coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type a string type that's more than one character long? \"\"\" coltype = _coltype_to_typeengine(coltype) return is_sqlatype_text_of_length_at_least(coltype, 2)", "def is_string_dtype(arr_or_dtype): \"\"\" Check whether the provided array or dtype is of the string dtype. Parameters ---------- arr_or_dtype : array-like The array or dtype to check. Returns ------- boolean Whether or not the array or dtype is of the string dtype. Examples -------- >>> is_string_dtype(str) True >>> is_string_dtype(object) True >>> is_string_dtype(int) False >>> >>> is_string_dtype(np.array(['a', 'b'])) True >>> is_string_dtype(pd.Series([1, 2])) False \"\"\" # TODO: gh-15585: consider making the checks stricter. def condition(dtype): return dtype.kind in ('O', 'S', 'U') and not is_period_dtype(dtype) return _is_dtype(arr_or_dtype, condition)", "def _isstring(dtype): \"\"\"Given a numpy dtype, determines whether it is a string. Returns True if the dtype is string or unicode. \"\"\" return dtype.type == numpy.unicode_ or dtype.type == numpy.string_", "def is_sqlatype_numeric(coltype: Union[TypeEngine, VisitableType]) -> bool: \"\"\" Is the SQLAlchemy column type one that inherits from :class:`Numeric`, such as :class:`Float`, :class:`Decimal`? \"\"\" coltype = _coltype_to_typeengine(coltype) return isinstance(coltype, sqltypes.Numeric)"], "rank": 1}
{"nl": "bin means python numpy", "code": ["def to_bin(data, width): \"\"\" Convert an unsigned integer to a numpy binary array with the first element the MSB and the last element the LSB. \"\"\" data_str = bin(data & (2**width-1))[2:].zfill(width) return [int(x) for x in tuple(data_str)]", "def getEdges(npArr): \"\"\"get np array of bin edges\"\"\" edges = np.concatenate(([0], npArr[:,0] + npArr[:,2])) return np.array([Decimal(str(i)) for i in edges])", "def abs_img(img): \"\"\" Return an image with the binarised version of the data of `img`.\"\"\" bool_img = np.abs(read_img(img).get_data()) return bool_img.astype(int)", "def val_to_bin(edges, x): \"\"\"Convert axis coordinate to bin index.\"\"\" ibin = np.digitize(np.array(x, ndmin=1), edges) - 1 return ibin", "def bytesize(arr): \"\"\" Returns the memory byte size of a Numpy array as an integer. \"\"\" byte_size = np.prod(arr.shape) * np.dtype(arr.dtype).itemsize return byte_size"], "rank": 4}
{"nl": "python boto3 delete key from s3", "code": ["def remove_file_from_s3(awsclient, bucket, key): \"\"\"Remove a file from an AWS S3 bucket. :param awsclient: :param bucket: :param key: :return: \"\"\" client_s3 = awsclient.get_client('s3') response = client_s3.delete_object(Bucket=bucket, Key=key)", "def delete_s3_bucket(client, resource): \"\"\"Delete an S3 bucket This function will try to delete an S3 bucket Args: client (:obj:`boto3.session.Session.client`): A boto3 client object resource (:obj:`Resource`): The resource object to terminate Returns: `ActionStatus` \"\"\" if dbconfig.get('enable_delete_s3_buckets', NS_AUDITOR_REQUIRED_TAGS, False): client.delete_bucket(Bucket=resource.id) return ActionStatus.SUCCEED, resource.metrics()", "def get_key(self, key, bucket_name=None): \"\"\" Returns a boto3.s3.Object :param key: the path to the key :type key: str :param bucket_name: the name of the bucket :type bucket_name: str \"\"\" if not bucket_name: (bucket_name, key) = self.parse_s3_url(key) obj = self.get_resource_type('s3').Object(bucket_name, key) obj.load() return obj", "def read_key(self, key, bucket_name=None): \"\"\" Reads a key from S3 :param key: S3 key that will point to the file :type key: str :param bucket_name: Name of the bucket in which the file is stored :type bucket_name: str \"\"\" obj = self.get_key(key, bucket_name) return obj.get()['Body'].read().decode('utf-8')", "def delete_entry(self, key): \"\"\"Delete an object from the redis table\"\"\" pipe = self.client.pipeline() pipe.srem(self.keys_container, key) pipe.delete(key) pipe.execute()"], "rank": 1}
{"nl": "random walk steps python", "code": ["def returned(n): \"\"\"Generate a random walk and return True if the walker has returned to the origin after taking `n` steps. \"\"\" ## `takei` yield lazily so we can short-circuit and avoid computing the rest of the walk for pos in randwalk() >> drop(1) >> takei(xrange(n-1)): if pos == Origin: return True return False", "def computeFactorial(n): \"\"\" computes factorial of n \"\"\" sleep_walk(10) ret = 1 for i in range(n): ret = ret * (i + 1) return ret", "def stepBy(self, steps): \"\"\"steps value up/down by a single step. Single step is defined in singleStep(). Args: steps (int): positiv int steps up, negativ steps down \"\"\" self.setValue(self.value() + steps*self.singleStep())", "def getRandomBinaryTreeLeafNode(binaryTree): \"\"\"Get random binary tree node. \"\"\" if binaryTree.internal == True: if random.random() > 0.5: return getRandomBinaryTreeLeafNode(binaryTree.left) else: return getRandomBinaryTreeLeafNode(binaryTree.right) else: return binaryTree", "def __call__(self, _): \"\"\"Print the current iteration.\"\"\" if self.iter % self.step == 0: print(self.fmt.format(self.iter), **self.kwargs) self.iter += 1"], "rank": 1}
{"nl": "python delete objects inside of objects", "code": ["def _removeTags(tags, objects): \"\"\" Removes tags from objects \"\"\" for t in tags: for o in objects: o.tags.remove(t) return True", "def cleanup(self): \"\"\"Forcefully delete objects from memory In an ideal world, this shouldn't be necessary. Garbage collection guarantees that anything without reference is automatically removed. However, because this application is designed to be run multiple times from the same interpreter process, extra case must be taken to ensure there are no memory leaks. Explicitly deleting objects shines a light on where objects may still be referenced in the form of an error. No errors means this was uneccesary, but that's ok. \"\"\" for instance in self.context: del(instance) for plugin in self.plugins: del(plugin)", "def del_object_from_parent(self): \"\"\" Delete object from parent object. \"\"\" if self.parent: self.parent.objects.pop(self.ref)", "def detach_all(self): \"\"\" Detach from all tracked classes and objects. Restore the original constructors and cleanse the tracking lists. \"\"\" self.detach_all_classes() self.objects.clear() self.index.clear() self._keepalive[:] = []", "def delete(self): \"\"\"Remove this object.\"\"\" self._client.remove_object(self._instance, self._bucket, self.name)"], "rank": 1}
{"nl": "truncate seconds from a timestamp in python code", "code": ["def RoundToSeconds(cls, timestamp): \"\"\"Takes a timestamp value and rounds it to a second precision.\"\"\" leftovers = timestamp % definitions.MICROSECONDS_PER_SECOND scrubbed = timestamp - leftovers rounded = round(float(leftovers) / definitions.MICROSECONDS_PER_SECOND) return int(scrubbed + rounded * definitions.MICROSECONDS_PER_SECOND)", "def datetime_to_timestamp(dt): \"\"\"Convert a UTC datetime to a Unix timestamp\"\"\" delta = dt - datetime.utcfromtimestamp(0) return delta.seconds + delta.days * 24 * 3600", "def dt2ts(dt): \"\"\"Converts to float representing number of seconds since 1970-01-01 GMT.\"\"\" # Note: no assertion to really keep this fast assert isinstance(dt, (datetime.datetime, datetime.date)) ret = time.mktime(dt.timetuple()) if isinstance(dt, datetime.datetime): ret += 1e-6 * dt.microsecond return ret", "def clip_to_seconds(m: Union[int, pd.Series]) -> Union[int, pd.Series]: \"\"\"Clips UTC datetime in nanoseconds to seconds.\"\"\" return m // pd.Timedelta(1, unit='s').value", "def ms_to_datetime(ms): \"\"\" Converts a millisecond accuracy timestamp to a datetime \"\"\" dt = datetime.datetime.utcfromtimestamp(ms / 1000) return dt.replace(microsecond=(ms % 1000) * 1000).replace(tzinfo=pytz.utc)"], "rank": 1}
{"nl": "python json load unorde", "code": ["def load_from_file(cls, file_path: str): \"\"\" Read and reconstruct the data from a JSON file. \"\"\" with open(file_path, \"r\") as f: data = json.load(f) item = cls.decode(data=data) return item", "def load(cls, fp, **kwargs): \"\"\"wrapper for :py:func:`json.load`\"\"\" json_obj = json.load(fp, **kwargs) return parse(cls, json_obj)", "def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def read_json(location): \"\"\"Open and load JSON from file. location (Path): Path to JSON file. RETURNS (dict): Loaded JSON content. \"\"\" location = ensure_path(location) with location.open('r', encoding='utf8') as f: return ujson.load(f)", "def load(raw_bytes): \"\"\" given a bytes object, should return a base python data structure that represents the object. \"\"\" try: if not isinstance(raw_bytes, string_type): raw_bytes = raw_bytes.decode() return json.loads(raw_bytes) except ValueError as e: raise SerializationException(str(e))"], "rank": 4}
{"nl": "python yaml expected single document", "code": ["def ParseMany(text): \"\"\"Parses many YAML documents into a list of Python objects. Args: text: A YAML source with multiple documents embedded. Returns: A list of Python data structures corresponding to the YAML documents. \"\"\" precondition.AssertType(text, Text) if compatibility.PY2: text = text.encode(\"utf-8\") return list(yaml.safe_load_all(text))", "def yaml_to_param(obj, name): \"\"\" Return the top-level element of a document sub-tree containing the YAML serialization of a Python object. \"\"\" return from_pyvalue(u\"yaml:%s\" % name, unicode(yaml.dump(obj)))", "def _ParseYamlFromFile(filedesc): \"\"\"Parses given YAML file.\"\"\" content = filedesc.read() return yaml.Parse(content) or collections.OrderedDict()", "def load_yaml(file): \"\"\"If pyyaml > 5.1 use full_load to avoid warning\"\"\" if hasattr(yaml, \"full_load\"): return yaml.full_load(file) else: return yaml.load(file)", "def getYamlDocument(filePath): \"\"\" Return a yaml file's contents as a dictionary \"\"\" with open(filePath) as stream: doc = yaml.load(stream) return doc"], "rank": 2}
{"nl": "python filter object at", "code": ["def filter_(stream_spec, filter_name, *args, **kwargs): \"\"\"Alternate name for ``filter``, so as to not collide with the built-in python ``filter`` operator. \"\"\" return filter(stream_spec, filter_name, *args, **kwargs)", "def __init__(self): \"\"\"Initializes a filter object.\"\"\" super(FilterObject, self).__init__() self._filter_expression = None self._matcher = None", "def filter(self, obj, *args, **kwargs): \"\"\" Filter the given object through the filter chain. :param obj: The object to filter :param args: Additional arguments to pass to each filter function. :param kwargs: Additional keyword arguments to pass to each filter function. :return: The filtered object or :data:`None` See the documentation of :class:`Filter` on how filtering operates. Returns the object returned by the last function in the filter chain or :data:`None` if any function returned :data:`None`. \"\"\" for _, _, func in self._filter_order: obj = func(obj, *args, **kwargs) if obj is None: return None return obj", "def __init__(self, function): \"\"\"function: to be called with each stream element as its only argument \"\"\" super(filter, self).__init__() self.function = function", "def filter(self, f, operator=\"and\"): \"\"\" Add a filter to the query Takes a Filter object, or a filterable DSL object. \"\"\" if self._filtered: self._filter_dsl.filter(f) else: self._build_filtered_query(f, operator) return self"], "rank": 4}
{"nl": "python nonetype object has no attributte", "code": ["def set_attrs(self): \"\"\" set our object attributes \"\"\" self.attrs.encoding = self.encoding self.attrs.errors = self.errors", "def validate_type(self, type_): \"\"\"Take an str/unicode `type_` and raise a ValueError if it's not a valid type for the object. A valid type for a field is a value from the types_set attribute of that field's class. \"\"\" if type_ is not None and type_ not in self.types_set: raise ValueError('Invalid type for %s:%s' % (self.__class__, type_))", "def get_type(self): \"\"\"Get the type of the item. :return: the type of the item. :returntype: `unicode`\"\"\" item_type = self.xmlnode.prop(\"type\") if not item_type: item_type = \"?\" return item_type.decode(\"utf-8\")", "def listlike(obj): \"\"\"Is an object iterable like a list (and not a string)?\"\"\" return hasattr(obj, \"__iter__\") \\ and not issubclass(type(obj), str)\\ and not issubclass(type(obj), unicode)", "def is_iterable(etype) -> bool: \"\"\" Determine whether etype is a List or other iterable \"\"\" return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)"], "rank": 4}
{"nl": "python tkinter how to create scrollable canvas", "code": ["def set_scrollregion(self, event=None): \"\"\" Set the scroll region on the canvas\"\"\" self.canvas.configure(scrollregion=self.canvas.bbox('all'))", "def __grid_widgets(self): \"\"\"Places all the child widgets in the appropriate positions.\"\"\" scrollbar_column = 0 if self.__compound is tk.LEFT else 2 self._canvas.grid(row=0, column=1, sticky=\"nswe\") self._scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")", "def _set_scroll_v(self, *args): \"\"\"Scroll both categories Canvas and scrolling container\"\"\" self._canvas_categories.yview(*args) self._canvas_scroll.yview(*args)", "def place(self): \"\"\"Place this container's canvas onto the parent container's canvas.\"\"\" self.place_children() self.canvas.append(self.parent.canvas, float(self.left), float(self.top))", "def build_gui(self, container): \"\"\" This is responsible for building the viewer's UI. It should place the UI in `container`. Override this to make a custom UI. \"\"\" vbox = Widgets.VBox() vbox.set_border_width(0) w = Viewers.GingaViewerWidget(viewer=self) vbox.add_widget(w, stretch=1) # need to put this in an hbox with an expanding label or the # browser wants to resize the canvas, distorting it hbox = Widgets.HBox() hbox.add_widget(vbox, stretch=0) hbox.add_widget(Widgets.Label(''), stretch=1) container.set_widget(hbox)"], "rank": 3}
{"nl": "geojson to topojson python", "code": ["def to_topojson(self): \"\"\"Adds points and converts to topojson string.\"\"\" topojson = self.topojson topojson[\"objects\"][\"points\"] = { \"type\": \"GeometryCollection\", \"geometries\": [point.to_topojson() for point in self.points.all()], } return json.dumps(topojson)", "def _loadfilepath(self, filepath, **kwargs): \"\"\"This loads a geojson file into a geojson python dictionary using the json module. Note: to load with a different text encoding use the encoding argument. \"\"\" with open(filepath, \"r\") as f: data = json.load(f, **kwargs) return data", "def CreateVertices(self, points): \"\"\" Returns a dictionary object with keys that are 2tuples represnting a point. \"\"\" gr = digraph() for z, x, Q in points: node = (z, x, Q) gr.add_nodes([node]) return gr", "def graphql_queries_to_json(*queries): \"\"\" Queries should be a list of GraphQL objects \"\"\" rtn = {} for i, query in enumerate(queries): rtn[\"q{}\".format(i)] = query.value return json.dumps(rtn)", "def geodetic_to_ecef(latitude, longitude, altitude): \"\"\"Convert WGS84 geodetic coordinates into ECEF Parameters ---------- latitude : float or array_like Geodetic latitude (degrees) longitude : float or array_like Geodetic longitude (degrees) altitude : float or array_like Geodetic Height (km) above WGS84 reference ellipsoid. Returns ------- x, y, z numpy arrays of x, y, z locations in km \"\"\" ellip = np.sqrt(1. - earth_b ** 2 / earth_a ** 2) r_n = earth_a / np.sqrt(1. - ellip ** 2 * np.sin(np.deg2rad(latitude)) ** 2) # colatitude = 90. - latitude x = (r_n + altitude) * np.cos(np.deg2rad(latitude)) * np.cos(np.deg2rad(longitude)) y = (r_n + altitude) * np.cos(np.deg2rad(latitude)) * np.sin(np.deg2rad(longitude)) z = (r_n * (1. - ellip ** 2) + altitude) * np.sin(np.deg2rad(latitude)) return x, y, z"], "rank": 1}
{"nl": "python get png image dimensions", "code": ["def get_image_dimension(self, url): \"\"\" Return a tuple that contains (width, height) Pass in a url to an image and find out its size without loading the whole file If the image wxh could not be found, the tuple will contain `None` values \"\"\" w_h = (None, None) try: if url.startswith('//'): url = 'http:' + url data = requests.get(url).content im = Image.open(BytesIO(data)) w_h = im.size except Exception: logger.warning(\"Error getting image size {}\".format(url), exc_info=True) return w_h", "def get_shape(img): \"\"\"Return the shape of img. Paramerers ----------- img: Returns ------- shape: tuple \"\"\" if hasattr(img, 'shape'): shape = img.shape else: shape = img.get_data().shape return shape", "def get_combined_size(tiles): \"\"\"Calculate combined size of tiles.\"\"\" # TODO: Refactor calculating layout to avoid repetition. columns, rows = calc_columns_rows(len(tiles)) tile_size = tiles[0].image.size return (tile_size[0] * columns, tile_size[1] * rows)", "def calculate_dimensions(image, long_side, short_side): \"\"\"Returns the thumbnail dimensions depending on the images format.\"\"\" if image.width >= image.height: return '{0}x{1}'.format(long_side, short_side) return '{0}x{1}'.format(short_side, long_side)", "def horz_dpi(self): \"\"\" Integer dots per inch for the width of this image. Defaults to 72 when not present in the file, as is often the case. \"\"\" pHYs = self._chunks.pHYs if pHYs is None: return 72 return self._dpi(pHYs.units_specifier, pHYs.horz_px_per_unit)"], "rank": 6}
{"nl": "python flask separate functions get and post", "code": ["def handleFlaskPostRequest(flaskRequest, endpoint): \"\"\" Handles the specified flask request for one of the POST URLS Invokes the specified endpoint to generate a response. \"\"\" if flaskRequest.method == \"POST\": return handleHttpPost(flaskRequest, endpoint) elif flaskRequest.method == \"OPTIONS\": return handleHttpOptions() else: raise exceptions.MethodNotAllowedException()", "def _post(self, url, params, uploads=None): \"\"\" Wrapper method for POST calls. \"\"\" self._call(self.POST, url, params, uploads)", "def getFlaskResponse(responseString, httpStatus=200): \"\"\" Returns a Flask response object for the specified data and HTTP status. \"\"\" return flask.Response(responseString, status=httpStatus, mimetype=MIMETYPE)", "def post(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP POST operation to API endpoint.\"\"\" return self._request('POST', endpoint, **kwargs)", "def POST(self, *args, **kwargs): \"\"\" POST request \"\"\" return self._handle_api(self.API_POST, args, kwargs)"], "rank": 1}
{"nl": "return year from date python", "code": ["def year(date): \"\"\" Returns the year. :param date: The string date with this format %m/%d/%Y :type date: String :returns: int :example: >>> year('05/1/2015') 2015 \"\"\" try: fmt = '%m/%d/%Y' return datetime.strptime(date, fmt).timetuple().tm_year except ValueError: return 0", "def get_year_start(day=None): \"\"\"Returns January 1 of the given year.\"\"\" day = add_timezone(day or datetime.date.today()) return day.replace(month=1).replace(day=1)", "def today(year=None): \"\"\"this day, last year\"\"\" return datetime.date(int(year), _date.month, _date.day) if year else _date", "def int_to_date(date): \"\"\" Convert an int of form yyyymmdd to a python date object. \"\"\" year = date // 10**4 month = date % 10**4 // 10**2 day = date % 10**2 return datetime.date(year, month, day)", "def datetime_to_year_quarter(dt): \"\"\" Args: dt: a datetime Returns: tuple of the datetime's year and quarter \"\"\" year = dt.year quarter = int(math.ceil(float(dt.month)/3)) return (year, quarter)"], "rank": 1}
{"nl": "how to get all modes python", "code": ["def __iter__(self): \"\"\" Returns the list of modes. :return: \"\"\" return iter([v for k, v in sorted(self._modes.items())])", "def values(self): \"\"\"return a list of all state values\"\"\" values = [] for __, data in self.items(): values.append(data) return values", "def items(cls): \"\"\" All values for this enum :return: list of tuples \"\"\" return [ cls.PRECIPITATION, cls.WIND, cls.TEMPERATURE, cls.PRESSURE ]", "def fields(self): \"\"\"Returns the list of field names of the model.\"\"\" return (self.attributes.values() + self.lists.values() + self.references.values())", "def __dir__(self): u\"\"\"Returns a list of children and available helper methods.\"\"\" return sorted(self.keys() | {m for m in dir(self.__class__) if m.startswith('to_')})"], "rank": 1}
{"nl": "integer and returns a random bitstring of size python", "code": ["def binary(length): \"\"\" returns a a random string that represent a binary representation :param length: number of bits \"\"\" num = randint(1, 999999) mask = '0' * length return (mask + ''.join([str(num >> i & 1) for i in range(7, -1, -1)]))[-length:]", "def random_int(maximum_value): \"\"\" Random generator (PyCrypto getrandbits wrapper). The result is a non-negative value. :param maximum_value: maximum integer value :return: int \"\"\" if maximum_value == 0: return 0 elif maximum_value == 1: return random_bits(1) bits = math.floor(math.log2(maximum_value)) result = random_bits(bits) + random_int(maximum_value - ((2 ** bits) - 1)) return result", "def rndstr(size=16): \"\"\" Returns a string of random ascii characters or digits :param size: The length of the string :return: string \"\"\" _basech = string.ascii_letters + string.digits return \"\".join([rnd.choice(_basech) for _ in range(size)])", "def random_str(size=10): \"\"\" create random string of selected size :param size: int, length of the string :return: the string \"\"\" return ''.join(random.choice(string.ascii_lowercase) for _ in range(size))", "def qrandom(n): \"\"\" Creates an array of n true random numbers obtained from the quantum random number generator at qrng.anu.edu.au This function requires the package quantumrandom and an internet connection. Args: n (int): length of the random array Return: array of ints: array of truly random unsigned 16 bit int values \"\"\" import quantumrandom return np.concatenate([ quantumrandom.get_data(data_type='uint16', array_length=1024) for i in range(int(np.ceil(n/1024.0))) ])[:n]"], "rank": 1}
{"nl": "how to get the encoding of a file python", "code": ["def get_encoding(binary): \"\"\"Return the encoding type.\"\"\" try: from chardet import detect except ImportError: LOGGER.error(\"Please install the 'chardet' module\") sys.exit(1) encoding = detect(binary).get('encoding') return 'iso-8859-1' if encoding == 'CP949' else encoding", "def detect(filename, include_confidence=False): \"\"\" Detect the encoding of a file. Returns only the predicted current encoding as a string. If `include_confidence` is True, Returns tuple containing: (str encoding, float confidence) \"\"\" f = open(filename) detection = chardet.detect(f.read()) f.close() encoding = detection.get('encoding') confidence = detection.get('confidence') if include_confidence: return (encoding, confidence) return encoding", "def smartread(path): \"\"\"Read text from file, automatically detect encoding. ``chardet`` required. \"\"\" with open(path, \"rb\") as f: content = f.read() result = chardet.detect(content) return content.decode(result[\"encoding\"])", "def get_best_encoding(stream): \"\"\"Returns the default stream encoding if not found.\"\"\" rv = getattr(stream, 'encoding', None) or sys.getdefaultencoding() if is_ascii_encoding(rv): return 'utf-8' return rv", "def open_with_encoding(filename, encoding, mode='r'): \"\"\"Return opened file with a specific encoding.\"\"\" return io.open(filename, mode=mode, encoding=encoding, newline='')"], "rank": 3}
{"nl": "python connect to aws rds", "code": ["def connect_rds(aws_access_key_id=None, aws_secret_access_key=None, **kwargs): \"\"\" :type aws_access_key_id: string :param aws_access_key_id: Your AWS Access Key ID :type aws_secret_access_key: string :param aws_secret_access_key: Your AWS Secret Access Key :rtype: :class:`boto.rds.RDSConnection` :return: A connection to RDS \"\"\" from boto.rds import RDSConnection return RDSConnection(aws_access_key_id, aws_secret_access_key, **kwargs)", "def list_rds(region, filter_by_kwargs): \"\"\"List all RDS thingys.\"\"\" conn = boto.rds.connect_to_region(region) instances = conn.get_all_dbinstances() return lookup(instances, filter_by=filter_by_kwargs)", "def s3_connect(bucket_name, s3_access_key_id, s3_secret_key): \"\"\" Returns a Boto connection to the provided S3 bucket. \"\"\" conn = connect_s3(s3_access_key_id, s3_secret_key) try: return conn.get_bucket(bucket_name) except S3ResponseError as e: if e.status == 403: raise Exception(\"Bad Amazon S3 credentials.\") raise", "def _aws_get_instance_by_tag(region, name, tag, raw): \"\"\"Get all instances matching a tag.\"\"\" client = boto3.session.Session().client('ec2', region) matching_reservations = client.describe_instances(Filters=[{'Name': tag, 'Values': [name]}]).get('Reservations', []) instances = [] [[instances.append(_aws_instance_from_dict(region, instance, raw)) # pylint: disable=expression-not-assigned for instance in reservation.get('Instances')] for reservation in matching_reservations if reservation] return instances", "def from_url(url, db=None, **kwargs): \"\"\" Returns an active Redis client generated from the given database URL. Will attempt to extract the database id from the path url fragment, if none is provided. \"\"\" from redis.client import Redis return Redis.from_url(url, db, **kwargs)"], "rank": 1}
{"nl": "python cv2 check if image is empty", "code": ["def is_empty(self): \"\"\"Checks for an empty image. \"\"\" if(((self.channels == []) and (not self.shape == (0, 0))) or ((not self.channels == []) and (self.shape == (0, 0)))): raise RuntimeError(\"Channels-shape mismatch.\") return self.channels == [] and self.shape == (0, 0)", "def imdecode(image_path): \"\"\"Return BGR image read by opencv\"\"\" import os assert os.path.exists(image_path), image_path + ' not found' im = cv2.imread(image_path) return im", "def _is_image_sequenced(image): \"\"\"Determine if the image is a sequenced image.\"\"\" try: image.seek(1) image.seek(0) result = True except EOFError: result = False return result", "def screen_cv2(self): \"\"\"cv2 Image of current window screen\"\"\" pil_image = self.screen.convert('RGB') cv2_image = np.array(pil_image) pil_image.close() # Convert RGB to BGR cv2_image = cv2_image[:, :, ::-1] return cv2_image", "def _validate_image_rank(self, img_array): \"\"\" Images must be either 2D or 3D. \"\"\" if img_array.ndim == 1 or img_array.ndim > 3: msg = \"{0}D imagery is not allowed.\".format(img_array.ndim) raise IOError(msg)"], "rank": 1}
{"nl": "how to hide a window using a button in python", "code": ["def hide(self): \"\"\"Hides the main window of the terminal and sets the visible flag to False. \"\"\" if not HidePrevention(self.window).may_hide(): return self.hidden = True self.get_widget('window-root').unstick() self.window.hide()", "def hide(self): \"\"\"Hide the window.\"\"\" self.tk.withdraw() self._visible = False if self._modal: self.tk.grab_release()", "def reject(self): \"\"\" Rejects the snapshot and closes the widget. \"\"\" if self.hideWindow(): self.hideWindow().show() self.close() self.deleteLater()", "def inFocus(self): \"\"\"Set GUI on-top flag\"\"\" previous_flags = self.window.flags() self.window.setFlags(previous_flags | QtCore.Qt.WindowStaysOnTopHint)", "def disable(self): \"\"\" Disable the button, if in non-expert mode; unset its activity flag come-what-may. \"\"\" if not self._expert: self.config(state='disable') self._active = False"], "rank": 1}
{"nl": "python marshmallow validation schema from parent", "code": ["def validate(schema, data, owner=None): \"\"\"Validate input data with input schema. :param Schema schema: schema able to validate input data. :param data: data to validate. :param Schema owner: input schema parent schema. :raises: Exception if the data is not validated. \"\"\" schema._validate(data=data, owner=owner)", "def validate(raw_schema, target=None, **kwargs): \"\"\" Given the python representation of a JSONschema as defined in the swagger spec, validate that the schema complies to spec. If `target` is provided, that target will be validated against the provided schema. \"\"\" schema = schema_validator(raw_schema, **kwargs) if target is not None: validate_object(target, schema=schema, **kwargs)", "def validate(self, *args, **kwargs): # pylint: disable=arguments-differ \"\"\" Validate a parameter dict against a parameter schema from an ocrd-tool.json Args: obj (dict): schema (dict): \"\"\" return super(ParameterValidator, self)._validate(*args, **kwargs)", "def validate(request: Union[Dict, List], schema: dict) -> Union[Dict, List]: \"\"\" Wraps jsonschema.validate, returning the same object passed in. Args: request: The deserialized-from-json request. schema: The jsonschema schema to validate against. Raises: jsonschema.ValidationError \"\"\" jsonschema_validate(request, schema) return request", "def validate(payload, schema): \"\"\"Validate `payload` against `schema`, returning an error list. jsonschema provides lots of information in it's errors, but it can be a bit of work to extract all the information. \"\"\" v = jsonschema.Draft4Validator( schema, format_checker=jsonschema.FormatChecker()) error_list = [] for error in v.iter_errors(payload): message = error.message location = '/' + '/'.join([str(c) for c in error.absolute_path]) error_list.append(message + ' at ' + location) return error_list"], "rank": 1}
{"nl": "fillna with string for specific columnin python", "code": ["def stringify_col(df, col_name): \"\"\" Take a dataframe and string-i-fy a column of values. Turn nan/None into \"\" and all other values into strings. Parameters ---------- df : dataframe col_name : string \"\"\" df = df.copy() df[col_name] = df[col_name].fillna(\"\") df[col_name] = df[col_name].astype(str) return df", "def fill_nulls(self, col: str): \"\"\" Fill all null values with NaN values in a column. Null values are ``None`` or en empty string :param col: column name :type col: str :example: ``ds.fill_nulls(\"mycol\")`` \"\"\" n = [None, \"\"] try: self.df[col] = self.df[col].replace(n, nan) except Exception as e: self.err(e)", "def fillna(series_or_arr, missing_value=0.0): \"\"\"Fill missing values in pandas objects and numpy arrays. Arguments --------- series_or_arr : pandas.Series, numpy.ndarray The numpy array or pandas series for which the missing values need to be replaced. missing_value : float, int, str The value to replace the missing value with. Default 0.0. Returns ------- pandas.Series, numpy.ndarray The numpy array or pandas series with the missing values filled. \"\"\" if pandas.notnull(missing_value): if isinstance(series_or_arr, (numpy.ndarray)): series_or_arr[numpy.isnan(series_or_arr)] = missing_value else: series_or_arr.fillna(missing_value, inplace=True) return series_or_arr", "def _maybe_fill(arr, fill_value=np.nan): \"\"\" if we have a compatible fill_value and arr dtype, then fill \"\"\" if _isna_compat(arr, fill_value): arr.fill(fill_value) return arr", "def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df"], "rank": 1}
{"nl": "how to separate list elements by white space python", "code": ["def strip_spaces(value, sep=None, join=True): \"\"\"Cleans trailing whitespaces and replaces also multiple whitespaces with a single space.\"\"\" value = value.strip() value = [v.strip() for v in value.split(sep)] join_sep = sep or ' ' return join_sep.join(value) if join else value", "def split_strings_in_list_retain_spaces(orig_list): \"\"\" Function to split every line in a list, and retain spaces for a rejoin :param orig_list: Original list :return: A List with split lines \"\"\" temp_list = list() for line in orig_list: line_split = __re.split(r'(\\s+)', line) temp_list.append(line_split) return temp_list", "def __normalize_list(self, msg): \"\"\"Split message to list by commas and trim whitespace.\"\"\" if isinstance(msg, list): msg = \"\".join(msg) return list(map(lambda x: x.strip(), msg.split(\",\")))", "def split_elements(value): \"\"\"Split a string with comma or space-separated elements into a list.\"\"\" l = [v.strip() for v in value.split(',')] if len(l) == 1: l = value.split() return l", "def clean_strings(iterable): \"\"\" Take a list of strings and clear whitespace on each one. If a value in the list is not a string pass it through untouched. Args: iterable: mixed list Returns: mixed list \"\"\" retval = [] for val in iterable: try: retval.append(val.strip()) except(AttributeError): retval.append(val) return retval"], "rank": 2}
{"nl": "how to default value in python", "code": ["def set_default(self_,param_name,value): \"\"\" Set the default value of param_name. Equivalent to setting param_name on the class. \"\"\" cls = self_.cls setattr(cls,param_name,value)", "def setdefault(obj, field, default): \"\"\"Set an object's field to default if it doesn't have a value\"\"\" setattr(obj, field, getattr(obj, field, default))", "def setdefault(self, name: str, default: Any=None) -> Any: \"\"\"Set an attribute with a default value.\"\"\" return self.__dict__.setdefault(name, default)", "def set_default(self, key, value): \"\"\"Set the default value for this key. Default only used when no value is provided by the user via arg, config or env. \"\"\" k = self._real_key(key.lower()) self._defaults[k] = value", "def safe_int(val, default=None): \"\"\" Returns int() of val if val is not convertable to int use default instead :param val: :param default: \"\"\" try: val = int(val) except (ValueError, TypeError): val = default return val"], "rank": 5}
{"nl": "python dict keys lowercase", "code": ["def keys_to_snake_case(camel_case_dict): \"\"\" Make a copy of a dictionary with all keys converted to snake case. This is just calls to_snake_case on each of the keys in the dictionary and returns a new dictionary. :param camel_case_dict: Dictionary with the keys to convert. :type camel_case_dict: Dictionary. :return: Dictionary with the keys converted to snake case. \"\"\" return dict((to_snake_case(key), value) for (key, value) in camel_case_dict.items())", "def _keys_to_camel_case(self, obj): \"\"\" Make a copy of a dictionary with all keys converted to camel case. This is just calls to_camel_case on each of the keys in the dictionary and returns a new dictionary. :param obj: Dictionary to convert keys to camel case. :return: Dictionary with the input values and all keys in camel case \"\"\" return dict((to_camel_case(key), value) for (key, value) in obj.items())", "def get_case_insensitive_dict_key(d: Dict, k: str) -> Optional[str]: \"\"\" Within the dictionary ``d``, find a key that matches (in case-insensitive fashion) the key ``k``, and return it (or ``None`` if there isn't one). \"\"\" for key in d.keys(): if k.lower() == key.lower(): return key return None", "def __contains__ (self, key): \"\"\"Check lowercase key item.\"\"\" assert isinstance(key, basestring) return dict.__contains__(self, key.lower())", "def to_snake_case(name): \"\"\" Given a name in camelCase return in snake_case \"\"\" s1 = FIRST_CAP_REGEX.sub(r'\\1_\\2', name) return ALL_CAP_REGEX.sub(r'\\1_\\2', s1).lower()"], "rank": 1}
{"nl": "python get index of list values that equal", "code": ["def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def index(m, val): \"\"\" Return the indices of all the ``val`` in ``m`` \"\"\" mm = np.array(m) idx_tuple = np.where(mm == val) idx = idx_tuple[0].tolist() return idx", "def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def find_geom(geom, geoms): \"\"\" Returns the index of a geometry in a list of geometries avoiding expensive equality checks of `in` operator. \"\"\" for i, g in enumerate(geoms): if g is geom: return i"], "rank": 13}
{"nl": "python requests logging not work", "code": ["def _log_response(response): \"\"\"Log out information about a ``Request`` object. After calling ``requests.request`` or one of its convenience methods, the object returned can be passed to this method. If done, information about the object returned is logged. :return: Nothing is returned. \"\"\" message = u'Received HTTP {0} response: {1}'.format( response.status_code, response.text ) if response.status_code >= 400: # pragma: no cover logger.warning(message) else: logger.debug(message)", "def process_request(self, request, response): \"\"\"Logs the basic endpoint requested\"\"\" self.logger.info('Requested: {0} {1} {2}'.format(request.method, request.relative_uri, request.content_type))", "def log_request(self, code='-', size='-'): \"\"\"Selectively log an accepted request.\"\"\" if self.server.logRequests: BaseHTTPServer.BaseHTTPRequestHandler.log_request(self, code, size)", "def info(self, message, *args, **kwargs): \"\"\"More important level : default for print and save \"\"\" self._log(logging.INFO, message, *args, **kwargs)", "def log(self, level, msg=None, *args, **kwargs): \"\"\"Writes log out at any arbitray level.\"\"\" return self._log(level, msg, args, kwargs)"], "rank": 2}
{"nl": "python static files flask", "code": ["def serve_static(request, path, insecure=False, **kwargs): \"\"\"Collect and serve static files. This view serves up static files, much like Django's :py:func:`~django.views.static.serve` view, with the addition that it collects static files first (if enabled). This allows images, fonts, and other assets to be served up without first loading a page using the ``{% javascript %}`` or ``{% stylesheet %}`` template tags. You can use this view by adding the following to any :file:`urls.py`:: urlpatterns += static('static/', view='pipeline.views.serve_static') \"\"\" # Follow the same logic Django uses for determining access to the # static-serving view. if not django_settings.DEBUG and not insecure: raise ImproperlyConfigured(\"The staticfiles view can only be used in \" \"debug mode or if the --insecure \" \"option of 'runserver' is used\") if not settings.PIPELINE_ENABLED and settings.PIPELINE_COLLECTOR_ENABLED: # Collect only the requested file, in order to serve the result as # fast as possible. This won't interfere with the template tags in any # way, as those will still cause Django to collect all media. default_collector.collect(request, files=[path]) return serve(request, path, document_root=django_settings.STATIC_ROOT, **kwargs)", "def glr_path_static(): \"\"\"Returns path to packaged static files\"\"\" return os.path.abspath(os.path.join(os.path.dirname(__file__), '_static'))", "def static_url(path, absolute=False): \"\"\" Shorthand for returning a URL for the requested static file. Arguments: path -- the path to the file (relative to the static files directory) absolute -- whether the link should be absolute or relative \"\"\" if os.sep != '/': path = '/'.join(path.split(os.sep)) return flask.url_for('static', filename=path, _external=absolute)", "def get_handler(self, *args, **options): \"\"\" Returns the default WSGI handler for the runner. \"\"\" handler = get_internal_wsgi_application() from django.contrib.staticfiles.handlers import StaticFilesHandler return StaticFilesHandler(handler)", "def collect_static() -> bool: \"\"\" Runs Django ``collectstatic`` command in silent mode. :return: always ``True`` \"\"\" from django.core.management import execute_from_command_line # from django.conf import settings # if not os.listdir(settings.STATIC_ROOT): wf('Collecting static files... ', False) execute_from_command_line(['./manage.py', 'collectstatic', '-c', '--noinput', '-v0']) wf('[+]\\n') return True"], "rank": 2}
{"nl": "rest json schema validation python", "code": ["def validate(request: Union[Dict, List], schema: dict) -> Union[Dict, List]: \"\"\" Wraps jsonschema.validate, returning the same object passed in. Args: request: The deserialized-from-json request. schema: The jsonschema schema to validate against. Raises: jsonschema.ValidationError \"\"\" jsonschema_validate(request, schema) return request", "def validate(payload, schema): \"\"\"Validate `payload` against `schema`, returning an error list. jsonschema provides lots of information in it's errors, but it can be a bit of work to extract all the information. \"\"\" v = jsonschema.Draft4Validator( schema, format_checker=jsonschema.FormatChecker()) error_list = [] for error in v.iter_errors(payload): message = error.message location = '/' + '/'.join([str(c) for c in error.absolute_path]) error_list.append(message + ' at ' + location) return error_list", "def _validate(data, schema, ac_schema_safe=True, **options): \"\"\" See the descritpion of :func:`validate` for more details of parameters and return value. Validate target object 'data' with given schema object. \"\"\" try: jsonschema.validate(data, schema, **options) except (jsonschema.ValidationError, jsonschema.SchemaError, Exception) as exc: if ac_schema_safe: return (False, str(exc)) # Validation was failed. raise return (True, '')", "def validate(raw_schema, target=None, **kwargs): \"\"\" Given the python representation of a JSONschema as defined in the swagger spec, validate that the schema complies to spec. If `target` is provided, that target will be validated against the provided schema. \"\"\" schema = schema_validator(raw_schema, **kwargs) if target is not None: validate_object(target, schema=schema, **kwargs)", "def validate(self, *args, **kwargs): # pylint: disable=arguments-differ \"\"\" Validate a parameter dict against a parameter schema from an ocrd-tool.json Args: obj (dict): schema (dict): \"\"\" return super(ParameterValidator, self)._validate(*args, **kwargs)"], "rank": 1}
{"nl": "write in bold and read in color of the print mesaage in python", "code": ["def good(txt): \"\"\"Print, emphasized 'good', the given 'txt' message\"\"\" print(\"%s# %s%s%s\" % (PR_GOOD_CC, get_time_stamp(), txt, PR_NC)) sys.stdout.flush()", "def info(txt): \"\"\"Print, emphasized 'neutral', the given 'txt' message\"\"\" print(\"%s# %s%s%s\" % (PR_EMPH_CC, get_time_stamp(), txt, PR_NC)) sys.stdout.flush()", "def printc(cls, txt, color=colors.red): \"\"\"Print in color.\"\"\" print(cls.color_txt(txt, color))", "def err(msg): \"\"\"Pretty-print an error.\"\"\" click.echo(click.style(msg, fg=\"red\", bold=True))", "def underline(self, msg): \"\"\"Underline the input\"\"\" return click.style(msg, underline=True) if self.colorize else msg"], "rank": 1}
{"nl": "python type cast to bigint", "code": ["def _to_numeric(val): \"\"\" Helper function for conversion of various data types into numeric representation. \"\"\" if isinstance(val, (int, float, datetime.datetime, datetime.timedelta)): return val return float(val)", "def bin_to_int(string): \"\"\"Convert a one element byte string to signed int for python 2 support.\"\"\" if isinstance(string, str): return struct.unpack(\"b\", string)[0] else: return struct.unpack(\"b\", bytes([string]))[0]", "def _get_type(self, value): \"\"\"Get the data type for *value*.\"\"\" if value is None: return type(None) elif type(value) in int_types: return int elif type(value) in float_types: return float elif isinstance(value, binary_type): return binary_type else: return text_type", "def convert_types(cls, value): \"\"\" Takes a value from MSSQL, and converts it to a value that's safe for JSON/Google Cloud Storage/BigQuery. \"\"\" if isinstance(value, decimal.Decimal): return float(value) else: return value", "def safe_int_conv(number): \"\"\"Safely convert a single number to integer.\"\"\" try: return int(np.array(number).astype(int, casting='safe')) except TypeError: raise ValueError('cannot safely convert {} to integer'.format(number))"], "rank": 7}
{"nl": "converts matrix to pictures by python", "code": ["def im2mat(I): \"\"\"Converts and image to matrix (one pixel per line)\"\"\" return I.reshape((I.shape[0] * I.shape[1], I.shape[2]))", "def trans_from_matrix(matrix): \"\"\" Convert a vtk matrix to a numpy.ndarray \"\"\" t = np.zeros((4, 4)) for i in range(4): for j in range(4): t[i, j] = matrix.GetElement(i, j) return t", "def a2s(a): \"\"\" convert 3,3 a matrix to 6 element \"s\" list (see Tauxe 1998) \"\"\" s = np.zeros((6,), 'f') # make the a matrix for i in range(3): s[i] = a[i][i] s[3] = a[0][1] s[4] = a[1][2] s[5] = a[0][2] return s", "def get_matrix(self): \"\"\" Use numpy to create a real matrix object from the data :return: the matrix representation of the fvm \"\"\" return np.array([ self.get_row_list(i) for i in range(self.row_count()) ])", "def segments_to_numpy(segments): \"\"\"given a list of 4-element tuples, transforms it into a numpy array\"\"\" segments = numpy.array(segments, dtype=SEGMENT_DATATYPE, ndmin=2) # each segment in a row segments = segments if SEGMENTS_DIRECTION == 0 else numpy.transpose(segments) return segments"], "rank": 1}
{"nl": "python random selection from function", "code": ["def select_random(engine, table_or_columns, limit=5): \"\"\" Randomly select some rows from table. \"\"\" s = select(table_or_columns).order_by(func.random()).limit(limit) return engine.execute(s).fetchall()", "def sometimesish(fn): \"\"\" Has a 50/50 chance of calling a function \"\"\" def wrapped(*args, **kwargs): if random.randint(1, 2) == 1: return fn(*args, **kwargs) return wrapped", "def runiform(lower, upper, size=None): \"\"\" Random uniform variates. \"\"\" return np.random.uniform(lower, upper, size)", "def _weighted_selection(l, n): \"\"\" Selects n random elements from a list of (weight, item) tuples. Based on code snippet by Nick Johnson \"\"\" cuml = [] items = [] total_weight = 0.0 for weight, item in l: total_weight += weight cuml.append(total_weight) items.append(item) return [items[bisect.bisect(cuml, random.random()*total_weight)] for _ in range(n)]", "def random_choice(sequence): \"\"\" Same as :meth:`random.choice`, but also supports :class:`set` type to be passed as sequence. \"\"\" return random.choice(tuple(sequence) if isinstance(sequence, set) else sequence)"], "rank": 2}
{"nl": "python keep processpool open until tasks complete", "code": ["def wait(self, timeout=None): \"\"\" Block until all jobs in the ThreadPool are finished. Beware that this can make the program run into a deadlock if another thread adds new jobs to the pool! # Raises Timeout: If the timeout is exceeded. \"\"\" if not self.__running: raise RuntimeError(\"ThreadPool ain't running\") self.__queue.wait(timeout)", "def Join(self): \"\"\"Waits until all outstanding tasks are completed.\"\"\" for _ in range(self.JOIN_TIMEOUT_DECISECONDS): if self._queue.empty() and not self.busy_threads: return time.sleep(0.1) raise ValueError(\"Timeout during Join() for threadpool %s.\" % self.name)", "def _clean_workers(self): \"\"\"Delete periodically workers in workers bag.\"\"\" while self._bag_collector: self._bag_collector.popleft() self._timer_worker_delete.stop()", "def finish(): \"\"\"Print warning about interrupt and empty the job queue.\"\"\" out.warn(\"Interrupted!\") for t in threads: t.stop() jobs.clear() out.warn(\"Waiting for download threads to finish.\")", "def get(self): \"\"\"retrieve a result from the pool if nothing is already completed when this method is called, it will block until something comes back if the pool's function exited via exception, that will come back as a result here as well, but will be re-raised in :meth:`get`. .. note:: if there is nothing in the pool's output queue when this method is called, it will block until something is ready :returns: a return value from one of the function's invocations if it exited normally :raises: :class:`PoolClosed` if the pool was closed before a result could be produced for thie call :raises: any exception that was raised inside the worker function \"\"\" if self.closed: raise PoolClosed() while self._getcount not in self._cache: counter, result = self.outq.get() self._cache[counter] = result result, succeeded = self._cache.pop(self._getcount) self._getcount += 1 if not succeeded: klass, exc, tb = result raise klass, exc, tb return result"], "rank": 1}
{"nl": "running a def a specified amount of time python 3", "code": ["def set(self, f): \"\"\"Call a function after a delay, unless another function is set in the meantime.\"\"\" self.stop() self._create_timer(f) self.start()", "def seconds(num): \"\"\" Pause for this many seconds \"\"\" now = pytime.time() end = now + num until(end)", "def timed (log=sys.stderr, limit=2.0): \"\"\"Decorator to run a function with timing info.\"\"\" return lambda func: timeit(func, log, limit)", "def nTimes(n, f, *args, **kwargs): r\"\"\"Call `f` `n` times with `args` and `kwargs`. Useful e.g. for simplistic timing. Examples: >>> nTimes(3, sys.stdout.write, 'hallo\\n') hallo hallo hallo \"\"\" for i in xrange(n): f(*args, **kwargs)", "def test3(): \"\"\"Test the multiprocess \"\"\" import time p = MVisionProcess() p.start() time.sleep(5) p.stop()"], "rank": 2}
{"nl": "python str to dateal time", "code": ["def str_to_time(time_str: str) -> datetime.datetime: \"\"\" Convert human readable string to datetime.datetime. \"\"\" pieces: Any = [int(piece) for piece in time_str.split('-')] return datetime.datetime(*pieces)", "def convert_time_string(date_str): \"\"\" Change a date string from the format 2018-08-15T23:55:17 into a datetime object \"\"\" dt, _, _ = date_str.partition(\".\") dt = datetime.strptime(dt, \"%Y-%m-%dT%H:%M:%S\") return dt", "def timestamp_to_datetime(cls, dt, dt_format=DATETIME_FORMAT): \"\"\"Convert unix timestamp to human readable date/time string\"\"\" return cls.convert_datetime(cls.get_datetime(dt), dt_format=dt_format)", "def deserialize_date(string): \"\"\" Deserializes string to date. :param string: str. :type string: str :return: date. :rtype: date \"\"\" try: from dateutil.parser import parse return parse(string).date() except ImportError: return string", "def string_to_date(value): \"\"\" Return a Python date that corresponds to the specified string representation. @param value: string representation of a date. @return: an instance ``datetime.datetime`` represented by the string. \"\"\" if isinstance(value, datetime.date): return value return dateutil.parser.parse(value).date()"], "rank": 1}
{"nl": "python check if file is executable", "code": ["def is_executable(path): \"\"\"Returns whether a path names an existing executable file.\"\"\" return os.path.isfile(path) and os.access(path, os.X_OK)", "def is_exe(fpath): \"\"\" Path references an executable file. \"\"\" return os.path.isfile(fpath) and os.access(fpath, os.X_OK)", "def executable_exists(executable): \"\"\"Test if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False", "def set_executable(filename): \"\"\"Set the exectuable bit on the given filename\"\"\" st = os.stat(filename) os.chmod(filename, st.st_mode | stat.S_IEXEC)", "def is_readable(filename): \"\"\"Check if file is a regular file and is readable.\"\"\" return os.path.isfile(filename) and os.access(filename, os.R_OK)"], "rank": 1}
{"nl": "python how to check the queue lenght", "code": ["def full(self): \"\"\"Return True if the queue is full\"\"\" if not self.size: return False return len(self.pq) == (self.size + self.removed_count)", "def qsize(self): \"\"\"Return the approximate size of the queue (not reliable!).\"\"\" self.mutex.acquire() n = self._qsize() self.mutex.release() return n", "def full(self): \"\"\"Return ``True`` if the queue is full, ``False`` otherwise (not reliable!). Only applicable if :attr:`maxsize` is set. \"\"\" return self.maxsize and len(self.list) >= self.maxsize or False", "def count(self, elem): \"\"\" Return the number of elements equal to elem present in the queue >>> pdeque([1, 2, 1]).count(1) 2 \"\"\" return self._left_list.count(elem) + self._right_list.count(elem)", "def ncores_reserved(self): \"\"\" Returns the number of cores reserved in this moment. A core is reserved if it's still not running but we have submitted the task to the queue manager. \"\"\" return sum(task.manager.num_cores for task in self if task.status == task.S_SUB)"], "rank": 3}
{"nl": "query server objects ldap in python", "code": ["def search(self, filterstr, attrlist): \"\"\"Query the configured LDAP server.\"\"\" return self._paged_search_ext_s(self.settings.BASE, ldap.SCOPE_SUBTREE, filterstr=filterstr, attrlist=attrlist, page_size=self.settings.PAGE_SIZE)", "def query(self, base, filterstr, attrlist=None): \"\"\" wrapper to search_s \"\"\" return self.conn.search_s(base, ldap.SCOPE_SUBTREE, filterstr, attrlist)", "def members(self, uid=\"*\", objects=False): \"\"\" members() issues an ldap query for all users, and returns a dict for each matching entry. This can be quite slow, and takes roughly 3s to complete. You may optionally restrict the scope by specifying a uid, which is roughly equivalent to a search(uid='foo') \"\"\" entries = self.search(uid='*') if objects: return self.memberObjects(entries) result = [] for entry in entries: result.append(entry[1]) return result", "def get_groups(self, username): \"\"\"Get all groups of a user\"\"\" username = ldap.filter.escape_filter_chars(self._byte_p2(username)) userdn = self._get_user(username, NO_ATTR) searchfilter = self.group_filter_tmpl % { 'userdn': userdn, 'username': username } groups = self._search(searchfilter, NO_ATTR, self.groupdn) ret = [] for entry in groups: ret.append(self._uni(entry[0])) return ret", "def compare(self, dn, attr, value): \"\"\" Compare the ``attr`` of the entry ``dn`` with given ``value``. This is a convenience wrapper for the ldap library's ``compare`` function that returns a boolean value instead of 1 or 0. \"\"\" return self.connection.compare_s(dn, attr, value) == 1"], "rank": 1}
{"nl": "python docx document section different page", "code": ["def process_docstring(app, what, name, obj, options, lines): \"\"\"Process the docstring for a given python object. Called when autodoc has read and processed a docstring. `lines` is a list of docstring lines that `_process_docstring` modifies in place to change what Sphinx outputs. The following settings in conf.py control what styles of docstrings will be parsed: * ``napoleon_google_docstring`` -- parse Google style docstrings * ``napoleon_numpy_docstring`` -- parse NumPy style docstrings Parameters ---------- app : sphinx.application.Sphinx Application object representing the Sphinx process. what : str A string specifying the type of the object to which the docstring belongs. Valid values: \"module\", \"class\", \"exception\", \"function\", \"method\", \"attribute\". name : str The fully qualified name of the object. obj : module, class, exception, function, method, or attribute The object to which the docstring belongs. options : sphinx.ext.autodoc.Options The options given to the directive: an object with attributes inherited_members, undoc_members, show_inheritance and noindex that are True if the flag option of same name was given to the auto directive. lines : list of str The lines of the docstring, see above. .. note:: `lines` is modified *in place* Notes ----- This function is (to most parts) taken from the :mod:`sphinx.ext.napoleon` module, sphinx version 1.3.1, and adapted to the classes defined here\"\"\" result_lines = lines if app.config.napoleon_numpy_docstring: docstring = ExtendedNumpyDocstring( result_lines, app.config, app, what, name, obj, options) result_lines = docstring.lines() if app.config.napoleon_google_docstring: docstring = ExtendedGoogleDocstring( result_lines, app.config, app, what, name, obj, options) result_lines = docstring.lines() lines[:] = result_lines[:]", "def fill_document(doc): \"\"\"Add a section, a subsection and some text to the document. :param doc: the document :type doc: :class:`pylatex.document.Document` instance \"\"\" with doc.create(Section('A section')): doc.append('Some regular text and some ') doc.append(italic('italic text. ')) with doc.create(Subsection('A subsection')): doc.append('Also some crazy characters: $&#{}')", "def doc_to_html(doc, doc_format=\"ROBOT\"): \"\"\"Convert documentation to HTML\"\"\" from robot.libdocpkg.htmlwriter import DocToHtml return DocToHtml(doc_format)(doc)", "def readme(): \"\"\"Try converting the README to an RST document. Return it as is on failure.\"\"\" try: import pypandoc readme_content = pypandoc.convert('README.md', 'rst') except(IOError, ImportError): print(\"Warning: no pypandoc module found.\") try: readme_content = open('README.md').read() except IOError: readme_content = '' return readme_content", "def RunSphinxAPIDoc(_): \"\"\"Runs sphinx-apidoc to auto-generate documentation.\"\"\" current_directory = os.path.abspath(os.path.dirname(__file__)) module = os.path.join(current_directory, '..', 'plaso') api_directory = os.path.join(current_directory, 'sources', 'api') apidoc.main(['-o', api_directory, module, '--force'])"], "rank": 2}
{"nl": "write a json object to file python", "code": ["def _write_json(obj, path): # type: (object, str) -> None \"\"\"Writes a serializeable object as a JSON file\"\"\" with open(path, 'w') as f: json.dump(obj, f)", "def _serialize_json(obj, fp): \"\"\" Serialize ``obj`` as a JSON formatted stream to ``fp`` \"\"\" json.dump(obj, fp, indent=4, default=serialize)", "def save(self, fname): \"\"\" Saves the dictionary in json format :param fname: file to save to \"\"\" with open(fname, 'wb') as f: json.dump(self, f)", "def _write_json(file, contents): \"\"\"Write a dict to a JSON file.\"\"\" with open(file, 'w') as f: return json.dump(contents, f, indent=2, sort_keys=True)", "def save_json(object, handle, indent=2): \"\"\"Save object as json on CNS.\"\"\" obj_json = json.dumps(object, indent=indent, cls=NumpyJSONEncoder) handle.write(obj_json)"], "rank": 2}
{"nl": "how to compute the minimum value of a tensor in python", "code": ["def last_location_of_minimum(x): \"\"\" Returns the last location of the minimal value of x. The position is calculated relatively to the length of x. :param x: the time series to calculate the feature of :type x: numpy.ndarray :return: the value of this feature :return type: float \"\"\" x = np.asarray(x) return 1.0 - np.argmin(x[::-1]) / len(x) if len(x) > 0 else np.NaN", "def fn_min(self, a, axis=None): \"\"\" Return the minimum of an array, ignoring any NaNs. :param a: The array. :return: The minimum value of the array. \"\"\" return numpy.nanmin(self._to_ndarray(a), axis=axis)", "def Min(a, axis, keep_dims): \"\"\" Min reduction op. \"\"\" return np.amin(a, axis=axis if not isinstance(axis, np.ndarray) else tuple(axis), keepdims=keep_dims),", "def top_1_tpu(inputs): \"\"\"find max and argmax over the last dimension. Works well on TPU Args: inputs: A tensor with shape [..., depth] Returns: values: a Tensor with shape [...] indices: a Tensor with shape [...] \"\"\" inputs_max = tf.reduce_max(inputs, axis=-1, keepdims=True) mask = tf.to_int32(tf.equal(inputs_max, inputs)) index = tf.range(tf.shape(inputs)[-1]) * mask return tf.squeeze(inputs_max, -1), tf.reduce_max(index, axis=-1)", "def findMin(arr): \"\"\" in comparison to argrelmax() more simple and reliable peak finder \"\"\" out = np.zeros(shape=arr.shape, dtype=bool) _calcMin(arr, out) return out"], "rank": 1}
{"nl": "python unittest how to assert 2 lists are almost equal", "code": ["def expect_all(a, b): \"\"\"\\ Asserts that two iterables contain the same values. \"\"\" assert all(_a == _b for _a, _b in zip_longest(a, b))", "def assert_equal(first, second, msg_fmt=\"{msg}\"): \"\"\"Fail unless first equals second, as determined by the '==' operator. >>> assert_equal(5, 5.0) >>> assert_equal(\"Hello World!\", \"Goodbye!\") Traceback (most recent call last): ... AssertionError: 'Hello World!' != 'Goodbye!' The following msg_fmt arguments are supported: * msg - the default error message * first - the first argument * second - the second argument \"\"\" if isinstance(first, dict) and isinstance(second, dict): assert_dict_equal(first, second, msg_fmt) elif not first == second: msg = \"{!r} != {!r}\".format(first, second) fail(msg_fmt.format(msg=msg, first=first, second=second))", "def _check_elements_equal(lst): \"\"\" Returns true if all of the elements in the list are equal. \"\"\" assert isinstance(lst, list), \"Input value must be a list.\" return not lst or lst.count(lst[0]) == len(lst)", "def allsame(list_, strict=True): \"\"\" checks to see if list is equal everywhere Args: list_ (list): Returns: True if all items in the list are equal \"\"\" if len(list_) == 0: return True first_item = list_[0] return list_all_eq_to(list_, first_item, strict)", "def assert_in(first, second, msg_fmt=\"{msg}\"): \"\"\"Fail if first is not in collection second. >>> assert_in(\"foo\", [4, \"foo\", {}]) >>> assert_in(\"bar\", [4, \"foo\", {}]) Traceback (most recent call last): ... AssertionError: 'bar' not in [4, 'foo', {}] The following msg_fmt arguments are supported: * msg - the default error message * first - the element looked for * second - the container looked in \"\"\" if first not in second: msg = \"{!r} not in {!r}\".format(first, second) fail(msg_fmt.format(msg=msg, first=first, second=second))"], "rank": 1}
{"nl": "how to check if 2 inputs are equal in python assert equal", "code": ["def assert_equal(first, second, msg_fmt=\"{msg}\"): \"\"\"Fail unless first equals second, as determined by the '==' operator. >>> assert_equal(5, 5.0) >>> assert_equal(\"Hello World!\", \"Goodbye!\") Traceback (most recent call last): ... AssertionError: 'Hello World!' != 'Goodbye!' The following msg_fmt arguments are supported: * msg - the default error message * first - the first argument * second - the second argument \"\"\" if isinstance(first, dict) and isinstance(second, dict): assert_dict_equal(first, second, msg_fmt) elif not first == second: msg = \"{!r} != {!r}\".format(first, second) fail(msg_fmt.format(msg=msg, first=first, second=second))", "def expect_all(a, b): \"\"\"\\ Asserts that two iterables contain the same values. \"\"\" assert all(_a == _b for _a, _b in zip_longest(a, b))", "def assert_is_not(expected, actual, message=None, extra=None): \"\"\"Raises an AssertionError if expected is actual.\"\"\" assert expected is not actual, _assert_fail_message( message, expected, actual, \"is\", extra )", "def hard_equals(a, b): \"\"\"Implements the '===' operator.\"\"\" if type(a) != type(b): return False return a == b", "def all_equal(arg1,arg2): \"\"\" Return a single boolean for arg1==arg2, even for numpy arrays using element-wise comparison. Uses all(arg1==arg2) for sequences, and arg1==arg2 otherwise. If both objects have an '_infinitely_iterable' attribute, they are not be zipped together and are compared directly instead. \"\"\" if all(hasattr(el, '_infinitely_iterable') for el in [arg1,arg2]): return arg1==arg2 try: return all(a1 == a2 for a1, a2 in zip(arg1, arg2)) except TypeError: return arg1==arg2"], "rank": 2}
{"nl": "python loop through proxies request", "code": ["def dispatch(self, request, *args, **kwargs): \"\"\"Dispatch all HTTP methods to the proxy.\"\"\" self.request = DownstreamRequest(request) self.args = args self.kwargs = kwargs self._verify_config() self.middleware = MiddlewareSet(self.proxy_middleware) return self.proxy()", "def load(self): \"\"\"Load proxy list from configured proxy source\"\"\" self._list = self._source.load() self._list_iter = itertools.cycle(self._list)", "def _GetProxies(self): \"\"\"Gather a list of proxies to use.\"\"\" # Detect proxies from the OS environment. result = client_utils.FindProxies() # Also try to connect directly if all proxies fail. result.append(\"\") # Also try all proxies configured in the config system. result.extend(config.CONFIG[\"Client.proxy_servers\"]) return result", "def _prepare_proxy(self, conn): \"\"\" Establish tunnel connection early, because otherwise httplib would improperly set Host: header to proxy's IP:port. \"\"\" conn.set_tunnel(self._proxy_host, self.port, self.proxy_headers) conn.connect()", "def _iter_response(self, url, params=None): \"\"\"Return an enumerable that iterates through a multi-page API request\"\"\" if params is None: params = {} params['page_number'] = 1 # Last page lists itself as next page while True: response = self._request(url, params) for item in response['result_data']: yield item # Last page lists itself as next page if response['service_meta']['next_page_number'] == params['page_number']: break params['page_number'] += 1"], "rank": 2}
{"nl": "python loess with gaussian kernel", "code": ["def kernel(self, spread=1): \"\"\" This will return whatever kind of kernel we want to use. Must have signature (ndarray size NxM, ndarray size 1xM) -> ndarray size Nx1 \"\"\" # TODO: use self.kernel_type to choose function def gaussian(data, pixel): return mvn.pdf(data, mean=pixel, cov=spread) return gaussian", "def _gaussian_function(self, datalength: int, values: np.ndarray, height: int, index: int) -> np.ndarray: \"\"\" i'th Regression Model Gaussian :param: len(x) :param: x values :param: height of gaussian :param: position of gaussian :return: gaussian bumps over domain \"\"\" return height * np.exp(-(1 / (self.spread_number * datalength)) * (values - ((datalength / self.function_number) * index)) ** 2)", "def gaussian_variogram_model(m, d): \"\"\"Gaussian model, m is [psill, range, nugget]\"\"\" psill = float(m[0]) range_ = float(m[1]) nugget = float(m[2]) return psill * (1. - np.exp(-d**2./(range_*4./7.)**2.)) + nugget", "def gaussian_kernel(gstd): \"\"\"Generate odd sized truncated Gaussian The generated filter kernel has a cutoff at $3\\sigma$ and is normalized to sum to 1 Parameters ------------- gstd : float Standard deviation of filter Returns ------------- g : ndarray Array with kernel coefficients \"\"\" Nc = np.ceil(gstd*3)*2+1 x = np.linspace(-(Nc-1)/2,(Nc-1)/2,Nc,endpoint=True) g = np.exp(-.5*((x/gstd)**2)) g = g/np.sum(g) return g", "def gauss_box_model(x, amplitude=1.0, mean=0.0, stddev=1.0, hpix=0.5): \"\"\"Integrate a Gaussian profile.\"\"\" z = (x - mean) / stddev z2 = z + hpix / stddev z1 = z - hpix / stddev return amplitude * (norm.cdf(z2) - norm.cdf(z1))"], "rank": 31}
{"nl": "greatest common divisor function in python", "code": ["def _gcd_array(X): \"\"\" Return the largest real value h such that all elements in x are integer multiples of h. \"\"\" greatest_common_divisor = 0.0 for x in X: greatest_common_divisor = _gcd(greatest_common_divisor, x) return greatest_common_divisor", "def gcd_float(numbers, tol=1e-8): \"\"\" Returns the greatest common divisor for a sequence of numbers. Uses a numerical tolerance, so can be used on floats Args: numbers: Sequence of numbers. tol: Numerical tolerance Returns: (int) Greatest common divisor of numbers. \"\"\" def pair_gcd_tol(a, b): \"\"\"Calculate the Greatest Common Divisor of a and b. Unless b==0, the result will have the same sign as b (so that when b is divided by it, the result comes out positive). \"\"\" while b > tol: a, b = b, a % b return a n = numbers[0] for i in numbers: n = pair_gcd_tol(n, i) return n", "def lcm(num1, num2): \"\"\" Find the lowest common multiple of 2 numbers :type num1: number :param num1: The first number to find the lcm for :type num2: number :param num2: The second number to find the lcm for \"\"\" if num1 > num2: bigger = num1 else: bigger = num2 while True: if bigger % num1 == 0 and bigger % num2 == 0: return bigger bigger += 1", "def most_common(items): \"\"\" Wanted functionality from Counters (new in Python 2.7). \"\"\" counts = {} for i in items: counts.setdefault(i, 0) counts[i] += 1 return max(six.iteritems(counts), key=operator.itemgetter(1))", "def _most_common(iterable): \"\"\"Returns the most common element in `iterable`.\"\"\" data = Counter(iterable) return max(data, key=data.__getitem__)"], "rank": 2}
{"nl": "how to clear up memory python", "code": ["def clear(self): \"\"\"Remove all items.\"\"\" self._fwdm.clear() self._invm.clear() self._sntl.nxt = self._sntl.prv = self._sntl", "def Flush(self): \"\"\"Flush all items from cache.\"\"\" while self._age: node = self._age.PopLeft() self.KillObject(node.data) self._hash = dict()", "def trim(self): \"\"\"Clear not used counters\"\"\" for key, value in list(iteritems(self.counters)): if value.empty(): del self.counters[key]", "def cleanup(self): \"\"\"Forcefully delete objects from memory In an ideal world, this shouldn't be necessary. Garbage collection guarantees that anything without reference is automatically removed. However, because this application is designed to be run multiple times from the same interpreter process, extra case must be taken to ensure there are no memory leaks. Explicitly deleting objects shines a light on where objects may still be referenced in the form of an error. No errors means this was uneccesary, but that's ok. \"\"\" for instance in self.context: del(instance) for plugin in self.plugins: del(plugin)", "def _release(self): \"\"\"Destroy self since closures cannot be called again.\"\"\" del self.funcs del self.variables del self.variable_values del self.satisfied"], "rank": 2}
{"nl": "get eucliedan distance between two vectors python", "code": ["def vector_distance(a, b): \"\"\"The Euclidean distance between two vectors.\"\"\" a = np.array(a) b = np.array(b) return np.linalg.norm(a - b)", "def _euclidean_dist(vector_a, vector_b): \"\"\" :param vector_a: A list of numbers. :param vector_b: A list of numbers. :returns: The euclidean distance between the two vectors. \"\"\" dist = 0 for (x, y) in zip(vector_a, vector_b): dist += (x-y)*(x-y) return math.sqrt(dist)", "def euclidean(c1, c2): \"\"\"Square of the euclidean distance\"\"\" diffs = ((i - j) for i, j in zip(c1, c2)) return sum(x * x for x in diffs)", "def euclidean(x, y): \"\"\"Standard euclidean distance. ..math:: D(x, y) = \\sqrt{\\sum_i (x_i - y_i)^2} \"\"\" result = 0.0 for i in range(x.shape[0]): result += (x[i] - y[i]) ** 2 return np.sqrt(result)", "def dist(x1, x2, axis=0): \"\"\"Return the distance between two points. Set axis=1 if x1 is a vector and x2 a matrix to get a vector of distances. \"\"\" return np.linalg.norm(x2 - x1, axis=axis)"], "rank": 1}
{"nl": "python interactive shell color", "code": ["def auto(): \"\"\"set colouring on if STDOUT is a terminal device, off otherwise\"\"\" try: Style.enabled = False Style.enabled = sys.stdout.isatty() except (AttributeError, TypeError): pass", "def determine_interactive(self): \"\"\"Determine whether we're in an interactive shell. Sets interactivity off if appropriate. cf http://stackoverflow.com/questions/24861351/how-to-detect-if-python-script-is-being-run-as-a-background-process \"\"\" try: if not sys.stdout.isatty() or os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno()): self.interactive = 0 return False except Exception: self.interactive = 0 return False if self.interactive == 0: return False return True", "def set(cls, color): \"\"\" Sets the terminal to the passed color. :param color: one of the availabe colors. \"\"\" sys.stdout.write(cls.colors.get(color, cls.colors['RESET']))", "def sprint(text, *colors): \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\" return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "def colorize(txt, fg=None, bg=None): \"\"\" Print escape codes to set the terminal color. fg and bg are indices into the color palette for the foreground and background colors. \"\"\" setting = '' setting += _SET_FG.format(fg) if fg else '' setting += _SET_BG.format(bg) if bg else '' return setting + str(txt) + _STYLE_RESET"], "rank": 1}
{"nl": "passing a range of values python years", "code": ["def from_years_range(start_year, end_year): \"\"\"Transform a range of years (two ints) to a DateRange object.\"\"\" start = datetime.date(start_year, 1 , 1) end = datetime.date(end_year, 12 , 31) return DateRange(start, end)", "def growthfromrange(rangegrowth, startdate, enddate): \"\"\" Annual growth given growth from start date to end date. \"\"\" _yrs = (pd.Timestamp(enddate) - pd.Timestamp(startdate)).total_seconds() /\\ dt.timedelta(365.25).total_seconds() return yrlygrowth(rangegrowth, _yrs)", "def daterange(start_date, end_date): \"\"\" Yield one date per day from starting date to ending date. Args: start_date (date): starting date. end_date (date): ending date. Yields: date: a date for each day within the range. \"\"\" for n in range(int((end_date - start_date).days)): yield start_date + timedelta(n)", "def get_year_start(day=None): \"\"\"Returns January 1 of the given year.\"\"\" day = add_timezone(day or datetime.date.today()) return day.replace(month=1).replace(day=1)", "def _xxrange(self, start, end, step_count): \"\"\"Generate n values between start and end.\"\"\" _step = (end - start) / float(step_count) return (start + (i * _step) for i in xrange(int(step_count)))"], "rank": 1}
{"nl": "python yaml for each key value", "code": ["def convert_to_yaml( name, value, indentation, indexOfColon, show_multi_line_character): \"\"\"converts a value list into yaml syntax :param name: name of object (example: phone) :type name: str :param value: object contents :type value: str, list(str), list(list(str)) :param indentation: indent all by number of spaces :type indentation: int :param indexOfColon: use to position : at the name string (-1 for no space) :type indexOfColon: int :param show_multi_line_character: option to hide \"|\" :type show_multi_line_character: boolean :returns: yaml formatted string array of name, value pair :rtype: list(str) \"\"\" strings = [] if isinstance(value, list): # special case for single item lists: if len(value) == 1 \\ and isinstance(value[0], str): # value = [\"string\"] should not be converted to # name: # - string # but to \"name: string\" instead value = value[0] elif len(value) == 1 \\ and isinstance(value[0], list) \\ and len(value[0]) == 1 \\ and isinstance(value[0][0], str): # same applies to value = [[\"string\"]] value = value[0][0] if isinstance(value, str): strings.append(\"%s%s%s: %s\" % ( ' ' * indentation, name, ' ' * (indexOfColon-len(name)), indent_multiline_string(value, indentation+4, show_multi_line_character))) elif isinstance(value, list): strings.append(\"%s%s%s: \" % ( ' ' * indentation, name, ' ' * (indexOfColon-len(name)))) for outer in value: # special case for single item sublists if isinstance(outer, list) \\ and len(outer) == 1 \\ and isinstance(outer[0], str): # outer = [\"string\"] should not be converted to # - # - string # but to \"- string\" instead outer = outer[0] if isinstance(outer, str): strings.append(\"%s- %s\" % ( ' ' * (indentation+4), indent_multiline_string( outer, indentation+8, show_multi_line_character))) elif isinstance(outer, list): strings.append(\"%s- \" % (' ' * (indentation+4))) for inner in outer: if isinstance(inner, str): strings.append(\"%s- %s\" % ( ' ' * (indentation+8), indent_multiline_string( inner, indentation+12, show_multi_line_character))) return strings", "def yaml(self): \"\"\" returns the yaml output of the dict. \"\"\" return ordered_dump(OrderedDict(self), Dumper=yaml.SafeDumper, default_flow_style=False)", "def string_presenter(self, dumper, data): \"\"\"Presenter to force yaml.dump to use multi-line string style.\"\"\" if '\\n' in data: return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|') else: return dumper.represent_scalar('tag:yaml.org,2002:str', data)", "def yaml_to_param(obj, name): \"\"\" Return the top-level element of a document sub-tree containing the YAML serialization of a Python object. \"\"\" return from_pyvalue(u\"yaml:%s\" % name, unicode(yaml.dump(obj)))", "def _dump_spec(spec): \"\"\"Dump bel specification dictionary using YAML Formats this with an extra indentation for lists to make it easier to use cold folding on the YAML version of the spec dictionary. \"\"\" with open(\"spec.yaml\", \"w\") as f: yaml.dump(spec, f, Dumper=MyDumper, default_flow_style=False)"], "rank": 7}
{"nl": "python test if value is ctypes array", "code": ["def is_array(type_): \"\"\"returns True, if type represents C++ array type, False otherwise\"\"\" nake_type = remove_alias(type_) nake_type = remove_reference(nake_type) nake_type = remove_cv(nake_type) return isinstance(nake_type, cpptypes.array_t)", "def c_array(ctype, values): \"\"\"Convert a python string to c array.\"\"\" if isinstance(values, np.ndarray) and values.dtype.itemsize == ctypes.sizeof(ctype): return (ctype * len(values)).from_buffer_copy(values) return (ctype * len(values))(*values)", "def cint8_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int8)): return np.fromiter(cptr, dtype=np.int8, count=length) else: raise RuntimeError('Expected int pointer')", "def cint32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes int pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_int32)): return np.fromiter(cptr, dtype=np.int32, count=length) else: raise RuntimeError('Expected int pointer')", "def cfloat64_array_to_numpy(cptr, length): \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_double)): return np.fromiter(cptr, dtype=np.float64, count=length) else: raise RuntimeError('Expected double pointer')"], "rank": 1}
{"nl": "function return apply async python", "code": ["def apply(self, func, args=(), kwds=dict()): \"\"\"Equivalent of the apply() builtin function. It blocks till the result is ready.\"\"\" return self.apply_async(func, args, kwds).get()", "def delegate(self, fn, *args, **kwargs): \"\"\"Return the given operation as an asyncio future.\"\"\" callback = functools.partial(fn, *args, **kwargs) coro = self.loop.run_in_executor(self.subexecutor, callback) return asyncio.ensure_future(coro)", "async def result_processor(tasks): \"\"\"An async result aggregator that combines all the results This gets executed in unsync.loop and unsync.thread\"\"\" output = {} for task in tasks: num, res = await task output[num] = res return output", "def run_task(func): \"\"\" Decorator to wrap an async function in an event loop. Use for main sync interface methods. \"\"\" def _wrapped(*a, **k): loop = asyncio.get_event_loop() return loop.run_until_complete(func(*a, **k)) return _wrapped", "async def _thread_coro(self, *args): \"\"\" Coroutine called by MapAsync. It's wrapping the call of run_in_executor to run the synchronous function as thread \"\"\" return await self._loop.run_in_executor( self._executor, self._function, *args)"], "rank": 1}
{"nl": "reload device program code in python", "code": ["def reload(self, save_config=True): \"\"\"Reload the device. !!!WARNING! there is unsaved configuration!!! This command will reboot the system. (y/n)? [n] \"\"\" if save_config: self.device.send(\"copy running-config startup-config\") self.device(\"reload\", wait_for_string=\"This command will reboot the system\") self.device.ctrl.sendline(\"y\")", "def autoreload(self, parameter_s=''): r\"\"\"%autoreload => Reload modules automatically %autoreload Reload all modules (except those excluded by %aimport) automatically now. %autoreload 0 Disable automatic reloading. %autoreload 1 Reload all modules imported with %aimport every time before executing the Python code typed. %autoreload 2 Reload all modules (except those excluded by %aimport) every time before executing the Python code typed. Reloading Python modules in a reliable way is in general difficult, and unexpected things may occur. %autoreload tries to work around common pitfalls by replacing function code objects and parts of classes previously in the module with new versions. This makes the following things to work: - Functions and classes imported via 'from xxx import foo' are upgraded to new versions when 'xxx' is reloaded. - Methods and properties of classes are upgraded on reload, so that calling 'c.foo()' on an object 'c' created before the reload causes the new code for 'foo' to be executed. Some of the known remaining caveats are: - Replacing code objects does not always succeed: changing a @property in a class to an ordinary method or a method to a member variable can cause problems (but in old objects only). - Functions that are removed (eg. via monkey-patching) from a module before it is reloaded are not upgraded. - C extension modules cannot be reloaded, and so cannot be autoreloaded. \"\"\" if parameter_s == '': self._reloader.check(True) elif parameter_s == '0': self._reloader.enabled = False elif parameter_s == '1': self._reloader.check_all = False self._reloader.enabled = True elif parameter_s == '2': self._reloader.check_all = True self._reloader.enabled = True", "def _load_autoreload_magic(self): \"\"\"Load %autoreload magic.\"\"\" from IPython.core.getipython import get_ipython try: get_ipython().run_line_magic('reload_ext', 'autoreload') get_ipython().run_line_magic('autoreload', '2') except Exception: pass", "def restart_program(): \"\"\" DOES NOT WORK WELL WITH MOPIDY Hack from https://www.daniweb.com/software-development/python/code/260268/restart-your-python-program to support updating the settings, since mopidy is not able to do that yet Restarts the current program Note: this function does not return. Any cleanup action (like saving data) must be done before calling this function \"\"\" python = sys.executable os.execl(python, python, * sys.argv)", "def clear_globals_reload_modules(self): \"\"\"Clears globals and reloads modules\"\"\" self.code_array.clear_globals() self.code_array.reload_modules() # Clear result cache self.code_array.result_cache.clear()"], "rank": 1}
{"nl": "split string into n parts python", "code": ["def _split_str(s, n): \"\"\" split string into list of strings by specified number. \"\"\" length = len(s) return [s[i:i + n] for i in range(0, length, n)]", "def schunk(string, size): \"\"\"Splits string into n sized chunks.\"\"\" return [string[i:i+size] for i in range(0, len(string), size)]", "def split_len(s, length): \"\"\"split string *s* into list of strings no longer than *length*\"\"\" return [s[i:i+length] for i in range(0, len(s), length)]", "def _split(string, splitters): \"\"\"Splits a string into parts at multiple characters\"\"\" part = '' for character in string: if character in splitters: yield part part = '' else: part += character yield part", "def group(data, num): \"\"\" Split data into chunks of num chars each \"\"\" return [data[i:i+num] for i in range(0, len(data), num)]"], "rank": 1}
{"nl": "python get object as dict", "code": ["def object_as_dict(obj): \"\"\"Turn an SQLAlchemy model into a dict of field names and values. Based on https://stackoverflow.com/a/37350445/1579058 \"\"\" return {c.key: getattr(obj, c.key) for c in inspect(obj).mapper.column_attrs}", "def as_dict(self): \"\"\"Package up the public attributes as a dict.\"\"\" attrs = vars(self) return {key: attrs[key] for key in attrs if not key.startswith('_')}", "def dict_self(self): \"\"\"Return the self object attributes not inherited as dict.\"\"\" return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}", "def _to_json(self): \"\"\" Gets a dict of this object's properties so that it can be used to send a dump to the client \"\"\" return dict(( (k, v) for k, v in self.__dict__.iteritems() if k != 'server'))", "def dict_from_object(obj: object): \"\"\"Convert a object into dictionary with all of its readable attributes.\"\"\" # If object is a dict instance, no need to convert. return (obj if isinstance(obj, dict) else {attr: getattr(obj, attr) for attr in dir(obj) if not attr.startswith('_')})"], "rank": 10}
{"nl": "selecting a range of 2d elements from a numpy array gives empty array in python 3", "code": ["def array(self): \"\"\" return the underlying numpy array \"\"\" return np.arange(self.start, self.stop, self.step)", "def select_from_array(cls, array, identifier): \"\"\"Return a region from a numpy array. :param array: :class:`numpy.ndarray` :param identifier: value representing the region to select in the array :returns: :class:`jicimagelib.region.Region` \"\"\" base_array = np.zeros(array.shape) array_coords = np.where(array == identifier) base_array[array_coords] = 1 return cls(base_array)", "def find_start_point(self): \"\"\" Find the first location in our array that is not empty \"\"\" for i, row in enumerate(self.data): for j, _ in enumerate(row): if self.data[i, j] != 0: # or not np.isfinite(self.data[i,j]): return i, j", "def _split_arrs(array_2d, slices): \"\"\" Equivalent to numpy.split(array_2d, slices), but avoids fancy indexing \"\"\" if len(array_2d) == 0: return np.empty(0, dtype=np.object) rtn = np.empty(len(slices) + 1, dtype=np.object) start = 0 for i, s in enumerate(slices): rtn[i] = array_2d[start:s] start = s rtn[-1] = array_2d[start:] return rtn", "def bounding_box(img): r\"\"\" Return the bounding box incorporating all non-zero values in the image. Parameters ---------- img : array_like An array containing non-zero objects. Returns ------- bbox : a list of slicer objects defining the bounding box \"\"\" locations = numpy.argwhere(img) mins = locations.min(0) maxs = locations.max(0) + 1 return [slice(x, y) for x, y in zip(mins, maxs)]"], "rank": 60}
{"nl": "python read tokens from line", "code": ["def get_tokens(line: str) -> Iterator[str]: \"\"\" Yields tokens from input string. :param line: Input string. :return: Iterator over tokens. \"\"\" for token in line.rstrip().split(): if len(token) > 0: yield token", "def listified_tokenizer(source): \"\"\"Tokenizes *source* and returns the tokens as a list of lists.\"\"\" io_obj = io.StringIO(source) return [list(a) for a in tokenize.generate_tokens(io_obj.readline)]", "def tokenize(string): \"\"\"Match and yield all the tokens of the input string.\"\"\" for match in TOKENS_REGEX.finditer(string): yield Token(match.lastgroup, match.group().strip(), match.span())", "def _next_token(self, skipws=True): \"\"\"Increment _token to the next token and return it.\"\"\" self._token = next(self._tokens).group(0) return self._next_token() if skipws and self._token.isspace() else self._token", "def extract_words(lines): \"\"\" Extract from the given iterable of lines the list of words. :param lines: an iterable of lines; :return: a generator of words of lines. \"\"\" for line in lines: for word in re.findall(r\"\\w+\", line): yield word"], "rank": 1}
{"nl": "write data into fits file python", "code": ["def write_fits(data, header, file_name): \"\"\" Combine data and a fits header to write a fits file. Parameters ---------- data : numpy.ndarray The data to be written. header : astropy.io.fits.hduheader The header for the fits file. file_name : string The file to write Returns ------- None \"\"\" hdu = fits.PrimaryHDU(data) hdu.header = header hdulist = fits.HDUList([hdu]) hdulist.writeto(file_name, overwrite=True) logging.info(\"Wrote {0}\".format(file_name)) return", "def write_fits(self, fitsfile): \"\"\"Write the ROI model to a FITS file.\"\"\" tab = self.create_table() hdu_data = fits.table_to_hdu(tab) hdus = [fits.PrimaryHDU(), hdu_data] fits_utils.write_hdus(hdus, fitsfile)", "def create_table_from_fits(fitsfile, hduname, colnames=None): \"\"\"Memory efficient function for loading a table from a FITS file.\"\"\" if colnames is None: return Table.read(fitsfile, hduname) cols = [] with fits.open(fitsfile, memmap=True) as h: for k in colnames: data = h[hduname].data.field(k) cols += [Column(name=k, data=data)] return Table(cols)", "def save_hdf(self,filename,path=''): \"\"\"Saves all relevant data to .h5 file; so state can be restored. \"\"\" self.dataframe.to_hdf(filename,'{}/df'.format(path))", "def add_column(filename,column,formula,force=False): \"\"\" Add a column to a FITS file. ADW: Could this be replaced by a ftool? \"\"\" columns = parse_formula(formula) logger.info(\"Running file: %s\"%filename) logger.debug(\" Reading columns: %s\"%columns) data = fitsio.read(filename,columns=columns) logger.debug(' Evaluating formula: %s'%formula) col = eval(formula) col = np.asarray(col,dtype=[(column,col.dtype)]) insert_columns(filename,col,force=force) return True"], "rank": 2}
{"nl": "python print string with visible ansi codes", "code": ["def sprint(text, *colors): \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\" return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "def println(msg): \"\"\" Convenience function to print messages on a single line in the terminal \"\"\" sys.stdout.write(msg) sys.stdout.flush() sys.stdout.write('\\x08' * len(msg)) sys.stdout.flush()", "def info(txt): \"\"\"Print, emphasized 'neutral', the given 'txt' message\"\"\" print(\"%s# %s%s%s\" % (PR_EMPH_CC, get_time_stamp(), txt, PR_NC)) sys.stdout.flush()", "def good(txt): \"\"\"Print, emphasized 'good', the given 'txt' message\"\"\" print(\"%s# %s%s%s\" % (PR_GOOD_CC, get_time_stamp(), txt, PR_NC)) sys.stdout.flush()", "def cprint(string, fg=None, bg=None, end='\\n', target=sys.stdout): \"\"\"Print a colored string to the target handle. fg and bg specify foreground- and background colors, respectively. The remaining keyword arguments are the same as for Python's built-in print function. Colors are returned to their defaults before the function returns. \"\"\" _color_manager.set_color(fg, bg) target.write(string + end) target.flush() # Needed for Python 3.x _color_manager.set_defaults()"], "rank": 11}
{"nl": "python check if value in enum", "code": ["def has_value(cls, value: int) -> bool: \"\"\"True if specified value exists in int enum; otherwise, False.\"\"\" return any(value == item.value for item in cls)", "def has_value_name(self, name): \"\"\"Check if this `enum` has a particular name among its values. :param name: Enumeration value name :type name: str :rtype: True if there is an enumeration value with the given name \"\"\" for val, _ in self._values: if val == name: return True return False", "def check(self, var): \"\"\"Check whether the provided value is a valid enum constant.\"\"\" if not isinstance(var, _str_type): return False return _enum_mangle(var) in self._consts", "def validate(self, val): \"\"\" Validates that the val is in the list of values for this Enum. Returns two element tuple: (bool, string) - `bool` - True if valid, False if not - `string` - Description of validation error, or None if valid :Parameters: val Value to validate. Should be a string. \"\"\" if val in self.values: return True, None else: return False, \"'%s' is not in enum: %s\" % (val, str(self.values))", "def has_enumerated_namespace_name(self, namespace: str, name: str) -> bool: \"\"\"Check that the namespace is defined by an enumeration and that the name is a member.\"\"\" return self.has_enumerated_namespace(namespace) and name in self.namespace_to_terms[namespace]"], "rank": 1}
{"nl": "python sqlite table names in database", "code": ["def get_table_names(connection): \"\"\" Return a list of the table names in the database. \"\"\" cursor = connection.cursor() cursor.execute(\"SELECT name FROM sqlite_master WHERE type == 'table'\") return [name for (name,) in cursor]", "def get_tablenames(cur): \"\"\" Conveinience: \"\"\" cur.execute(\"SELECT name FROM sqlite_master WHERE type='table'\") tablename_list_ = cur.fetchall() tablename_list = [str(tablename[0]) for tablename in tablename_list_ ] return tablename_list", "def get_table_list(dbconn): \"\"\" Get a list of tables that exist in dbconn :param dbconn: database connection :return: List of table names \"\"\" cur = dbconn.cursor() cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\") try: return [item[0] for item in cur.fetchall()] except IndexError: return get_table_list(dbconn)", "def get_table_names_from_metadata(metadata: MetaData) -> List[str]: \"\"\" Returns all database table names found in an SQLAlchemy :class:`MetaData` object. \"\"\" return [table.name for table in metadata.tables.values()]", "def get_table_columns(dbconn, tablename): \"\"\" Return a list of tuples specifying the column name and type \"\"\" cur = dbconn.cursor() cur.execute(\"PRAGMA table_info('%s');\" % tablename) info = cur.fetchall() cols = [(i[1], i[2]) for i in info] return cols"], "rank": 1}
{"nl": "how to remove all element from a python dictionary", "code": ["def _delete_keys(dct, keys): \"\"\"Returns a copy of dct without `keys` keys \"\"\" c = deepcopy(dct) assert isinstance(keys, list) for k in keys: c.pop(k) return c", "def rm_keys_from_dict(d, keys): \"\"\" Given a dictionary and a key list, remove any data in the dictionary with the given keys. :param dict d: Metadata :param list keys: Keys to be removed :return dict d: Metadata \"\"\" # Loop for each key given for key in keys: # Is the key in the dictionary? if key in d: try: d.pop(key, None) except KeyError: # Not concerned with an error. Keep going. pass return d", "def _remove_none_values(dictionary): \"\"\" Remove dictionary keys whose value is None \"\"\" return list(map(dictionary.pop, [i for i in dictionary if dictionary[i] is None]))", "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]: \"\"\" Return a new copied dictionary without the keys with ``None`` values from the given Mapping object. \"\"\" return {k: v for k, v in obj.items() if v is not None}", "def filter_dict_by_key(d, keys): \"\"\"Filter the dict *d* to remove keys not in *keys*.\"\"\" return {k: v for k, v in d.items() if k in keys}"], "rank": 4}
{"nl": "python parse query string from url", "code": ["def parse_query_string(query): \"\"\" parse_query_string: very simplistic. won't do the right thing with list values \"\"\" result = {} qparts = query.split('&') for item in qparts: key, value = item.split('=') key = key.strip() value = value.strip() result[key] = unquote_plus(value) return result", "def get_querystring(uri): \"\"\"Get Querystring information from uri. :param uri: uri :return: querystring info or {} \"\"\" parts = urlparse.urlsplit(uri) return urlparse.parse_qs(parts.query)", "def get_url_args(url): \"\"\" Returns a dictionary from a URL params \"\"\" url_data = urllib.parse.urlparse(url) arg_dict = urllib.parse.parse_qs(url_data.query) return arg_dict", "def urlencoded(body, charset='ascii', **kwargs): \"\"\"Converts query strings into native Python objects\"\"\" return parse_query_string(text(body, charset=charset), False)", "def strip_querystring(url): \"\"\"Remove the querystring from the end of a URL.\"\"\" p = six.moves.urllib.parse.urlparse(url) return p.scheme + \"://\" + p.netloc + p.path"], "rank": 1}
{"nl": "python remove element set", "code": ["def discard(self, element): \"\"\"Remove element from the RangeSet if it is a member. If the element is not a member, do nothing. \"\"\" try: i = int(element) set.discard(self, i) except ValueError: pass", "def remove_once(gset, elem): \"\"\"Remove the element from a set, lists or dict. >>> L = [\"Lucy\"]; S = set([\"Sky\"]); D = { \"Diamonds\": True }; >>> remove_once(L, \"Lucy\"); remove_once(S, \"Sky\"); remove_once(D, \"Diamonds\"); >>> print L, S, D [] set([]) {} Returns the element if it was removed. Raises one of the exceptions in :obj:`RemoveError` otherwise. \"\"\" remove = getattr(gset, 'remove', None) if remove is not None: remove(elem) else: del gset[elem] return elem", "def isolate_element(self, x): \"\"\"Isolates `x` from its equivalence class.\"\"\" members = list(self.members(x)) self.delete_set(x) self.union(*(v for v in members if v != x))", "def add(self, value): \"\"\"Add the element *value* to the set.\"\"\" if value not in self._set: self._set.add(value) self._list.add(value)", "def __isub__(self, other): \"\"\"Remove all elements of another set from this RangeSet.\"\"\" self._binary_sanity_check(other) set.difference_update(self, other) return self"], "rank": 3}
{"nl": "manhattan distance in python using longitude and latitude", "code": ["def _manhattan_distance(vec_a, vec_b): \"\"\"Return manhattan distance between two lists of numbers.\"\"\" if len(vec_a) != len(vec_b): raise ValueError('len(vec_a) must equal len(vec_b)') return sum(map(lambda a, b: abs(a - b), vec_a, vec_b))", "def manhattan_distance_numpy(object1, object2): \"\"\"! @brief Calculate Manhattan distance between two objects using numpy. @param[in] object1 (array_like): The first array_like object. @param[in] object2 (array_like): The second array_like object. @return (double) Manhattan distance between two objects. \"\"\" return numpy.sum(numpy.absolute(object1 - object2), axis=1).T", "def distL1(x1,y1,x2,y2): \"\"\"Compute the L1-norm (Manhattan) distance between two points. The distance is rounded to the closest integer, for compatibility with the TSPLIB convention. The two points are located on coordinates (x1,y1) and (x2,y2), sent as parameters\"\"\" return int(abs(x2-x1) + abs(y2-y1)+.5)", "def _calculate_distance(latlon1, latlon2): \"\"\"Calculates the distance between two points on earth. \"\"\" lat1, lon1 = latlon1 lat2, lon2 = latlon2 dlon = lon2 - lon1 dlat = lat2 - lat1 R = 6371 # radius of the earth in kilometers a = np.sin(dlat / 2)**2 + np.cos(lat1) * np.cos(lat2) * (np.sin(dlon / 2))**2 c = 2 * np.pi * R * np.arctan2(np.sqrt(a), np.sqrt(1 - a)) / 180 return c", "def manhattan(h1, h2): # # 7 us @array, 31 us @list \\w 100 bins r\"\"\" Equal to Minowski distance with :math:`p=1`. See also -------- minowski \"\"\" h1, h2 = __prepare_histogram(h1, h2) return scipy.sum(scipy.absolute(h1 - h2))"], "rank": 1}
{"nl": "how to read from a file to a list python", "code": ["def get_list_from_file(file_name): \"\"\"read the lines from a file into a list\"\"\" with open(file_name, mode='r', encoding='utf-8') as f1: lst = f1.readlines() return lst", "def r_num(obj): \"\"\"Read list of numbers.\"\"\" if isinstance(obj, (list, tuple)): it = iter else: it = LinesIterator dataset = Dataset([Dataset.FLOAT]) return dataset.load(it(obj))", "def csvtolist(inputstr): \"\"\" converts a csv string into a list \"\"\" reader = csv.reader([inputstr], skipinitialspace=True) output = [] for r in reader: output += r return output", "def readCommaList(fileList): \"\"\" Return a list of the files with the commas removed. \"\"\" names=fileList.split(',') fileList=[] for item in names: fileList.append(item) return fileList", "def get_dt_list(fn_list): \"\"\"Get list of datetime objects, extracted from a filename \"\"\" dt_list = np.array([fn_getdatetime(fn) for fn in fn_list]) return dt_list"], "rank": 1}
{"nl": "dynamically update value in dictionary python", "code": ["def update(self, params): \"\"\"Update the dev_info data from a dictionary. Only updates if it already exists in the device. \"\"\" dev_info = self.json_state.get('deviceInfo') dev_info.update({k: params[k] for k in params if dev_info.get(k)})", "def set_property(self, key, value): \"\"\" Update only one property in the dict \"\"\" self.properties[key] = value self.sync_properties()", "def update_dict(obj, dict, attributes): \"\"\"Update dict with fields from obj.attributes. :param obj: the object updated into dict :param dict: the result dictionary :param attributes: a list of attributes belonging to obj \"\"\" for attribute in attributes: if hasattr(obj, attribute) and getattr(obj, attribute) is not None: dict[attribute] = getattr(obj, attribute)", "def copy_and_update(dictionary, update): \"\"\"Returns an updated copy of the dictionary without modifying the original\"\"\" newdict = dictionary.copy() newdict.update(update) return newdict", "def update(self, *args, **kwargs): \"\"\" A handy update() method which returns self :) :rtype: DictProxy \"\"\" super(DictProxy, self).update(*args, **kwargs) return self"], "rank": 1}
{"nl": "python replace string from right", "code": ["def right_replace(string, old, new, count=1): \"\"\" Right replaces ``count`` occurrences of ``old`` with ``new`` in ``string``. For example:: right_replace('one_two_two', 'two', 'three') -> 'one_two_three' \"\"\" if not string: return string return new.join(string.rsplit(old, count))", "def replace(self, text): \"\"\"Do j/v replacement\"\"\" for (pattern, repl) in self.patterns: text = re.subn(pattern, repl, text)[0] return text", "def subat(orig, index, replace): \"\"\"Substitutes the replacement string/character at the given index in the given string, returns the modified string. **Examples**: :: auxly.stringy.subat(\"bit\", 2, \"n\") \"\"\" return \"\".join([(orig[x] if x != index else replace) for x in range(len(orig))])", "def replace(s, replace): \"\"\"Replace multiple values in a string\"\"\" for r in replace: s = s.replace(*r) return s", "def replace(s, old, new, maxreplace=-1): \"\"\"replace (str, old, new[, maxreplace]) -> string Return a copy of string str with all occurrences of substring old replaced by new. If the optional argument maxreplace is given, only the first maxreplace occurrences are replaced. \"\"\" return s.replace(old, new, maxreplace)"], "rank": 1}
{"nl": "how to check if a path is writeable python", "code": ["def _writable_dir(path): \"\"\"Whether `path` is a directory, to which the user has write access.\"\"\" return os.path.isdir(path) and os.access(path, os.W_OK)", "def readable(path): \"\"\"Test whether a path exists and is readable. Returns None for broken symbolic links or a failing stat() and False if the file exists but does not have read permission. True is returned if the file is readable.\"\"\" try: st = os.stat(path) return 0 != st.st_mode & READABLE_MASK except os.error: return None return True", "def is_writable_by_others(filename): \"\"\"Check if file or directory is world writable.\"\"\" mode = os.stat(filename)[stat.ST_MODE] return mode & stat.S_IWOTH", "def make_writeable(filename): \"\"\" Make sure that the file is writeable. Useful if our source is read-only. \"\"\" if not os.access(filename, os.W_OK): st = os.stat(filename) new_permissions = stat.S_IMODE(st.st_mode) | stat.S_IWUSR os.chmod(filename, new_permissions)", "def make_file_readable (filename): \"\"\"Make file user readable if it is not a link.\"\"\" if not os.path.islink(filename): util.set_mode(filename, stat.S_IRUSR)"], "rank": 1}
{"nl": "get attribute type in python", "code": ["def get_kind(self, value): \"\"\"Return the kind (type) of the attribute\"\"\" if isinstance(value, float): return 'f' elif isinstance(value, int): return 'i' else: raise ValueError(\"Only integer or floating point values can be stored.\")", "def get_type(self): \"\"\"Get the type of the item. :return: the type of the item. :returntype: `unicode`\"\"\" item_type = self.xmlnode.prop(\"type\") if not item_type: item_type = \"?\" return item_type.decode(\"utf-8\")", "def _get_type(self, value): \"\"\"Get the data type for *value*.\"\"\" if value is None: return type(None) elif type(value) in int_types: return int elif type(value) in float_types: return float elif isinstance(value, binary_type): return binary_type else: return text_type", "def static_get_type_attr(t, name): \"\"\" Get a type attribute statically, circumventing the descriptor protocol. \"\"\" for type_ in t.mro(): try: return vars(type_)[name] except KeyError: pass raise AttributeError(name)", "def type(self): \"\"\"Returns type of the data for the given FeatureType.\"\"\" if self is FeatureType.TIMESTAMP: return list if self is FeatureType.BBOX: return BBox return dict"], "rank": 14}
{"nl": "get fields of object python", "code": ["def fields(self): \"\"\"Returns the list of field names of the model.\"\"\" return (self.attributes.values() + self.lists.values() + self.references.values())", "def dict_from_object(obj: object): \"\"\"Convert a object into dictionary with all of its readable attributes.\"\"\" # If object is a dict instance, no need to convert. return (obj if isinstance(obj, dict) else {attr: getattr(obj, attr) for attr in dir(obj) if not attr.startswith('_')})", "def dict_self(self): \"\"\"Return the self object attributes not inherited as dict.\"\"\" return {k: v for k, v in self.__dict__.items() if k in FSM_ATTRS}", "def _basic_field_data(field, obj): \"\"\"Returns ``obj.field`` data as a dict\"\"\" value = field.value_from_object(obj) return {Field.TYPE: FieldType.VAL, Field.VALUE: value}", "def object_as_dict(obj): \"\"\"Turn an SQLAlchemy model into a dict of field names and values. Based on https://stackoverflow.com/a/37350445/1579058 \"\"\" return {c.key: getattr(obj, c.key) for c in inspect(obj).mapper.column_attrs}"], "rank": 5}
{"nl": "how to fetch one value from one row from mysql query in python", "code": ["def fetchvalue(self, sql: str, *args) -> Optional[Any]: \"\"\"Executes SQL; returns the first value of the first row, or None.\"\"\" row = self.fetchone(sql, *args) if row is None: return None return row[0]", "def query_fetch_one(self, query, values): \"\"\" Executes a db query, gets the first value, and closes the connection. \"\"\" self.cursor.execute(query, values) retval = self.cursor.fetchone() self.__close_db() return retval", "def query_proc_row(procname, args=(), factory=None): \"\"\" Execute a stored procedure. Returns the first row of the result set, or `None`. \"\"\" for row in query_proc(procname, args, factory): return row return None", "def fetchallfirstvalues(self, sql: str, *args) -> List[Any]: \"\"\"Executes SQL; returns list of first values of each row.\"\"\" rows = self.fetchall(sql, *args) return [row[0] for row in rows]", "def fetch(table, cols=\"*\", where=(), group=\"\", order=(), limit=(), **kwargs): \"\"\"Convenience wrapper for database SELECT and fetch all.\"\"\" return select(table, cols, where, group, order, limit, **kwargs).fetchall()"], "rank": 1}
{"nl": "python flask routes add", "code": ["def add_url_rule(self, route, endpoint, handler): \"\"\"Add a new url route. Args: See flask.Flask.add_url_route(). \"\"\" self.app.add_url_rule(route, endpoint, handler)", "def register(self, target): \"\"\"Registers url_rules on the blueprint \"\"\" for rule, options in self.url_rules: target.add_url_rule(rule, self.name, self.dispatch_request, **options)", "def http(self, *args, **kwargs): \"\"\"Starts the process of building a new HTTP route linked to this API instance\"\"\" kwargs['api'] = self.api return http(*args, **kwargs)", "def add_swagger(app, json_route, html_route): \"\"\" a convenience method for both adding a swagger.json route, as well as adding a page showing the html documentation \"\"\" app.router.add_route('GET', json_route, create_swagger_json_handler(app)) add_swagger_api_route(app, html_route, json_route)", "def initialize_api(flask_app): \"\"\"Initialize an API.\"\"\" if not flask_restplus: return api = flask_restplus.Api(version=\"1.0\", title=\"My Example API\") api.add_resource(HelloWorld, \"/hello\") blueprint = flask.Blueprint(\"api\", __name__, url_prefix=\"/api\") api.init_app(blueprint) flask_app.register_blueprint(blueprint)"], "rank": 1}
{"nl": "define function arg type and default values python", "code": ["def get_default_args(func): \"\"\" returns a dictionary of arg_name:default_values for the input function \"\"\" args, varargs, keywords, defaults = getargspec_no_self(func) return dict(zip(args[-len(defaults):], defaults))", "def validate_args(**args): \"\"\" function to check if input query is not None and set missing arguments to default value \"\"\" if not args['query']: print(\"\\nMissing required query argument.\") sys.exit() for key in DEFAULTS: if key not in args: args[key] = DEFAULTS[key] return args", "def with_defaults(method, nparams, defaults=None): \"\"\"Call method with nparams positional parameters, all non-specified defaults are passed None. :method: the method to call :nparams: the number of parameters the function expects :defaults: the default values to pass in for the last len(defaults) params \"\"\" args = [None] * nparams if not defaults else defaults + max(nparams - len(defaults), 0) * [None] return method(*args)", "def arg_default(*args, **kwargs): \"\"\"Return default argument value as given by argparse's add_argument(). The argument is passed through a mocked-up argument parser. This way, we get default parameters even if the feature is called directly and not through the CLI. \"\"\" parser = argparse.ArgumentParser() parser.add_argument(*args, **kwargs) args = vars(parser.parse_args([])) _, default = args.popitem() return default", "def return_type(type_name, formatter=None): \"\"\"Specify that this function returns a typed value. Args: type_name (str): A type name known to the global typedargs type system formatter (str): An optional name of a formatting function specified for the type given in type_name. \"\"\" def _returns(func): annotated(func) func.metadata.typed_returnvalue(type_name, formatter) return func return _returns"], "rank": 1}
{"nl": "python image shape detect", "code": ["def get_shape(img): \"\"\"Return the shape of img. Paramerers ----------- img: Returns ------- shape: tuple \"\"\" if hasattr(img, 'shape'): shape = img.shape else: shape = img.get_data().shape return shape", "def _validate_image_rank(self, img_array): \"\"\" Images must be either 2D or 3D. \"\"\" if img_array.ndim == 1 or img_array.ndim > 3: msg = \"{0}D imagery is not allowed.\".format(img_array.ndim) raise IOError(msg)", "def calculate_dimensions(image, long_side, short_side): \"\"\"Returns the thumbnail dimensions depending on the images format.\"\"\" if image.width >= image.height: return '{0}x{1}'.format(long_side, short_side) return '{0}x{1}'.format(short_side, long_side)", "def get_shape_mask(self, shape_obj): \"\"\" Return full mask where True marks pixels within the given shape. \"\"\" wd, ht = self.get_size() yi = np.mgrid[:ht].reshape(-1, 1) xi = np.mgrid[:wd].reshape(1, -1) pts = np.asarray((xi, yi)).T contains = shape_obj.contains_pts(pts) return contains", "def _scale_shape(dshape, scale = (1,1,1)): \"\"\"returns the shape after scaling (should be the same as ndimage.zoom\"\"\" nshape = np.round(np.array(dshape) * np.array(scale)) return tuple(nshape.astype(np.int))"], "rank": 1}
{"nl": "what can iterators be iterated only once in python", "code": ["def __iter__(self): \"\"\"Define a generator function and return it\"\"\" def generator(): for i, obj in enumerate(self._sequence): if i >= self._limit: break yield obj raise StopIteration return generator", "def __iter__(self): \"\"\"Iterate through all elements. Multiple copies will be returned if they exist. \"\"\" for value, count in self.counts(): for _ in range(count): yield value", "def peekiter(iterable): \"\"\"Return first row and also iterable with same items as original\"\"\" it = iter(iterable) one = next(it) def gen(): \"\"\"Generator that returns first and proxy other items from source\"\"\" yield one while True: yield next(it) return (one, gen())", "def iterate(obj): \"\"\"Loop over an iterable and track progress, including first and last state. On each iteration yield an Iteration named tuple with the first and last flags, current element index, total iterable length (if possible to acquire), and value, in that order. for iteration in iterate(something): iteration.value # Do something. You can unpack these safely: for first, last, index, total, value in iterate(something): pass If you want to unpack the values you are iterating across, you can by wrapping the nested unpacking in parenthesis: for first, last, index, total, (foo, bar, baz) in iterate(something): pass Even if the length of the iterable can't be reliably determined this function will still capture the \"last\" state of the final loop iteration. (Basically: this works with generators.) This process is about 10x slower than simple enumeration on CPython 3.4, so only use it where you actually need to track state. Use `enumerate()` elsewhere. \"\"\" global next, Iteration next = next Iteration = Iteration total = len(obj) if isinstance(obj, Sized) else None iterator = iter(obj) first = True last = False i = 0 try: value = next(iterator) except StopIteration: return while True: try: next_value = next(iterator) except StopIteration: last = True yield Iteration(first, last, i, total, value) if last: return value = next_value i += 1 first = False", "def _fill(self): \"\"\"Advance the iterator without returning the old head.\"\"\" try: self._head = self._iterable.next() except StopIteration: self._head = None"], "rank": 5}
{"nl": "remove all characters in string in python", "code": ["def clean(self, text): \"\"\"Remove all unwanted characters from text.\"\"\" return ''.join([c for c in text if c in self.alphabet])", "def clean(ctx, text): \"\"\" Removes all non-printable characters from a text string \"\"\" text = conversions.to_string(text, ctx) return ''.join([c for c in text if ord(c) >= 32])", "def strip_accents(string): \"\"\" Strip all the accents from the string \"\"\" return u''.join( (character for character in unicodedata.normalize('NFD', string) if unicodedata.category(character) != 'Mn'))", "def strip_accents(text): \"\"\" Strip agents from a string. \"\"\" normalized_str = unicodedata.normalize('NFD', text) return ''.join([ c for c in normalized_str if unicodedata.category(c) != 'Mn'])", "def strip_accents(s): \"\"\" Strip accents to prepare for slugification. \"\"\" nfkd = unicodedata.normalize('NFKD', unicode(s)) return u''.join(ch for ch in nfkd if not unicodedata.combining(ch))"], "rank": 16}
{"nl": "python circle in a square bitmap array", "code": ["def circles_pycairo(width, height, color): \"\"\" Implementation of circle border with PyCairo. \"\"\" cairo_color = color / rgb(255, 255, 255) surface = cairo.ImageSurface(cairo.FORMAT_ARGB32, width, height) ctx = cairo.Context(surface) # draw a circle in the center ctx.new_path() ctx.set_source_rgb(cairo_color.red, cairo_color.green, cairo_color.blue) ctx.arc(width / 2, height / 2, width / 2, 0, 2 * pi) ctx.fill() surface.write_to_png('circles.png')", "def round_corner(radius, fill): \"\"\"Draw a round corner\"\"\" corner = Image.new('L', (radius, radius), 0) # (0, 0, 0, 0)) draw = ImageDraw.Draw(corner) draw.pieslice((0, 0, radius * 2, radius * 2), 180, 270, fill=fill) return corner", "def getBitmap(self): \"\"\" Captures screen area of this region, at least the part that is on the screen Returns image as numpy array \"\"\" return PlatformManager.getBitmapFromRect(self.x, self.y, self.w, self.h)", "def border(self): \"\"\"Region formed by taking border elements. :returns: :class:`jicimagelib.region.Region` \"\"\" border_array = self.bitmap - self.inner.bitmap return Region(border_array)", "def draw_image(self, ax, image): \"\"\"Process a matplotlib image object and call renderer.draw_image\"\"\" self.renderer.draw_image(imdata=utils.image_to_base64(image), extent=image.get_extent(), coordinates=\"data\", style={\"alpha\": image.get_alpha(), \"zorder\": image.get_zorder()}, mplobj=image)"], "rank": 3}
{"nl": "location of maya python exe", "code": ["def setup_environment(): \"\"\"Set up neccessary environment variables This appends all path of sys.path to the python path so mayapy will find all installed modules. We have to make sure, that we use maya libs instead of libs of the virtual env. So we insert all the libs for mayapy first. :returns: None :rtype: None :raises: None \"\"\" osinter = ostool.get_interface() pypath = osinter.get_maya_envpath() for p in sys.path: pypath = os.pathsep.join((pypath, p)) os.environ['PYTHONPATH'] = pypath", "def main_func(args=None): \"\"\"Main funcion when executing this module as script :param args: commandline arguments :type args: list :returns: None :rtype: None :raises: None \"\"\" # we have to initialize a gui even if we dont need one right now. # as soon as you call maya.standalone.initialize(), a QApplication # with type Tty is created. This is the type for conosle apps. # Because i have not found a way to replace that, we just init the gui. guimain.init_gui() main.init() launcher = Launcher() parsed, unknown = launcher.parse_args(args) parsed.func(parsed, unknown)", "def _find_conda(): \"\"\"Find the conda executable robustly across conda versions. Returns ------- conda : str Path to the conda executable. Raises ------ IOError If the executable cannot be found in either the CONDA_EXE environment variable or in the PATH. Notes ----- In POSIX platforms in conda >= 4.4, conda can be set up as a bash function rather than an executable. (This is to enable the syntax ``conda activate env-name``.) In this case, the environment variable ``CONDA_EXE`` contains the path to the conda executable. In other cases, we use standard search for the appropriate name in the PATH. See https://github.com/airspeed-velocity/asv/issues/645 for more details. \"\"\" if 'CONDA_EXE' in os.environ: conda = os.environ['CONDA_EXE'] else: conda = util.which('conda') return conda", "def locate(command, on): \"\"\"Locate the command's man page.\"\"\" location = find_page_location(command, on) click.echo(location)", "def perl_cmd(): \"\"\"Retrieve path to locally installed conda Perl or first in PATH. \"\"\" perl = which(os.path.join(get_bcbio_bin(), \"perl\")) if perl: return perl else: return which(\"perl\")"], "rank": 1}
{"nl": "how to make letters uppercase in python skipping spaces", "code": ["def uppercase_chars(string: any) -> str: \"\"\"Return all (and only) the uppercase chars in the given string.\"\"\" return ''.join([c if c.isupper() else '' for c in str(string)])", "def to_pascal_case(s): \"\"\"Transform underscore separated string to pascal case \"\"\" return re.sub(r'(?!^)_([a-zA-Z])', lambda m: m.group(1).upper(), s.capitalize())", "def snake_to_camel(s: str) -> str: \"\"\"Convert string from snake case to camel case.\"\"\" fragments = s.split('_') return fragments[0] + ''.join(x.title() for x in fragments[1:])", "def to_camel_case(text): \"\"\"Convert to camel case. :param str text: :rtype: str :return: \"\"\" split = text.split('_') return split[0] + \"\".join(x.title() for x in split[1:])", "def _upper(val_list): \"\"\" :param val_list: a list of strings :return: a list of upper-cased strings \"\"\" res = [] for ele in val_list: res.append(ele.upper()) return res"], "rank": 1}
{"nl": "python delete element from set", "code": ["def discard(self, element): \"\"\"Remove element from the RangeSet if it is a member. If the element is not a member, do nothing. \"\"\" try: i = int(element) set.discard(self, i) except ValueError: pass", "def remove_once(gset, elem): \"\"\"Remove the element from a set, lists or dict. >>> L = [\"Lucy\"]; S = set([\"Sky\"]); D = { \"Diamonds\": True }; >>> remove_once(L, \"Lucy\"); remove_once(S, \"Sky\"); remove_once(D, \"Diamonds\"); >>> print L, S, D [] set([]) {} Returns the element if it was removed. Raises one of the exceptions in :obj:`RemoveError` otherwise. \"\"\" remove = getattr(gset, 'remove', None) if remove is not None: remove(elem) else: del gset[elem] return elem", "def add(self, value): \"\"\"Add the element *value* to the set.\"\"\" if value not in self._set: self._set.add(value) self._list.add(value)", "def __isub__(self, other): \"\"\"Remove all elements of another set from this RangeSet.\"\"\" self._binary_sanity_check(other) set.difference_update(self, other) return self", "def isolate_element(self, x): \"\"\"Isolates `x` from its equivalence class.\"\"\" members = list(self.members(x)) self.delete_set(x) self.union(*(v for v in members if v != x))"], "rank": 2}
{"nl": "python get current git branch", "code": ["def get_current_branch(): \"\"\" Return the current branch \"\"\" cmd = [\"git\", \"rev-parse\", \"--abbrev-ref\", \"HEAD\"] output = subprocess.check_output(cmd, stderr=subprocess.STDOUT) return output.strip().decode(\"utf-8\")", "def get_git_branch(git_path='git'): \"\"\"Returns the name of the current git branch \"\"\" branch_match = call((git_path, 'rev-parse', '--symbolic-full-name', 'HEAD')) if branch_match == \"HEAD\": return None else: return os.path.basename(branch_match)", "def branches(): # type: () -> List[str] \"\"\" Return a list of branches in the current repo. Returns: list[str]: A list of branches in the current repo. \"\"\" out = shell.run( 'git branch', capture=True, never_pretend=True ).stdout.strip() return [x.strip('* \\t\\n') for x in out.splitlines()]", "def branches(self): \"\"\"All branches in a list\"\"\" result = self.git(self.default + ['branch', '-a', '--no-color']) return [l.strip(' *\\n') for l in result.split('\\n') if l.strip(' *\\n')]", "def get_last_commit(git_path=None): \"\"\" Get the HEAD commit SHA1 of repository in current dir. \"\"\" if git_path is None: git_path = GIT_PATH line = get_last_commit_line(git_path) revision_id = line.split()[1] return revision_id"], "rank": 5}
{"nl": "how to make a restart button using python", "code": ["def do_restart(self, line): \"\"\"Request that the Outstation perform a cold restart. Command syntax is: restart\"\"\" self.application.master.Restart(opendnp3.RestartType.COLD, restart_callback)", "def restart_program(): \"\"\" DOES NOT WORK WELL WITH MOPIDY Hack from https://www.daniweb.com/software-development/python/code/260268/restart-your-python-program to support updating the settings, since mopidy is not able to do that yet Restarts the current program Note: this function does not return. Any cleanup action (like saving data) must be done before calling this function \"\"\" python = sys.executable os.execl(python, python, * sys.argv)", "async def restart(request): \"\"\" Returns OK, then waits approximately 1 second and restarts container \"\"\" def wait_and_restart(): log.info('Restarting server') sleep(1) os.system('kill 1') Thread(target=wait_and_restart).start() return web.json_response({\"message\": \"restarting\"})", "def restart(self, reset=False): \"\"\" Quit and Restart Spyder application. If reset True it allows to reset spyder on restart. \"\"\" # Get start path to use in restart script spyder_start_directory = get_module_path('spyder') restart_script = osp.join(spyder_start_directory, 'app', 'restart.py') # Get any initial argument passed when spyder was started # Note: Variables defined in bootstrap.py and spyder/app/start.py env = os.environ.copy() bootstrap_args = env.pop('SPYDER_BOOTSTRAP_ARGS', None) spyder_args = env.pop('SPYDER_ARGS') # Get current process and python running spyder pid = os.getpid() python = sys.executable # Check if started with bootstrap.py if bootstrap_args is not None: spyder_args = bootstrap_args is_bootstrap = True else: is_bootstrap = False # Pass variables as environment variables (str) to restarter subprocess env['SPYDER_ARGS'] = spyder_args env['SPYDER_PID'] = str(pid) env['SPYDER_IS_BOOTSTRAP'] = str(is_bootstrap) env['SPYDER_RESET'] = str(reset) if DEV: if os.name == 'nt': env['PYTHONPATH'] = ';'.join(sys.path) else: env['PYTHONPATH'] = ':'.join(sys.path) # Build the command and popen arguments depending on the OS if os.name == 'nt': # Hide flashing command prompt startupinfo = subprocess.STARTUPINFO() startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW shell = False else: startupinfo = None shell = True command = '\"{0}\" \"{1}\"' command = command.format(python, restart_script) try: if self.closing(True): subprocess.Popen(command, shell=shell, env=env, startupinfo=startupinfo) self.console.quit() except Exception as error: # If there is an error with subprocess, Spyder should not quit and # the error can be inspected in the internal console print(error) # spyder: test-skip print(command)", "def do_restart(self, line): \"\"\" Attempt to restart the bot. \"\"\" self.bot._frame = 0 self.bot._namespace.clear() self.bot._namespace.update(self.bot._initial_namespace)"], "rank": 1}
{"nl": "tracing python code execution", "code": ["def _trace_full (frame, event, arg): \"\"\"Trace every executed line.\"\"\" if event == \"line\": _trace_line(frame, event, arg) else: _trace(frame, event, arg) return _trace_full", "def __run(self): \"\"\"Hacked run function, which installs the trace.\"\"\" sys.settrace(self.globaltrace) self.__run_backup() self.run = self.__run_backup", "def memory_usage(method): \"\"\"Log memory usage before and after a method.\"\"\" def wrapper(*args, **kwargs): logging.info('Memory before method %s is %s.', method.__name__, runtime.memory_usage().current()) result = method(*args, **kwargs) logging.info('Memory after method %s is %s', method.__name__, runtime.memory_usage().current()) return result return wrapper", "def set_trace(): \"\"\"Start a Pdb instance at the calling frame, with stdout routed to sys.__stdout__.\"\"\" # https://github.com/nose-devs/nose/blob/master/nose/tools/nontrivial.py pdb.Pdb(stdout=sys.__stdout__).set_trace(sys._getframe().f_back)", "def timeit(method): \"\"\" A Python decorator for printing out the execution time for a function. Adapted from: www.andreas-jung.com/contents/a-python-decorator-for-measuring-the-execution-time-of-methods \"\"\" def timed(*args, **kw): time_start = time.time() result = method(*args, **kw) time_end = time.time() print('timeit: %r %2.2f sec (%r, %r) ' % (method.__name__, time_end-time_start, str(args)[:20], kw)) return result return timed"], "rank": 2}
{"nl": "delete pyc files from python script", "code": ["def clean(dry_run='n'): \"\"\"Wipes compiled and cached python files. To simulate: pynt clean[dry_run=y]\"\"\" file_patterns = ['*.pyc', '*.pyo', '*~'] dir_patterns = ['__pycache__'] recursive_pattern_delete(project_paths.root, file_patterns, dir_patterns, dry_run=bool(dry_run.lower() == 'y'))", "def clean(): \"\"\"clean - remove build artifacts.\"\"\" run('rm -rf build/') run('rm -rf dist/') run('rm -rf puzzle.egg-info') run('find . -name __pycache__ -delete') run('find . -name *.pyc -delete') run('find . -name *.pyo -delete') run('find . -name *~ -delete') log.info('cleaned up')", "def cli(ctx, project_dir): \"\"\"Clean the previous generated files.\"\"\" exit_code = SCons(project_dir).clean() ctx.exit(exit_code)", "def clean_py_files(path): \"\"\" Removes all .py files. :param path: the path :return: None \"\"\" for dirname, subdirlist, filelist in os.walk(path): for f in filelist: if f.endswith('py'): os.remove(os.path.join(dirname, f))", "def clean_all(self, args): \"\"\"Delete all build components; the package cache, package builds, bootstrap builds and distributions.\"\"\" self.clean_dists(args) self.clean_builds(args) self.clean_download_cache(args)"], "rank": 1}
{"nl": "sleep holding up python", "code": ["def seconds(num): \"\"\" Pause for this many seconds \"\"\" now = pytime.time() end = now + num until(end)", "def main(idle): \"\"\"Any normal python logic which runs a loop. Can take arguments.\"\"\" while True: LOG.debug(\"Sleeping for {0} seconds.\".format(idle)) time.sleep(idle)", "def sleep(self, time): \"\"\" Perform an asyncio sleep for the time specified in seconds. T his method should be used in place of time.sleep() :param time: time in seconds :returns: No return value \"\"\" try: task = asyncio.ensure_future(self.core.sleep(time)) self.loop.run_until_complete(task) except asyncio.CancelledError: pass except RuntimeError: pass", "def needs_check(self): \"\"\" Check if enough time has elapsed to perform a check(). If this time has elapsed, a state change check through has_state_changed() should be performed and eventually a sync(). :rtype: boolean \"\"\" if self.lastcheck is None: return True return time.time() - self.lastcheck >= self.ipchangedetection_sleep", "def test3(): \"\"\"Test the multiprocess \"\"\" import time p = MVisionProcess() p.start() time.sleep(5) p.stop()"], "rank": 5}
{"nl": "traversal in tree in python", "code": ["def walk_tree(root): \"\"\"Pre-order depth-first\"\"\" yield root for child in root.children: for el in walk_tree(child): yield el", "def __iter__(self): \"\"\" Iterate through tree, leaves first following http://stackoverflow.com/questions/6914803/python-iterator-through-tree-with-list-of-children \"\"\" for node in chain(*imap(iter, self.children)): yield node yield self", "def map_tree(visitor, tree): \"\"\"Apply function to nodes\"\"\" newn = [map_tree(visitor, node) for node in tree.nodes] return visitor(tree, newn)", "def debugTreePrint(node,pfx=\"->\"): \"\"\"Purely a debugging aid: Ascii-art picture of a tree descended from node\"\"\" print pfx,node.item for c in node.children: debugTreePrint(c,\" \"+pfx)", "def print_tree(self, indent=2): \"\"\" print_tree: prints out structure of tree Args: indent (int): What level of indentation at which to start printing Returns: None \"\"\" config.LOGGER.info(\"{indent}{data}\".format(indent=\" \" * indent, data=str(self))) for child in self.children: child.print_tree(indent + 1)"], "rank": 1}
{"nl": "timing a function call python", "code": ["def timed_call(func, *args, log_level='DEBUG', **kwargs): \"\"\"Logs a function's run time :param func: The function to run :param args: The args to pass to the function :param kwargs: The keyword args to pass to the function :param log_level: The log level at which to print the run time :return: The function's return value \"\"\" start = time() r = func(*args, **kwargs) t = time() - start log(log_level, \"Call to '{}' took {:0.6f}s\".format(func.__name__, t)) return r", "def timeit(self, metric, func, *args, **kwargs): \"\"\"Time execution of callable and emit metric then return result.\"\"\" return metrics.timeit(metric, func, *args, **kwargs)", "def timedcall(executable_function, *args): \"\"\"! @brief Executes specified method or function with measuring of execution time. @param[in] executable_function (pointer): Pointer to function or method. @param[in] args (*): Arguments of called function or method. @return (tuple) Execution time and result of execution of function or method (execution_time, result_execution). \"\"\" time_start = time.clock(); result = executable_function(*args); time_end = time.clock(); return (time_end - time_start, result);", "def timeit(method): \"\"\" A Python decorator for printing out the execution time for a function. Adapted from: www.andreas-jung.com/contents/a-python-decorator-for-measuring-the-execution-time-of-methods \"\"\" def timed(*args, **kw): time_start = time.time() result = method(*args, **kw) time_end = time.time() print('timeit: %r %2.2f sec (%r, %r) ' % (method.__name__, time_end-time_start, str(args)[:20], kw)) return result return timed", "def timed (log=sys.stderr, limit=2.0): \"\"\"Decorator to run a function with timing info.\"\"\" return lambda func: timeit(func, log, limit)"], "rank": 6}
{"nl": "python how to stop playsound", "code": ["def stop(self): \"\"\"Stops playback\"\"\" if self.isPlaying is True: self._execute(\"stop\") self._changePlayingState(False)", "def toggle_pause(self): \"\"\"Toggle pause mode\"\"\" self.controller.playing = not self.controller.playing self.music.toggle_pause()", "def pause(self): \"\"\"Pause the music\"\"\" mixer.music.pause() self.pause_time = self.get_time() self.paused = True", "def stop(self): \"\"\" Stops the video stream and resets the clock. \"\"\" logger.debug(\"Stopping playback\") # Stop the clock self.clock.stop() # Set plauyer status to ready self.status = READY", "def OnTogglePlay(self, event): \"\"\"Toggles the video status between play and hold\"\"\" if self.player.get_state() == vlc.State.Playing: self.player.pause() else: self.player.play() event.Skip()"], "rank": 1}
{"nl": "python change dictioinary values in place", "code": ["def copy_and_update(dictionary, update): \"\"\"Returns an updated copy of the dictionary without modifying the original\"\"\" newdict = dictionary.copy() newdict.update(update) return newdict", "def dict_update_newkeys(dict_, dict2): \"\"\" Like dict.update, but does not overwrite items \"\"\" for key, val in six.iteritems(dict2): if key not in dict_: dict_[key] = val", "def update(self, other_dict): \"\"\"update() extends rather than replaces existing key lists.\"\"\" for key, value in iter_multi_items(other_dict): MultiDict.add(self, key, value)", "def update(self, dictionary=None, **kwargs): \"\"\" Adds/overwrites all the keys and values from the dictionary. \"\"\" if not dictionary == None: kwargs.update(dictionary) for k in list(kwargs.keys()): self[k] = kwargs[k]", "def update(self, params): \"\"\"Update the dev_info data from a dictionary. Only updates if it already exists in the device. \"\"\" dev_info = self.json_state.get('deviceInfo') dev_info.update({k: params[k] for k in params if dev_info.get(k)})"], "rank": 6}
{"nl": "can i pass instance method as variable python", "code": ["def action(self): \"\"\" This class overrides this method \"\"\" self.return_value = self.function(*self.args, **self.kwargs)", "def __get__(self, obj, objtype): if not self.is_method: self.is_method = True \"\"\"Support instance methods.\"\"\" return functools.partial(self.__call__, obj)", "def Proxy(f): \"\"\"A helper to create a proxy method in a class.\"\"\" def Wrapped(self, *args): return getattr(self, f)(*args) return Wrapped", "def __get__(self, obj, objtype): \"\"\" Support instance methods \"\"\" import functools return functools.partial(self.__call__, obj)", "def __getattr__(self, item: str) -> Callable: \"\"\"Get a callable that sends the actual API request internally.\"\"\" return functools.partial(self.call_action, item)"], "rank": 7}
{"nl": "pickle python read entiere file", "code": ["def read_raw(data_path): \"\"\" Parameters ---------- data_path : str \"\"\" with open(data_path, 'rb') as f: data = pickle.load(f) return data", "def load(cls,filename): \"\"\"Load from stored files\"\"\" filename = cls.correct_file_extension(filename) with open(filename,'rb') as f: return pickle.load(f)", "def unpickle_file(picklefile, **kwargs): \"\"\"Helper function to unpickle data from `picklefile`.\"\"\" with open(picklefile, 'rb') as f: return pickle.load(f, **kwargs)", "def unpickle(pickle_file): \"\"\"Unpickle a python object from the given path.\"\"\" pickle = None with open(pickle_file, \"rb\") as pickle_f: pickle = dill.load(pickle_f) if not pickle: LOG.error(\"Could not load python object from file\") return pickle", "def pickle_load(fname): \"\"\"return the contents of a pickle file\"\"\" assert type(fname) is str and os.path.exists(fname) print(\"loaded\",fname) return pickle.load(open(fname,\"rb\"))"], "rank": 1}
{"nl": "should there be equal no of columns to concanate two df python", "code": ["def cross_join(df1, df2): \"\"\" Return a dataframe that is a cross between dataframes df1 and df2 ref: https://github.com/pydata/pandas/issues/5401 \"\"\" if len(df1) == 0: return df2 if len(df2) == 0: return df1 # Add as lists so that the new index keeps the items in # the order that they are added together all_columns = pd.Index(list(df1.columns) + list(df2.columns)) df1['key'] = 1 df2['key'] = 1 return pd.merge(df1, df2, on='key').loc[:, all_columns]", "def columns_equal(a: Column, b: Column) -> bool: \"\"\" Are two SQLAlchemy columns are equal? Checks based on: - column ``name`` - column ``type`` (see :func:`column_types_equal`) - ``nullable`` \"\"\" return ( a.name == b.name and column_types_equal(a.type, b.type) and a.nullable == b.nullable )", "def intersect(self, other): \"\"\" Return a new :class:`DataFrame` containing rows only in both this frame and another frame. This is equivalent to `INTERSECT` in SQL. \"\"\" return DataFrame(self._jdf.intersect(other._jdf), self.sql_ctx)", "def duplicated_rows(df, col_name): \"\"\" Return a DataFrame with the duplicated values of the column `col_name` in `df`.\"\"\" _check_cols(df, [col_name]) dups = df[pd.notnull(df[col_name]) & df.duplicated(subset=[col_name])] return dups", "def difference(ydata1, ydata2): \"\"\" Returns the number you should add to ydata1 to make it line up with ydata2 \"\"\" y1 = _n.array(ydata1) y2 = _n.array(ydata2) return(sum(y2-y1)/len(ydata1))"], "rank": 1}
{"nl": "python flatten deep nested list", "code": ["def flat_list(lst): \"\"\"This function flatten given nested list. Argument: nested list Returns: flat list \"\"\" if isinstance(lst, list): for item in lst: for i in flat_list(item): yield i else: yield lst", "def flat_list(input_list): r\"\"\" Given a list of nested lists of arbitrary depth, returns a single level or 'flat' list. \"\"\" x = input_list if isinstance(x, list): return [a for i in x for a in flat_list(i)] else: return [x]", "def flatten(nested): \"\"\" Return a flatten version of the nested argument \"\"\" flat_return = list() def __inner_flat(nested,flat): for i in nested: __inner_flat(i, flat) if isinstance(i, list) else flat.append(i) return flat __inner_flat(nested,flat_return) return flat_return", "def flatten(lis): \"\"\"Given a list, possibly nested to any level, return it flattened.\"\"\" new_lis = [] for item in lis: if isinstance(item, collections.Sequence) and not isinstance(item, basestring): new_lis.extend(flatten(item)) else: new_lis.append(item) return new_lis", "def flatten(l): \"\"\"Flatten a nested list.\"\"\" return sum(map(flatten, l), []) \\ if isinstance(l, list) or isinstance(l, tuple) else [l]"], "rank": 3}
{"nl": "how to show a variable amount of precision in python string format", "code": ["def _strvar(a, prec='{:G}'): r\"\"\"Return variable as a string to print, with given precision.\"\"\" return ' '.join([prec.format(i) for i in np.atleast_1d(a)])", "def format_exp_floats(decimals): \"\"\" sometimes the exp. column can be too large \"\"\" threshold = 10 ** 5 return ( lambda n: \"{:.{prec}e}\".format(n, prec=decimals) if n > threshold else \"{:4.{prec}f}\".format(n, prec=decimals) )", "def reportMemory(k, options, field=None, isBytes=False): \"\"\" Given k kilobytes, report back the correct format as string. \"\"\" if options.pretty: return prettyMemory(int(k), field=field, isBytes=isBytes) else: if isBytes: k /= 1024. if field is not None: return \"%*dK\" % (field - 1, k) # -1 for the \"K\" else: return \"%dK\" % int(k)", "def print_float(self, value, decimal_digits=2, justify_right=True): \"\"\"Print a numeric value to the display. If value is negative it will be printed with a leading minus sign. Decimal digits is the desired number of digits after the decimal point. \"\"\" format_string = '{{0:0.{0}F}}'.format(decimal_digits) self.print_number_str(format_string.format(value), justify_right)", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0"], "rank": 13}
{"nl": "python array get element by index with default", "code": ["def list_get(l, idx, default=None): \"\"\" Get from a list with an optional default value. \"\"\" try: if l[idx]: return l[idx] else: return default except IndexError: return default", "def _get_or_default(mylist, i, default=None): \"\"\"return list item number, or default if don't exist\"\"\" if i >= len(mylist): return default else : return mylist[i]", "def _nth(arr, n): \"\"\" Return the nth value of array If it is missing return NaN \"\"\" try: return arr.iloc[n] except (KeyError, IndexError): return np.nan", "def percentile_index(a, q): \"\"\" Returns the index of the value at the Qth percentile in array a. \"\"\" return np.where( a==np.percentile(a, q, interpolation='nearest') )[0][0]", "def get_cell(self, index): \"\"\" For a single index and return the value :param index: index value :return: value \"\"\" i = sorted_index(self._index, index) if self._sort else self._index.index(index) return self._data[i]"], "rank": 1}
{"nl": "check if string is int in python", "code": ["def _isint(string): \"\"\" >>> _isint(\"123\") True >>> _isint(\"123.45\") False \"\"\" return type(string) is int or \\ (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \\ _isconvertible(int, string)", "def check_int(integer): \"\"\" Check if number is integer or not. :param integer: Number as str :return: Boolean \"\"\" if not isinstance(integer, str): return False if integer[0] in ('-', '+'): return integer[1:].isdigit() return integer.isdigit()", "def is_int(string): \"\"\" Checks if a string is an integer. If the string value is an integer return True, otherwise return False. Args: string: a string to test. Returns: boolean \"\"\" try: a = float(string) b = int(a) except ValueError: return False else: return a == b", "def is_int_type(val): \"\"\"Return True if `val` is of integer type.\"\"\" try: # Python 2 return isinstance(val, (int, long)) except NameError: # Python 3 return isinstance(val, int)", "def is_int(value): \"\"\"Return `True` if ``value`` is an integer.\"\"\" if isinstance(value, bool): return False try: int(value) return True except (ValueError, TypeError): return False"], "rank": 1}
{"nl": "how to change numpy array to list in python", "code": ["def _to_array(value): \"\"\"As a convenience, turn Python lists and tuples into NumPy arrays.\"\"\" if isinstance(value, (tuple, list)): return array(value) elif isinstance(value, (float, int)): return np.float64(value) else: return value", "def A(*a): \"\"\"convert iterable object into numpy array\"\"\" return np.array(a[0]) if len(a)==1 else [np.array(o) for o in a]", "def listify(a): \"\"\" Convert a scalar ``a`` to a list and all iterables to list as well. Examples -------- >>> listify(0) [0] >>> listify([1,2,3]) [1, 2, 3] >>> listify('a') ['a'] >>> listify(np.array([1,2,3])) [1, 2, 3] >>> listify('string') ['string'] \"\"\" if a is None: return [] elif not isinstance(a, (tuple, list, np.ndarray)): return [a] return list(a)", "def arr_to_vector(arr): \"\"\"Reshape a multidimensional array to a vector. \"\"\" dim = array_dim(arr) tmp_arr = [] for n in range(len(dim) - 1): for inner in arr: for i in inner: tmp_arr.append(i) arr = tmp_arr tmp_arr = [] return arr", "def recarray(self): \"\"\"Returns data as :class:`numpy.recarray`.\"\"\" return numpy.rec.fromrecords(self.records, names=self.names)"], "rank": 2}
{"nl": "read a file into a set python", "code": ["def read_set_from_file(filename: str) -> Set[str]: \"\"\" Extract a de-duped collection (set) of text from a file. Expected file format is one item per line. \"\"\" collection = set() with open(filename, 'r') as file_: for line in file_: collection.add(line.rstrip()) return collection", "def get_system_flags() -> FrozenSet[Flag]: \"\"\"Return the set of implemented system flags.\"\"\" return frozenset({Seen, Recent, Deleted, Flagged, Answered, Draft})", "def ranges_to_set(lst): \"\"\" Convert a list of ranges to a set of numbers:: >>> ranges = [(1,3), (5,6)] >>> sorted(list(ranges_to_set(ranges))) [1, 2, 3, 5, 6] \"\"\" return set(itertools.chain(*(range(x[0], x[1]+1) for x in lst)))", "def get_list_from_file(file_name): \"\"\"read the lines from a file into a list\"\"\" with open(file_name, mode='r', encoding='utf-8') as f1: lst = f1.readlines() return lst", "def load_file(self, filename): \"\"\"Read in file contents and set the current string.\"\"\" with open(filename, 'r') as sourcefile: self.set_string(sourcefile.read())"], "rank": 1}
{"nl": "python round float to int", "code": ["def intround(value): \"\"\"Given a float returns a rounded int. Should give the same result on both Py2/3 \"\"\" return int(decimal.Decimal.from_float( value).to_integral_value(decimal.ROUND_HALF_EVEN))", "def proper_round(n): \"\"\" rounds float to closest int :rtype: int :param n: float \"\"\" return int(n) + (n / abs(n)) * int(abs(n - int(n)) >= 0.5) if n != 0 else 0", "def round_float(f, digits, rounding=ROUND_HALF_UP): \"\"\" Accurate float rounding from http://stackoverflow.com/a/15398691. \"\"\" return Decimal(str(f)).quantize(Decimal(10) ** (-1 * digits), rounding=rounding)", "def floor(self): \"\"\"Round `x` and `y` down to integers.\"\"\" return Point(int(math.floor(self.x)), int(math.floor(self.y)))", "def round_to_float(number, precision): \"\"\"Round a float to a precision\"\"\" rounded = Decimal(str(floor((number + precision / 2) // precision)) ) * Decimal(str(precision)) return float(rounded)"], "rank": 1}
{"nl": "how to write a parser on python", "code": ["def _get_parsing_plan_for_multifile_children(self, obj_on_fs: PersistedObject, desired_type: Type[Any], logger: Logger) -> Dict[str, Any]: \"\"\" Implementation of AnyParser API \"\"\" raise Exception('This should never happen, since this parser relies on underlying parsers')", "def build(self, **kwargs): \"\"\"Build the lexer.\"\"\" self.lexer = ply.lex.lex(object=self, **kwargs)", "def parse(self): \"\"\" Parse file specified by constructor. \"\"\" f = open(self.parse_log_path, \"r\") self.parse2(f) f.close()", "def cli_run(): \"\"\"docstring for argparse\"\"\" parser = argparse.ArgumentParser(description='Stupidly simple code answers from StackOverflow') parser.add_argument('query', help=\"What's the problem ?\", type=str, nargs='+') parser.add_argument('-t','--tags', help='semicolon separated tags -> python;lambda') args = parser.parse_args() main(args)", "def main(pargs): \"\"\"This should only be used for testing. The primary mode of operation is as an imported library. \"\"\" input_file = sys.argv[1] fp = ParseFileLineByLine(input_file) for i in fp: print(i)"], "rank": 13}
{"nl": "python dict with keys no value", "code": ["def nonull_dict(self): \"\"\"Like dict, but does not hold any null values. :return: \"\"\" return {k: v for k, v in six.iteritems(self.dict) if v and k != '_codes'}", "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]: \"\"\" Return a new copied dictionary without the keys with ``None`` values from the given Mapping object. \"\"\" return {k: v for k, v in obj.items() if v is not None}", "def _remove_dict_keys_with_value(dict_, val): \"\"\"Removes `dict` keys which have have `self` as value.\"\"\" return {k: v for k, v in dict_.items() if v is not val}", "def _clean_dict(target_dict, whitelist=None): \"\"\" Convenience function that removes a dicts keys that have falsy values \"\"\" assert isinstance(target_dict, dict) return { ustr(k).strip(): ustr(v).strip() for k, v in target_dict.items() if v not in (None, Ellipsis, [], (), \"\") and (not whitelist or k in whitelist) }", "def purge_dict(idict): \"\"\"Remove null items from a dictionary \"\"\" odict = {} for key, val in idict.items(): if is_null(val): continue odict[key] = val return odict"], "rank": 1}
{"nl": "how to remove blank lines from a text file in python", "code": ["def get_stripped_file_lines(filename): \"\"\" Return lines of a file with whitespace removed \"\"\" try: lines = open(filename).readlines() except FileNotFoundError: fatal(\"Could not open file: {!r}\".format(filename)) return [line.strip() for line in lines]", "def lines(input): \"\"\"Remove comments and empty lines\"\"\" for raw_line in input: line = raw_line.strip() if line and not line.startswith('#'): yield strip_comments(line)", "def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line", "def lint_file(in_file, out_file=None): \"\"\"Helps remove extraneous whitespace from the lines of a file :param file in_file: A readable file or file-like :param file out_file: A writable file or file-like \"\"\" for line in in_file: print(line.strip(), file=out_file)", "def text_remove_empty_lines(text): \"\"\" Whitespace normalization: - Strip empty lines - Strip trailing whitespace \"\"\" lines = [ line.rstrip() for line in text.splitlines() if line.strip() ] return \"\\n\".join(lines)"], "rank": 1}
{"nl": "how to hash a binary file in python", "code": ["def _hash_the_file(hasher, filename): \"\"\"Helper function for creating hash functions. See implementation of :func:`dtoolcore.filehasher.shasum` for more usage details. \"\"\" BUF_SIZE = 65536 with open(filename, 'rb') as f: buf = f.read(BUF_SIZE) while len(buf) > 0: hasher.update(buf) buf = f.read(BUF_SIZE) return hasher", "def generate_hash(filepath): \"\"\"Public function that reads a local file and generates a SHA256 hash digest for it\"\"\" fr = FileReader(filepath) data = fr.read_bin() return _calculate_sha256(data)", "def update_hash(cls, filelike, digest): \"\"\"Update the digest of a single file in a memory-efficient manner.\"\"\" block_size = digest.block_size * 1024 for chunk in iter(lambda: filelike.read(block_size), b''): digest.update(chunk)", "def hash_file(fileobj): \"\"\" :param fileobj: a file object :return: a hash of the file content \"\"\" hasher = hashlib.md5() buf = fileobj.read(65536) while len(buf) > 0: hasher.update(buf) buf = fileobj.read(65536) return hasher.hexdigest()", "def get_hash(self, handle): \"\"\"Return the hash.\"\"\" fpath = self._fpath_from_handle(handle) return DiskStorageBroker.hasher(fpath)"], "rank": 2}
{"nl": "python get function keyword names", "code": ["def get_args(method_or_func): \"\"\"Returns method or function arguments.\"\"\" try: # Python 3.0+ args = list(inspect.signature(method_or_func).parameters.keys()) except AttributeError: # Python 2.7 args = inspect.getargspec(method_or_func).args return args", "def get_all_args(fn) -> list: \"\"\" Returns a list of all arguments for the function fn. >>> def foo(x, y, z=100): return x + y + z >>> get_all_args(foo) ['x', 'y', 'z'] \"\"\" sig = inspect.signature(fn) return list(sig.parameters)", "def parse_func_kwarg_keys(func, with_vals=False): \"\"\" hacky inference of kwargs keys SeeAlso: argparse_funckw recursive_parse_kwargs parse_kwarg_keys parse_func_kwarg_keys get_func_kwargs \"\"\" sourcecode = get_func_sourcecode(func, strip_docstr=True, strip_comments=True) kwkeys = parse_kwarg_keys(sourcecode, with_vals=with_vals) #ut.get_func_kwargs TODO return kwkeys", "def get_func_posargs_name(f): \"\"\"Returns the name of the function f's keyword argument parameter if it exists, otherwise None\"\"\" sigparams = inspect.signature(f).parameters for p in sigparams: if sigparams[p].kind == inspect.Parameter.VAR_POSITIONAL: return sigparams[p].name return None", "def functions(self): \"\"\" A list of functions declared or defined in this module. \"\"\" return [v for v in self.globals.values() if isinstance(v, values.Function)]"], "rank": 3}
{"nl": "iterate through words in text file python", "code": ["def txt_line_iterator(path): \"\"\"Iterate through lines of file.\"\"\" with tf.gfile.Open(path) as f: for line in f: yield line.strip()", "def extract_words(lines): \"\"\" Extract from the given iterable of lines the list of words. :param lines: an iterable of lines; :return: a generator of words of lines. \"\"\" for line in lines: for word in re.findall(r\"\\w+\", line): yield word", "def count_words(file): \"\"\" Counts the word frequences in a list of sentences. Note: This is a helper function for parallel execution of `Vocabulary.from_text` method. \"\"\" c = Counter() with open(file, 'r') as f: for l in f: words = l.strip().split() c.update(words) return c", "def _read_words(filename): \"\"\"Reads words from a file.\"\"\" with tf.gfile.GFile(filename, \"r\") as f: if sys.version_info[0] >= 3: return f.read().replace(\"\\n\", \" %s \" % EOS).split() else: return f.read().decode(\"utf-8\").replace(\"\\n\", \" %s \" % EOS).split()", "def wordfreq(text, is_filename=False): \"\"\"Return a dictionary of words and word counts in a string.\"\"\" if is_filename: with open(text) as f: text = f.read() freqs = {} for word in text.split(): lword = word.lower() freqs[lword] = freqs.get(lword, 0) + 1 return freqs"], "rank": 2}
{"nl": "python make a put request to restful endpoint", "code": ["def put(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP PUT operation to API endpoint.\"\"\" return self._request('PUT', endpoint, **kwargs)", "def post(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP POST operation to API endpoint.\"\"\" return self._request('POST', endpoint, **kwargs)", "def _post(self, url, params, uploads=None): \"\"\" Wrapper method for POST calls. \"\"\" self._call(self.POST, url, params, uploads)", "def delete(self, endpoint: str, **kwargs) -> dict: \"\"\"HTTP DELETE operation to API endpoint.\"\"\" return self._request('DELETE', endpoint, **kwargs)", "def POST(self, *args, **kwargs): \"\"\" POST request \"\"\" return self._handle_api(self.API_POST, args, kwargs)"], "rank": 1}
{"nl": "python get the id of the current thread", "code": ["def threadid(self): \"\"\" Current thread ident. If current thread is main thread then it returns ``None``. :type: int or None \"\"\" current = self.thread.ident main = get_main_thread() if main is None: return current else: return current if current != main.ident else None", "def getWindowPID(self, hwnd): \"\"\" Gets the process ID that the specified window belongs to \"\"\" pid = ctypes.c_ulong() ctypes.windll.user32.GetWindowThreadProcessId(hwnd, ctypes.byref(pid)) return int(pid.value)", "def get_current_frames(): \"\"\"Return current threads prepared for further processing. \"\"\" return dict( (thread_id, {'frame': thread2list(frame), 'time': None}) for thread_id, frame in sys._current_frames().items() )", "def get_system_uid(): \"\"\"Get a (probably) unique ID to identify a system. Used to differentiate votes. \"\"\" try: if os.name == 'nt': return get_nt_system_uid() if sys.platform == 'darwin': return get_osx_system_uid() except Exception: return get_mac_uid() else: return get_mac_uid()", "def _id(self): \"\"\"What this object is equal to.\"\"\" return (self.__class__, self.number_of_needles, self.needle_positions, self.left_end_needle)"], "rank": 1}
{"nl": "python function returning a list of all entities is called", "code": ["def values(self): \"\"\"return a list of all state values\"\"\" values = [] for __, data in self.items(): values.append(data) return values", "def list_all(dev: Device): \"\"\"List all available API calls.\"\"\" for name, service in dev.services.items(): click.echo(click.style(\"\\nService %s\" % name, bold=True)) for method in service.methods: click.echo(\" %s\" % method.name)", "def getExperiments(uuid: str): \"\"\" list active (running or completed) experiments\"\"\" return jsonify([x.deserialize() for x in Experiment.query.all()])", "def functions(self): \"\"\" A list of functions declared or defined in this module. \"\"\" return [v for v in self.globals.values() if isinstance(v, values.Function)]", "def get_action_methods(self): \"\"\" return a list of methods on this class for executing actions. methods are return as a list of (name, func) tuples \"\"\" return [(name, getattr(self, name)) for name, _ in Action.get_command_types()]"], "rank": 36}
{"nl": "how to flat a list of list python", "code": ["def flatten_list(l: List[list]) -> list: \"\"\" takes a list of lists, l and returns a flat list \"\"\" return [v for inner_l in l for v in inner_l]", "def flatten_list(x: List[Any]) -> List[Any]: \"\"\" Converts a list of lists into a flat list. Args: x: list of lists Returns: flat list As per http://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python \"\"\" # noqa return [item for sublist in x for item in sublist]", "def flatten(l): \"\"\"Flatten a nested list.\"\"\" return sum(map(flatten, l), []) \\ if isinstance(l, list) or isinstance(l, tuple) else [l]", "def flat_list(lst): \"\"\"This function flatten given nested list. Argument: nested list Returns: flat list \"\"\" if isinstance(lst, list): for item in lst: for i in flat_list(item): yield i else: yield lst", "def flatten(l, types=(list, float)): \"\"\" Flat nested list of lists into a single list. \"\"\" l = [item if isinstance(item, types) else [item] for item in l] return [item for sublist in l for item in sublist]"], "rank": 2}
{"nl": "python read yaml to numpy", "code": ["def numpy_to_yaml(representer: Representer, data: np.ndarray) -> Sequence[Any]: \"\"\" Write a numpy array to YAML. It registers the array under the tag ``!numpy_array``. Use with: .. code-block:: python >>> yaml = ruamel.yaml.YAML() >>> yaml.representer.add_representer(np.ndarray, yaml.numpy_to_yaml) Note: We cannot use ``yaml.register_class`` because it won't register the proper type. (It would register the type of the class, rather than of `numpy.ndarray`). Instead, we use the above approach to register this method explicitly with the representer. \"\"\" return representer.represent_sequence( \"!numpy_array\", data.tolist() )", "def _openResources(self): \"\"\" Uses numpy.load to open the underlying file \"\"\" arr = np.load(self._fileName, allow_pickle=ALLOW_PICKLE) check_is_an_array(arr) self._array = arr", "def load_yaml(filepath): \"\"\"Convenience function for loading yaml-encoded data from disk.\"\"\" with open(filepath) as f: txt = f.read() return yaml.load(txt)", "def read_numpy(fd, byte_order, dtype, count): \"\"\"Read tag data from file and return as numpy array.\"\"\" return numpy.fromfile(fd, byte_order+dtype[-1], count)", "def ReadManyFromPath(filepath): \"\"\"Reads a Python object stored in a specified YAML file. Args: filepath: A filepath to the YAML file. Returns: A Python data structure corresponding to the YAML in the given file. \"\"\" with io.open(filepath, mode=\"r\", encoding=\"utf-8\") as filedesc: return ReadManyFromFile(filedesc)"], "rank": 1}
{"nl": "discord python get user from id string", "code": ["def get_user_by_id(self, id): \"\"\"Retrieve a User object by ID.\"\"\" return self.db_adapter.get_object(self.UserClass, id=id)", "def get_chat_member(self, user_id): \"\"\" Get information about a member of a chat. :param int user_id: Unique identifier of the target user \"\"\" return self.bot.api_call( \"getChatMember\", chat_id=str(self.id), user_id=str(user_id) )", "def get_account_id_by_fullname(self, fullname: str) -> str: \"\"\" Locates the account by fullname \"\"\" account = self.get_by_fullname(fullname) return account.guid", "def get_user_id_from_email(self, email): \"\"\" Uses the get-all-user-accounts Portals API to retrieve the user-id by supplying an email. \"\"\" accts = self.get_all_user_accounts() for acct in accts: if acct['email'] == email: return acct['id'] return None", "def me(self): \"\"\"Similar to :attr:`.Guild.me` except it may return the :class:`.ClientUser` in private message contexts.\"\"\" return self.guild.me if self.guild is not None else self.bot.user"], "rank": 1}
{"nl": "list of arbitrary objects to counts in python", "code": ["def count(args): \"\"\" count occurences in a list of lists >>> count([['a','b'],['a']]) defaultdict(int, {'a' : 2, 'b' : 1}) \"\"\" counts = defaultdict(int) for arg in args: for item in arg: counts[item] = counts[item] + 1 return counts", "def counter(items): \"\"\" Simplest required implementation of collections.Counter. Required as 2.6 does not have Counter in collections. \"\"\" results = {} for item in items: results[item] = results.get(item, 0) + 1 return results", "def count_generator(generator, memory_efficient=True): \"\"\"Count number of item in generator. memory_efficient=True, 3 times slower, but memory_efficient. memory_efficient=False, faster, but cost more memory. \"\"\" if memory_efficient: counter = 0 for _ in generator: counter += 1 return counter else: return len(list(generator))", "def count_list(the_list): \"\"\" Generates a count of the number of times each unique item appears in a list \"\"\" count = the_list.count result = [(item, count(item)) for item in set(the_list)] result.sort() return result", "def seq(): \"\"\" Counts up sequentially from a number based on the current time :rtype int: \"\"\" current_frame = inspect.currentframe().f_back trace_string = \"\" while current_frame.f_back: trace_string = trace_string + current_frame.f_back.f_code.co_name current_frame = current_frame.f_back return counter.get_from_trace(trace_string)"], "rank": 1}
{"nl": "how to know queue size in python", "code": ["def qsize(self): \"\"\"Return the approximate size of the queue (not reliable!).\"\"\" self.mutex.acquire() n = self._qsize() self.mutex.release() return n", "def count(self, elem): \"\"\" Return the number of elements equal to elem present in the queue >>> pdeque([1, 2, 1]).count(1) 2 \"\"\" return self._left_list.count(elem) + self._right_list.count(elem)", "def full(self): \"\"\"Return True if the queue is full\"\"\" if not self.size: return False return len(self.pq) == (self.size + self.removed_count)", "def full(self): \"\"\"Return ``True`` if the queue is full, ``False`` otherwise (not reliable!). Only applicable if :attr:`maxsize` is set. \"\"\" return self.maxsize and len(self.list) >= self.maxsize or False", "def ncores_reserved(self): \"\"\" Returns the number of cores reserved in this moment. A core is reserved if it's still not running but we have submitted the task to the queue manager. \"\"\" return sum(task.manager.num_cores for task in self if task.status == task.S_SUB)"], "rank": 1}
{"nl": "impute missing values in python", "code": ["def impute_data(self,x): \"\"\"Imputes data set containing Nan values\"\"\" imp = Imputer(missing_values='NaN', strategy='mean', axis=0) return imp.fit_transform(x)", "def check_precomputed_distance_matrix(X): \"\"\"Perform check_array(X) after removing infinite values (numpy.inf) from the given distance matrix. \"\"\" tmp = X.copy() tmp[np.isinf(tmp)] = 1 check_array(tmp)", "def _replace_nan(a, val): \"\"\" replace nan in a by val, and returns the replaced array and the nan position \"\"\" mask = isnull(a) return where_method(val, mask, a), mask", "def fillna(series_or_arr, missing_value=0.0): \"\"\"Fill missing values in pandas objects and numpy arrays. Arguments --------- series_or_arr : pandas.Series, numpy.ndarray The numpy array or pandas series for which the missing values need to be replaced. missing_value : float, int, str The value to replace the missing value with. Default 0.0. Returns ------- pandas.Series, numpy.ndarray The numpy array or pandas series with the missing values filled. \"\"\" if pandas.notnull(missing_value): if isinstance(series_or_arr, (numpy.ndarray)): series_or_arr[numpy.isnan(series_or_arr)] = missing_value else: series_or_arr.fillna(missing_value, inplace=True) return series_or_arr", "def fix_missing(df, col, name, na_dict): \"\"\" Fill missing data in a column of df with the median, and add a {name}_na column which specifies if the data was missing. Parameters: ----------- df: The data frame that will be changed. col: The column of data to fix by filling in missing data. name: The name of the new filled column in df. na_dict: A dictionary of values to create na's of and the value to insert. If name is not a key of na_dict the median will fill any missing data. Also if name is not a key of na_dict and there is no missing data in col, then no {name}_na column is not created. Examples: --------- >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]}) >>> df col1 col2 0 1 5 1 nan 2 2 3 2 >>> fix_missing(df, df['col1'], 'col1', {}) >>> df col1 col2 col1_na 0 1 5 False 1 2 2 True 2 3 2 False >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]}) >>> df col1 col2 0 1 5 1 nan 2 2 3 2 >>> fix_missing(df, df['col2'], 'col2', {}) >>> df col1 col2 0 1 5 1 nan 2 2 3 2 >>> df = pd.DataFrame({'col1' : [1, np.NaN, 3], 'col2' : [5, 2, 2]}) >>> df col1 col2 0 1 5 1 nan 2 2 3 2 >>> fix_missing(df, df['col1'], 'col1', {'col1' : 500}) >>> df col1 col2 col1_na 0 1 5 False 1 500 2 True 2 3 2 False \"\"\" if is_numeric_dtype(col): if pd.isnull(col).sum() or (name in na_dict): df[name+'_na'] = pd.isnull(col) filler = na_dict[name] if name in na_dict else col.median() df[name] = col.fillna(filler) na_dict[name] = filler return na_dict"], "rank": 1}
{"nl": "python dict rank by value", "code": ["def revrank_dict(dict, key=lambda t: t[1], as_tuple=False): \"\"\" Reverse sorts a #dict by a given key, optionally returning it as a #tuple. By default, the @dict is sorted by it's value. @dict: the #dict you wish to sorts @key: the #sorted key to use @as_tuple: returns result as a #tuple ((k, v),...) -> :class:OrderedDict or #tuple \"\"\" sorted_list = sorted(dict.items(), key=key, reverse=True) return OrderedDict(sorted_list) if not as_tuple else tuple(sorted_list)", "def _dict_values_sorted_by_key(dictionary): # This should be a yield from instead. \"\"\"Internal helper to return the values of a dictionary, sorted by key. \"\"\" for _, value in sorted(dictionary.iteritems(), key=operator.itemgetter(0)): yield value", "def get_keys_of_max_n(dict_obj, n): \"\"\"Returns the keys that maps to the top n max values in the given dict. Example: -------- >>> dict_obj = {'a':2, 'b':1, 'c':5} >>> get_keys_of_max_n(dict_obj, 2) ['a', 'c'] \"\"\" return sorted([ item[0] for item in sorted( dict_obj.items(), key=lambda item: item[1], reverse=True )[:n] ])", "def ranks(self, key, value): \"\"\"Populate the ``ranks`` key.\"\"\" return [normalize_rank(el) for el in force_list(value.get('a'))]", "def flattened_nested_key_indices(nested_dict): \"\"\" Combine the outer and inner keys of nested dictionaries into a single ordering. \"\"\" outer_keys, inner_keys = collect_nested_keys(nested_dict) combined_keys = list(sorted(set(outer_keys + inner_keys))) return {k: i for (i, k) in enumerate(combined_keys)}"], "rank": 1}
{"nl": "generate short unique id python", "code": ["def _unique_id(self, prefix): \"\"\" Generate a unique (within the graph) identifer internal to graph generation. \"\"\" _id = self._id_gen self._id_gen += 1 return prefix + str(_id)", "def generate_id(self): \"\"\"Generate a fresh id\"\"\" if self.use_repeatable_ids: self.repeatable_id_counter += 1 return 'autobaked-{}'.format(self.repeatable_id_counter) else: return str(uuid4())", "def generate_id(): \"\"\"Generate new UUID\"\"\" # TODO: Use six.string_type to Py3 compat try: return unicode(uuid1()).replace(u\"-\", u\"\") except NameError: return str(uuid1()).replace(u\"-\", u\"\")", "def _get_random_id(): \"\"\" Get a random (i.e., unique) string identifier\"\"\" symbols = string.ascii_uppercase + string.ascii_lowercase + string.digits return ''.join(random.choice(symbols) for _ in range(15))", "def _uniqueid(n=30): \"\"\"Return a unique string with length n. :parameter int N: number of character in the uniqueid :return: the uniqueid :rtype: str \"\"\" return ''.join(random.SystemRandom().choice( string.ascii_uppercase + string.ascii_lowercase) for _ in range(n))"], "rank": 7}
{"nl": "python numpy inverse of the matrix", "code": ["def MatrixInverse(a, adj): \"\"\" Matrix inversion op. \"\"\" return np.linalg.inv(a if not adj else _adjoint(a)),", "def inverse(self): \"\"\" Returns inverse of transformation. \"\"\" invr = np.linalg.inv(self.affine_matrix) return SymmOp(invr)", "def inh(table): \"\"\" inverse hyperbolic sine transformation \"\"\" t = [] for i in table: t.append(np.ndarray.tolist(np.arcsinh(i))) return t", "def Cinv(self): \"\"\"Inverse of the noise covariance.\"\"\" try: return np.linalg.inv(self.c) except np.linalg.linalg.LinAlgError: print('Warning: non-invertible noise covariance matrix c.') return np.eye(self.c.shape[0])", "def quaternion_imag(quaternion): \"\"\"Return imaginary part of quaternion. >>> quaternion_imag([3, 0, 1, 2]) array([ 0., 1., 2.]) \"\"\" return numpy.array(quaternion[1:4], dtype=numpy.float64, copy=True)"], "rank": 1}
{"nl": "python how to mix backslash and forward slashes in path", "code": ["def __unixify(self, s): \"\"\" stupid windows. converts the backslash to forwardslash for consistency \"\"\" return os.path.normpath(s).replace(os.sep, \"/\")", "def norm_slash(name): \"\"\"Normalize path slashes.\"\"\" if isinstance(name, str): return name.replace('/', \"\\\\\") if not is_case_sensitive() else name else: return name.replace(b'/', b\"\\\\\") if not is_case_sensitive() else name", "def normalize_pattern(pattern): \"\"\"Converts backslashes in path patterns to forward slashes. Doesn't normalize regular expressions - they may contain escapes. \"\"\" if not (pattern.startswith('RE:') or pattern.startswith('!RE:')): pattern = _slashes.sub('/', pattern) if len(pattern) > 1: pattern = pattern.rstrip('/') return pattern", "def fixpath(path): \"\"\"Uniformly format a path.\"\"\" return os.path.normpath(os.path.realpath(os.path.expanduser(path)))", "def escapePathForShell(path): \"\"\" Escapes a filesystem path for use as a command-line argument \"\"\" if platform.system() == 'Windows': return '\"{}\"'.format(path.replace('\"', '\"\"')) else: return shellescape.quote(path)"], "rank": 2}
{"nl": "reduce functon not defined in python", "code": ["def reduce(function, initval=None): \"\"\" Curried version of the built-in reduce. >>> reduce(lambda x,y: x+y)( [1, 2, 3, 4, 5] ) 15 \"\"\" if initval is None: return lambda s: __builtin__.reduce(function, s) else: return lambda s: __builtin__.reduce(function, s, initval)", "def compose(*funcs): \"\"\"compose a list of functions\"\"\" return lambda x: reduce(lambda v, f: f(v), reversed(funcs), x)", "def map_wrap(f): \"\"\"Wrap standard function to easily pass into 'map' processing. \"\"\" @functools.wraps(f) def wrapper(*args, **kwargs): return f(*args, **kwargs) return wrapper", "def compose(func_list): \"\"\" composion of preprocessing functions \"\"\" def f(G, bim): for func in func_list: G, bim = func(G, bim) return G, bim return f", "def apply(self, node): \"\"\" Apply transformation and return if an update happened. \"\"\" new_node = self.run(node) return self.update, new_node"], "rank": 4}
{"nl": "python detect key press linux", "code": ["def _kbhit_unix() -> bool: \"\"\" Under UNIX: is a keystroke available? \"\"\" dr, dw, de = select.select([sys.stdin], [], [], 0) return dr != []", "def read_key(suppress=False): \"\"\" Blocks until a keyboard event happens, then returns that event's name or, if missing, its scan code. \"\"\" event = read_event(suppress) return event.name or event.scan_code", "def on_key_press(self, symbol, modifiers): \"\"\" Pyglet specific key press callback. Forwards and translates the events to :py:func:`keyboard_event` \"\"\" self.keyboard_event(symbol, self.keys.ACTION_PRESS, modifiers)", "def on_press_key(key, callback, suppress=False): \"\"\" Invokes `callback` for KEY_DOWN event related to the given key. For details see `hook`. \"\"\" return hook_key(key, lambda e: e.event_type == KEY_UP or callback(e), suppress=suppress)", "def keyPressEvent(self, event): \"\"\" Pyqt specific key press callback function. Translates and forwards events to :py:func:`keyboard_event`. \"\"\" self.keyboard_event(event.key(), self.keys.ACTION_PRESS, 0)"], "rank": 1}
{"nl": "how to generate random binary tree in python", "code": ["def getRandomBinaryTreeLeafNode(binaryTree): \"\"\"Get random binary tree node. \"\"\" if binaryTree.internal == True: if random.random() > 0.5: return getRandomBinaryTreeLeafNode(binaryTree.left) else: return getRandomBinaryTreeLeafNode(binaryTree.right) else: return binaryTree", "def make_bintree(levels): \"\"\"Make a symmetrical binary tree with @levels\"\"\" G = nx.DiGraph() root = '0' G.add_node(root) add_children(G, root, levels, 2) return G", "def print_bintree(tree, indent=' '): \"\"\"print a binary tree\"\"\" for n in sorted(tree.keys()): print \"%s%s\" % (indent * depth(n,tree), n)", "def human__decision_tree(): \"\"\" Decision Tree \"\"\" # build data N = 1000000 M = 3 X = np.zeros((N,M)) X.shape y = np.zeros(N) X[0, 0] = 1 y[0] = 8 X[1, 1] = 1 y[1] = 8 X[2, 0:2] = 1 y[2] = 4 # fit model xor_model = sklearn.tree.DecisionTreeRegressor(max_depth=2) xor_model.fit(X, y) return xor_model", "def cric__decision_tree(): \"\"\" Decision Tree \"\"\" model = sklearn.tree.DecisionTreeClassifier(random_state=0, max_depth=4) # we want to explain the raw probability outputs of the trees model.predict = lambda X: model.predict_proba(X)[:,1] return model"], "rank": 1}
{"nl": "python get epoch milis from datetime", "code": ["def _dt_to_epoch(dt): \"\"\"Convert datetime to epoch seconds.\"\"\" try: epoch = dt.timestamp() except AttributeError: # py2 epoch = (dt - datetime(1970, 1, 1)).total_seconds() return epoch", "def _DateToEpoch(date): \"\"\"Converts python datetime to epoch microseconds.\"\"\" tz_zero = datetime.datetime.utcfromtimestamp(0) diff_sec = int((date - tz_zero).total_seconds()) return diff_sec * 1000000", "def AmericanDateToEpoch(self, date_str): \"\"\"Take a US format date and return epoch.\"\"\" try: epoch = time.strptime(date_str, \"%m/%d/%Y\") return int(calendar.timegm(epoch)) * 1000000 except ValueError: return 0", "def convertDatetime(t): \"\"\" Converts the specified datetime object into its appropriate protocol value. This is the number of milliseconds from the epoch. \"\"\" epoch = datetime.datetime.utcfromtimestamp(0) delta = t - epoch millis = delta.total_seconds() * 1000 return int(millis)", "def timestamp_to_microseconds(timestamp): \"\"\"Convert a timestamp string into a microseconds value :param timestamp :return time in microseconds \"\"\" timestamp_str = datetime.datetime.strptime(timestamp, ISO_DATETIME_REGEX) epoch_time_secs = calendar.timegm(timestamp_str.timetuple()) epoch_time_mus = epoch_time_secs * 1e6 + timestamp_str.microsecond return epoch_time_mus"], "rank": 1}
{"nl": "python get environ user windows", "code": ["def get_user_name(): \"\"\"Get user name provide by operating system \"\"\" if sys.platform == 'win32': #user = os.getenv('USERPROFILE') user = os.getenv('USERNAME') else: user = os.getenv('LOGNAME') return user", "def _get_os_environ_dict(keys): \"\"\"Return a dictionary of key/values from os.environ.\"\"\" return {k: os.environ.get(k, _UNDEFINED) for k in keys}", "def get_python(): \"\"\"Determine the path to the virtualenv python\"\"\" if sys.platform == 'win32': python = path.join(VE_ROOT, 'Scripts', 'python.exe') else: python = path.join(VE_ROOT, 'bin', 'python') return python", "def on_windows (): \"\"\" Returns true if running on windows, whether in cygwin or not. \"\"\" if bjam.variable(\"NT\"): return True elif bjam.variable(\"UNIX\"): uname = bjam.variable(\"JAMUNAME\") if uname and uname[0].startswith(\"CYGWIN\"): return True return False", "def get_environment_info() -> dict: \"\"\" Information about Cauldron and its Python interpreter. :return: A dictionary containing information about the Cauldron and its Python environment. This information is useful when providing feedback and bug reports. \"\"\" data = _environ.systems.get_system_data() data['cauldron'] = _environ.package_settings.copy() return data"], "rank": 1}
{"nl": "iterator is past the end python", "code": ["def next(self): \"\"\"Provides hook for Python2 iterator functionality.\"\"\" _LOGGER.debug(\"reading next\") if self.closed: _LOGGER.debug(\"stream is closed\") raise StopIteration() line = self.readline() if not line: _LOGGER.debug(\"nothing more to read\") raise StopIteration() return line", "def _fill(self): \"\"\"Advance the iterator without returning the old head.\"\"\" try: self._head = self._iterable.next() except StopIteration: self._head = None", "def next (self): # File-like object. \"\"\"This is to support iterators over a file-like object. \"\"\" result = self.readline() if result == self._empty_buffer: raise StopIteration return result", "def __next__(self): \"\"\"Pop the head off the iterator and return it.\"\"\" res = self._head self._fill() if res is None: raise StopIteration() return res", "def __next__(self): \"\"\" :return: int \"\"\" self.current += 1 if self.current > self.total: raise StopIteration else: return self.iterable[self.current - 1]"], "rank": 4}
{"nl": "python if file not exist then creat", "code": ["def check_create_folder(filename): \"\"\"Check if the folder exisits. If not, create the folder\"\"\" os.makedirs(os.path.dirname(filename), exist_ok=True)", "def ensure_dir(f): \"\"\" Ensure a a file exists and if not make the relevant path \"\"\" d = os.path.dirname(f) if not os.path.exists(d): os.makedirs(d)", "def check_exists(filename, oappend=False): \"\"\" Avoid overwriting some files accidentally. \"\"\" if op.exists(filename): if oappend: return oappend logging.error(\"`{0}` found, overwrite (Y/N)?\".format(filename)) overwrite = (raw_input() == 'Y') else: overwrite = True return overwrite", "def _replace_file(path, content): \"\"\"Writes a file if it doesn't already exist with the same content. This is useful because cargo uses timestamps to decide whether to compile things.\"\"\" if os.path.exists(path): with open(path, 'r') as f: if content == f.read(): print(\"Not overwriting {} because it is unchanged\".format(path), file=sys.stderr) return with open(path, 'w') as f: f.write(content)", "def file_found(filename,force): \"\"\"Check if a file exists\"\"\" if os.path.exists(filename) and not force: logger.info(\"Found %s; skipping...\"%filename) return True else: return False"], "rank": 1}
{"nl": "python howe to tell if path passed in is absolute or relative", "code": ["def is_relative_url(url): \"\"\" simple method to determine if a url is relative or absolute \"\"\" if url.startswith(\"#\"): return None if url.find(\"://\") > 0 or url.startswith(\"//\"): # either 'http(s)://...' or '//cdn...' and therefore absolute return False return True", "def get_absolute_path(*args): \"\"\"Transform relative pathnames into absolute pathnames.\"\"\" directory = os.path.dirname(os.path.abspath(__file__)) return os.path.join(directory, *args)", "def relative_path(path): \"\"\" Return the given path relative to this file. \"\"\" return os.path.join(os.path.dirname(__file__), path)", "def __absolute__(self, uri): \"\"\" Get the absolute uri for a file :param uri: URI of the resource to be retrieved :return: Absolute Path \"\"\" return op.abspath(op.join(self.__path__, uri))", "def relpath(path): \"\"\"Path helper, gives you a path relative to this file\"\"\" return os.path.normpath( os.path.join(os.path.abspath(os.path.dirname(__file__)), path) )"], "rank": 1}
{"nl": "maker a string lowercase pythong", "code": ["def python(string: str): \"\"\" :param string: String can be type, resource or python case \"\"\" return underscore(singularize(string) if Naming._pluralize(string) else string)", "def to_identifier(s): \"\"\" Convert snake_case to camel_case. \"\"\" if s.startswith('GPS'): s = 'Gps' + s[3:] return ''.join([i.capitalize() for i in s.split('_')]) if '_' in s else s", "def us2mc(string): \"\"\"Transform an underscore_case string to a mixedCase string\"\"\" return re.sub(r'_([a-z])', lambda m: (m.group(1).upper()), string)", "def case_us2mc(x): \"\"\" underscore to mixed case notation \"\"\" return re.sub(r'_([a-z])', lambda m: (m.group(1).upper()), x)", "def camel_to_under(name): \"\"\" Converts camel-case string to lowercase string separated by underscores. Written by epost (http://stackoverflow.com/questions/1175208). :param name: String to be converted :return: new String with camel-case converted to lowercase, underscored \"\"\" s1 = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name) return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", s1).lower()"], "rank": 57}
{"nl": "python sys stdout write new line", "code": ["def printOut(value, end='\\n'): \"\"\" This function prints the given String immediately and flushes the output. \"\"\" sys.stdout.write(value) sys.stdout.write(end) sys.stdout.flush()", "def println(msg): \"\"\" Convenience function to print messages on a single line in the terminal \"\"\" sys.stdout.write(msg) sys.stdout.flush() sys.stdout.write('\\x08' * len(msg)) sys.stdout.flush()", "def _stdout_raw(self, s): \"\"\"Writes the string to stdout\"\"\" print(s, end='', file=sys.stdout) sys.stdout.flush()", "def stdoutwriteline(*args): \"\"\" @type args: tuple @return: None \"\"\" s = \"\" for i in args: s += str(i) + \" \" s = s.strip() sys.stdout.write(str(s) + \"\\n\") sys.stdout.flush() return s", "def pstd(self, *args, **kwargs): \"\"\" Console to STDOUT \"\"\" kwargs['file'] = self.out self.print(*args, **kwargs) sys.stdout.flush()"], "rank": 2}
{"nl": "python how to create a iterable", "code": ["def fromiterable(cls, itr): \"\"\"Initialize from iterable\"\"\" x, y, z = itr return cls(x, y, z)", "def concat(cls, iterables): \"\"\" Similar to #itertools.chain.from_iterable(). \"\"\" def generator(): for it in iterables: for element in it: yield element return cls(generator())", "def force_iterable(f): \"\"\"Will make any functions return an iterable objects by wrapping its result in a list.\"\"\" def wrapper(*args, **kwargs): r = f(*args, **kwargs) if hasattr(r, '__iter__'): return r else: return [r] return wrapper", "def _izip(*iterables): \"\"\" Iterate through multiple lists or arrays of equal size \"\"\" # This izip routine is from itertools # izip('ABCD', 'xy') --> Ax By iterators = map(iter, iterables) while iterators: yield tuple(map(next, iterators))", "def __iter__(self): \"\"\"Iterate through all elements. Multiple copies will be returned if they exist. \"\"\" for value, count in self.counts(): for _ in range(count): yield value"], "rank": 3}
{"nl": "how to use python function in tensorflow", "code": ["def mul(a, b): \"\"\" A wrapper around tf multiplication that does more automatic casting of the input. \"\"\" def multiply(a, b): \"\"\"Multiplication\"\"\" return a * b return op_with_scalar_cast(a, b, multiply)", "def main(argv=None): \"\"\"Run a Tensorflow model on the Iris dataset.\"\"\" args = parse_arguments(sys.argv if argv is None else argv) tf.logging.set_verbosity(tf.logging.INFO) learn_runner.run( experiment_fn=get_experiment_fn(args), output_dir=args.job_dir)", "def batch_tensor(self, name): \"\"\" A buffer of a given value in a 'flat' (minibatch-indexed) format \"\"\" if name in self.transition_tensors: return tensor_util.merge_first_two_dims(self.transition_tensors[name]) else: return self.rollout_tensors[name]", "def Print(x, data, message, **kwargs): # pylint: disable=invalid-name \"\"\"Call tf.Print. Args: x: a Tensor. data: a list of Tensor message: a string **kwargs: keyword arguments to tf.Print Returns: a Tensor which is identical in value to x \"\"\" return PrintOperation(x, data, message, **kwargs).outputs[0]", "def _int64_feature(value): \"\"\"Wrapper for inserting int64 features into Example proto.\"\"\" if not isinstance(value, list): value = [value] return tf.train.Feature(int64_list=tf.train.Int64List(value=value))"], "rank": 8}
{"nl": "python how to get range of array with positive values numpy", "code": ["def array(self): \"\"\" return the underlying numpy array \"\"\" return np.arange(self.start, self.stop, self.step)", "def find_start_point(self): \"\"\" Find the first location in our array that is not empty \"\"\" for i, row in enumerate(self.data): for j, _ in enumerate(row): if self.data[i, j] != 0: # or not np.isfinite(self.data[i,j]): return i, j", "def _interval_to_bound_points(array): \"\"\" Helper function which returns an array with the Intervals' boundaries. \"\"\" array_boundaries = np.array([x.left for x in array]) array_boundaries = np.concatenate( (array_boundaries, np.array([array[-1].right]))) return array_boundaries", "def _index_range(self, version, symbol, from_version=None, **kwargs): \"\"\" Tuple describing range to read from the ndarray - closed:open \"\"\" from_index = None if from_version: from_index = from_version['up_to'] return from_index, None", "def findMin(arr): \"\"\" in comparison to argrelmax() more simple and reliable peak finder \"\"\" out = np.zeros(shape=arr.shape, dtype=bool) _calcMin(arr, out) return out"], "rank": 3}
{"nl": "python 3 a build string from iterable", "code": ["def encode(strs): \"\"\"Encodes a list of strings to a single string. :type strs: List[str] :rtype: str \"\"\" res = '' for string in strs.split(): res += str(len(string)) + \":\" + string return res", "def commajoin_as_strings(iterable): \"\"\" Join the given iterable with ',' \"\"\" return _(u',').join((six.text_type(i) for i in iterable))", "def flatten( iterables ): \"\"\" Flatten an iterable, except for string elements. \"\"\" for it in iterables: if isinstance(it, str): yield it else: for element in it: yield element", "def concat(cls, iterables): \"\"\" Similar to #itertools.chain.from_iterable(). \"\"\" def generator(): for it in iterables: for element in it: yield element return cls(generator())", "def fromiterable(cls, itr): \"\"\"Initialize from iterable\"\"\" x, y, z = itr return cls(x, y, z)"], "rank": 2}
{"nl": "unchecking a radio button python", "code": ["def checkbox_uncheck(self, force_check=False): \"\"\" Wrapper to uncheck a checkbox \"\"\" if self.get_attribute('checked'): self.click(force_click=force_check)", "def uncheck(self, locator=None, allow_label_click=None, **kwargs): \"\"\" Find a check box and uncheck it. The check box can be found via name, id, or label text. :: page.uncheck(\"German\") Args: locator (str, optional): Which check box to uncheck. allow_label_click (bool, optional): Attempt to click the label to toggle state if element is non-visible. Defaults to :data:`capybara.automatic_label_click`. **kwargs: Arbitrary keyword arguments for :class:`SelectorQuery`. \"\"\" self._check_with_label( \"checkbox\", False, locator=locator, allow_label_click=allow_label_click, **kwargs)", "def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setChecked(Qt.Checked) else: self.setChecked(Qt.Unchecked)", "def check_by_selector(self, selector): \"\"\"Check the checkbox matching the CSS selector.\"\"\" elem = find_element_by_jquery(world.browser, selector) if not elem.is_selected(): elem.click()", "def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setCheckState(Qt.Checked) else: self.setCheckState(Qt.Unchecked)"], "rank": 1}
{"nl": "python expected type sized", "code": ["def check_type_and_size_of_param_list(param_list, expected_length): \"\"\" Ensure that param_list is a list with the expected length. Raises a helpful ValueError if this is not the case. \"\"\" try: assert isinstance(param_list, list) assert len(param_list) == expected_length except AssertionError: msg = \"param_list must be a list containing {} elements.\" raise ValueError(msg.format(expected_length)) return None", "def _requiredSize(shape, dtype): \"\"\" Determines the number of bytes required to store a NumPy array with the specified shape and datatype. \"\"\" return math.floor(np.prod(np.asarray(shape, dtype=np.uint64)) * np.dtype(dtype).itemsize)", "def assert_is_instance(value, types, message=None, extra=None): \"\"\"Raises an AssertionError if value is not an instance of type(s).\"\"\" assert isinstance(value, types), _assert_fail_message( message, value, types, \"is not an instance of\", extra )", "def _assert_is_type(name, value, value_type): \"\"\"Assert that a value must be a given type.\"\"\" if not isinstance(value, value_type): if type(value_type) is tuple: types = ', '.join(t.__name__ for t in value_type) raise ValueError('{0} must be one of ({1})'.format(name, types)) else: raise ValueError('{0} must be {1}' .format(name, value_type.__name__))", "def _valid_other_type(x, types): \"\"\" Do all elements of x have a type from types? \"\"\" return all(any(isinstance(el, t) for t in types) for el in np.ravel(x))"], "rank": 2}
{"nl": "python load csv to numpy array", "code": ["def csv_to_numpy(string_like, dtype=None): # type: (str) -> np.array \"\"\"Convert a CSV object to a numpy array. Args: string_like (str): CSV string. dtype (dtype, optional): Data type of the resulting array. If None, the dtypes will be determined by the contents of each column, individually. This argument can only be used to 'upcast' the array. For downcasting, use the .astype(t) method. Returns: (np.array): numpy array \"\"\" stream = StringIO(string_like) return np.genfromtxt(stream, dtype=dtype, delimiter=',')", "def load_data(filename): \"\"\" :rtype : numpy matrix \"\"\" data = pandas.read_csv(filename, header=None, delimiter='\\t', skiprows=9) return data.as_matrix()", "def _openResources(self): \"\"\" Uses numpy.load to open the underlying file \"\"\" arr = np.load(self._fileName, allow_pickle=ALLOW_PICKLE) check_is_an_array(arr) self._array = arr", "def convert_array(array): \"\"\" Converts an ARRAY string stored in the database back into a Numpy array. Parameters ---------- array: ARRAY The array object to be converted back into a Numpy array. Returns ------- array The converted Numpy array. \"\"\" out = io.BytesIO(array) out.seek(0) return np.load(out)", "def from_array(cls, arr): \"\"\"Convert a structured NumPy array into a Table.\"\"\" return cls().with_columns([(f, arr[f]) for f in arr.dtype.names])"], "rank": 1}
{"nl": "python remove element from list time complexity", "code": ["def remove_elements(target, indices): \"\"\"Remove multiple elements from a list and return result. This implementation is faster than the alternative below. Also note the creation of a new list to avoid altering the original. We don't have any current use for the original intact list, but may in the future...\"\"\" copied = list(target) for index in reversed(indices): del copied[index] return copied", "def remove(self, key): \"\"\"remove the value found at key from the queue\"\"\" item = self.item_finder.pop(key) item[-1] = None self.removed_count += 1", "def pop(self, index=-1): \"\"\"Remove and return the item at index.\"\"\" value = self._list.pop(index) del self._dict[value] return value", "def remove(self, entry): \"\"\"Removes an entry\"\"\" try: list = self.cache[entry.key] list.remove(entry) except: pass", "def without(seq1, seq2): r\"\"\"Return a list with all elements in `seq2` removed from `seq1`, order preserved. Examples: >>> without([1,2,3,1,2], [1]) [2, 3, 2] \"\"\" if isSet(seq2): d2 = seq2 else: d2 = set(seq2) return [elt for elt in seq1 if elt not in d2]"], "rank": 1}
{"nl": "python property by string name", "code": ["def get_property_by_name(pif, name): \"\"\"Get a property by name\"\"\" return next((x for x in pif.properties if x.name == name), None)", "def get_propety_by_name(pif, name): \"\"\"Get a property by name\"\"\" warn(\"This method has been deprecated in favor of get_property_by_name\") return next((x for x in pif.properties if x.name == name), None)", "def get_property(self, property_name): \"\"\" Get a property's value. property_name -- the property to get the value of Returns the properties value, if found, else None. \"\"\" prop = self.find_property(property_name) if prop: return prop.get_value() return None", "def experiment_property(prop): \"\"\"Get a property of the experiment by name.\"\"\" exp = experiment(session) p = getattr(exp, prop) return success_response(field=prop, data=p, request_type=prop)", "def get_property(self, name): # type: (str) -> object \"\"\" Retrieves a framework or system property. As framework properties don't change while it's running, this method don't need to be protected. :param name: The property name \"\"\" with self.__properties_lock: return self.__properties.get(name, os.getenv(name))"], "rank": 2}
{"nl": "index of an entry in a list python", "code": ["def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def bisect_index(a, x): \"\"\" Find the leftmost index of an element in a list using binary search. Parameters ---------- a: list A sorted list. x: arbitrary The element. Returns ------- int The index. \"\"\" i = bisect.bisect_left(a, x) if i != len(a) and a[i] == x: return i raise ValueError", "def binSearch(arr, val): \"\"\" Function for running binary search on a sorted list. :param arr: (list) a sorted list of integers to search :param val: (int) a integer to search for in the sorted array :returns: (int) the index of the element if it is found and -1 otherwise. \"\"\" i = bisect_left(arr, val) if i != len(arr) and arr[i] == val: return i return -1"], "rank": 3}
{"nl": "python how to determine if an iterable is iterable", "code": ["def is_iterable(obj): \"\"\" Are we being asked to look up a list of things, instead of a single thing? We check for the `__iter__` attribute so that this can cover types that don't have to be known by this module, such as NumPy arrays. Strings, however, should be considered as atomic values to look up, not iterables. The same goes for tuples, since they are immutable and therefore valid entries. We don't need to check for the Python 2 `unicode` type, because it doesn't have an `__iter__` attribute anyway. \"\"\" return ( hasattr(obj, \"__iter__\") and not isinstance(obj, str) and not isinstance(obj, tuple) )", "def _is_iterable(item): \"\"\" Checks if an item is iterable (list, tuple, generator), but not string \"\"\" return isinstance(item, collections.Iterable) and not isinstance(item, six.string_types)", "def is_seq(obj): \"\"\" Returns True if object is not a string but is iterable \"\"\" if not hasattr(obj, '__iter__'): return False if isinstance(obj, basestring): return False return True", "def is_iterable(value): \"\"\"must be an iterable (list, array, tuple)\"\"\" return isinstance(value, np.ndarray) or isinstance(value, list) or isinstance(value, tuple), value", "def is_lazy_iterable(obj): \"\"\" Returns whether *obj* is iterable lazily, such as generators, range objects, etc. \"\"\" return isinstance(obj, (types.GeneratorType, collections.MappingView, six.moves.range, enumerate))"], "rank": 2}
{"nl": "python how to create date from string", "code": ["def string_to_date(value): \"\"\" Return a Python date that corresponds to the specified string representation. @param value: string representation of a date. @return: an instance ``datetime.datetime`` represented by the string. \"\"\" if isinstance(value, datetime.date): return value return dateutil.parser.parse(value).date()", "def _read_date_from_string(str1): \"\"\" Reads the date from a string in the format YYYY/MM/DD and returns :class: datetime.date \"\"\" full_date = [int(x) for x in str1.split('/')] return datetime.date(full_date[0], full_date[1], full_date[2])", "def deserialize_date(string): \"\"\" Deserializes string to date. :param string: str. :type string: str :return: date. :rtype: date \"\"\" try: from dateutil.parser import parse return parse(string).date() except ImportError: return string", "def datetime_from_str(string): \"\"\" Args: string: string of the form YYMMDD-HH_MM_SS, e.g 160930-18_43_01 Returns: a datetime object \"\"\" return datetime.datetime(year=2000+int(string[0:2]), month=int(string[2:4]), day=int(string[4:6]), hour=int(string[7:9]), minute=int(string[10:12]),second=int(string[13:15]))", "def get_date(date): \"\"\" Get the date from a value that could be a date object or a string. :param date: The date object or string. :returns: The date object. \"\"\" if type(date) is str: return datetime.strptime(date, '%Y-%m-%d').date() else: return date"], "rank": 7}
{"nl": "how to append a line in a file in the middle of file in python", "code": ["def prepend_line(filepath, line): \"\"\"Rewrite a file adding a line to its beginning. \"\"\" with open(filepath) as f: lines = f.readlines() lines.insert(0, line) with open(filepath, 'w') as f: f.writelines(lines)", "def append_text(self, txt): \"\"\" adds a line of text to a file \"\"\" with open(self.fullname, \"a\") as myfile: myfile.write(txt)", "def writefile(openedfile, newcontents): \"\"\"Set the contents of a file.\"\"\" openedfile.seek(0) openedfile.truncate() openedfile.write(newcontents)", "def write_line(self, line, count=1): \"\"\"writes the line and count newlines after the line\"\"\" self.write(line) self.write_newlines(count)", "def align_file_position(f, size): \"\"\" Align the position in the file to the next block of specified size \"\"\" align = (size - 1) - (f.tell() % size) f.seek(align, 1)"], "rank": 1}
{"nl": "how to split a string by every character in python", "code": ["def _split(string, splitters): \"\"\"Splits a string into parts at multiple characters\"\"\" part = '' for character in string: if character in splitters: yield part part = '' else: part += character yield part", "def split(s): \"\"\"Uses dynamic programming to infer the location of spaces in a string without spaces.\"\"\" l = [_split(x) for x in _SPLIT_RE.split(s)] return [item for sublist in l for item in sublist]", "def multi_split(s, split): # type: (S, Iterable[S]) -> List[S] \"\"\"Splits on multiple given separators.\"\"\" for r in split: s = s.replace(r, \"|\") return [i for i in s.split(\"|\") if len(i) > 0]", "def split_on(s, sep=\" \"): \"\"\"Split s by sep, unless it's inside a quote.\"\"\" pattern = '''((?:[^%s\"']|\"[^\"]*\"|'[^']*')+)''' % sep return [_strip_speechmarks(t) for t in re.split(pattern, s)[1::2]]", "def split(text: str) -> List[str]: \"\"\"Split a text into a list of tokens. :param text: the text to split :return: tokens \"\"\" return [word for word in SEPARATOR.split(text) if word.strip(' \\t')]"], "rank": 1}
{"nl": "python get index of element each time it appears in list", "code": ["def index(self, item): \"\"\" Not recommended for use on large lists due to time complexity, but it works -> #int list index of @item \"\"\" for i, x in enumerate(self.iter()): if x == item: return i return None", "def _duplicates(list_): \"\"\"Return dict mapping item -> indices.\"\"\" item_indices = {} for i, item in enumerate(list_): try: item_indices[item].append(i) except KeyError: # First time seen item_indices[item] = [i] return item_indices", "def is_in(self, search_list, pair): \"\"\" If pair is in search_list, return the index. Otherwise return -1 \"\"\" index = -1 for nr, i in enumerate(search_list): if(np.all(i == pair)): return nr return index", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def count_list(the_list): \"\"\" Generates a count of the number of times each unique item appears in a list \"\"\" count = the_list.count result = [(item, count(item)) for item in set(the_list)] result.sort() return result"], "rank": 1}
{"nl": "python how to make dot character", "code": ["def _dotify(cls, data): \"\"\"Add dots.\"\"\" return ''.join(char if char in cls.PRINTABLE_DATA else '.' for char in data)", "def _intermediary_to_dot(tables, relationships): \"\"\" Returns the dot source representing the database in a string. \"\"\" t = '\\n'.join(t.to_dot() for t in tables) r = '\\n'.join(r.to_dot() for r in relationships) return '{}\\n{}\\n{}\\n}}'.format(GRAPH_BEGINNING, t, r)", "def join_field(path): \"\"\" RETURN field SEQUENCE AS STRING \"\"\" output = \".\".join([f.replace(\".\", \"\\\\.\") for f in path if f != None]) return output if output else \".\"", "def to_dotfile(self): \"\"\" Writes a DOT graphviz file of the domain structure, and returns the filename\"\"\" domain = self.get_domain() filename = \"%s.dot\" % (self.__class__.__name__) nx.write_dot(domain, filename) return filename", "def export_to_dot(self, filename: str = 'output') -> None: \"\"\" Export the graph to the dot file \"filename.dot\". \"\"\" with open(filename + '.dot', 'w') as output: output.write(self.as_dot())"], "rank": 1}
{"nl": "python heap top element", "code": ["def pop(h): \"\"\"Pop the heap value from the heap.\"\"\" n = h.size() - 1 h.swap(0, n) down(h, 0, n) return h.pop()", "def heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt", "def _heappop_max(heap): \"\"\"Maxheap version of a heappop.\"\"\" lastelt = heap.pop() # raises appropriate IndexError if heap is empty if heap: returnitem = heap[0] heap[0] = lastelt _siftup_max(heap, 0) return returnitem return lastelt", "def heappush_max(heap, item): \"\"\"Push item onto heap, maintaining the heap invariant.\"\"\" heap.append(item) _siftdown_max(heap, 0, len(heap) - 1)", "def _heappush_max(heap, item): \"\"\" why is this not in heapq \"\"\" heap.append(item) heapq._siftdown_max(heap, 0, len(heap) - 1)"], "rank": 1}
{"nl": "in python, how to print strings in different colours", "code": ["def sprint(text, *colors): \"\"\"Format text with color or other effects into ANSI escaped string.\"\"\" return \"\\33[{}m{content}\\33[{}m\".format(\";\".join([str(color) for color in colors]), RESET, content=text) if IS_ANSI_TERMINAL and colors else text", "def write_color(string, name, style='normal', when='auto'): \"\"\" Write the given colored string to standard out. \"\"\" write(color(string, name, style, when))", "def cprint(string, fg=None, bg=None, end='\\n', target=sys.stdout): \"\"\"Print a colored string to the target handle. fg and bg specify foreground- and background colors, respectively. The remaining keyword arguments are the same as for Python's built-in print function. Colors are returned to their defaults before the function returns. \"\"\" _color_manager.set_color(fg, bg) target.write(string + end) target.flush() # Needed for Python 3.x _color_manager.set_defaults()", "def printc(cls, txt, color=colors.red): \"\"\"Print in color.\"\"\" print(cls.color_txt(txt, color))", "def colorize(txt, fg=None, bg=None): \"\"\" Print escape codes to set the terminal color. fg and bg are indices into the color palette for the foreground and background colors. \"\"\" setting = '' setting += _SET_FG.format(fg) if fg else '' setting += _SET_BG.format(bg) if bg else '' return setting + str(txt) + _STYLE_RESET"], "rank": 3}
{"nl": "python yield unsupported operand type(s)", "code": ["def visit_BoolOp(self, node): \"\"\" Return type may come from any boolop operand. \"\"\" return sum((self.visit(value) for value in node.values), [])", "def __init__(self,operand,operator,**args): \"\"\" Accepts a NumberGenerator operand, an operator, and optional arguments to be provided to the operator when calling it on the operand. \"\"\" # Note that it's currently not possible to set # parameters in the superclass when creating an instance, # because **args is used by this class itself. super(UnaryOperator,self).__init__() self.operand=operand self.operator=operator self.args=args", "def __iter__(self): \"\"\"Overloads iter(condition), and also, for bit in condition. The values yielded by the iterator are True (1), False (0), or None (#).\"\"\" for bit, mask in zip(self._bits, self._mask): yield bit if mask else None", "def visit_BinOp(self, node): \"\"\" Return type depend from both operand of the binary operation. \"\"\" args = [self.visit(arg) for arg in (node.left, node.right)] return list({frozenset.union(*x) for x in itertools.product(*args)})", "def storeByteArray(self, context, page, len, data, returnError): \"\"\"please override\"\"\" returnError.contents.value = self.IllegalStateError raise NotImplementedError(\"You must override this method.\")"], "rank": 4}
{"nl": "how to sort files by filename python", "code": ["def sort_filenames(filenames): \"\"\" sort a list of files by filename only, ignoring the directory names \"\"\" basenames = [os.path.basename(x) for x in filenames] indexes = [i[0] for i in sorted(enumerate(basenames), key=lambda x:x[1])] return [filenames[x] for x in indexes]", "def sort_fn_list(fn_list): \"\"\"Sort input filename list by datetime \"\"\" dt_list = get_dt_list(fn_list) fn_list_sort = [fn for (dt,fn) in sorted(zip(dt_list,fn_list))] return fn_list_sort", "def sort_by_modified(files_or_folders: list) -> list: \"\"\" Sort files or folders by modified time Args: files_or_folders: list of files or folders Returns: list \"\"\" return sorted(files_or_folders, key=os.path.getmtime, reverse=True)", "def _remove_duplicate_files(xs): \"\"\"Remove files specified multiple times in a list. \"\"\" seen = set([]) out = [] for x in xs: if x[\"path\"] not in seen: out.append(x) seen.add(x[\"path\"]) return out", "def newest_file(file_iterable): \"\"\" Returns the name of the newest file given an iterable of file names. \"\"\" return max(file_iterable, key=lambda fname: os.path.getmtime(fname))"], "rank": 1}
{"nl": "how to change the axis range in a plot in python for subplots", "code": ["def setAutoRangeOn(self, axisNumber): \"\"\" Sets the auto-range of the axis on. :param axisNumber: 0 (X-axis), 1 (Y-axis), 2, (Both X and Y axes). \"\"\" setXYAxesAutoRangeOn(self, self.xAxisRangeCti, self.yAxisRangeCti, axisNumber)", "def autozoom(self, n=None): \"\"\" Auto-scales the axes to fit all the data in plot index n. If n == None, auto-scale everyone. \"\"\" if n==None: for p in self.plot_widgets: p.autoRange() else: self.plot_widgets[n].autoRange() return self", "def ylim(self, low, high, index=1): \"\"\"Set yaxis limits. Parameters ---------- low : number high : number index : int, optional Returns ------- Chart \"\"\" self.layout['yaxis' + str(index)]['range'] = [low, high] return self", "def set_xlimits_widgets(self, set_min=True, set_max=True): \"\"\"Populate axis limits GUI with current plot values.\"\"\" xmin, xmax = self.tab_plot.ax.get_xlim() if set_min: self.w.x_lo.set_text('{0}'.format(xmin)) if set_max: self.w.x_hi.set_text('{0}'.format(xmax))", "def set_xlimits(self, row, column, min=None, max=None): \"\"\"Set x-axis limits of a subplot. :param row,column: specify the subplot. :param min: minimal axis value :param max: maximum axis value \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_xlimits(min, max)"], "rank": 5}
{"nl": "python view vector to asimuth elevation", "code": ["def world_to_view(v): \"\"\"world coords to view coords; v an eu.Vector2, returns (float, float)\"\"\" return v.x * config.scale_x, v.y * config.scale_y", "def _convert_latitude(self, latitude): \"\"\"Convert from latitude to the y position in overall map.\"\"\" return int((180 - (180 / pi * log(tan( pi / 4 + latitude * pi / 360)))) * (2 ** self._zoom) * self._size / 360)", "def euler(self): \"\"\"TODO DEPRECATE THIS?\"\"\" e_xyz = transformations.euler_from_matrix(self.rotation, 'sxyz') return np.array([180.0 / np.pi * a for a in e_xyz])", "def metres2latlon(mx, my, origin_shift= 2 * pi * 6378137 / 2.0): \"\"\"Converts XY point from Spherical Mercator EPSG:900913 to lat/lon in WGS84 Datum\"\"\" lon = (mx / origin_shift) * 180.0 lat = (my / origin_shift) * 180.0 lat = 180 / pi * (2 * atan( exp( lat * pi / 180.0)) - pi / 2.0) return lat, lon", "def earth_orientation(date): \"\"\"Earth orientation as a rotating matrix \"\"\" x_p, y_p, s_prime = np.deg2rad(_earth_orientation(date)) return rot3(-s_prime) @ rot2(x_p) @ rot1(y_p)"], "rank": 1}
{"nl": "python sort data by variable", "code": ["def sort_data(x, y): \"\"\"Sort the data.\"\"\" xy = sorted(zip(x, y)) x, y = zip(*xy) return x, y", "def sort_data(data, cols): \"\"\"Sort `data` rows and order columns\"\"\" return data.sort_values(cols)[cols + ['value']].reset_index(drop=True)", "def unsort_vector(data, indices_of_increasing): \"\"\"Upermutate 1-D data that is sorted by indices_of_increasing.\"\"\" return numpy.array([data[indices_of_increasing.index(i)] for i in range(len(data))])", "def sort_func(self, key): \"\"\"Sorting logic for `Quantity` objects.\"\"\" if key == self._KEYS.VALUE: return 'aaa' if key == self._KEYS.SOURCE: return 'zzz' return key", "def csort(objs, key): \"\"\"Order-preserving sorting function.\"\"\" idxs = dict((obj, i) for (i, obj) in enumerate(objs)) return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))"], "rank": 1}
{"nl": "compute the middle index in list python", "code": ["def bisect_index(a, x): \"\"\" Find the leftmost index of an element in a list using binary search. Parameters ---------- a: list A sorted list. x: arbitrary The element. Returns ------- int The index. \"\"\" i = bisect.bisect_left(a, x) if i != len(a) and a[i] == x: return i raise ValueError", "def sorted_index(values, x): \"\"\" For list, values, returns the index location of element x. If x does not exist will raise an error. :param values: list :param x: item :return: integer index \"\"\" i = bisect_left(values, x) j = bisect_right(values, x) return values[i:j].index(x) + i", "def _mid(pt1, pt2): \"\"\" (Point, Point) -> Point Return the point that lies in between the two input points. \"\"\" (x0, y0), (x1, y1) = pt1, pt2 return 0.5 * (x0 + x1), 0.5 * (y0 + y1)", "def find_lt(a, x): \"\"\"Find rightmost value less than x.\"\"\" i = bs.bisect_left(a, x) if i: return i - 1 raise ValueError", "def _mid(string, start, end=None): \"\"\" Returns a substring delimited by start and end position. \"\"\" if end is None: end = len(string) return string[start:start + end]"], "rank": 1}
{"nl": "python josn dump to file", "code": ["def save_notebook(work_notebook, write_file): \"\"\"Saves the Jupyter work_notebook to write_file\"\"\" with open(write_file, 'w') as out_nb: json.dump(work_notebook, out_nb, indent=2)", "def json_pretty_dump(obj, filename): \"\"\" Serialize obj as a JSON formatted stream to the given filename ( pretty printing version) \"\"\" with open(filename, \"wt\") as fh: json.dump(obj, fh, indent=4, sort_keys=4)", "def save_json(object, handle, indent=2): \"\"\"Save object as json on CNS.\"\"\" obj_json = json.dumps(object, indent=indent, cls=NumpyJSONEncoder) handle.write(obj_json)", "def save(self, fname): \"\"\" Saves the dictionary in json format :param fname: file to save to \"\"\" with open(fname, 'wb') as f: json.dump(self, f)", "def write_document(doc, fnm): \"\"\"Write a Text document to file. Parameters ---------- doc: Text The document to save. fnm: str The filename to save the document \"\"\" with codecs.open(fnm, 'wb', 'ascii') as f: f.write(json.dumps(doc, indent=2))"], "rank": 1}
{"nl": "python remove condition apply to dict", "code": ["def _remove_dict_keys_with_value(dict_, val): \"\"\"Removes `dict` keys which have have `self` as value.\"\"\" return {k: v for k, v in dict_.items() if v is not val}", "def _remove_keywords(d): \"\"\" copy the dict, filter_keywords Parameters ---------- d : dict \"\"\" return { k:v for k, v in iteritems(d) if k not in RESERVED }", "def filter_dict_by_key(d, keys): \"\"\"Filter the dict *d* to remove keys not in *keys*.\"\"\" return {k: v for k, v in d.items() if k in keys}", "def _clean_dict(target_dict, whitelist=None): \"\"\" Convenience function that removes a dicts keys that have falsy values \"\"\" assert isinstance(target_dict, dict) return { ustr(k).strip(): ustr(v).strip() for k, v in target_dict.items() if v not in (None, Ellipsis, [], (), \"\") and (not whitelist or k in whitelist) }", "def filter_dict(d, keys): \"\"\" Creates a new dict from an existing dict that only has the given keys \"\"\" return {k: v for k, v in d.items() if k in keys}"], "rank": 1}
{"nl": "python remove directory tree if no files", "code": ["def clean_out_dir(directory): \"\"\" Delete all the files and subdirectories in a directory. \"\"\" if not isinstance(directory, path): directory = path(directory) for file_path in directory.files(): file_path.remove() for dir_path in directory.dirs(): dir_path.rmtree()", "def _clear_dir(dirName): \"\"\" Remove a directory and it contents. Ignore any failures. \"\"\" # If we got here, clear dir for fname in os.listdir(dirName): try: os.remove( os.path.join(dirName, fname) ) except Exception: pass try: os.rmdir(dirName) except Exception: pass", "def clean_py_files(path): \"\"\" Removes all .py files. :param path: the path :return: None \"\"\" for dirname, subdirlist, filelist in os.walk(path): for f in filelist: if f.endswith('py'): os.remove(os.path.join(dirname, f))", "def safe_rmtree(directory): \"\"\"Delete a directory if it's present. If it's not present, no-op.\"\"\" if os.path.exists(directory): shutil.rmtree(directory, True)", "def cleanup(self): \"\"\"Clean up any temporary files.\"\"\" for file in glob.glob(self.basename + '*'): os.unlink(file)"], "rank": 1}
{"nl": "how to pop a node off a stack python", "code": ["def pop(self): \"\"\" return the last stack element and delete it from the list \"\"\" if not self.empty(): val = self.stack[-1] del self.stack[-1] return val", "def __pop_top_frame(self): \"\"\"Pops the top frame off the frame stack.\"\"\" popped = self.__stack.pop() if self.__stack: self.__stack[-1].process_subframe(popped)", "def pop(h): \"\"\"Pop the heap value from the heap.\"\"\" n = h.size() - 1 h.swap(0, n) down(h, 0, n) return h.pop()", "def up(self): \"\"\"Moves the layer up in the stacking order. \"\"\" i = self.index() if i != None: del self.canvas.layers[i] i = min(len(self.canvas.layers), i+1) self.canvas.layers.insert(i, self)", "def up(self): \"\"\"Go up in stack and return True if top frame\"\"\" if self.frame: self.frame = self.frame.f_back return self.frame is None"], "rank": 7}
{"nl": "python open file with exclusive access permissions", "code": ["def fopenat(base_fd, path): \"\"\" Does openat read-only, then does fdopen to get a file object \"\"\" return os.fdopen(openat(base_fd, path, os.O_RDONLY), 'rb')", "def open_file(file, mode): \"\"\"Open a file. :arg file: file-like or path-like object. :arg str mode: ``mode`` argument for :func:`open`. \"\"\" if hasattr(file, \"read\"): return file if hasattr(file, \"open\"): return file.open(mode) return open(file, mode)", "def make_file_read_only(file_path): \"\"\" Removes the write permissions for the given file for owner, groups and others. :param file_path: The file whose privileges are revoked. :raise FileNotFoundError: If the given file does not exist. \"\"\" old_permissions = os.stat(file_path).st_mode os.chmod(file_path, old_permissions & ~WRITE_PERMISSIONS)", "def make_writeable(filename): \"\"\" Make sure that the file is writeable. Useful if our source is read-only. \"\"\" if not os.access(filename, os.W_OK): st = os.stat(filename) new_permissions = stat.S_IMODE(st.st_mode) | stat.S_IWUSR os.chmod(filename, new_permissions)", "def make_file_readable (filename): \"\"\"Make file user readable if it is not a link.\"\"\" if not os.path.islink(filename): util.set_mode(filename, stat.S_IRUSR)"], "rank": 12}
{"nl": "python gevent combine multiprocessing", "code": ["def fetch_event(urls): \"\"\" This parallel fetcher uses gevent one uses gevent \"\"\" rs = (grequests.get(u) for u in urls) return [content.json() for content in grequests.map(rs)]", "def compute(args): x, y, params = args \"\"\"Callable function for the multiprocessing pool.\"\"\" return x, y, mandelbrot(x, y, params)", "def _gevent_patch(): \"\"\"Patch the modules with gevent :return: Default is GEVENT. If it not supports gevent then return MULTITHREAD :rtype: int \"\"\" try: assert gevent assert grequests except NameError: logger.warn('gevent not exist, fallback to multiprocess...') return MULTITHREAD else: monkey.patch_all() # Must patch before get_photos_info return GEVENT", "def compute_capture(args): x, y, w, h, params = args \"\"\"Callable function for the multiprocessing pool.\"\"\" return x, y, mandelbrot_capture(x, y, w, h, params)", "def _spawn(self, func, *args, **kwargs): \"\"\"Spawn a handler function. Spawns the supplied ``func`` with ``*args`` and ``**kwargs`` as a gevent greenlet. :param func: A callable to call. :param args: Arguments to ``func``. :param kwargs: Keyword arguments to ``func``. \"\"\" gevent.spawn(func, *args, **kwargs)"], "rank": 1}
{"nl": "create copy that doesn't alter original python", "code": ["def __copy__(self): \"\"\"A magic method to implement shallow copy behavior.\"\"\" return self.__class__.load(self.dump(), context=self.context)", "def copy(self): \"\"\"Return a shallow copy.\"\"\" return self.__class__(self.operations.copy(), self.collection, self.document)", "def copy(self): \"\"\"Create an identical (deep) copy of this element.\"\"\" result = self.space.element() result.assign(self) return result", "def copy(obj): def copy(self): \"\"\" Copy self to a new object. \"\"\" from copy import deepcopy return deepcopy(self) obj.copy = copy return obj", "def copy(self): \"\"\" Creates a copy of model \"\"\" return self.__class__(field_type=self.get_field_type(), data=self.export_data())"], "rank": 4}
{"nl": "remove trailing whitespace in python", "code": ["def clean(s): \"\"\"Removes trailing whitespace on each line.\"\"\" lines = [l.rstrip() for l in s.split('\\n')] return '\\n'.join(lines)", "def text_remove_empty_lines(text): \"\"\" Whitespace normalization: - Strip empty lines - Strip trailing whitespace \"\"\" lines = [ line.rstrip() for line in text.splitlines() if line.strip() ] return \"\\n\".join(lines)", "def clean_whitespace(statement): \"\"\" Remove any consecutive whitespace characters from the statement text. \"\"\" import re # Replace linebreaks and tabs with spaces statement.text = statement.text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ') # Remove any leeding or trailing whitespace statement.text = statement.text.strip() # Remove consecutive spaces statement.text = re.sub(' +', ' ', statement.text) return statement", "def _delete_whitespace(self): \"\"\"Delete all whitespace from the end of the line.\"\"\" while isinstance(self._lines[-1], (self._Space, self._LineBreak, self._Indent)): del self._lines[-1]", "def fix_line_breaks(s): \"\"\" Convert \\r\\n and \\r to \\n chars. Strip any leading or trailing whitespace on each line. Remove blank lines. \"\"\" l = s.splitlines() x = [i.strip() for i in l] x = [i for i in x if i] # remove blank lines return \"\\n\".join(x)"], "rank": 1}
{"nl": "python default menuitem select", "code": ["def contextMenuEvent(self, event): \"\"\"Override Qt method\"\"\" self.update_menu() self.menu.popup(event.globalPos())", "def select_default(self): \"\"\" Resets the combo box to the original \"selected\" value from the constructor (or the first value if no selected value was specified). \"\"\" if self._default is None: if not self._set_option_by_index(0): utils.error_format(self.description + \"\\n\" + \"Unable to select default option as the Combo is empty\") else: if not self._set_option(self._default): utils.error_format( self.description + \"\\n\" + \"Unable to select default option as it doesnt exist in the Combo\")", "def add(self, name, desc, func=None, args=None, krgs=None): \"\"\"Add a menu entry.\"\"\" self.entries.append(MenuEntry(name, desc, func, args or [], krgs or {}))", "def contextMenuEvent(self, event): \"\"\"Reimplement Qt method\"\"\" self.menu.popup(event.globalPos()) event.accept()", "def _selectItem(self, index): \"\"\"Select item in the list \"\"\" self._selectedIndex = index self.setCurrentIndex(self.model().createIndex(index, 0))"], "rank": 19}
{"nl": "get sort indexes in a list python", "code": ["def _index_ordering(redshift_list): \"\"\" :param redshift_list: list of redshifts :return: indexes in acending order to be evaluated (from z=0 to z=z_source) \"\"\" redshift_list = np.array(redshift_list) sort_index = np.argsort(redshift_list) return sort_index", "def naturalsortkey(s): \"\"\"Natural sort order\"\"\" return [int(part) if part.isdigit() else part for part in re.split('([0-9]+)', s)]", "def argsort_indices(a, axis=-1): \"\"\"Like argsort, but returns an index suitable for sorting the the original array even if that array is multidimensional \"\"\" a = np.asarray(a) ind = list(np.ix_(*[np.arange(d) for d in a.shape])) ind[axis] = a.argsort(axis) return tuple(ind)", "def sort_data(x, y): \"\"\"Sort the data.\"\"\" xy = sorted(zip(x, y)) x, y = zip(*xy) return x, y", "def csort(objs, key): \"\"\"Order-preserving sorting function.\"\"\" idxs = dict((obj, i) for (i, obj) in enumerate(objs)) return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))"], "rank": 1}
{"nl": "how to start a new line in python gui", "code": ["def go_to_new_line(self): \"\"\"Go to the end of the current line and create a new line\"\"\" self.stdkey_end(False, False) self.insert_text(self.get_line_separator())", "def page_guiref(arg_s=None): \"\"\"Show a basic reference about the GUI Console.\"\"\" from IPython.core import page page.page(gui_reference, auto_html=True)", "def separator(self, menu=None): \"\"\"Add a separator\"\"\" self.gui.get_menu(menu or self.menu).addSeparator()", "def get_hline(): \"\"\" gets a horiztonal line \"\"\" return Window( width=LayoutDimension.exact(1), height=LayoutDimension.exact(1), content=FillControl('-', token=Token.Line))", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0"], "rank": 1}
{"nl": "how to get tuple of colors in image python", "code": ["def get_colors(img): \"\"\" Returns a list of all the image's colors. \"\"\" w, h = img.size return [color[:3] for count, color in img.convert('RGB').getcolors(w * h)]", "def GetAllPixelColors(self) -> ctypes.Array: \"\"\" Return `ctypes.Array`, an iterable array of int values in argb. \"\"\" return self.GetPixelColorsOfRect(0, 0, self.Width, self.Height)", "def rgba_bytes_tuple(self, x): \"\"\"Provides the color corresponding to value `x` in the form of a tuple (R,G,B,A) with int values between 0 and 255. \"\"\" return tuple(int(u*255.9999) for u in self.rgba_floats_tuple(x))", "def _get_background_color(self): \"\"\"Returns background color rgb tuple of right line\"\"\" color = self.cell_attributes[self.key][\"bgcolor\"] return tuple(c / 255.0 for c in color_pack2rgb(color))", "def hex_color_to_tuple(hex): \"\"\" convent hex color to tuple \"#ffffff\" -> (255, 255, 255) \"#ffff00ff\" -> (255, 255, 0, 255) \"\"\" hex = hex[1:] length = len(hex) // 2 return tuple(int(hex[i*2:i*2+2], 16) for i in range(length))"], "rank": 3}
{"nl": "make datetime aware python", "code": ["def date_to_datetime(x): \"\"\"Convert a date into a datetime\"\"\" if not isinstance(x, datetime) and isinstance(x, date): return datetime.combine(x, time()) return x", "def date_to_datetime(d): \"\"\" >>> date_to_datetime(date(2000, 1, 2)) datetime.datetime(2000, 1, 2, 0, 0) >>> date_to_datetime(datetime(2000, 1, 2, 3, 4, 5)) datetime.datetime(2000, 1, 2, 3, 4, 5) \"\"\" if not isinstance(d, datetime): d = datetime.combine(d, datetime.min.time()) return d", "def from_pydatetime(cls, pydatetime): \"\"\" Creates sql datetime2 object from Python datetime object ignoring timezone @param pydatetime: Python datetime object @return: sql datetime2 object \"\"\" return cls(date=Date.from_pydate(pydatetime.date), time=Time.from_pytime(pydatetime.time))", "def is_datetime_like(dtype): \"\"\"Check if a dtype is a subclass of the numpy datetime types \"\"\" return (np.issubdtype(dtype, np.datetime64) or np.issubdtype(dtype, np.timedelta64))", "def to_datetime(value): \"\"\"Converts a string to a datetime.\"\"\" if value is None: return None if isinstance(value, six.integer_types): return parser.parse(value) return parser.isoparse(value)"], "rank": 1}
{"nl": "python dict drop empty", "code": ["def _clean_dict(target_dict, whitelist=None): \"\"\" Convenience function that removes a dicts keys that have falsy values \"\"\" assert isinstance(target_dict, dict) return { ustr(k).strip(): ustr(v).strip() for k, v in target_dict.items() if v not in (None, Ellipsis, [], (), \"\") and (not whitelist or k in whitelist) }", "def _remove_none_values(dictionary): \"\"\" Remove dictionary keys whose value is None \"\"\" return list(map(dictionary.pop, [i for i in dictionary if dictionary[i] is None]))", "def clean_map(obj: Mapping[Any, Any]) -> Mapping[Any, Any]: \"\"\" Return a new copied dictionary without the keys with ``None`` values from the given Mapping object. \"\"\" return {k: v for k, v in obj.items() if v is not None}", "def purge_dict(idict): \"\"\"Remove null items from a dictionary \"\"\" odict = {} for key, val in idict.items(): if is_null(val): continue odict[key] = val return odict", "def nonull_dict(self): \"\"\"Like dict, but does not hold any null values. :return: \"\"\" return {k: v for k, v in six.iteritems(self.dict) if v and k != '_codes'}"], "rank": 4}
{"nl": "python pretty print without sort", "code": ["def pformat(o, indent=1, width=80, depth=None): \"\"\"Format a Python o into a pretty-printed representation.\"\"\" return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(o)", "def pprint(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Like `pretty` but print to stdout. \"\"\" printer = RepresentationPrinter(sys.stdout, verbose, max_width, newline) printer.pretty(obj) printer.flush() sys.stdout.write(newline) sys.stdout.flush()", "def pformat(object, indent=1, width=80, depth=None): \"\"\"Format a Python object into a pretty-printed representation.\"\"\" return PrettyPrinter(indent=indent, width=width, depth=depth).pformat(object)", "def pretty(obj, verbose=False, max_width=79, newline='\\n'): \"\"\" Pretty print the object's representation. \"\"\" stream = StringIO() printer = RepresentationPrinter(stream, verbose, max_width, newline) printer.pretty(obj) printer.flush() return stream.getvalue()", "def pprint(self, ind): \"\"\"pretty prints the tree with indentation\"\"\" pp = pprint.PrettyPrinter(indent=ind) pp.pprint(self.tree)"], "rank": 2}
{"nl": "python check if interactive", "code": ["def isInteractive(): \"\"\" A basic check of if the program is running in interactive mode \"\"\" if sys.stdout.isatty() and os.name != 'nt': #Hopefully everything but ms supports '\\r' try: import threading except ImportError: return False else: return True else: return False", "def is_interactive(self): \"\"\" Determine if the user requested interactive mode. \"\"\" # The Python interpreter sets sys.flags correctly, so use them! if sys.flags.interactive: return True # IPython does not set sys.flags when -i is specified, so first # check it if it is already imported. if '__IPYTHON__' not in dir(six.moves.builtins): return False # Then we check the application singleton and determine based on # a variable it sets. try: from IPython.config.application import Application as App return App.initialized() and App.instance().interact except (ImportError, AttributeError): return False", "def determine_interactive(self): \"\"\"Determine whether we're in an interactive shell. Sets interactivity off if appropriate. cf http://stackoverflow.com/questions/24861351/how-to-detect-if-python-script-is-being-run-as-a-background-process \"\"\" try: if not sys.stdout.isatty() or os.getpgrp() != os.tcgetpgrp(sys.stdout.fileno()): self.interactive = 0 return False except Exception: self.interactive = 0 return False if self.interactive == 0: return False return True", "def _in_qtconsole() -> bool: \"\"\" A small utility function which determines if we're running in QTConsole's context. \"\"\" try: from IPython import get_ipython try: from ipykernel.zmqshell import ZMQInteractiveShell shell_object = ZMQInteractiveShell except ImportError: from IPython.kernel.zmq import zmqshell shell_object = zmqshell.ZMQInteractiveShell return isinstance(get_ipython(), shell_object) except Exception: return False", "def intty(cls): \"\"\" Check if we are in a tty. \"\"\" # XXX: temporary hack until we can detect if we are in a pipe or not return True if hasattr(sys.stdout, 'isatty') and sys.stdout.isatty(): return True return False"], "rank": 3}
{"nl": "python pid determine existence", "code": ["def pid_exists(pid): \"\"\" Determines if a system process identifer exists in process table. \"\"\" try: os.kill(pid, 0) except OSError as exc: return exc.errno == errno.EPERM else: return True", "def is_running(process_id: int) -> bool: \"\"\" Uses the Unix ``ps`` program to see if a process is running. \"\"\" pstr = str(process_id) encoding = sys.getdefaultencoding() s = subprocess.Popen([\"ps\", \"-p\", pstr], stdout=subprocess.PIPE) for line in s.stdout: strline = line.decode(encoding) if pstr in strline: return True return False", "def get_pid_list(): \"\"\"Returns a list of PIDs currently running on the system.\"\"\" pids = [int(x) for x in os.listdir('/proc') if x.isdigit()] return pids", "def write_pid_file(): \"\"\"Write a file with the PID of this server instance. Call when setting up a command line testserver. \"\"\" pidfile = os.path.basename(sys.argv[0])[:-3] + '.pid' # strip .py, add .pid with open(pidfile, 'w') as fh: fh.write(\"%d\\n\" % os.getpid()) fh.close()", "def is_alive(self): \"\"\" @rtype: bool @return: C{True} if the process is currently running. \"\"\" try: self.wait(0) except WindowsError: e = sys.exc_info()[1] return e.winerror == win32.WAIT_TIMEOUT return False"], "rank": 1}
{"nl": "python pymongo insert without duplicatte", "code": ["def upsert_multi(db, collection, object, match_params=None): \"\"\" Wrapper for pymongo.insert_many() and update_many() :param db: db connection :param collection: collection to update :param object: the modifications to apply :param match_params: a query that matches the documents to update :return: ids of inserted/updated document \"\"\" if isinstance(object, list) and len(object) > 0: return str(db[collection].insert_many(object).inserted_ids) elif isinstance(object, dict): return str(db[collection].update_many(match_params, {\"$set\": object}, upsert=False).upserted_id)", "def upsert_single(db, collection, object, match_params=None): \"\"\" Wrapper for pymongo.update_one() :param db: db connection :param collection: collection to update :param object: the modifications to apply :param match_params: a query that matches the documents to update :return: id of updated document \"\"\" return str(db[collection].update_one(match_params, {\"$set\": object}, upsert=True).upserted_id)", "def insert_one(self, mongo_collection, doc, mongo_db=None, **kwargs): \"\"\" Inserts a single document into a mongo collection https://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.insert_one \"\"\" collection = self.get_collection(mongo_collection, mongo_db=mongo_db) return collection.insert_one(doc, **kwargs)", "def update_one(self, query, doc): \"\"\" Updates one element of the collection :param query: dictionary representing the mongo query :param doc: dictionary representing the item to be updated :return: UpdateResult \"\"\" if self.table is None: self.build_table() if u\"$set\" in doc: doc = doc[u\"$set\"] allcond = self.parse_query(query) try: result = self.table.update(doc, allcond) except: # TODO: check table.update result # check what pymongo does in that case result = None return UpdateResult(raw_result=result)", "def forceupdate(self, *args, **kw): \"\"\"Like a bulk :meth:`forceput`.\"\"\" self._update(False, self._ON_DUP_OVERWRITE, *args, **kw)"], "rank": 3}
{"nl": "how to read json files with multiple object python", "code": ["def load_from_file(cls, file_path: str): \"\"\" Read and reconstruct the data from a JSON file. \"\"\" with open(file_path, \"r\") as f: data = json.load(f) item = cls.decode(data=data) return item", "def json_iter (path): \"\"\" iterator for JSON-per-line in a file pattern \"\"\" with open(path, 'r') as f: for line in f.readlines(): yield json.loads(line)", "def load(cls, fp, **kwargs): \"\"\"wrapper for :py:func:`json.load`\"\"\" json_obj = json.load(fp, **kwargs) return parse(cls, json_obj)", "def read(self): \"\"\"Iterate over all JSON input (Generator)\"\"\" for line in self.io.read(): with self.parse_line(line) as j: yield j", "def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)"], "rank": 6}
{"nl": "python discord leave voice channel", "code": ["async def join(self, ctx, *, channel: discord.VoiceChannel): \"\"\"Joins a voice channel\"\"\" if ctx.voice_client is not None: return await ctx.voice_client.move_to(channel) await channel.connect()", "async def connect(self): \"\"\" Connects to the voice channel associated with this Player. \"\"\" await self.node.join_voice_channel(self.channel.guild.id, self.channel.id)", "def log_leave(event, nick, channel): \"\"\" Log a quit or part event. \"\"\" if channel not in pmxbot.config.log_channels: return ParticipantLogger.store.log(nick, channel, event.type)", "def send_notice(self, text): \"\"\"Send a notice (from bot) message to the room.\"\"\" return self.client.api.send_notice(self.room_id, text)", "def on_welcome(self, connection, event): \"\"\" Join the channel once connected to the IRC server. \"\"\" connection.join(self.channel, key=settings.IRC_CHANNEL_KEY or \"\")"], "rank": 1}
{"nl": "python unittest make tests discoverable", "code": ["def test(): \"\"\"Run the unit tests.\"\"\" import unittest tests = unittest.TestLoader().discover('tests') unittest.TextTestRunner(verbosity=2).run(tests)", "def test(): \"\"\" Run all Tests [nose] \"\"\" command = 'nosetests --with-coverage --cover-package=pwnurl' status = subprocess.call(shlex.split(command)) sys.exit(status)", "def test(*args): \"\"\" Run unit tests. \"\"\" subprocess.call([\"py.test-2.7\"] + list(args)) subprocess.call([\"py.test-3.4\"] + list(args))", "def test(ctx, all=False, verbose=False): \"\"\"Run the tests.\"\"\" cmd = 'tox' if all else 'py.test' if verbose: cmd += ' -v' return ctx.run(cmd, pty=True).return_code", "def main(argv=sys.argv, stream=sys.stderr): \"\"\"Entry point for ``tappy`` command.\"\"\" args = parse_args(argv) suite = build_suite(args) runner = unittest.TextTestRunner(verbosity=args.verbose, stream=stream) result = runner.run(suite) return get_status(result)"], "rank": 1}
{"nl": "multiline text send message python", "code": ["async def _send_plain_text(self, request: Request, stack: Stack): \"\"\" Sends plain text using `_send_text()`. \"\"\" await self._send_text(request, stack, None)", "def send_text(self, text): \"\"\"Send a plain text message to the room.\"\"\" return self.client.api.send_message(self.room_id, text)", "async def send_message(): \"\"\"Example of sending a message.\"\"\" jar = aiohttp.CookieJar(unsafe=True) websession = aiohttp.ClientSession(cookie_jar=jar) modem = eternalegypt.Modem(hostname=sys.argv[1], websession=websession) await modem.login(password=sys.argv[2]) await modem.sms(phone=sys.argv[3], message=sys.argv[4]) await modem.logout() await websession.close()", "async def repeat(ctx, times: int, content='repeating...'): \"\"\"Repeats a message multiple times.\"\"\" for i in range(times): await ctx.send(content)", "def println(msg): \"\"\" Convenience function to print messages on a single line in the terminal \"\"\" sys.stdout.write(msg) sys.stdout.flush() sys.stdout.write('\\x08' * len(msg)) sys.stdout.flush()"], "rank": 1}
{"nl": "extract integers from string in python", "code": ["def get_numbers(s): \"\"\"Extracts all integers from a string an return them in a list\"\"\" result = map(int, re.findall(r'[0-9]+', unicode(s))) return result + [1] * (2 - len(result))", "def str2int(string_with_int): \"\"\" Collect digits from a string \"\"\" return int(\"\".join([char for char in string_with_int if char in string.digits]) or 0)", "def try_cast_int(s): \"\"\"(str) -> int All the digits in a given string are concatenated and converted into a single number. \"\"\" try: temp = re.findall('\\d', str(s)) temp = ''.join(temp) return int(temp) except: return s", "def prsint(string): \"\"\" Parse a string as an integer, encapsulating error handling. http://naif.jpl.nasa.gov/pub/naif/toolkit_docs/C/cspice/prsint_c.html :param string: String representing an integer. :type string: str :return: Integer value obtained by parsing string. :rtype: int \"\"\" string = stypes.stringToCharP(string) intval = ctypes.c_int() libspice.prsint_c(string, ctypes.byref(intval)) return intval.value", "def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)"], "rank": 1}
{"nl": "python take a string after the title", "code": ["def get_title(soup): \"\"\"Given a soup, pick out a title\"\"\" if soup.title: return soup.title.string if soup.h1: return soup.h1.string return ''", "def titleize(text): \"\"\"Capitalizes all the words and replaces some characters in the string to create a nicer looking title. \"\"\" if len(text) == 0: # if empty string, return it return text else: text = text.lower() # lower all char # delete redundant empty space chunks = [chunk[0].upper() + chunk[1:] for chunk in text.split(\" \") if len(chunk) >= 1] return \" \".join(chunks)", "def guess_title(basename): \"\"\" Attempt to guess the title from the filename \"\"\" base, _ = os.path.splitext(basename) return re.sub(r'[ _-]+', r' ', base).title()", "def _format_title_string(self, title_string): \"\"\" format mpv's title \"\"\" return self._title_string_format_text_tag(title_string.replace(self.icy_tokkens[0], self.icy_title_prefix))", "def fix_title_capitalization(title): \"\"\"Try to capitalize properly a title string.\"\"\" if re.search(\"[A-Z]\", title) and re.search(\"[a-z]\", title): return title word_list = re.split(' +', title) final = [word_list[0].capitalize()] for word in word_list[1:]: if word.upper() in COMMON_ACRONYMS: final.append(word.upper()) elif len(word) > 3: final.append(word.capitalize()) else: final.append(word.lower()) return \" \".join(final)"], "rank": 4}
{"nl": "how to randomly select rows in ndarray in python", "code": ["def downsample(array, k): \"\"\"Choose k random elements of array.\"\"\" length = array.shape[0] indices = random.sample(xrange(length), k) return array[indices]", "def bootstrap_indexes(data, n_samples=10000): \"\"\" Given data points data, where axis 0 is considered to delineate points, return an generator for sets of bootstrap indexes. This can be used as a list of bootstrap indexes (with list(bootstrap_indexes(data))) as well. \"\"\" for _ in xrange(n_samples): yield randint(data.shape[0], size=(data.shape[0],))", "def _unique_rows_numpy(a): \"\"\"return unique rows\"\"\" a = np.ascontiguousarray(a) unique_a = np.unique(a.view([('', a.dtype)] * a.shape[1])) return unique_a.view(a.dtype).reshape((unique_a.shape[0], a.shape[1]))", "def extract(self): \"\"\" Creates a copy of this tabarray in the form of a numpy ndarray. Useful if you want to do math on array elements, e.g. if you have a subset of the columns that are all numerical, you can construct a numerical matrix and do matrix operations. \"\"\" return np.vstack([self[r] for r in self.dtype.names]).T.squeeze()", "def sp_rand(m,n,a): \"\"\" Generates an mxn sparse 'd' matrix with round(a*m*n) nonzeros. \"\"\" if m == 0 or n == 0: return spmatrix([], [], [], (m,n)) nnz = min(max(0, int(round(a*m*n))), m*n) nz = matrix(random.sample(range(m*n), nnz), tc='i') return spmatrix(normal(nnz,1), nz%m, matrix([int(ii) for ii in nz/m]), (m,n))"], "rank": 1}
{"nl": "how to open a file with a path in python", "code": ["def fopenat(base_fd, path): \"\"\" Does openat read-only, then does fdopen to get a file object \"\"\" return os.fdopen(openat(base_fd, path, os.O_RDONLY), 'rb')", "def get_file_string(filepath): \"\"\"Get string from file.\"\"\" with open(os.path.abspath(filepath)) as f: return f.read()", "def read(fname): \"\"\"Quick way to read a file content.\"\"\" content = None with open(os.path.join(here, fname)) as f: content = f.read() return content", "def read(*args): \"\"\"Reads complete file contents.\"\"\" return io.open(os.path.join(HERE, *args), encoding=\"utf-8\").read()", "def open_file(file, mode): \"\"\"Open a file. :arg file: file-like or path-like object. :arg str mode: ``mode`` argument for :func:`open`. \"\"\" if hasattr(file, \"read\"): return file if hasattr(file, \"open\"): return file.open(mode) return open(file, mode)"], "rank": 2}
{"nl": "python count distance between two vectors", "code": ["def distance(vec1, vec2): \"\"\"Calculate the distance between two Vectors\"\"\" if isinstance(vec1, Vector2) \\ and isinstance(vec2, Vector2): dist_vec = vec2 - vec1 return dist_vec.length() else: raise TypeError(\"vec1 and vec2 must be Vector2's\")", "def vector_distance(a, b): \"\"\"The Euclidean distance between two vectors.\"\"\" a = np.array(a) b = np.array(b) return np.linalg.norm(a - b)", "def dist(x1, x2, axis=0): \"\"\"Return the distance between two points. Set axis=1 if x1 is a vector and x2 a matrix to get a vector of distances. \"\"\" return np.linalg.norm(x2 - x1, axis=axis)", "def _euclidean_dist(vector_a, vector_b): \"\"\" :param vector_a: A list of numbers. :param vector_b: A list of numbers. :returns: The euclidean distance between the two vectors. \"\"\" dist = 0 for (x, y) in zip(vector_a, vector_b): dist += (x-y)*(x-y) return math.sqrt(dist)", "def angle(vec1, vec2): \"\"\"Returns the angle between two vectors\"\"\" dot_vec = dot(vec1, vec2) mag1 = vec1.length() mag2 = vec2.length() result = dot_vec / (mag1 * mag2) return math.acos(result)"], "rank": 1}
{"nl": "python get dimensions of list", "code": ["def get_list_dimensions(_list): \"\"\" Takes a nested list and returns the size of each dimension followed by the element type in the list \"\"\" if isinstance(_list, list) or isinstance(_list, tuple): return [len(_list)] + get_list_dimensions(_list[0]) return []", "def array_dim(arr): \"\"\"Return the size of a multidimansional array. \"\"\" dim = [] while True: try: dim.append(len(arr)) arr = arr[0] except TypeError: return dim", "def get_dimension_array(array): \"\"\" Get dimension of an array getting the number of rows and the max num of columns. \"\"\" if all(isinstance(el, list) for el in array): result = [len(array), len(max([x for x in array], key=len,))] # elif array and isinstance(array, list): else: result = [len(array), 1] return result", "def get_shape(self): \"\"\" Return a tuple of this array's dimensions. This is done by querying the Dim children. Note that once it has been created, it is also possible to examine an Array object's .array attribute directly, and doing that is much faster. \"\"\" return tuple(int(c.pcdata) for c in self.getElementsByTagName(ligolw.Dim.tagName))[::-1]", "def shape(self): \"\"\"Compute the shape of the dataset as (rows, cols).\"\"\" if not self.data: return (0, 0) return (len(self.data), len(self.dimensions))"], "rank": 3}
{"nl": "python filter lowpass minmum cutoff frequency", "code": ["def highpass(cutoff): \"\"\" This strategy uses an exponential approximation for cut-off frequency calculation, found by matching the one-pole Laplace lowpass filter and mirroring the resulting filter to get a highpass. \"\"\" R = thub(exp(cutoff - pi), 2) return (1 - R) / (1 + R * z ** -1)", "def fft_bandpassfilter(data, fs, lowcut, highcut): \"\"\" http://www.swharden.com/blog/2009-01-21-signal-filtering-with-python/#comment-16801 \"\"\" fft = np.fft.fft(data) # n = len(data) # timestep = 1.0 / fs # freq = np.fft.fftfreq(n, d=timestep) bp = fft.copy() # Zero out fft coefficients # bp[10:-10] = 0 # Normalise # bp *= real(fft.dot(fft))/real(bp.dot(bp)) bp *= fft.dot(fft) / bp.dot(bp) # must multipy by 2 to get the correct amplitude ibp = 12 * np.fft.ifft(bp) return ibp", "def clip_image(image, clip_min, clip_max): \"\"\" Clip an image, or an image batch, with upper and lower threshold. \"\"\" return np.minimum(np.maximum(clip_min, image), clip_max)", "def lowPass(self, *args): \"\"\" Creates a copy of the signal with the low pass applied, args specifed are passed through to _butter. :return: \"\"\" return Signal(self._butter(self.samples, 'low', *args), fs=self.fs)", "def local_minima(img, min_distance = 4): r\"\"\" Returns all local minima from an image. Parameters ---------- img : array_like The image. min_distance : integer The minimal distance between the minimas in voxels. If it is less, only the lower minima is returned. Returns ------- indices : sequence List of all minima indices. values : sequence List of all minima values. \"\"\" # @TODO: Write a unittest for this. fits = numpy.asarray(img) minfits = minimum_filter(fits, size=min_distance) # default mode is reflect minima_mask = fits == minfits good_indices = numpy.transpose(minima_mask.nonzero()) good_fits = fits[minima_mask] order = good_fits.argsort() return good_indices[order], good_fits[order]"], "rank": 2}
{"nl": "python 3, seperate a string into a list at comma", "code": ["def split_elements(value): \"\"\"Split a string with comma or space-separated elements into a list.\"\"\" l = [v.strip() for v in value.split(',')] if len(l) == 1: l = value.split() return l", "def _str_to_list(s): \"\"\"Converts a comma separated string to a list\"\"\" _list = s.split(\",\") return list(map(lambda i: i.lstrip(), _list))", "def string_to_list(string, sep=\",\", filter_empty=False): \"\"\"Transforma una string con elementos separados por `sep` en una lista.\"\"\" return [value.strip() for value in string.split(sep) if (not filter_empty or value)]", "def comma_delimited_to_list(list_param): \"\"\"Convert comma-delimited list / string into a list of strings :param list_param: Comma-delimited string :type list_param: str | unicode :return: A list of strings :rtype: list \"\"\" if isinstance(list_param, list): return list_param if isinstance(list_param, str): return list_param.split(',') else: return []", "def split_comma_argument(comma_sep_str): \"\"\"Split a comma separated option into a list.\"\"\" terms = [] for term in comma_sep_str.split(','): if term: terms.append(term) return terms"], "rank": 4}
{"nl": "how to delete an element in a python dictionary", "code": ["def __delitem__ (self, key): \"\"\"Remove key from dict.\"\"\" self._keys.remove(key) super(ListDict, self).__delitem__(key)", "def dictlist_wipe_key(dict_list: Iterable[Dict], key: str) -> None: \"\"\" Process an iterable of dictionaries. For each dictionary ``d``, delete ``d[key]`` if it exists. \"\"\" for d in dict_list: d.pop(key, None)", "def pop (self, key): \"\"\"Remove key from dict and return value.\"\"\" if key in self._keys: self._keys.remove(key) super(ListDict, self).pop(key)", "def remove(parent, idx): \"\"\"Remove a value from a dict.\"\"\" if isinstance(parent, dict): del parent[idx] elif isinstance(parent, list): del parent[int(idx)] else: raise JSONPathError(\"Invalid path for operation\")", "def __delitem__(self, key): \"\"\"Remove item with given key from the mapping. Runs in O(n), unless removing last item, then in O(1). \"\"\" index, value = self._dict.pop(key) key2, value2 = self._list.pop(index) assert key == key2 assert value is value2 self._fix_indices_after_delete(index)"], "rank": 2}
{"nl": "how know if the box was selected in checkbox in python", "code": ["def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setCheckState(Qt.Checked) else: self.setCheckState(Qt.Unchecked)", "def set_value(self, value): \"\"\"Set value of the checkbox. Parameters ---------- value : bool value for the checkbox \"\"\" if value: self.setChecked(Qt.Checked) else: self.setChecked(Qt.Unchecked)", "def check_by_selector(self, selector): \"\"\"Check the checkbox matching the CSS selector.\"\"\" elem = find_element_by_jquery(world.browser, selector) if not elem.is_selected(): elem.click()", "def checkbox_uncheck(self, force_check=False): \"\"\" Wrapper to uncheck a checkbox \"\"\" if self.get_attribute('checked'): self.click(force_click=force_check)", "def uncheck(self, locator=None, allow_label_click=None, **kwargs): \"\"\" Find a check box and uncheck it. The check box can be found via name, id, or label text. :: page.uncheck(\"German\") Args: locator (str, optional): Which check box to uncheck. allow_label_click (bool, optional): Attempt to click the label to toggle state if element is non-visible. Defaults to :data:`capybara.automatic_label_click`. **kwargs: Arbitrary keyword arguments for :class:`SelectorQuery`. \"\"\" self._check_with_label( \"checkbox\", False, locator=locator, allow_label_click=allow_label_click, **kwargs)"], "rank": 2}
{"nl": "standard scalar function in python", "code": ["def Softsign(a): \"\"\" Softsign op. \"\"\" return np.divide(a, np.add(np.abs(a), 1)),", "def is_scalar(value): \"\"\"Test if the given value is a scalar. This function also works with memory mapped array values, in contrast to the numpy is_scalar method. Args: value: the value to test for being a scalar value Returns: boolean: if the given value is a scalar or not \"\"\" return np.isscalar(value) or (isinstance(value, np.ndarray) and (len(np.squeeze(value).shape) == 0))", "def get_scalar_product(self, other): \"\"\"Returns the scalar product of this vector with the given other vector.\"\"\" return self.x*other.x+self.y*other.y", "def mul(a, b): \"\"\" A wrapper around tf multiplication that does more automatic casting of the input. \"\"\" def multiply(a, b): \"\"\"Multiplication\"\"\" return a * b return op_with_scalar_cast(a, b, multiply)", "def asin(x): \"\"\" Inverse sine \"\"\" if isinstance(x, UncertainFunction): mcpts = np.arcsin(x._mcpts) return UncertainFunction(mcpts) else: return np.arcsin(x)"], "rank": 1}
{"nl": "python random gaussian distribution noise", "code": ["def _gauss(mean: int, sigma: int) -> int: \"\"\" Creates a variation from a base value Args: mean: base value sigma: gaussian sigma Returns: random value \"\"\" return int(random.gauss(mean, sigma))", "def rnormal(mu, tau, size=None): \"\"\" Random normal variates. \"\"\" return np.random.normal(mu, 1. / np.sqrt(tau), size)", "def block(seed): \"\"\" Return block of normal random numbers Parameters ---------- seed : {None, int} The seed to generate the noise.sd Returns -------- noise : numpy.ndarray Array of random numbers \"\"\" num = SAMPLE_RATE * BLOCK_SIZE rng = RandomState(seed % 2**32) variance = SAMPLE_RATE / 2 return rng.normal(size=num, scale=variance**0.5)", "def sample_normal(mean, var, rng): \"\"\"Sample from independent normal distributions Each element is an independent normal distribution. Parameters ---------- mean : numpy.ndarray Means of the normal distribution. Shape --> (batch_num, sample_dim) var : numpy.ndarray Variance of the normal distribution. Shape --> (batch_num, sample_dim) rng : numpy.random.RandomState Returns ------- ret : numpy.ndarray The sampling result. Shape --> (batch_num, sample_dim) \"\"\" ret = numpy.sqrt(var) * rng.randn(*mean.shape) + mean return ret", "def get_2D_samples_gauss(n, m, sigma, random_state=None): \"\"\" Deprecated see make_2D_samples_gauss \"\"\" return make_2D_samples_gauss(n, m, sigma, random_state=None)"], "rank": 10}
{"nl": "return the number of numeric attributes in python", "code": ["def __len__(self): \"\"\"Get a list of the public data attributes.\"\"\" return len([i for i in (set(dir(self)) - self._STANDARD_ATTRS) if i[0] != '_'])", "def __len__(self): \"\"\" This will equal 124 for the V1 database. \"\"\" length = 0 for typ, siz, _ in self.format: length += siz return length", "def objectcount(data, key): \"\"\"return the count of objects of key\"\"\" objkey = key.upper() return len(data.dt[objkey])", "def get_size(objects): \"\"\"Compute the total size of all elements in objects.\"\"\" res = 0 for o in objects: try: res += _getsizeof(o) except AttributeError: print(\"IGNORING: type=%s; o=%s\" % (str(type(o)), str(o))) return res", "def size(self): \"\"\"The size of this parameter, equivalent to self.value.size\"\"\" return np.multiply.reduce(self.shape, dtype=np.int32)"], "rank": 1}
{"nl": "python pil camera capture", "code": ["def read(self): \"\"\"https://picamera.readthedocs.io/en/release-1.13/recipes1.html#capturing-to-a-pil-image\"\"\" stream = BytesIO() self.cam.capture(stream, format='png') # \"Rewind\" the stream to the beginning so we can read its content stream.seek(0) return Image.open(stream)", "def match_aspect_to_viewport(self): \"\"\"Updates Camera.aspect to match the viewport's aspect ratio.\"\"\" viewport = self.viewport self.aspect = float(viewport.width) / viewport.height", "def screen_cv2(self): \"\"\"cv2 Image of current window screen\"\"\" pil_image = self.screen.convert('RGB') cv2_image = np.array(pil_image) pil_image.close() # Convert RGB to BGR cv2_image = cv2_image[:, :, ::-1] return cv2_image", "def viewport_to_screen_space(framebuffer_size: vec2, point: vec4) -> vec2: \"\"\"Transform point in viewport space to screen space.\"\"\" return (framebuffer_size * point.xy) / point.w", "def vec(self): \"\"\":obj:`numpy.ndarray` : Vector representation for this camera. \"\"\" return np.r_[self.fx, self.fy, self.cx, self.cy, self.skew, self.height, self.width]"], "rank": 1}
{"nl": "python flask create cookie expiration", "code": ["def save_config_value(request, response, key, value): \"\"\"Sets value of key `key` to `value` in both session and cookies.\"\"\" request.session[key] = value response.set_cookie(key, value, expires=one_year_from_now()) return response", "def __set_token_expired(self, value): \"\"\"Internal helper for oauth code\"\"\" self._token_expired = datetime.datetime.now() + datetime.timedelta(seconds=value) return", "def _save_cookies(requests_cookiejar, filename): \"\"\"Save cookies to a file.\"\"\" with open(filename, 'wb') as handle: pickle.dump(requests_cookiejar, handle)", "def logout(cache): \"\"\" Logs out the current session by removing it from the cache. This is expected to only occur when a session has \"\"\" cache.set(flask.session['auth0_key'], None) flask.session.clear() return True", "def setup_cache(app: Flask, cache_config) -> Optional[Cache]: \"\"\"Setup the flask-cache on a flask app\"\"\" if cache_config and cache_config.get('CACHE_TYPE') != 'null': return Cache(app, config=cache_config) return None"], "rank": 4}
{"nl": "python sqlalchemy model *", "code": ["def createdb(): \"\"\"Create database tables from sqlalchemy models\"\"\" manager.db.engine.echo = True manager.db.create_all() set_alembic_revision()", "def _store_helper(model: Action, session: Optional[Session] = None) -> None: \"\"\"Help store an action.\"\"\" if session is None: session = _make_session() session.add(model) session.commit() session.close()", "def compile(expr, params=None): \"\"\" Force compilation of expression for the SQLite target \"\"\" from ibis.sql.alchemy import to_sqlalchemy return to_sqlalchemy(expr, dialect.make_context(params=params))", "def handle_m2m_user(self, sender, instance, **kwargs): \"\"\" Handle many to many relationships for user field \"\"\" self.handle_save(instance.user.__class__, instance.user)", "def create_db(app, appbuilder): \"\"\" Create all your database objects (SQLAlchemy specific). \"\"\" from flask_appbuilder.models.sqla import Base _appbuilder = import_application(app, appbuilder) engine = _appbuilder.get_session.get_bind(mapper=None, clause=None) Base.metadata.create_all(engine) click.echo(click.style(\"DB objects created\", fg=\"green\"))"], "rank": 8}
{"nl": "python how to move to next command in for loop", "code": ["def do_next(self, args): \"\"\"Step over the next statement \"\"\" self._do_print_from_last_cmd = True self._interp.step_over() return True", "def advance_one_line(self): \"\"\"Advances to next line.\"\"\" current_line = self._current_token.line_number while current_line == self._current_token.line_number: self._current_token = ConfigParser.Token(*next(self._token_generator))", "def step_next_line(self): \"\"\"Sets cursor as beginning of next line.\"\"\" self._eol.append(self.position) self._lineno += 1 self._col_offset = 0", "def _get_history_next(self): \"\"\" callback function for key down \"\"\" if self._has_history: ret = self._input_history.return_history(1) self.string = ret self._curs_pos = len(ret)", "def do_rewind(self, line): \"\"\" rewind \"\"\" self.print_response(\"Rewinding from frame %s to 0\" % self.bot._frame) self.bot._frame = 0"], "rank": 1}
{"nl": "python check if object is a char", "code": ["def is_text(obj, name=None): \"\"\" returns True if object is text-like \"\"\" try: # python2 ans = isinstance(obj, basestring) except NameError: # python3 ans = isinstance(obj, str) if name: print(\"is_text: (%s) %s = %s\" % (ans, name, obj.__class__), file=sys.stderr) return ans", "def isstring(value): \"\"\"Report whether the given value is a byte or unicode string.\"\"\" classes = (str, bytes) if pyutils.PY3 else basestring # noqa: F821 return isinstance(value, classes)", "def is_string(obj): \"\"\"Is this a string. :param object obj: :rtype: bool \"\"\" if PYTHON3: str_type = (bytes, str) else: str_type = (bytes, str, unicode) return isinstance(obj, str_type)", "def is_unicode(string): \"\"\"Validates that the object itself is some kinda string\"\"\" str_type = str(type(string)) if str_type.find('str') > 0 or str_type.find('unicode') > 0: return True return False", "def is_sequence(obj): \"\"\"Check if `obj` is a sequence, but not a string or bytes.\"\"\" return isinstance(obj, Sequence) and not ( isinstance(obj, str) or BinaryClass.is_valid_type(obj))"], "rank": 3}
{"nl": "how to achieve logarithmic complexity in python", "code": ["def log(x): \"\"\" Natural logarithm \"\"\" if isinstance(x, UncertainFunction): mcpts = np.log(x._mcpts) return UncertainFunction(mcpts) else: return np.log(x)", "def glog(x,l = 2): \"\"\" Generalised logarithm :param x: number :param p: number added befor logarithm \"\"\" return np.log((x+np.sqrt(x**2+l**2))/2)/np.log(l)", "def logx_linear(x, a, b): \"\"\"logx linear Parameters ---------- x: int a: float b: float Returns ------- float a * np.log(x) + b \"\"\" x = np.log(x) return a*x + b", "def _protected_log(x1): \"\"\"Closure of log for zero arguments.\"\"\" with np.errstate(divide='ignore', invalid='ignore'): return np.where(np.abs(x1) > 0.001, np.log(np.abs(x1)), 0.)", "def filter_symlog(y, base=10.0): \"\"\"Symmetrical logarithmic scale. Optional arguments: *base*: The base of the logarithm. \"\"\" log_base = np.log(base) sign = np.sign(y) logs = np.log(np.abs(y) / log_base) return sign * logs"], "rank": 7}
{"nl": "python listbox scrollbar not tk", "code": ["def __init__(self, master=None, compound=tk.RIGHT, autohidescrollbar=True, **kwargs): \"\"\" Create a Listbox with a vertical scrollbar. :param master: master widget :type master: widget :param compound: side for the Scrollbar to be on (:obj:`tk.LEFT` or :obj:`tk.RIGHT`) :type compound: str :param autohidescrollbar: whether to use an :class:`~ttkwidgets.AutoHideScrollbar` or a :class:`ttk.Scrollbar` :type autohidescrollbar: bool :param kwargs: keyword arguments passed on to the :class:`tk.Listbox` initializer \"\"\" ttk.Frame.__init__(self, master) self.columnconfigure(1, weight=1) self.rowconfigure(0, weight=1) self.listbox = tk.Listbox(self, **kwargs) if autohidescrollbar: self.scrollbar = AutoHideScrollbar(self, orient=tk.VERTICAL, command=self.listbox.yview) else: self.scrollbar = ttk.Scrollbar(self, orient=tk.VERTICAL, command=self.listbox.yview) self.config_listbox(yscrollcommand=self.scrollbar.set) if compound is not tk.LEFT and compound is not tk.RIGHT: raise ValueError(\"Invalid compound value passed: {0}\".format(compound)) self.__compound = compound self._grid_widgets()", "def _grid_widgets(self): \"\"\"Puts the two whole widgets in the correct position depending on compound.\"\"\" scrollbar_column = 0 if self.__compound is tk.LEFT else 2 self.listbox.grid(row=0, column=1, sticky=\"nswe\") self.scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")", "def __grid_widgets(self): \"\"\"Places all the child widgets in the appropriate positions.\"\"\" scrollbar_column = 0 if self.__compound is tk.LEFT else 2 self._canvas.grid(row=0, column=1, sticky=\"nswe\") self._scrollbar.grid(row=0, column=scrollbar_column, sticky=\"ns\")", "def set_scrollbars_cb(self, w, tf): \"\"\"This callback is invoked when the user checks the 'Use Scrollbars' box in the preferences pane.\"\"\" scrollbars = 'on' if tf else 'off' self.t_.set(scrollbars=scrollbars)", "def yview(self, *args): \"\"\"Update inplace widgets position when doing vertical scroll\"\"\" self.after_idle(self.__updateWnds) ttk.Treeview.yview(self, *args)"], "rank": 1}
{"nl": "identify the most common number in an array python", "code": ["def _most_common(iterable): \"\"\"Returns the most common element in `iterable`.\"\"\" data = Counter(iterable) return max(data, key=data.__getitem__)", "def _gcd_array(X): \"\"\" Return the largest real value h such that all elements in x are integer multiples of h. \"\"\" greatest_common_divisor = 0.0 for x in X: greatest_common_divisor = _gcd(greatest_common_divisor, x) return greatest_common_divisor", "def most_common(items): \"\"\" Wanted functionality from Counters (new in Python 2.7). \"\"\" counts = {} for i in items: counts.setdefault(i, 0) counts[i] += 1 return max(six.iteritems(counts), key=operator.itemgetter(1))", "def find_commons(lists): \"\"\"Finds common values :param lists: List of lists :return: List of values that are in common between inner lists \"\"\" others = lists[1:] return [ val for val in lists[0] if is_in_all(val, others) ]", "def most_significant_bit(lst: np.ndarray) -> int: \"\"\" A helper function that finds the position of the most significant bit in a 1darray of 1s and 0s, i.e. the first position where a 1 appears, reading left to right. :param lst: a 1d array of 0s and 1s with at least one 1 :return: the first position in lst that a 1 appears \"\"\" return np.argwhere(np.asarray(lst) == 1)[0][0]"], "rank": 5}
{"nl": "pull multiple values to make table python", "code": ["def from_df(data_frame): \"\"\"Parses data and builds an instance of this class :param data_frame: pandas DataFrame :return: SqlTable \"\"\" labels = data_frame.keys().tolist() data = data_frame.values.tolist() return SqlTable(labels, data, \"{:.3f}\", \"\\n\")", "def make_table_map(table, headers): \"\"\"Create a function to map from rows with the structure of the headers to the structure of the table.\"\"\" header_parts = {} for i, h in enumerate(headers): header_parts[h] = 'row[{}]'.format(i) body_code = 'lambda row: [{}]'.format(','.join(header_parts.get(c.name, 'None') for c in table.columns)) header_code = 'lambda row: [{}]'.format( ','.join(header_parts.get(c.name, \"'{}'\".format(c.name)) for c in table.columns)) return eval(header_code), eval(body_code)", "def filter_bolts(table, header): \"\"\" filter to keep bolts \"\"\" bolts_info = [] for row in table: if row[0] == 'bolt': bolts_info.append(row) return bolts_info, header", "def adapter(data, headers, **kwargs): \"\"\"Wrap vertical table in a function for TabularOutputFormatter.\"\"\" keys = ('sep_title', 'sep_character', 'sep_length') return vertical_table(data, headers, **filter_dict_by_key(kwargs, keys))", "def update_table_row(self, table, row_idx): \"\"\"Add this instance as a row on a `astropy.table.Table` \"\"\" try: table[row_idx]['timestamp'] = self.timestamp table[row_idx]['status'] = self.status except IndexError: print(\"Index error\", len(table), row_idx)"], "rank": 40}
{"nl": "python adjust data to normal distribution", "code": ["def normalize(data): \"\"\"Normalize the data to be in the [0, 1] range. :param data: :return: normalized data \"\"\" out_data = data.copy() for i, sample in enumerate(out_data): out_data[i] /= sum(out_data[i]) return out_data", "def normalize(df, style = 'mean'): \"\"\" Returns a normalized version of a DataFrame or Series Parameters: df - DataFrame or Series The data to normalize style - function or string, default 'mean' The style to use when computing the norms. Takes 'mean' or 'minmax' to do mean or min-max normalization respectively. User-defined functions that take a pandas Series as input and return a normalized pandas Series are also accepted \"\"\" if style == 'mean': df_mean,df_std = df.mean(),df.std() return (df-df_mean)/df_std elif style == 'minmax': col_min,col_max = df.min(),df.max() return (df-col_min)/(col_max-col_min) else: return style(df)", "def normalize(data): \"\"\" Function to normalize data to have mean 0 and unity standard deviation (also called z-transform) Parameters ---------- data : numpy.ndarray Returns ------- numpy.ndarray z-transform of input array \"\"\" data = data.astype(float) data -= data.mean() return data / data.std()", "def normalize(self): \"\"\" Normalize data. \"\"\" if self.preprocessed_data.empty: data = self.original_data else: data = self.preprocessed_data data = pd.DataFrame(preprocessing.normalize(data), columns=data.columns, index=data.index) self.preprocessed_data = data", "def convert_column(self, values): \"\"\"Normalize values.\"\"\" assert all(values >= 0), 'Cannot normalize a column with negatives' total = sum(values) if total > 0: return values / total else: return values"], "rank": 1}
{"nl": "python check if float has no floating points", "code": ["def is_floating(self): \"\"\"Returns whether this is a (non-quantized, real) floating point type.\"\"\" return ( self.is_numpy_compatible and np.issubdtype(self.as_numpy_dtype, np.floating) ) or self.base_dtype == bfloat16", "def is_float(value): \"\"\"must be a float\"\"\" return isinstance(value, float) or isinstance(value, int) or isinstance(value, np.float64), float(value)", "def is_finite(value: Any) -> bool: \"\"\"Return true if a value is a finite number.\"\"\" return isinstance(value, int) or (isinstance(value, float) and isfinite(value))", "def is_real_floating_dtype(dtype): \"\"\"Return ``True`` if ``dtype`` is a real floating point type.\"\"\" dtype = np.dtype(dtype) return np.issubsctype(getattr(dtype, 'base', None), np.floating)", "def is_float_array(val): \"\"\" Checks whether a variable is a numpy float array. Parameters ---------- val The variable to check. Returns ------- bool True if the variable is a numpy float array. Otherwise False. \"\"\" return is_np_array(val) and issubclass(val.dtype.type, np.floating)"], "rank": 3}
{"nl": "string remove the last blank python", "code": ["def remove_blank_lines(string): \"\"\" Removes all blank lines in @string -> #str without blank lines \"\"\" return \"\\n\".join(line for line in string.split(\"\\n\") if len(line.strip()))", "def get_line_ending(line): \"\"\"Return line ending.\"\"\" non_whitespace_index = len(line.rstrip()) - len(line) if not non_whitespace_index: return '' else: return line[non_whitespace_index:]", "def _repr_strip(mystring): \"\"\" Returns the string without any initial or final quotes. \"\"\" r = repr(mystring) if r.startswith(\"'\") and r.endswith(\"'\"): return r[1:-1] else: return r", "def remove_leading_zeros(num: str) -> str: \"\"\" Strips zeros while handling -, M, and empty strings \"\"\" if not num: return num if num.startswith('M'): ret = 'M' + num[1:].lstrip('0') elif num.startswith('-'): ret = '-' + num[1:].lstrip('0') else: ret = num.lstrip('0') return '0' if ret in ('', 'M', '-') else ret", "def uncomment_line(line, prefix): \"\"\"Remove prefix (and space) from line\"\"\" if not prefix: return line if line.startswith(prefix + ' '): return line[len(prefix) + 1:] if line.startswith(prefix): return line[len(prefix):] return line"], "rank": 1}
{"nl": "python how to display object attributes", "code": ["def _repr(obj): \"\"\"Show the received object as precise as possible.\"\"\" vals = \", \".join(\"{}={!r}\".format( name, getattr(obj, name)) for name in obj._attribs) if vals: t = \"{}(name={}, {})\".format(obj.__class__.__name__, obj.name, vals) else: t = \"{}(name={})\".format(obj.__class__.__name__, obj.name) return t", "def format_repr(obj, attributes) -> str: \"\"\"Format an object's repr method with specific attributes.\"\"\" attribute_repr = ', '.join(('{}={}'.format(attr, repr(getattr(obj, attr))) for attr in attributes)) return \"{0}({1})\".format(obj.__class__.__qualname__, attribute_repr)", "def __repr__(self): \"\"\"Return string representation of object.\"\"\" return str(self.__class__) + '(' + ', '.join([list.__repr__(d) for d in self.data]) + ')'", "def prnt(self): \"\"\" Prints DB data representation of the object. \"\"\" print(\"= = = =\\n\\n%s object key: \\033[32m%s\\033[0m\" % (self.__class__.__name__, self.key)) pprnt(self._data or self.clean_value())", "def dict_from_object(obj: object): \"\"\"Convert a object into dictionary with all of its readable attributes.\"\"\" # If object is a dict instance, no need to convert. return (obj if isinstance(obj, dict) else {attr: getattr(obj, attr) for attr in dir(obj) if not attr.startswith('_')})"], "rank": 1}
{"nl": "determine if a list of numbers contains duplicates python", "code": ["def find_duplicates(l: list) -> set: \"\"\" Return the duplicates in a list. The function relies on https://stackoverflow.com/questions/9835762/find-and-list-duplicates-in-a-list . Parameters ---------- l : list Name Returns ------- set Duplicated values >>> find_duplicates([1,2,3]) set() >>> find_duplicates([1,2,1]) {1} \"\"\" return set([x for x in l if l.count(x) > 1])", "def remove_list_duplicates(lista, unique=False): \"\"\" Remove duplicated elements in a list. Args: lista: List with elements to clean duplicates. \"\"\" result = [] allready = [] for elem in lista: if elem not in result: result.append(elem) else: allready.append(elem) if unique: for elem in allready: result = list(filter((elem).__ne__, result)) return result", "def check_if_numbers_are_consecutive(list_): \"\"\" Returns True if numbers in the list are consecutive :param list_: list of integers :return: Boolean \"\"\" return all((True if second - first == 1 else False for first, second in zip(list_[:-1], list_[1:])))", "def remove_dups(seq): \"\"\"remove duplicates from a sequence, preserving order\"\"\" seen = set() seen_add = seen.add return [x for x in seq if not (x in seen or seen_add(x))]", "def purge_duplicates(list_in): \"\"\"Remove duplicates from list while preserving order. Parameters ---------- list_in: Iterable Returns ------- list List of first occurences in order \"\"\" _list = [] for item in list_in: if item not in _list: _list.append(item) return _list"], "rank": 1}
{"nl": "python figure add title label size", "code": ["def ylabelsize(self, size, index=1): \"\"\"Set the size of the label. Parameters ---------- size : int Returns ------- Chart \"\"\" self.layout['yaxis' + str(index)]['titlefont']['size'] = size return self", "def set_title(self, title, **kwargs): \"\"\"Sets the title on the underlying matplotlib AxesSubplot.\"\"\" ax = self.get_axes() ax.set_title(title, **kwargs)", "def legend_title_header_element(feature, parent): \"\"\"Retrieve legend title header string from definitions.\"\"\" _ = feature, parent # NOQA header = legend_title_header['string_format'] return header.capitalize()", "def add_xlabel(self, text=None): \"\"\" Add a label to the x-axis. \"\"\" x = self.fit.meta['independent'] if not text: text = '$' + x['tex_symbol'] + r'$ $(\\si{' + x['siunitx'] + r'})$' self.plt.set_xlabel(text)", "def _enter_plotting(self, fontsize=9): \"\"\"assumes that a figure is open \"\"\" # interactive_status = matplotlib.is_interactive() self.original_fontsize = pyplot.rcParams['font.size'] pyplot.rcParams['font.size'] = fontsize pyplot.hold(False) # opens a figure window, if non exists pyplot.ioff()"], "rank": 11}
{"nl": "python networkx longest path directed acyclic graph", "code": ["def dag_longest_path(graph, source, target): \"\"\" Finds the longest path in a dag between two nodes \"\"\" if source == target: return [source] allpaths = nx.all_simple_paths(graph, source, target) longest_path = [] for l in allpaths: if len(l) > len(longest_path): longest_path = l return longest_path", "def dfs_recursive(graph, node, seen): \"\"\"DFS, detect connected component, recursive implementation :param graph: directed graph in listlist or listdict format :param int node: to start graph exploration :param boolean-table seen: will be set true for the connected component containing node. :complexity: `O(|V|+|E|)` \"\"\" seen[node] = True for neighbor in graph[node]: if not seen[neighbor]: dfs_recursive(graph, neighbor, seen)", "def is_cyclic(graph): \"\"\" Return True if the directed graph g has a cycle. The directed graph should be represented as a dictionary mapping of edges for each node. \"\"\" path = set() def visit(vertex): path.add(vertex) for neighbour in graph.get(vertex, ()): if neighbour in path or visit(neighbour): return True path.remove(vertex) return False return any(visit(v) for v in graph)", "def _create_complete_graph(node_ids): \"\"\"Create a complete graph from the list of node ids. Args: node_ids: a list of node ids Returns: An undirected graph (as a networkx.Graph) \"\"\" g = nx.Graph() g.add_nodes_from(node_ids) for (i, j) in combinations(node_ids, 2): g.add_edge(i, j) return g", "def _dfs_cycle_detect(graph, node, path, visited_nodes): \"\"\" search graph for cycle using DFS continuing from node path contains the list of visited nodes currently on the stack visited_nodes is the set of already visited nodes :param graph: :param node: :param path: :param visited_nodes: :return: \"\"\" visited_nodes.add(node) for target in graph[node]: if target in path: # cycle found => return current path return path + [target] else: return _dfs_cycle_detect(graph, target, path + [target], visited_nodes) return None"], "rank": 1}
{"nl": "how to take list as input in python seperated with spaces", "code": ["def split_elements(value): \"\"\"Split a string with comma or space-separated elements into a list.\"\"\" l = [v.strip() for v in value.split(',')] if len(l) == 1: l = value.split() return l", "def split_strings_in_list_retain_spaces(orig_list): \"\"\" Function to split every line in a list, and retain spaces for a rejoin :param orig_list: Original list :return: A List with split lines \"\"\" temp_list = list() for line in orig_list: line_split = __re.split(r'(\\s+)', line) temp_list.append(line_split) return temp_list", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids", "def __normalize_list(self, msg): \"\"\"Split message to list by commas and trim whitespace.\"\"\" if isinstance(msg, list): msg = \"\".join(msg) return list(map(lambda x: x.strip(), msg.split(\",\")))", "def list_to_str(lst): \"\"\" Turn a list into a comma- and/or and-separated string. Parameters ---------- lst : :obj:`list` A list of strings to join into a single string. Returns ------- str_ : :obj:`str` A string with commas and/or ands separating th elements from ``lst``. \"\"\" if len(lst) == 1: str_ = lst[0] elif len(lst) == 2: str_ = ' and '.join(lst) elif len(lst) > 2: str_ = ', '.join(lst[:-1]) str_ += ', and {0}'.format(lst[-1]) else: raise ValueError('List of length 0 provided.') return str_"], "rank": 8}
{"nl": "is a list in python an array", "code": ["def is_listish(obj): \"\"\"Check if something quacks like a list.\"\"\" if isinstance(obj, (list, tuple, set)): return True return is_sequence(obj)", "def listify(a): \"\"\" Convert a scalar ``a`` to a list and all iterables to list as well. Examples -------- >>> listify(0) [0] >>> listify([1,2,3]) [1, 2, 3] >>> listify('a') ['a'] >>> listify(np.array([1,2,3])) [1, 2, 3] >>> listify('string') ['string'] \"\"\" if a is None: return [] elif not isinstance(a, (tuple, list, np.ndarray)): return [a] return list(a)", "def is_iterable(value): \"\"\"must be an iterable (list, array, tuple)\"\"\" return isinstance(value, np.ndarray) or isinstance(value, list) or isinstance(value, tuple), value", "def _to_array(value): \"\"\"As a convenience, turn Python lists and tuples into NumPy arrays.\"\"\" if isinstance(value, (tuple, list)): return array(value) elif isinstance(value, (float, int)): return np.float64(value) else: return value", "def is_int_vector(l): r\"\"\"Checks if l is a numpy array of integers \"\"\" if isinstance(l, np.ndarray): if l.ndim == 1 and (l.dtype.kind == 'i' or l.dtype.kind == 'u'): return True return False"], "rank": 34}
{"nl": "python cosine similarity of two vectors", "code": ["def similarity(self, other): \"\"\"Calculates the cosine similarity between this vector and another vector.\"\"\" if self.magnitude == 0 or other.magnitude == 0: return 0 return self.dot(other) / self.magnitude", "def _cosine(a, b): \"\"\" Return the len(a & b) / len(a) \"\"\" return 1. * len(a & b) / (math.sqrt(len(a)) * math.sqrt(len(b)))", "def similarity(word1: str, word2: str) -> float: \"\"\" Get cosine similarity between two words. If a word is not in the vocabulary, KeyError will be raised. :param string word1: first word :param string word2: second word :return: the cosine similarity between the two word vectors \"\"\" return _MODEL.similarity(word1, word2)", "def tanimoto_coefficient(a, b): \"\"\"Measured similarity between two points in a multi-dimensional space. Returns: 1.0 if the two points completely overlap, 0.0 if the two points are infinitely far apart. \"\"\" return sum(map(lambda (x,y): float(x)*float(y), zip(a,b))) / sum([ -sum(map(lambda (x,y): float(x)*float(y), zip(a,b))), sum(map(lambda x: float(x)**2, a)), sum(map(lambda x: float(x)**2, b))])", "def tanimoto_set_similarity(x: Iterable[X], y: Iterable[X]) -> float: \"\"\"Calculate the tanimoto set similarity.\"\"\" a, b = set(x), set(y) union = a | b if not union: return 0.0 return len(a & b) / len(union)"], "rank": 11}
{"nl": "how to model a sphere python", "code": ["def arcball_map_to_sphere(point, center, radius): \"\"\"Return unit sphere coordinates from window coordinates.\"\"\" v0 = (point[0] - center[0]) / radius v1 = (center[1] - point[1]) / radius n = v0*v0 + v1*v1 if n > 1.0: # position outside of sphere n = math.sqrt(n) return numpy.array([v0/n, v1/n, 0.0]) else: return numpy.array([v0, v1, math.sqrt(1.0 - n)])", "def Fsphere(q, R): \"\"\"Scattering form-factor amplitude of a sphere normalized to F(q=0)=V Inputs: ------- ``q``: independent variable ``R``: sphere radius Formula: -------- ``4*pi/q^3 * (sin(qR) - qR*cos(qR))`` \"\"\" return 4 * np.pi / q ** 3 * (np.sin(q * R) - q * R * np.cos(q * R))", "def surface(self, zdata, **kwargs): \"\"\"Show a 3D surface plot. Extra keyword arguments are passed to `SurfacePlot()`. Parameters ---------- zdata : array-like A 2D array of the surface Z values. \"\"\" self._configure_3d() surf = scene.SurfacePlot(z=zdata, **kwargs) self.view.add(surf) self.view.camera.set_range() return surf", "def volume(self): \"\"\" The volume of the primitive extrusion. Calculated from polygon and height to avoid mesh creation. Returns ---------- volume: float, volume of 3D extrusion \"\"\" volume = abs(self.primitive.polygon.area * self.primitive.height) return volume", "def gauss_box_model(x, amplitude=1.0, mean=0.0, stddev=1.0, hpix=0.5): \"\"\"Integrate a Gaussian profile.\"\"\" z = (x - mean) / stddev z2 = z + hpix / stddev z1 = z - hpix / stddev return amplitude * (norm.cdf(z2) - norm.cdf(z1))"], "rank": 2}
{"nl": "how to check paths in python", "code": ["def executable_exists(executable): \"\"\"Test if an executable is available on the system.\"\"\" for directory in os.getenv(\"PATH\").split(\":\"): if os.path.exists(os.path.join(directory, executable)): return True return False", "def is_executable(path): \"\"\"Returns whether a path names an existing executable file.\"\"\" return os.path.isfile(path) and os.access(path, os.X_OK)", "def is_exe(fpath): \"\"\" Path references an executable file. \"\"\" return os.path.isfile(fpath) and os.access(fpath, os.X_OK)", "def _file_exists(path, filename): \"\"\"Checks if the filename exists under the path.\"\"\" return os.path.isfile(os.path.join(path, filename))", "def is_valid_folder(parser, arg): \"\"\"Check if arg is a valid file that already exists on the file system.\"\"\" arg = os.path.abspath(arg) if not os.path.isdir(arg): parser.error(\"The folder %s does not exist!\" % arg) else: return arg"], "rank": 9}
{"nl": "python test truth value of list", "code": ["def assert_exactly_one_true(bool_list): \"\"\"This method asserts that only one value of the provided list is True. :param bool_list: List of booleans to check :return: True if only one value is True, False otherwise \"\"\" assert isinstance(bool_list, list) counter = 0 for item in bool_list: if item: counter += 1 return counter == 1", "def All(sequence): \"\"\" :param sequence: Any sequence whose elements can be evaluated as booleans. :returns: true if all elements of the sequence satisfy True and x. \"\"\" return bool(reduce(lambda x, y: x and y, sequence, True))", "def _check_elements_equal(lst): \"\"\" Returns true if all of the elements in the list are equal. \"\"\" assert isinstance(lst, list), \"Input value must be a list.\" return not lst or lst.count(lst[0]) == len(lst)", "def list_of(cls): \"\"\" Returns a function that checks that each element in a list is of a specific type. \"\"\" return lambda l: isinstance(l, list) and all(isinstance(x, cls) for x in l)", "def is_list_of_list(item): \"\"\" check whether the item is list (tuple) and consist of list (tuple) elements \"\"\" if ( type(item) in (list, tuple) and len(item) and isinstance(item[0], (list, tuple)) ): return True return False"], "rank": 1}
{"nl": "python string value of enum", "code": ["def from_string(cls, string): \"\"\" Simply logs a warning if the desired enum value is not found. :param string: :return: \"\"\" # find enum value for attr in dir(cls): value = getattr(cls, attr) if value == string: return value # if not found, log warning and return the value passed in logger.warning(\"{} is not a valid enum value for {}.\".format(string, cls.__name__)) return string", "def to_python(self, value): \"\"\" Convert a string from a form into an Enum value. \"\"\" if value is None: return value if isinstance(value, self.enum): return value return self.enum[value]", "def EnumValueName(self, enum, value): \"\"\"Returns the string name of an enum value. This is just a small helper method to simplify a common operation. Args: enum: string name of the Enum. value: int, value of the enum. Returns: string name of the enum value. Raises: KeyError if either the Enum doesn't exist or the value is not a valid value for the enum. \"\"\" return self.enum_types_by_name[enum].values_by_number[value].name", "def Value(self, name): \"\"\"Returns the value coresponding to the given enum name.\"\"\" if name in self._enum_type.values_by_name: return self._enum_type.values_by_name[name].number raise ValueError('Enum %s has no value defined for name %s' % ( self._enum_type.name, name))", "def get_enum_from_name(self, enum_name): \"\"\" Return an enum from a name Args: enum_name (str): name of the enum Returns: Enum \"\"\" return next((e for e in self.enums if e.name == enum_name), None)"], "rank": 3}
{"nl": "python create null pointer with ctypes", "code": ["def POINTER(obj): \"\"\" Create ctypes pointer to object. Notes ----- This function converts None to a real NULL pointer because of bug in how ctypes handles None on 64-bit platforms. \"\"\" p = ctypes.POINTER(obj) if not isinstance(p.from_param, classmethod): def from_param(cls, x): if x is None: return cls() else: return x p.from_param = classmethod(from_param) return p", "def pointer(self): \"\"\"Get a ctypes void pointer to the memory mapped region. :type: ctypes.c_void_p \"\"\" return ctypes.cast(ctypes.pointer(ctypes.c_uint8.from_buffer(self.mapping, 0)), ctypes.c_void_p)", "def cudaMalloc(count, ctype=None): \"\"\" Allocate device memory. Allocate memory on the device associated with the current active context. Parameters ---------- count : int Number of bytes of memory to allocate ctype : _ctypes.SimpleType, optional ctypes type to cast returned pointer. Returns ------- ptr : ctypes pointer Pointer to allocated device memory. \"\"\" ptr = ctypes.c_void_p() status = _libcudart.cudaMalloc(ctypes.byref(ptr), count) cudaCheckStatus(status) if ctype != None: ptr = ctypes.cast(ptr, ctypes.POINTER(ctype)) return ptr", "def to_bytes_or_none(value): \"\"\"Converts C char arrays to bytes and C NULL values to None.\"\"\" if value == ffi.NULL: return None elif isinstance(value, ffi.CData): return ffi.string(value) else: raise ValueError('Value must be char[] or NULL')", "def get_ctype(rtype, cfunc, *args): \"\"\" Call a C function that takes a pointer as its last argument and return the C object that it contains after the function has finished. :param rtype: C data type is filled by the function :param cfunc: C function to call :param args: Arguments to call function with :return: A pointer to the specified data type \"\"\" val_p = backend.ffi.new(rtype) args = args + (val_p,) cfunc(*args) return val_p[0]"], "rank": 1}
{"nl": "python md5 hash string", "code": ["def md5_string(s): \"\"\" Shortcut to create md5 hash :param s: :return: \"\"\" m = hashlib.md5() m.update(s) return str(m.hexdigest())", "def filehash(path): \"\"\"Make an MD5 hash of a file, ignoring any differences in line ending characters.\"\"\" with open(path, \"rU\") as f: return md5(py3compat.str_to_bytes(f.read())).hexdigest()", "def sha1(s): \"\"\" Returns a sha1 of the given string \"\"\" h = hashlib.new('sha1') h.update(s) return h.hexdigest()", "def get_file_md5sum(path): \"\"\"Calculate the MD5 hash for a file.\"\"\" with open(path, 'rb') as fh: h = str(hashlib.md5(fh.read()).hexdigest()) return h", "def _string_hash(s): \"\"\"String hash (djb2) with consistency between py2/py3 and persistency between runs (unlike `hash`).\"\"\" h = 5381 for c in s: h = h * 33 + ord(c) return h"], "rank": 1}
{"nl": "python how to match dictionarys", "code": ["def intersect(d1, d2): \"\"\"Intersect dictionaries d1 and d2 by key *and* value.\"\"\" return dict((k, d1[k]) for k in d1 if k in d2 and d1[k] == d2[k])", "def is_same_dict(d1, d2): \"\"\"Test two dictionary is equal on values. (ignore order) \"\"\" for k, v in d1.items(): if isinstance(v, dict): is_same_dict(v, d2[k]) else: assert d1[k] == d2[k] for k, v in d2.items(): if isinstance(v, dict): is_same_dict(v, d1[k]) else: assert d1[k] == d2[k]", "def compare_dict(da, db): \"\"\" Compare differencs from two dicts \"\"\" sa = set(da.items()) sb = set(db.items()) diff = sa & sb return dict(sa - diff), dict(sb - diff)", "def get_single_item(d): \"\"\"Get an item from a dict which contains just one item.\"\"\" assert len(d) == 1, 'Single-item dict must have just one item, not %d.' % len(d) return next(six.iteritems(d))", "def fuzzy_get_tuple(dict_obj, approximate_key, dict_keys=None, key_and_value=False, similarity=0.6, default=None): \"\"\"Find the closest matching key and/or value in a dictionary (must have all string keys!)\"\"\" return fuzzy_get(dict(('|'.join(str(k2) for k2 in k), v) for (k, v) in viewitems(dict_obj)), '|'.join(str(k) for k in approximate_key), dict_keys=dict_keys, key_and_value=key_and_value, similarity=similarity, default=default)"], "rank": 1}
{"nl": "select elements from a list, then delete these elements in the original list python", "code": ["def remove_elements(target, indices): \"\"\"Remove multiple elements from a list and return result. This implementation is faster than the alternative below. Also note the creation of a new list to avoid altering the original. We don't have any current use for the original intact list, but may in the future...\"\"\" copied = list(target) for index in reversed(indices): del copied[index] return copied", "def rm_empty_indices(*args): \"\"\" Remove unwanted list indices. First argument is the list of indices to remove. Other elements are the lists to trim. \"\"\" rm_inds = args[0] if not rm_inds: return args[1:] keep_inds = [i for i in range(len(args[1])) if i not in rm_inds] return [[a[i] for i in keep_inds] for a in args[1:]]", "def without(seq1, seq2): r\"\"\"Return a list with all elements in `seq2` removed from `seq1`, order preserved. Examples: >>> without([1,2,3,1,2], [1]) [2, 3, 2] \"\"\" if isSet(seq2): d2 = seq2 else: d2 = set(seq2) return [elt for elt in seq1 if elt not in d2]", "def distinct(l): \"\"\" Return a list where the duplicates have been removed. Args: l (list): the list to filter. Returns: list: the same list without duplicates. \"\"\" seen = set() seen_add = seen.add return (_ for _ in l if not (_ in seen or seen_add(_)))", "def filter_list_by_indices(lst, indices): \"\"\"Return a modified list containing only the indices indicated. Args: lst: Original list of values indices: List of indices to keep from the original list Returns: list: Filtered list of values \"\"\" return [x for i, x in enumerate(lst) if i in indices]"], "rank": 1}
{"nl": "pybool to c++ python 3", "code": ["def convertToBool(): \"\"\" Convert a byte value to boolean (0 or 1) if the global flag strictBool is True \"\"\" if not OPTIONS.strictBool.value: return [] REQUIRES.add('strictbool.asm') result = [] result.append('pop af') result.append('call __NORMALIZE_BOOLEAN') result.append('push af') return result", "def _encode_bool(name, value, dummy0, dummy1): \"\"\"Encode a python boolean (True/False).\"\"\" return b\"\\x08\" + name + (value and b\"\\x01\" or b\"\\x00\")", "def boolean(value): \"\"\" Configuration-friendly boolean type converter. Supports both boolean-valued and string-valued inputs (e.g. from env vars). \"\"\" if isinstance(value, bool): return value if value == \"\": return False return strtobool(value)", "def less_strict_bool(x): \"\"\"Idempotent and None-safe version of strict_bool.\"\"\" if x is None: return False elif x is True or x is False: return x else: return strict_bool(x)", "def process_bool_arg(arg): \"\"\" Determine True/False from argument \"\"\" if isinstance(arg, bool): return arg elif isinstance(arg, basestring): if arg.lower() in [\"true\", \"1\"]: return True elif arg.lower() in [\"false\", \"0\"]: return False"], "rank": 1}
{"nl": "how to add a ? in python url", "code": ["def strip_querystring(url): \"\"\"Remove the querystring from the end of a URL.\"\"\" p = six.moves.urllib.parse.urlparse(url) return p.scheme + \"://\" + p.netloc + p.path", "def append_query_parameter(url, parameters, ignore_if_exists=True): \"\"\" quick and dirty appending of query parameters to a url \"\"\" if ignore_if_exists: for key in parameters.keys(): if key + \"=\" in url: del parameters[key] parameters_str = \"&\".join(k + \"=\" + v for k, v in parameters.items()) append_token = \"&\" if \"?\" in url else \"?\" return url + append_token + parameters_str", "def create_search_url(self): \"\"\" Generates (urlencoded) query string from stored key-values tuples :returns: A string containing all arguments in a url-encoded format \"\"\" url = '?' for key, value in self.arguments.items(): url += '%s=%s&' % (quote_plus(key), quote_plus(value)) self.url = url[:-1] return self.url", "def add_params_to_url(url, params): \"\"\"Adds params to url :param url: Url :param params: Params to add :return: original url with new params \"\"\" url_parts = list(urlparse.urlparse(url)) # get url parts query = dict(urlparse.parse_qsl(url_parts[4])) # get url query query.update(params) # add new params url_parts[4] = urlencode(query) return urlparse.urlunparse(url_parts)", "def url_concat(url, args): \"\"\"Concatenate url and argument dictionary regardless of whether url has existing query parameters. >>> url_concat(\"http://example.com/foo?a=b\", dict(c=\"d\")) 'http://example.com/foo?a=b&c=d' \"\"\" if not args: return url if url[-1] not in ('?', '&'): url += '&' if ('?' in url) else '?' return url + urllib.urlencode(args)"], "rank": 2}
{"nl": "python how to check whether the process with pid exist", "code": ["def pid_exists(pid): \"\"\" Determines if a system process identifer exists in process table. \"\"\" try: os.kill(pid, 0) except OSError as exc: return exc.errno == errno.EPERM else: return True", "def is_running(process_id: int) -> bool: \"\"\" Uses the Unix ``ps`` program to see if a process is running. \"\"\" pstr = str(process_id) encoding = sys.getdefaultencoding() s = subprocess.Popen([\"ps\", \"-p\", pstr], stdout=subprocess.PIPE) for line in s.stdout: strline = line.decode(encoding) if pstr in strline: return True return False", "def get_pid_list(): \"\"\"Returns a list of PIDs currently running on the system.\"\"\" pids = [int(x) for x in os.listdir('/proc') if x.isdigit()] return pids", "def is_alive(self): \"\"\" @rtype: bool @return: C{True} if the process is currently running. \"\"\" try: self.wait(0) except WindowsError: e = sys.exc_info()[1] return e.winerror == win32.WAIT_TIMEOUT return False", "def kill_mprocess(process): \"\"\"kill process Args: process - Popen object for process \"\"\" if process and proc_alive(process): process.terminate() process.communicate() return not proc_alive(process)"], "rank": 1}
{"nl": "how to cehck if somethign is a constant python", "code": ["def is_static(*p): \"\"\" A static value (does not change at runtime) which is known at compile time \"\"\" return all(is_CONST(x) or is_number(x) or is_const(x) for x in p)", "def check(self, var): \"\"\"Check whether the provided value is a valid enum constant.\"\"\" if not isinstance(var, _str_type): return False return _enum_mangle(var) in self._consts", "def is_symbol(string): \"\"\" Return true if the string is a mathematical symbol. \"\"\" return ( is_int(string) or is_float(string) or is_constant(string) or is_unary(string) or is_binary(string) or (string == '(') or (string == ')') )", "def is_enum_type(type_): \"\"\" Checks if the given type is an enum type. :param type_: The type to check :return: True if the type is a enum type, otherwise False :rtype: bool \"\"\" return isinstance(type_, type) and issubclass(type_, tuple(_get_types(Types.ENUM)))", "def isreal(obj): \"\"\" Test if the argument is a real number (float or integer). :param obj: Object :type obj: any :rtype: boolean \"\"\" return ( (obj is not None) and (not isinstance(obj, bool)) and isinstance(obj, (int, float)) )"], "rank": 1}
{"nl": "python check if variable exists in locals", "code": ["def getvariable(name): \"\"\"Get the value of a local variable somewhere in the call stack.\"\"\" import inspect fr = inspect.currentframe() try: while fr: fr = fr.f_back vars = fr.f_locals if name in vars: return vars[name] except: pass return None", "def caller_locals(): \"\"\"Get the local variables in the caller's frame.\"\"\" import inspect frame = inspect.currentframe() try: return frame.f_back.f_back.f_locals finally: del frame", "def get_var(name, factory=None): \"\"\"Gets a global variable given its name. If factory is not None and the variable is not set, factory is a callable that will set the variable. If not set, returns None. \"\"\" if name not in _VARS and factory is not None: _VARS[name] = factory() return _VARS.get(name)", "def is_defined(self, objtxt, force_import=False): \"\"\"Return True if object is defined\"\"\" return self.interpreter.is_defined(objtxt, force_import)", "def _check_env_var(envvar: str) -> bool: \"\"\"Check Environment Variable to verify that it is set and not empty. :param envvar: Environment Variable to Check. :returns: True if Environment Variable is set and not empty. :raises: KeyError if Environment Variable is not set or is empty. .. versionadded:: 0.0.12 \"\"\" if os.getenv(envvar) is None: raise KeyError( \"Required ENVVAR: {0} is not set\".format(envvar)) if not os.getenv(envvar): # test if env var is empty raise KeyError( \"Required ENVVAR: {0} is empty\".format(envvar)) return True"], "rank": 1}
{"nl": "python filter a dictionary by value", "code": ["def _(f, x): \"\"\" filter for dict, note `f` should have signature: `f::key->value->bool` \"\"\" return {k: v for k, v in x.items() if f(k, v)}", "def filter_dict(d, keys): \"\"\" Creates a new dict from an existing dict that only has the given keys \"\"\" return {k: v for k, v in d.items() if k in keys}", "def filter_dict_by_key(d, keys): \"\"\"Filter the dict *d* to remove keys not in *keys*.\"\"\" return {k: v for k, v in d.items() if k in keys}", "def filter_by_ids(original_list, ids_to_filter): \"\"\"Filter a list of dicts by IDs using an id key on each dict.\"\"\" if not ids_to_filter: return original_list return [i for i in original_list if i['id'] in ids_to_filter]", "def _remove_keywords(d): \"\"\" copy the dict, filter_keywords Parameters ---------- d : dict \"\"\" return { k:v for k, v in iteritems(d) if k not in RESERVED }"], "rank": 1}
{"nl": "read json file and turn into dictionary using python", "code": ["def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def read_json(location): \"\"\"Open and load JSON from file. location (Path): Path to JSON file. RETURNS (dict): Loaded JSON content. \"\"\" location = ensure_path(location) with location.open('r', encoding='utf8') as f: return ujson.load(f)", "def load(cls, fname): \"\"\" Loads the dictionary from json file :param fname: file to load from :return: loaded dictionary \"\"\" with open(fname) as f: return Config(**json.load(f))", "def load_from_file(cls, file_path: str): \"\"\" Read and reconstruct the data from a JSON file. \"\"\" with open(file_path, \"r\") as f: data = json.load(f) item = cls.decode(data=data) return item", "def json_get_data(filename): \"\"\"Get data from json file \"\"\" with open(filename) as fp: json_data = json.load(fp) return json_data return False"], "rank": 1}
{"nl": "python change the shape of list", "code": ["def shape_list(l,shape,dtype): \"\"\" Shape a list of lists into the appropriate shape and data type \"\"\" return np.array(l, dtype=dtype).reshape(shape)", "def _crop_list_to_size(l, size): \"\"\"Make a list a certain size\"\"\" for x in range(size - len(l)): l.append(False) for x in range(len(l) - size): l.pop() return l", "def arr_to_vector(arr): \"\"\"Reshape a multidimensional array to a vector. \"\"\" dim = array_dim(arr) tmp_arr = [] for n in range(len(dim) - 1): for inner in arr: for i in inner: tmp_arr.append(i) arr = tmp_arr tmp_arr = [] return arr", "def grow_slice(slc, size): \"\"\" Grow a slice object by 1 in each direction without overreaching the list. Parameters ---------- slc: slice slice object to grow size: int list length Returns ------- slc: slice extended slice \"\"\" return slice(max(0, slc.start-1), min(size, slc.stop+1))", "def batchify(data, batch_size): \"\"\"Reshape data into (num_example, batch_size)\"\"\" nbatch = data.shape[0] // batch_size data = data[:nbatch * batch_size] data = data.reshape((batch_size, nbatch)).T return data"], "rank": 1}
{"nl": "how to get the datatypes in python", "code": ["def type(self): \"\"\"Returns type of the data for the given FeatureType.\"\"\" if self is FeatureType.TIMESTAMP: return list if self is FeatureType.BBOX: return BBox return dict", "def dtypes(self): \"\"\"Returns all column names and their data types as a list. >>> df.dtypes [('age', 'int'), ('name', 'string')] \"\"\" return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]", "def types(self): \"\"\" Return a list of all the variable types that exist in the Variables object. \"\"\" output = set() for var in self.values(): if var.has_value(): output.update(var.types()) return list(output)", "def maybe_infer_dtype_type(element): \"\"\"Try to infer an object's dtype, for use in arithmetic ops Uses `element.dtype` if that's available. Objects implementing the iterator protocol are cast to a NumPy array, and from there the array's type is used. Parameters ---------- element : object Possibly has a `.dtype` attribute, and possibly the iterator protocol. Returns ------- tipo : type Examples -------- >>> from collections import namedtuple >>> Foo = namedtuple(\"Foo\", \"dtype\") >>> maybe_infer_dtype_type(Foo(np.dtype(\"i8\"))) numpy.int64 \"\"\" tipo = None if hasattr(element, 'dtype'): tipo = element.dtype elif is_list_like(element): element = np.asarray(element) tipo = element.dtype return tipo", "def _get_column_types(self, data): \"\"\"Get a list of the data types for each column in *data*.\"\"\" columns = list(zip_longest(*data)) return [self._get_column_type(column) for column in columns]"], "rank": 2}
{"nl": "initializing an empty string with a size python", "code": ["def stn(s, length, encoding, errors): \"\"\"Convert a string to a null-terminated bytes object. \"\"\" s = s.encode(encoding, errors) return s[:length] + (length - len(s)) * NUL", "def __init__(self, ba=None): \"\"\"Constructor.\"\"\" self.bytearray = ba or (bytearray(b'\\0') * self.SIZEOF)", "def add_0x(string): \"\"\"Add 0x to string at start. \"\"\" if isinstance(string, bytes): string = string.decode('utf-8') return '0x' + str(string)", "def random_str(size=10): \"\"\" create random string of selected size :param size: int, length of the string :return: the string \"\"\" return ''.join(random.choice(string.ascii_lowercase) for _ in range(size))", "def gen_random_string(str_len): \"\"\" generate random string with specified length \"\"\" return ''.join( random.choice(string.ascii_letters + string.digits) for _ in range(str_len))"], "rank": 4}
{"nl": "cast str as int in python", "code": ["def try_cast_int(s): \"\"\"(str) -> int All the digits in a given string are concatenated and converted into a single number. \"\"\" try: temp = re.findall('\\d', str(s)) temp = ''.join(temp) return int(temp) except: return s", "def cast_int(x): \"\"\" Cast unknown type into integer :param any x: :return int: \"\"\" try: x = int(x) except ValueError: try: x = x.strip() except AttributeError as e: logger_misc.warn(\"parse_str: AttributeError: String not number or word, {}, {}\".format(x, e)) return x", "def get_number(s, cast=int): \"\"\" Try to get a number out of a string, and cast it. \"\"\" import string d = \"\".join(x for x in str(s) if x in string.digits) return cast(d)", "def str2int(num, radix=10, alphabet=BASE85): \"\"\"helper function for quick base conversions from strings to integers\"\"\" return NumConv(radix, alphabet).str2int(num)", "def str_to_num(str_value): \"\"\"Convert str_value to an int or a float, depending on the numeric value represented by str_value. \"\"\" str_value = str(str_value) try: return int(str_value) except ValueError: return float(str_value)"], "rank": 1}
{"nl": "python, sql table column details", "code": ["def get_table_columns(dbconn, tablename): \"\"\" Return a list of tuples specifying the column name and type \"\"\" cur = dbconn.cursor() cur.execute(\"PRAGMA table_info('%s');\" % tablename) info = cur.fetchall() cols = [(i[1], i[2]) for i in info] return cols", "def get_column_definition(self, table, column): \"\"\"Retrieve the column definition statement for a column from a table.\"\"\" # Parse column definitions for match for col in self.get_column_definition_all(table): if col.strip('`').startswith(column): return col.strip(',')", "def _columns_for_table(table_name): \"\"\" Return all of the columns registered for a given table. Parameters ---------- table_name : str Returns ------- columns : dict of column wrappers Keys will be column names. \"\"\" return {cname: col for (tname, cname), col in _COLUMNS.items() if tname == table_name}", "def get_column_keys_and_names(table): \"\"\" Return a generator of tuples k, c such that k is the name of the python attribute for the column and c is the name of the column in the sql table. \"\"\" ins = inspect(table) return ((k, c.name) for k, c in ins.mapper.c.items())", "def column_names(self, table): \"\"\"An iterable of column names, for a particular table or view.\"\"\" table_info = self.execute( u'PRAGMA table_info(%s)' % quote(table)) return (column['name'] for column in table_info)"], "rank": 5}
{"nl": "python pathlib to traverse directories", "code": ["def listfolderpath(p): \"\"\" generator of list folder in the path. folders only \"\"\" for entry in scandir.scandir(p): if entry.is_dir(): yield entry.path", "def get_files(dir_name): \"\"\"Simple directory walker\"\"\" return [(os.path.join('.', d), [os.path.join(d, f) for f in files]) for d, _, files in os.walk(dir_name)]", "def get_all_files(folder): \"\"\" Generator that loops through all absolute paths of the files within folder Parameters ---------- folder: str Root folder start point for recursive search. Yields ------ fpath: str Absolute path of one file in the folders \"\"\" for path, dirlist, filelist in os.walk(folder): for fn in filelist: yield op.join(path, fn)", "def recursively_get_files_from_directory(directory): \"\"\" Return all filenames under recursively found in a directory \"\"\" return [ os.path.join(root, filename) for root, directories, filenames in os.walk(directory) for filename in filenames ]", "def globlookup(pattern, root): \"\"\"globlookup finds filesystem objects whose relative path matches the given pattern. :param pattern: The pattern to wish to match relative filepaths to. :param root: The root director to search within. \"\"\" for subdir, dirnames, filenames in os.walk(root): d = subdir[len(root) + 1:] files = (os.path.join(d, f) for f in filenames) for f in fnmatch.filter(files, pattern): yield f"], "rank": 2}
{"nl": "python create list of columns with their dtype", "code": ["def dtypes(self): \"\"\"Returns all column names and their data types as a list. >>> df.dtypes [('age', 'int'), ('name', 'string')] \"\"\" return [(str(f.name), f.dataType.simpleString()) for f in self.schema.fields]", "def get_obj_cols(df): \"\"\" Returns names of 'object' columns in the DataFrame. \"\"\" obj_cols = [] for idx, dt in enumerate(df.dtypes): if dt == 'object' or is_category(dt): obj_cols.append(df.columns.values[idx]) return obj_cols", "def _get_str_columns(sf): \"\"\" Returns a list of names of columns that are string type. \"\"\" return [name for name in sf.column_names() if sf[name].dtype == str]", "def _get_column_types(self, data): \"\"\"Get a list of the data types for each column in *data*.\"\"\" columns = list(zip_longest(*data)) return [self._get_column_type(column) for column in columns]", "def to_array(self): \"\"\"Convert the table to a structured NumPy array.\"\"\" dt = np.dtype(list(zip(self.labels, (c.dtype for c in self.columns)))) arr = np.empty_like(self.columns[0], dt) for label in self.labels: arr[label] = self[label] return arr"], "rank": 3}
{"nl": "python turn all nested object to dict", "code": ["def as_dict(self): \"\"\"Return all child objects in nested dict.\"\"\" dicts = [x.as_dict for x in self.children] return {'{0} {1}'.format(self.name, self.value): dicts}", "def to_dict(self): \"\"\" Returns: the tree item as a dictionary \"\"\" if self.childCount() > 0: value = {} for index in range(self.childCount()): value.update(self.child(index).to_dict()) else: value = self.value return {self.name: value}", "def _zeep_to_dict(cls, obj): \"\"\"Convert a zeep object to a dictionary.\"\"\" res = serialize_object(obj) res = cls._get_non_empty_dict(res) return res", "def object_as_dict(obj): \"\"\"Turn an SQLAlchemy model into a dict of field names and values. Based on https://stackoverflow.com/a/37350445/1579058 \"\"\" return {c.key: getattr(obj, c.key) for c in inspect(obj).mapper.column_attrs}", "def get_all_items(obj): \"\"\" dict.items() but with a separate row for each value in a MultiValueDict \"\"\" if hasattr(obj, 'getlist'): items = [] for key in obj: for value in obj.getlist(key): items.append((key, value)) return items else: return obj.items()"], "rank": 1}
{"nl": "python read dicom images", "code": ["def numpy(self): \"\"\" Grabs image data and converts it to a numpy array \"\"\" # load GDCM's image reading functionality image_reader = gdcm.ImageReader() image_reader.SetFileName(self.fname) if not image_reader.Read(): raise IOError(\"Could not read DICOM image\") pixel_array = self._gdcm_to_numpy(image_reader.GetImage()) return pixel_array", "def ReadTif(tifFile): \"\"\"Reads a tif file to a 2D NumPy array\"\"\" img = Image.open(tifFile) img = np.array(img) return img", "def read_flat(self): \"\"\" Read a PNG file and decode it into flat row flat pixel format. Returns (*width*, *height*, *pixels*, *metadata*). May use excessive memory. `pixels` are returned in flat row flat pixel format. See also the :meth:`read` method which returns pixels in the more stream-friendly boxed row flat pixel format. \"\"\" x, y, pixel, meta = self.read() arraycode = 'BH'[meta['bitdepth'] > 8] pixel = array(arraycode, itertools.chain(*pixel)) return x, y, pixel, meta", "def load_tiff(file): \"\"\" Load a geotiff raster keeping ndv values using a masked array Usage: data = load_tiff(file) \"\"\" ndv, xsize, ysize, geot, projection, datatype = get_geo_info(file) data = gdalnumeric.LoadFile(file) data = np.ma.masked_array(data, mask=data == ndv, fill_value=ndv) return data", "def delete(self, mutagen_file): \"\"\"Remove all images from the file. \"\"\" for cover_tag in self.TAG_NAMES.values(): try: del mutagen_file[cover_tag] except KeyError: pass"], "rank": 1}
{"nl": "python making string lower case", "code": ["def snake_case(a_string): \"\"\"Returns a snake cased version of a string. :param a_string: any :class:`str` object. Usage: >>> snake_case('FooBar') \"foo_bar\" \"\"\" partial = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', a_string) return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', partial).lower()", "def camel_to_snake_case(string): \"\"\"Converts 'string' presented in camel case to snake case. e.g.: CamelCase => snake_case \"\"\" s = _1.sub(r'\\1_\\2', string) return _2.sub(r'\\1_\\2', s).lower()", "def _lower(string): \"\"\"Custom lower string function. Examples: FooBar -> foo_bar \"\"\" if not string: return \"\" new_string = [string[0].lower()] for char in string[1:]: if char.isupper(): new_string.append(\"_\") new_string.append(char.lower()) return \"\".join(new_string)", "def convert_camel_case_string(name: str) -> str: \"\"\"Convert camel case string to snake case\"\"\" string = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name) return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", string).lower()", "def camel_to_snake(s: str) -> str: \"\"\"Convert string from camel case to snake case.\"\"\" return CAMEL_CASE_RE.sub(r'_\\1', s).strip().lower()"], "rank": 14}
{"nl": "how to implement a macro in python", "code": ["def define_macro(self, name, themacro): \"\"\"Define a new macro Parameters ---------- name : str The name of the macro. themacro : str or Macro The action to do upon invoking the macro. If a string, a new Macro object is created by passing the string to it. \"\"\" from IPython.core import macro if isinstance(themacro, basestring): themacro = macro.Macro(themacro) if not isinstance(themacro, macro.Macro): raise ValueError('A macro must be a string or a Macro instance.') self.user_ns[name] = themacro", "def magic(self, alias): \"\"\"Returns the appropriate IPython code magic when called with an alias for a language. \"\"\" if alias in self.aliases: return self.aliases[alias] else: return \"%%{}\\n\".format(alias)", "def _multilingual(function, *args, **kwargs): \"\"\" Returns the value from the function with the given name in the given language module. By default, language=\"en\". \"\"\" return getattr(_module(kwargs.pop(\"language\", \"en\")), function)(*args, **kwargs)", "def extend_with(func): \"\"\"Extends with class or function\"\"\" if not func.__name__ in ArgParseInator._plugins: ArgParseInator._plugins[func.__name__] = func", "def build_code(self, lang, body): \"\"\"Wrap text with markdown specific flavour.\"\"\" self.out.append(\"```\" + lang) self.build_markdown(lang, body) self.out.append(\"```\")"], "rank": 1}
{"nl": "python subprocess close stdin", "code": ["def _finish(self): \"\"\" Closes and waits for subprocess to exit. \"\"\" if self._process.returncode is None: self._process.stdin.flush() self._process.stdin.close() self._process.wait() self.closed = True", "def correspond(text): \"\"\"Communicate with the child process without closing stdin.\"\"\" subproc.stdin.write(text) subproc.stdin.flush() return drain()", "def send(self, data): \"\"\" Send data to the child process through. \"\"\" self.stdin.write(data) self.stdin.flush()", "def safe_exit(output): \"\"\"exit without breaking pipes.\"\"\" try: sys.stdout.write(output) sys.stdout.flush() except IOError: pass", "def close(self): \"\"\"Close child subprocess\"\"\" if self._subprocess is not None: os.killpg(self._subprocess.pid, signal.SIGTERM) self._subprocess = None"], "rank": 1}
{"nl": "sum within a comprehension python", "code": ["def _accumulate(sequence, func): \"\"\" Python2 accumulate implementation taken from https://docs.python.org/3/library/itertools.html#itertools.accumulate \"\"\" iterator = iter(sequence) total = next(iterator) yield total for element in iterator: total = func(total, element) yield total", "def lcumsum (inlist): \"\"\" Returns a list consisting of the cumulative sum of the items in the passed list. Usage: lcumsum(inlist) \"\"\" newlist = copy.deepcopy(inlist) for i in range(1,len(newlist)): newlist[i] = newlist[i] + newlist[i-1] return newlist", "def _cumprod(l): \"\"\"Cumulative product of a list. Args: l: a list of integers Returns: a list with one more element (starting with 1) \"\"\" ret = [1] for item in l: ret.append(ret[-1] * item) return ret", "def cumsum(inlist): \"\"\" Returns a list consisting of the cumulative sum of the items in the passed list. Usage: lcumsum(inlist) \"\"\" newlist = copy.deepcopy(inlist) for i in range(1, len(newlist)): newlist[i] = newlist[i] + newlist[i - 1] return newlist", "def query_sum(queryset, field): \"\"\" Let the DBMS perform a sum on a queryset \"\"\" return queryset.aggregate(s=models.functions.Coalesce(models.Sum(field), 0))['s']"], "rank": 1}
{"nl": "python timedelta without microseconds", "code": ["def timedelta2millisecond(td): \"\"\"Get milliseconds from a timedelta.\"\"\" milliseconds = td.days * 24 * 60 * 60 * 1000 milliseconds += td.seconds * 1000 milliseconds += td.microseconds / 1000 return milliseconds", "def timedelta_seconds(timedelta): \"\"\"Returns the total timedelta duration in seconds.\"\"\" return (timedelta.total_seconds() if hasattr(timedelta, \"total_seconds\") else timedelta.days * 24 * 3600 + timedelta.seconds + timedelta.microseconds / 1000000.)", "def total_seconds(td): \"\"\"convert a timedelta to seconds. This is patterned after timedelta.total_seconds, which is only available in python 27. Args: td: a timedelta object. Returns: total seconds within a timedelta. Rounded up to seconds. \"\"\" secs = td.seconds + td.days * 24 * 3600 if td.microseconds: secs += 1 return secs", "def datetime_delta_to_ms(delta): \"\"\" Given a datetime.timedelta object, return the delta in milliseconds \"\"\" delta_ms = delta.days * 24 * 60 * 60 * 1000 delta_ms += delta.seconds * 1000 delta_ms += delta.microseconds / 1000 delta_ms = int(delta_ms) return delta_ms", "def ToDatetime(self): \"\"\"Converts Timestamp to datetime.\"\"\" return datetime.utcfromtimestamp( self.seconds + self.nanos / float(_NANOS_PER_SECOND))"], "rank": 2}
{"nl": "change python object to string", "code": ["def __repr__(self): \"\"\"Returns a stringified representation of this object.\"\"\" return str({'name': self._name, 'watts': self._watts, 'type': self._output_type, 'id': self._integration_id})", "def to_str(obj): \"\"\"Attempts to convert given object to a string object \"\"\" if not isinstance(obj, str) and PY3 and isinstance(obj, bytes): obj = obj.decode('utf-8') return obj if isinstance(obj, string_types) else str(obj)", "def __repr__(self) -> str: \"\"\"Return the string representation of self.\"\"\" return '{0}({1})'.format(type(self).__name__, repr(self.string))", "def obj_to_string(obj, top=True): \"\"\" Turn an arbitrary object into a unicode string. If complex (dict/list/tuple), will be json-encoded. \"\"\" obj = prepare_for_json_encoding(obj) if type(obj) == six.text_type: return obj return json.dumps(obj)", "def check_str(obj): \"\"\" Returns a string for various input types \"\"\" if isinstance(obj, str): return obj if isinstance(obj, float): return str(int(obj)) else: return str(obj)"], "rank": 12}
{"nl": "eit request header in python flask", "code": ["def get_trace_id_from_flask(): \"\"\"Get trace_id from flask request headers. :rtype: str :returns: TraceID in HTTP request headers. \"\"\" if flask is None or not flask.request: return None header = flask.request.headers.get(_FLASK_TRACE_HEADER) if header is None: return None trace_id = header.split(\"/\", 1)[0] return trace_id", "def fetch_header(self): \"\"\"Make a header request to the endpoint.\"\"\" query = self.query().add_query_parameter(req='header') return self._parse_messages(self.get_query(query).content)[0]", "def get_from_headers(request, key): \"\"\"Try to read a value named ``key`` from the headers. \"\"\" value = request.headers.get(key) return to_native(value)", "def getheader(self, name, default=None): \"\"\"Returns a given response header.\"\"\" return self.aiohttp_response.headers.get(name, default)", "def get_header(request, header_service): \"\"\"Return request's 'X_POLYAXON_...:' header, as a bytestring. Hide some test client ickyness where the header can be unicode. \"\"\" service = request.META.get('HTTP_{}'.format(header_service), b'') if isinstance(service, str): # Work around django test client oddness service = service.encode(HTTP_HEADER_ENCODING) return service"], "rank": 1}
{"nl": "python lower all elements in list", "code": ["def gen_lower(x: Iterable[str]) -> Generator[str, None, None]: \"\"\" Args: x: iterable of strings Yields: each string in lower case \"\"\" for string in x: yield string.lower()", "def _lower(string): \"\"\"Custom lower string function. Examples: FooBar -> foo_bar \"\"\" if not string: return \"\" new_string = [string[0].lower()] for char in string[1:]: if char.isupper(): new_string.append(\"_\") new_string.append(char.lower()) return \"\".join(new_string)", "def _upper(val_list): \"\"\" :param val_list: a list of strings :return: a list of upper-cased strings \"\"\" res = [] for ele in val_list: res.append(ele.upper()) return res", "def downcaseTokens(s,l,t): \"\"\"Helper parse action to convert tokens to lower case.\"\"\" return [ tt.lower() for tt in map(_ustr,t) ]", "def lowercase_chars(string: any) -> str: \"\"\"Return all (and only) the lowercase chars in the given string.\"\"\" return ''.join([c if c.islower() else '' for c in str(string)])"], "rank": 1}
{"nl": "using sort to move element in to new position in list python", "code": ["def insort_no_dup(lst, item): \"\"\" If item is not in lst, add item to list at its sorted position \"\"\" import bisect ix = bisect.bisect_left(lst, item) if lst[ix] != item: lst[ix:ix] = [item]", "def list_move_to_front(l,value='other'): \"\"\"if the value is in the list, move it to the front and return it.\"\"\" l=list(l) if value in l: l.remove(value) l.insert(0,value) return l", "def insert_ordered(value, array): \"\"\" This will insert the value into the array, keeping it sorted, and returning the index where it was inserted \"\"\" index = 0 # search for the last array item that value is larger than for n in range(0,len(array)): if value >= array[n]: index = n+1 array.insert(index, value) return index", "def fix(h, i): \"\"\"Rearrange the heap after the item at position i got updated.\"\"\" down(h, i, h.size()) up(h, i)", "def unsort_vector(data, indices_of_increasing): \"\"\"Upermutate 1-D data that is sorted by indices_of_increasing.\"\"\" return numpy.array([data[indices_of_increasing.index(i)] for i in range(len(data))])"], "rank": 1}
{"nl": "remove special characters from column names in python", "code": ["def normalize_column_names(df): r\"\"\" Clean up whitespace in column names. See better version at `pugnlp.clean_columns` >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['Hello World', 'not here']) >>> normalize_column_names(df) ['hello_world', 'not_here'] \"\"\" columns = df.columns if hasattr(df, 'columns') else df columns = [c.lower().replace(' ', '_') for c in columns] return columns", "def columnclean(column): \"\"\" Modifies column header format to be importable into a database :param column: raw column header :return: cleanedcolumn: reformatted column header \"\"\" cleanedcolumn = str(column) \\ .replace('%', 'percent') \\ .replace('(', '_') \\ .replace(')', '') \\ .replace('As', 'Adenosines') \\ .replace('Cs', 'Cytosines') \\ .replace('Gs', 'Guanines') \\ .replace('Ts', 'Thymines') \\ .replace('Ns', 'Unknowns') \\ .replace('index', 'adapterIndex') return cleanedcolumn", "def clean_colnames(df): \"\"\" Cleans the column names on a DataFrame Parameters: df - DataFrame The DataFrame to clean \"\"\" col_list = [] for index in range(_dutils.cols(df)): col_list.append(df.columns[index].strip().lower().replace(' ','_')) df.columns = col_list", "def _clean_up_name(self, name): \"\"\" Cleans up the name according to the rules specified in this exact function. Uses self.naughty, a list of naughty characters. \"\"\" for n in self.naughty: name = name.replace(n, '_') return name", "def standard_db_name(file_column_name): \"\"\"return a standard name by following rules: 1. find all regular expression partners ((IDs)|(ID)|([A-Z][a-z]+)|([A-Z]{2,})) 2. lower very part and join again with _ This method is only used if values in table[model]['columns'] are str :param str file_column_name: name of column in file :return: standard name :rtype: str \"\"\" found = id_re.findall(file_column_name) if not found: return file_column_name return '_'.join(x[0].lower() for x in found)"], "rank": 1}
{"nl": "calculate the average of a given list in python", "code": ["def calc_list_average(l): \"\"\" Calculates the average value of a list of numbers Returns a float \"\"\" total = 0.0 for value in l: total += value return total / len(l)", "def mean(inlist): \"\"\" Returns the arithematic mean of the values in the passed list. Assumes a '1D' list, but will function on the 1st dim of an array(!). Usage: lmean(inlist) \"\"\" sum = 0 for item in inlist: sum = sum + item return sum / float(len(inlist))", "def average(arr): \"\"\"average of the values, must have more than 0 entries. :param arr: list of numbers :type arr: number[] a number array :return: average :rtype: float \"\"\" if len(arr) == 0: sys.stderr.write(\"ERROR: no content in array to take average\\n\") sys.exit() if len(arr) == 1: return arr[0] return float(sum(arr))/float(len(arr))", "def average(iterator): \"\"\"Iterative mean.\"\"\" count = 0 total = 0 for num in iterator: count += 1 total += num return float(total)/count", "def _aggr_mean(inList): \"\"\" Returns mean of non-None elements of the list \"\"\" aggrSum = 0 nonNone = 0 for elem in inList: if elem != SENTINEL_VALUE_FOR_MISSING_DATA: aggrSum += elem nonNone += 1 if nonNone != 0: return aggrSum / nonNone else: return None"], "rank": 2}
{"nl": "past python git clone", "code": ["def mkhead(repo, path): \"\"\":return: New branch/head instance\"\"\" return git.Head(repo, git.Head.to_full_path(path))", "def check_git(): \"\"\"Check if git command is available.\"\"\" try: with open(os.devnull, \"wb\") as devnull: subprocess.check_call([\"git\", \"--version\"], stdout=devnull, stderr=devnull) except: raise RuntimeError(\"Please make sure git is installed and on your path.\")", "def activate(): \"\"\" Usage: containment activate \"\"\" # This is derived from the clone cli = CommandLineInterface() cli.ensure_config() cli.write_dockerfile() cli.build() cli.run()", "def commit(self, message=None, amend=False, stage=True): \"\"\"Commit any changes, optionally staging all changes beforehand.\"\"\" return git_commit(self.repo_dir, message=message, amend=amend, stage=stage)", "def is_git_repo(): \"\"\"Check whether the current folder is a Git repo.\"\"\" cmd = \"git\", \"rev-parse\", \"--git-dir\" try: subprocess.run(cmd, stdout=subprocess.DEVNULL, check=True) return True except subprocess.CalledProcessError: return False"], "rank": 1}
{"nl": "python lambda function with 3 params", "code": ["def lambda_tuple_converter(func): \"\"\" Converts a Python 2 function as lambda (x,y): x + y In the Python 3 format: lambda x,y : x + y \"\"\" if func is not None and func.__code__.co_argcount == 1: return lambda *args: func(args[0] if len(args) == 1 else args) else: return func", "def make_lambda(call): \"\"\"Wrap an AST Call node to lambda expression node. call: ast.Call node \"\"\" empty_args = ast.arguments(args=[], vararg=None, kwarg=None, defaults=[]) return ast.Lambda(args=empty_args, body=call)", "def call_with_context(func, context, *args): \"\"\" Check if given function has more arguments than given. Call it with context as last argument or without it. \"\"\" return make_context_aware(func, len(args))(*args + (context,))", "def def_linear(fun): \"\"\"Flags that a function is linear wrt all args\"\"\" defjvp_argnum(fun, lambda argnum, g, ans, args, kwargs: fun(*subval(args, argnum, g), **kwargs))", "def reduce(function, initval=None): \"\"\" Curried version of the built-in reduce. >>> reduce(lambda x,y: x+y)( [1, 2, 3, 4, 5] ) 15 \"\"\" if initval is None: return lambda s: __builtin__.reduce(function, s) else: return lambda s: __builtin__.reduce(function, s, initval)"], "rank": 2}
{"nl": "check for punctuation python", "code": ["def is_punctuation(text): \"\"\"Check if given string is a punctuation\"\"\" return not (text.lower() in config.AVRO_VOWELS or text.lower() in config.AVRO_CONSONANTS)", "def is_delimiter(line): \"\"\" True if a line consists only of a single punctuation character.\"\"\" return bool(line) and line[0] in punctuation and line[0]*len(line) == line", "def check(text): \"\"\"Check the text.\"\"\" err = \"malapropisms.misc\" msg = u\"'{}' is a malapropism.\" illogics = [ \"the infinitesimal universe\", \"a serial experience\", \"attack my voracity\", ] return existence_check(text, illogics, err, msg, offset=1)", "def remove_punctuation(text, exceptions=[]): \"\"\" Return a string with punctuation removed. Parameters: text (str): The text to remove punctuation from. exceptions (list): List of symbols to keep in the given text. Return: str: The input text without the punctuation. \"\"\" all_but = [ r'\\w', r'\\s' ] all_but.extend(exceptions) pattern = '[^{}]'.format(''.join(all_but)) return re.sub(pattern, '', text)", "def check(text): \"\"\"Check the text.\"\"\" err = \"misc.currency\" msg = u\"Incorrect use of symbols in {}.\" symbols = [ \"\\$[\\d]* ?(?:dollars|usd|us dollars)\" ] return existence_check(text, symbols, err, msg)"], "rank": 1}
{"nl": "unsupported media type json python", "code": ["def get_serialize_format(self, mimetype): \"\"\" Get the serialization format for the given mimetype \"\"\" format = self.formats.get(mimetype, None) if format is None: format = formats.get(mimetype, None) return format", "def parse(self, data, mimetype): \"\"\" Parses a byte array containing a JSON document and returns a Python object. :param data: The byte array containing a JSON document. :param MimeType mimetype: The mimetype chose to parse the data. :return: A Python object. \"\"\" encoding = mimetype.params.get('charset') or 'utf-8' return json.loads(data.decode(encoding))", "def content_type(self, data): \"\"\"The Content-Type header value for this request.\"\"\" self._content_type = str(data) self.add_header('Content-Type', str(data))", "def mimetype(self): \"\"\"MIME type of the asset.\"\"\" return (self.environment.mimetypes.get(self.format_extension) or self.compiler_mimetype or 'application/octet-stream')", "def python_mime(fn): \"\"\" Decorator, which adds correct MIME type for python source to the decorated bottle API function. \"\"\" @wraps(fn) def python_mime_decorator(*args, **kwargs): response.content_type = \"text/x-python\" return fn(*args, **kwargs) return python_mime_decorator"], "rank": 2}
{"nl": "python separate string to list", "code": ["def string_to_list(string, sep=\",\", filter_empty=False): \"\"\"Transforma una string con elementos separados por `sep` en una lista.\"\"\" return [value.strip() for value in string.split(sep) if (not filter_empty or value)]", "def _str_to_list(s): \"\"\"Converts a comma separated string to a list\"\"\" _list = s.split(\",\") return list(map(lambda i: i.lstrip(), _list))", "def split_elements(value): \"\"\"Split a string with comma or space-separated elements into a list.\"\"\" l = [v.strip() for v in value.split(',')] if len(l) == 1: l = value.split() return l", "def path_to_list(pathstr): \"\"\"Conver a path string to a list of path elements.\"\"\" return [elem for elem in pathstr.split(os.path.pathsep) if elem]", "def comma_delimited_to_list(list_param): \"\"\"Convert comma-delimited list / string into a list of strings :param list_param: Comma-delimited string :type list_param: str | unicode :return: A list of strings :rtype: list \"\"\" if isinstance(list_param, list): return list_param if isinstance(list_param, str): return list_param.split(',') else: return []"], "rank": 6}
{"nl": "python remove phrase from list of strings", "code": ["def delistify(x): \"\"\" A basic slug version of a given parameter list. \"\"\" if isinstance(x, list): x = [e.replace(\"'\", \"\") for e in x] return '-'.join(sorted(x)) return x", "def _removeStopwords(text_list): \"\"\" Removes stopwords contained in a list of words. :param text_string: A list of strings. :type text_string: list. :returns: The input ``text_list`` with stopwords removed. :rtype: list \"\"\" output_list = [] for word in text_list: if word.lower() not in _stopwords: output_list.append(word) return output_list", "def unpunctuate(s, *, char_blacklist=string.punctuation): \"\"\" Remove punctuation from string s. \"\"\" # remove punctuation s = \"\".join(c for c in s if c not in char_blacklist) # remove consecutive spaces return \" \".join(filter(None, s.split(\" \")))", "def detokenize(s): \"\"\" Detokenize a string by removing spaces before punctuation.\"\"\" print(s) s = re.sub(\"\\s+([;:,\\.\\?!])\", \"\\\\1\", s) s = re.sub(\"\\s+(n't)\", \"\\\\1\", s) return s", "def _clean_str(self, s): \"\"\" Returns a lowercase string with punctuation and bad chars removed :param s: string to clean \"\"\" return s.translate(str.maketrans('', '', punctuation)).replace('\\u200b', \" \").strip().lower()"], "rank": 10}
{"nl": "python fastest way to load data", "code": ["def Load(file): \"\"\" Loads a model from specified file \"\"\" with open(file, 'rb') as file: model = dill.load(file) return model", "def load(cls,filename): \"\"\"Load from stored files\"\"\" filename = cls.correct_file_extension(filename) with open(filename,'rb') as f: return pickle.load(f)", "def load(filename): \"\"\"Load a pickled obj from the filesystem. You better know what you expect from the given pickle, because we don't check it. Args: filename (str): The filename we load the object from. Returns: The object we were able to unpickle, else None. \"\"\" if not os.path.exists(filename): LOG.error(\"load object - File '%s' does not exist.\", filename) return None obj = None with open(filename, 'rb') as obj_file: obj = dill.load(obj_file) return obj", "def read_raw(data_path): \"\"\" Parameters ---------- data_path : str \"\"\" with open(data_path, 'rb') as f: data = pickle.load(f) return data", "def load_object_at_path(path): \"\"\"Load an object from disk at explicit path\"\"\" with open(path, 'r') as f: data = _deserialize(f.read()) return aadict(data)"], "rank": 42}
{"nl": "add noise to the audio python", "code": ["def synthesize(self, duration): \"\"\" Synthesize white noise Args: duration (numpy.timedelta64): The duration of the synthesized sound \"\"\" sr = self.samplerate.samples_per_second seconds = duration / Seconds(1) samples = np.random.uniform(low=-1., high=1., size=int(sr * seconds)) return AudioSamples(samples, self.samplerate)", "def calc_volume(self, sample: np.ndarray): \"\"\"Find the RMS of the audio\"\"\" return sqrt(np.mean(np.square(sample)))", "def add_noise(Y, sigma): \"\"\"Adds noise to Y\"\"\" return Y + np.random.normal(0, sigma, Y.shape)", "def uniform_noise(points): \"\"\"Init a uniform noise variable.\"\"\" return np.random.rand(1) * np.random.uniform(points, 1) \\ + random.sample([2, -2], 1)", "def normal_noise(points): \"\"\"Init a noise variable.\"\"\" return np.random.rand(1) * np.random.randn(points, 1) \\ + random.sample([2, -2], 1)"], "rank": 1}
{"nl": "object with commas turned into list python", "code": ["def _tostr(self,obj): \"\"\" converts a object to list, if object is a list, it creates a comma seperated string. \"\"\" if not obj: return '' if isinstance(obj, list): return ', '.join(map(self._tostr, obj)) return str(obj)", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids", "def seq_to_str(obj, sep=\",\"): \"\"\" Given a sequence convert it to a comma separated string. If, however, the argument is a single object, return its string representation. \"\"\" if isinstance(obj, string_classes): return obj elif isinstance(obj, (list, tuple)): return sep.join([str(x) for x in obj]) else: return str(obj)", "def vectorize(values): \"\"\" Takes a value or list of values and returns a single result, joined by \",\" if necessary. \"\"\" if isinstance(values, list): return ','.join(str(v) for v in values) return values", "def list_to_csv(value): \"\"\" Converts list to string with comma separated values. For string is no-op. \"\"\" if isinstance(value, (list, tuple, set)): value = \",\".join(value) return value"], "rank": 3}
{"nl": "python set limits on x axis", "code": ["def set_xlimits(self, min=None, max=None): \"\"\"Set limits for the x-axis. :param min: minimum value to be displayed. If None, it will be calculated. :param max: maximum value to be displayed. If None, it will be calculated. \"\"\" self.limits['xmin'] = min self.limits['xmax'] = max", "def set_xlimits_widgets(self, set_min=True, set_max=True): \"\"\"Populate axis limits GUI with current plot values.\"\"\" xmin, xmax = self.tab_plot.ax.get_xlim() if set_min: self.w.x_lo.set_text('{0}'.format(xmin)) if set_max: self.w.x_hi.set_text('{0}'.format(xmax))", "def set_xlimits(self, row, column, min=None, max=None): \"\"\"Set x-axis limits of a subplot. :param row,column: specify the subplot. :param min: minimal axis value :param max: maximum axis value \"\"\" subplot = self.get_subplot_at(row, column) subplot.set_xlimits(min, max)", "def _set_axis_limits(self, which, lims, d, scale, reverse=False): \"\"\"Private method for setting axis limits. Sets the axis limits on each axis for an individual plot. Args: which (str): The indicator of which part of the plots to adjust. This currently handles `x` and `y`. lims (len-2 list of floats): The limits for the axis. d (float): Amount to increment by between the limits. scale (str): Scale of the axis. Either `log` or `lin`. reverse (bool, optional): If True, reverse the axis tick marks. Default is False. \"\"\" setattr(self.limits, which + 'lims', lims) setattr(self.limits, 'd' + which, d) setattr(self.limits, which + 'scale', scale) if reverse: setattr(self.limits, 'reverse_' + which + '_axis', True) return", "def set_ylim(self, xlims, dx, xscale, reverse=False): \"\"\"Set y limits for plot. This will set the limits for the y axis for the specific plot. Args: ylims (len-2 list of floats): The limits for the axis. dy (float): Amount to increment by between the limits. yscale (str): Scale of the axis. Either `log` or `lin`. reverse (bool, optional): If True, reverse the axis tick marks. Default is False. \"\"\" self._set_axis_limits('y', xlims, dx, xscale, reverse) return"], "rank": 1}
{"nl": "is python ssl socket connection thread safe", "code": ["def enable_ssl(self, *args, **kwargs): \"\"\" Transforms the regular socket.socket to an ssl.SSLSocket for secure connections. Any arguments are passed to ssl.wrap_socket: http://docs.python.org/dev/library/ssl.html#ssl.wrap_socket \"\"\" if self.handshake_sent: raise SSLError('can only enable SSL before handshake') self.secure = True self.sock = ssl.wrap_socket(self.sock, *args, **kwargs)", "def _sslobj(sock): \"\"\"Returns the underlying PySLLSocket object with which the C extension functions interface. \"\"\" pass if isinstance(sock._sslobj, _ssl._SSLSocket): return sock._sslobj else: return sock._sslobj._sslobj", "def send_request(self, *args, **kwargs): \"\"\"Wrapper for session.request Handle connection reset error even from pyopenssl \"\"\" try: return self.session.request(*args, **kwargs) except ConnectionError: self.session.close() return self.session.request(*args, **kwargs)", "def inject_into_urllib3(): \"\"\" Monkey-patch urllib3 with SecureTransport-backed SSL-support. \"\"\" util.ssl_.SSLContext = SecureTransportContext util.HAS_SNI = HAS_SNI util.ssl_.HAS_SNI = HAS_SNI util.IS_SECURETRANSPORT = True util.ssl_.IS_SECURETRANSPORT = True", "def connect(self): \"\"\"Connects to the given host\"\"\" self.socket = socket.create_connection(self.address, self.timeout)"], "rank": 1}
{"nl": "python3 encode decode bytes", "code": ["def to_bytes(value): \"\"\" str to bytes (py3k) \"\"\" vtype = type(value) if vtype == bytes or vtype == type(None): return value try: return vtype.encode(value) except UnicodeEncodeError: pass return value", "def tob(data, enc='utf8'): \"\"\" Convert anything to bytes \"\"\" return data.encode(enc) if isinstance(data, six.text_type) else bytes(data)", "def decodebytes(input): \"\"\"Decode base64 string to byte array.\"\"\" py_version = sys.version_info[0] if py_version >= 3: return _decodebytes_py3(input) return _decodebytes_py2(input)", "def u2b(string): \"\"\" unicode to bytes\"\"\" if ((PY2 and isinstance(string, unicode)) or ((not PY2) and isinstance(string, str))): return string.encode('utf-8') return string", "def cast_bytes(s, encoding=None): \"\"\"Source: https://github.com/ipython/ipython_genutils\"\"\" if not isinstance(s, bytes): return encode(s, encoding) return s"], "rank": 1}
{"nl": "xsd file to python object", "code": ["def from_file(cls, file_path, validate=True): \"\"\" Creates a Python object from a XML file :param file_path: Path to the XML file :param validate: XML should be validated against the embedded XSD definition :type validate: Boolean :returns: the Python object \"\"\" return xmlmap.load_xmlobject_from_file(file_path, xmlclass=cls, validate=validate)", "def schemaParse(self): \"\"\"parse a schema definition resource and build an internal XML Shema struture which can be used to validate instances. \"\"\" ret = libxml2mod.xmlSchemaParse(self._o) if ret is None:raise parserError('xmlSchemaParse() failed') __tmp = Schema(_obj=ret) return __tmp", "def SchemaValidate(self, xsd): \"\"\"Use W3C XSD schema to validate the document as it is processed. Activation is only possible before the first Read(). If @xsd is None, then XML Schema validation is deactivated. \"\"\" ret = libxml2mod.xmlTextReaderSchemaValidate(self._o, xsd) return ret", "def schemaValidateFile(self, filename, options): \"\"\"Do a schemas validation of the given resource, it will use the SAX streamable validation internally. \"\"\" ret = libxml2mod.xmlSchemaValidateFile(self._o, filename, options) return ret", "def validate(self, xml_input): \"\"\" This method validate the parsing and schema, return a boolean \"\"\" parsed_xml = etree.parse(self._handle_xml(xml_input)) try: return self.xmlschema.validate(parsed_xml) except AttributeError: raise CannotValidate('Set XSD to validate the XML')"], "rank": 1}
{"nl": "python expand a path", "code": ["def expandpath(path): \"\"\" Expand a filesystem path that may or may not contain user/env vars. :param str path: path to expand :return str: expanded version of input path \"\"\" return os.path.expandvars(os.path.expanduser(path)).replace(\"//\", \"/\")", "def expand_path(path): \"\"\"Returns ``path`` as an absolute path with ~user and env var expansion applied. :API: public \"\"\" return os.path.abspath(os.path.expandvars(os.path.expanduser(path)))", "def fixpath(path): \"\"\"Uniformly format a path.\"\"\" return os.path.normpath(os.path.realpath(os.path.expanduser(path)))", "def get_absolute_path(*args): \"\"\"Transform relative pathnames into absolute pathnames.\"\"\" directory = os.path.dirname(os.path.abspath(__file__)) return os.path.join(directory, *args)", "def relpath(path): \"\"\"Path helper, gives you a path relative to this file\"\"\" return os.path.normpath( os.path.join(os.path.abspath(os.path.dirname(__file__)), path) )"], "rank": 1}
{"nl": "python flask template table example", "code": ["def html(header_rows): \"\"\" Convert a list of tuples describing a table into a HTML string \"\"\" name = 'table%d' % next(tablecounter) return HtmlTable([map(str, row) for row in header_rows], name).render()", "def _render_table(data, fields=None): \"\"\" Helper to render a list of dictionaries as an HTML display object. \"\"\" return IPython.core.display.HTML(datalab.utils.commands.HtmlBuilder.render_table(data, fields))", "def index(): \"\"\" Display productpage with normal user and test user buttons\"\"\" global productpage table = json2html.convert(json = json.dumps(productpage), table_attributes=\"class=\\\"table table-condensed table-bordered table-hover\\\"\") return render_template('index.html', serviceTable=table)", "def tree_render(request, upy_context, vars_dictionary): \"\"\" It renders template defined in upy_context's page passed in arguments \"\"\" page = upy_context['PAGE'] return render_to_response(page.template.file_name, vars_dictionary, context_instance=RequestContext(request))", "def print_table(*args, **kwargs): \"\"\" if csv: import csv t = csv.writer(sys.stdout, delimiter=\";\") t.writerow(header) else: t = PrettyTable(header) t.align = \"r\" t.align[\"details\"] = \"l\" \"\"\" t = format_table(*args, **kwargs) click.echo(t)"], "rank": 3}
{"nl": "how to check if object defined python", "code": ["def is_defined(self, objtxt, force_import=False): \"\"\"Return True if object is defined\"\"\" return self.interpreter.is_defined(objtxt, force_import)", "def is_defined(self, obj, force_import=False): \"\"\"Return True if object is defined in current namespace\"\"\" from spyder_kernels.utils.dochelpers import isdefined ns = self._get_current_namespace(with_magics=True) return isdefined(obj, force_import=force_import, namespace=ns)", "def is_builtin_object(node: astroid.node_classes.NodeNG) -> bool: \"\"\"Returns True if the given node is an object from the __builtin__ module.\"\"\" return node and node.root().name == BUILTINS_NAME", "def isroutine(object): \"\"\"Return true if the object is any kind of function or method.\"\"\" return (isbuiltin(object) or isfunction(object) or ismethod(object) or ismethoddescriptor(object))", "def is_descriptor_class(desc, include_abstract=False): r\"\"\"Check calculatable descriptor class or not. Returns: bool \"\"\" return ( isinstance(desc, type) and issubclass(desc, Descriptor) and (True if include_abstract else not inspect.isabstract(desc)) )"], "rank": 1}
{"nl": "s3 sync between bucket python", "code": ["def sync_s3(self): \"\"\"Walk the media/static directories and syncs files to S3\"\"\" bucket, key = self.open_s3() for directory in self.DIRECTORIES: for root, dirs, files in os.walk(directory): self.upload_s3((bucket, key, self.AWS_BUCKET_NAME, directory), root, files, dirs)", "def s3_connect(bucket_name, s3_access_key_id, s3_secret_key): \"\"\" Returns a Boto connection to the provided S3 bucket. \"\"\" conn = connect_s3(s3_access_key_id, s3_secret_key) try: return conn.get_bucket(bucket_name) except S3ResponseError as e: if e.status == 403: raise Exception(\"Bad Amazon S3 credentials.\") raise", "def s3(ctx, bucket_name, data_file, region): \"\"\"Use the S3 SWAG backend.\"\"\" if not ctx.data_file: ctx.data_file = data_file if not ctx.bucket_name: ctx.bucket_name = bucket_name if not ctx.region: ctx.region = region ctx.type = 's3'", "def download_file_from_bucket(self, bucket, file_path, key): \"\"\" Download file from S3 Bucket \"\"\" with open(file_path, 'wb') as data: self.__s3.download_fileobj(bucket, key, data) return file_path", "def remove_file_from_s3(awsclient, bucket, key): \"\"\"Remove a file from an AWS S3 bucket. :param awsclient: :param bucket: :param key: :return: \"\"\" client_s3 = awsclient.get_client('s3') response = client_s3.delete_object(Bucket=bucket, Key=key)"], "rank": 3}
{"nl": "change the position of 3d coordinate in python", "code": ["def transform_to_3d(points,normal,z=0): \"\"\"Project points into 3d from 2d points.\"\"\" d = np.cross(normal, (0, 0, 1)) M = rotation_matrix(d) transformed_points = M.dot(points.T).T + z return transformed_points", "def list(self): \"\"\"position in 3d space\"\"\" return [self._pos3d.x, self._pos3d.y, self._pos3d.z]", "def world_to_view(v): \"\"\"world coords to view coords; v an eu.Vector2, returns (float, float)\"\"\" return v.x * config.scale_x, v.y * config.scale_y", "def __call__(self, xy): \"\"\"Project x and y\"\"\" x, y = xy return (self.x(x), self.y(y))", "def unproject(self, xy): \"\"\" Returns the coordinates from position in meters \"\"\" (x, y) = xy lng = x/EARTH_RADIUS * RAD_TO_DEG lat = 2 * atan(exp(y/EARTH_RADIUS)) - pi/2 * RAD_TO_DEG return (lng, lat)"], "rank": 2}
{"nl": "python how to get the number of cores in a computer", "code": ["def cpu_count() -> int: \"\"\"Returns the number of processors on this machine.\"\"\" if multiprocessing is None: return 1 try: return multiprocessing.cpu_count() except NotImplementedError: pass try: return os.sysconf(\"SC_NPROCESSORS_CONF\") except (AttributeError, ValueError): pass gen_log.error(\"Could not detect number of processors; assuming 1\") return 1", "def _num_cpus_darwin(): \"\"\"Return the number of active CPUs on a Darwin system.\"\"\" p = subprocess.Popen(['sysctl','-n','hw.ncpu'],stdout=subprocess.PIPE) return p.stdout.read()", "def machine_info(): \"\"\"Retrieve core and memory information for the current machine. \"\"\" import psutil BYTES_IN_GIG = 1073741824.0 free_bytes = psutil.virtual_memory().total return [{\"memory\": float(\"%.1f\" % (free_bytes / BYTES_IN_GIG)), \"cores\": multiprocessing.cpu_count(), \"name\": socket.gethostname()}]", "def ncores_reserved(self): \"\"\" Returns the number of cores reserved in this moment. A core is reserved if it's still not running but we have submitted the task to the queue manager. \"\"\" return sum(task.manager.num_cores for task in self if task.status == task.S_SUB)", "def get_size(objects): \"\"\"Compute the total size of all elements in objects.\"\"\" res = 0 for o in objects: try: res += _getsizeof(o) except AttributeError: print(\"IGNORING: type=%s; o=%s\" % (str(type(o)), str(o))) return res"], "rank": 2}
{"nl": "python how to use pdb set trace", "code": ["def set_trace(): \"\"\"Start a Pdb instance at the calling frame, with stdout routed to sys.__stdout__.\"\"\" # https://github.com/nose-devs/nose/blob/master/nose/tools/nontrivial.py pdb.Pdb(stdout=sys.__stdout__).set_trace(sys._getframe().f_back)", "def __run(self): \"\"\"Hacked run function, which installs the trace.\"\"\" sys.settrace(self.globaltrace) self.__run_backup() self.run = self.__run_backup", "def debug_on_error(type, value, tb): \"\"\"Code due to Thomas Heller - published in Python Cookbook (O'Reilley)\"\"\" traceback.print_exc(type, value, tb) print() pdb.pm()", "def _trace_full (frame, event, arg): \"\"\"Trace every executed line.\"\"\" if event == \"line\": _trace_line(frame, event, arg) else: _trace(frame, event, arg) return _trace_full", "def user_return(self, frame, return_value): \"\"\"This function is called when a return trap is set here.\"\"\" pdb.Pdb.user_return(self, frame, return_value)"], "rank": 1}
{"nl": "python dictionary url encode", "code": ["def get_dict_to_encoded_url(data): \"\"\" Converts a dict to an encoded URL. Example: given data = {'a': 1, 'b': 2}, it returns 'a=1&b=2' \"\"\" unicode_data = dict([(k, smart_str(v)) for k, v in data.items()]) encoded = urllib.urlencode(unicode_data) return encoded", "def dict_to_querystring(dictionary): \"\"\"Converts a dict to a querystring suitable to be appended to a URL.\"\"\" s = u\"\" for d in dictionary.keys(): s = unicode.format(u\"{0}{1}={2}&\", s, d, dictionary[d]) return s[:-1]", "def url_encode(url): \"\"\" Convert special characters using %xx escape. :param url: str :return: str - encoded url \"\"\" if isinstance(url, text_type): url = url.encode('utf8') return quote(url, ':/%?&=')", "def create_search_url(self): \"\"\" Generates (urlencoded) query string from stored key-values tuples :returns: A string containing all arguments in a url-encoded format \"\"\" url = '?' for key, value in self.arguments.items(): url += '%s=%s&' % (quote_plus(key), quote_plus(value)) self.url = url[:-1] return self.url", "def add_params_to_url(url, params): \"\"\"Adds params to url :param url: Url :param params: Params to add :return: original url with new params \"\"\" url_parts = list(urlparse.urlparse(url)) # get url parts query = dict(urlparse.parse_qsl(url_parts[4])) # get url query query.update(params) # add new params url_parts[4] = urlencode(query) return urlparse.urlunparse(url_parts)"], "rank": 1}
{"nl": "how to check whether a string is int in python", "code": ["def _isint(string): \"\"\" >>> _isint(\"123\") True >>> _isint(\"123.45\") False \"\"\" return type(string) is int or \\ (isinstance(string, _binary_type) or isinstance(string, _text_type)) and \\ _isconvertible(int, string)", "def check_int(integer): \"\"\" Check if number is integer or not. :param integer: Number as str :return: Boolean \"\"\" if not isinstance(integer, str): return False if integer[0] in ('-', '+'): return integer[1:].isdigit() return integer.isdigit()", "def is_int(string): \"\"\" Checks if a string is an integer. If the string value is an integer return True, otherwise return False. Args: string: a string to test. Returns: boolean \"\"\" try: a = float(string) b = int(a) except ValueError: return False else: return a == b", "def is_int_type(val): \"\"\"Return True if `val` is of integer type.\"\"\" try: # Python 2 return isinstance(val, (int, long)) except NameError: # Python 3 return isinstance(val, int)", "def is_int(value): \"\"\"Return `True` if ``value`` is an integer.\"\"\" if isinstance(value, bool): return False try: int(value) return True except (ValueError, TypeError): return False"], "rank": 1}
{"nl": "python 3 change permission of file chmod", "code": ["def chmod(f): \"\"\" change mod to writeable \"\"\" try: os.chmod(f, S_IWRITE) # windows (cover all) except Exception as e: pass try: os.chmod(f, 0o777) # *nix except Exception as e: pass", "def chmod_plus_w(path): \"\"\"Equivalent of unix `chmod +w path`\"\"\" path_mode = os.stat(path).st_mode path_mode &= int('777', 8) path_mode |= stat.S_IWRITE os.chmod(path, path_mode)", "def chmod(scope, filename, mode): \"\"\" Changes the permissions of the given file (or list of files) to the given mode. You probably want to use an octal representation for the integer, e.g. \"chmod(myfile, 0644)\". :type filename: string :param filename: A filename. :type mode: int :param mode: The access permissions. \"\"\" for file in filename: os.chmod(file, mode[0]) return True", "def add_exec_permission_to(target_file): \"\"\"Add executable permissions to the file :param target_file: the target file whose permission to be changed \"\"\" mode = os.stat(target_file).st_mode os.chmod(target_file, mode | stat.S_IXUSR)", "def chmod_add_excute(filename): \"\"\" Adds execute permission to file. :param filename: :return: \"\"\" st = os.stat(filename) os.chmod(filename, st.st_mode | stat.S_IEXEC)"], "rank": 4}
{"nl": "python gaussian filter array", "code": ["def smooth_gaussian(image, sigma=1): \"\"\"Returns Gaussian smoothed image. :param image: numpy array or :class:`jicimagelib.image.Image` :param sigma: standard deviation :returns: :class:`jicimagelib.image.Image` \"\"\" return scipy.ndimage.filters.gaussian_filter(image, sigma=sigma, mode=\"nearest\")", "def gaussian_kernel(gstd): \"\"\"Generate odd sized truncated Gaussian The generated filter kernel has a cutoff at $3\\sigma$ and is normalized to sum to 1 Parameters ------------- gstd : float Standard deviation of filter Returns ------------- g : ndarray Array with kernel coefficients \"\"\" Nc = np.ceil(gstd*3)*2+1 x = np.linspace(-(Nc-1)/2,(Nc-1)/2,Nc,endpoint=True) g = np.exp(-.5*((x/gstd)**2)) g = g/np.sum(g) return g", "def _gaussian_function(self, datalength: int, values: np.ndarray, height: int, index: int) -> np.ndarray: \"\"\" i'th Regression Model Gaussian :param: len(x) :param: x values :param: height of gaussian :param: position of gaussian :return: gaussian bumps over domain \"\"\" return height * np.exp(-(1 / (self.spread_number * datalength)) * (values - ((datalength / self.function_number) * index)) ** 2)", "def gaussian_variogram_model(m, d): \"\"\"Gaussian model, m is [psill, range, nugget]\"\"\" psill = float(m[0]) range_ = float(m[1]) nugget = float(m[2]) return psill * (1. - np.exp(-d**2./(range_*4./7.)**2.)) + nugget", "def kernel(self, spread=1): \"\"\" This will return whatever kind of kernel we want to use. Must have signature (ndarray size NxM, ndarray size 1xM) -> ndarray size Nx1 \"\"\" # TODO: use self.kernel_type to choose function def gaussian(data, pixel): return mvn.pdf(data, mean=pixel, cov=spread) return gaussian"], "rank": 1}
{"nl": "how to cut off a calculated number to two decimals in python", "code": ["def price_rounding(price, decimals=2): \"\"\"Takes a decimal price and rounds to a number of decimal places\"\"\" try: exponent = D('.' + decimals * '0') except InvalidOperation: # Currencies with no decimal places, ex. JPY, HUF exponent = D() return price.quantize(exponent, rounding=ROUND_UP)", "def truncate(value: Decimal, n_digits: int) -> Decimal: \"\"\"Truncates a value to a number of decimals places\"\"\" return Decimal(math.trunc(value * (10 ** n_digits))) / (10 ** n_digits)", "def round_to_x_digits(number, digits): \"\"\" Returns 'number' rounded to 'digits' digits. \"\"\" return round(number * math.pow(10, digits)) / math.pow(10, digits)", "def trim_decimals(s, precision=-3): \"\"\" Convert from scientific notation using precision \"\"\" encoded = s.encode('ascii', 'ignore') str_val = \"\" if six.PY3: str_val = str(encoded, encoding='ascii', errors='ignore')[:precision] else: # If precision is 0, this must be handled seperately if precision == 0: str_val = str(encoded) else: str_val = str(encoded)[:precision] if len(str_val) > 0: return float(str_val) else: return 0", "def get_decimal_quantum(precision): \"\"\"Return minimal quantum of a number, as defined by precision.\"\"\" assert isinstance(precision, (int, decimal.Decimal)) return decimal.Decimal(10) ** (-precision)"], "rank": 2}
{"nl": "replace many value at once in python", "code": ["def replace(s, replace): \"\"\"Replace multiple values in a string\"\"\" for r in replace: s = s.replace(*r) return s", "def replace_one(self, replacement): \"\"\"Replace one entire document matching the selector criteria. :Parameters: - `replacement` (dict): the replacement document \"\"\" self.__bulk.add_replace(self.__selector, replacement, upsert=True, collation=self.__collation)", "def forceupdate(self, *args, **kw): \"\"\"Like a bulk :meth:`forceput`.\"\"\" self._update(False, self._ON_DUP_OVERWRITE, *args, **kw)", "def replace_in_list(stringlist: Iterable[str], replacedict: Dict[str, str]) -> List[str]: \"\"\" Returns a list produced by applying :func:`multiple_replace` to every string in ``stringlist``. Args: stringlist: list of source strings replacedict: dictionary mapping \"original\" to \"replacement\" strings Returns: list of final strings \"\"\" newlist = [] for fromstring in stringlist: newlist.append(multiple_replace(fromstring, replacedict)) return newlist", "def remove_from_string(string, values): \"\"\" Parameters ---------- string: values: Returns ------- \"\"\" for v in values: string = string.replace(v, '') return string"], "rank": 20}
{"nl": "python numpy conver to float64", "code": ["def _maybe_cast_to_float64(da): \"\"\"Cast DataArrays to np.float64 if they are of type np.float32. Parameters ---------- da : xr.DataArray Input DataArray Returns ------- DataArray \"\"\" if da.dtype == np.float32: logging.warning('Datapoints were stored using the np.float32 datatype.' 'For accurate reduction operations using bottleneck, ' 'datapoints are being cast to the np.float64 datatype.' ' For more information see: https://github.com/pydata/' 'xarray/issues/1346') return da.astype(np.float64) else: return da", "def cfloat64_array_to_numpy(cptr, length): \"\"\"Convert a ctypes double pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_double)): return np.fromiter(cptr, dtype=np.float64, count=length) else: raise RuntimeError('Expected double pointer')", "def ensure_dtype_float(x, default=np.float64): r\"\"\"Makes sure that x is type of float \"\"\" if isinstance(x, np.ndarray): if x.dtype.kind == 'f': return x elif x.dtype.kind == 'i': return x.astype(default) else: raise TypeError('x is of type '+str(x.dtype)+' that cannot be converted to float') else: raise TypeError('x is not an array')", "def cfloat32_array_to_numpy(cptr, length): \"\"\"Convert a ctypes float pointer array to a numpy array.\"\"\" if isinstance(cptr, ctypes.POINTER(ctypes.c_float)): return np.fromiter(cptr, dtype=np.float32, count=length) else: raise RuntimeError('Expected float pointer')", "def c2f(r, i, ctype_name): \"\"\" Convert strings to complex number instance with specified numpy type. \"\"\" ftype = c2f_dict[ctype_name] return np.typeDict[ctype_name](ftype(r) + 1j * ftype(i))"], "rank": 9}
{"nl": "python how to rank a list object", "code": ["def rank(self): \"\"\"how high in sorted list each key is. inverse permutation of sorter, such that sorted[rank]==keys\"\"\" r = np.empty(self.size, np.int) r[self.sorter] = np.arange(self.size) return r", "def __sort_up(self): \"\"\"Sort the updatable objects according to ascending order\"\"\" if self.__do_need_sort_up: self.__up_objects.sort(key=cmp_to_key(self.__up_cmp)) self.__do_need_sort_up = False", "def sort_by_name(self): \"\"\"Sort list elements by name.\"\"\" super(JSSObjectList, self).sort(key=lambda k: k.name)", "def count_list(the_list): \"\"\" Generates a count of the number of times each unique item appears in a list \"\"\" count = the_list.count result = [(item, count(item)) for item in set(the_list)] result.sort() return result", "def csort(objs, key): \"\"\"Order-preserving sorting function.\"\"\" idxs = dict((obj, i) for (i, obj) in enumerate(objs)) return sorted(objs, key=lambda obj: (key(obj), idxs[obj]))"], "rank": 11}
{"nl": "how to check if file doesn't exist in python", "code": ["def file_found(filename,force): \"\"\"Check if a file exists\"\"\" if os.path.exists(filename) and not force: logger.info(\"Found %s; skipping...\"%filename) return True else: return False", "def _file_exists(path, filename): \"\"\"Checks if the filename exists under the path.\"\"\" return os.path.isfile(os.path.join(path, filename))", "def is_valid_file(parser, arg): \"\"\"Check if arg is a valid file that already exists on the file system.\"\"\" arg = os.path.abspath(arg) if not os.path.exists(arg): parser.error(\"The file %s does not exist!\" % arg) else: return arg", "def do_files_exist(filenames): \"\"\"Whether any of the filenames exist.\"\"\" preexisting = [tf.io.gfile.exists(f) for f in filenames] return any(preexisting)", "def file_exists(fname): \"\"\"Check if a file exists and is non-empty. \"\"\" try: return fname and os.path.exists(fname) and os.path.getsize(fname) > 0 except OSError: return False"], "rank": 3}
{"nl": "how to get the parent directory in python", "code": ["def get_parent_dir(name): \"\"\"Get the parent directory of a filename.\"\"\" parent_dir = os.path.dirname(os.path.dirname(name)) if parent_dir: return parent_dir return os.path.abspath('.')", "def get_parent_folder_name(file_path): \"\"\"Finds parent folder of file :param file_path: path :return: Name of folder container \"\"\" return os.path.split(os.path.split(os.path.abspath(file_path))[0])[-1]", "def grandparent_path(self): \"\"\" return grandparent's path string \"\"\" return os.path.basename(os.path.join(self.path, '../..'))", "def go_to_parent_directory(self): \"\"\"Go to parent directory\"\"\" self.chdir(osp.abspath(osp.join(getcwd_or_home(), os.pardir)))", "def get_mac_dot_app_dir(directory): \"\"\"Returns parent directory of mac .app Args: directory (str): Current directory Returns: (str): Parent directory of mac .app \"\"\" return os.path.dirname(os.path.dirname(os.path.dirname(directory)))"], "rank": 1}
{"nl": "resize an image python pil", "code": ["def resize(self, size): \"\"\"Return a new Image instance with the given size.\"\"\" return Image(self.pil_image.resize(size, PIL.Image.ANTIALIAS))", "def resize_image(self, data, size): \"\"\" Resizes the given image to fit inside a box of the given size. \"\"\" from machina.core.compat import PILImage as Image image = Image.open(BytesIO(data)) # Resize! image.thumbnail(size, Image.ANTIALIAS) string = BytesIO() image.save(string, format='PNG') return string.getvalue()", "def resize(self): \"\"\" Get target size for a cropped image and do the resizing if we got anything usable. \"\"\" resized_size = self.get_resized_size() if not resized_size: return self.image = self.image.resize(resized_size, Image.ANTIALIAS)", "def resize_image_to_fit_width(image, dest_w): \"\"\" Resize and image to fit the passed in width, keeping the aspect ratio the same :param image: PIL.Image :param dest_w: The desired width \"\"\" scale_factor = dest_w / image.size[0] dest_h = image.size[1] * scale_factor scaled_image = image.resize((int(dest_w), int(dest_h)), PIL.Image.ANTIALIAS) return scaled_image", "def resize_by_area(img, size): \"\"\"image resize function used by quite a few image problems.\"\"\" return tf.to_int64( tf.image.resize_images(img, [size, size], tf.image.ResizeMethod.AREA))"], "rank": 1}
{"nl": "python boxplot data frame", "code": ["def compute_boxplot(self, series): \"\"\" Compute boxplot for given pandas Series. \"\"\" from matplotlib.cbook import boxplot_stats series = series[series.notnull()] if len(series.values) == 0: return {} elif not is_numeric_dtype(series): return self.non_numeric_stats(series) stats = boxplot_stats(list(series.values))[0] stats['count'] = len(series.values) stats['fliers'] = \"|\".join(map(str, stats['fliers'])) return stats", "def swap(self): \"\"\"Return the box (for horizontal graphs)\"\"\" self.xmin, self.ymin = self.ymin, self.xmin self.xmax, self.ymax = self.ymax, self.xmax", "def __init__(self, xmin=0, ymin=0, xmax=1, ymax=1): \"\"\" Create the chart bounds with min max horizontal and vertical values \"\"\" self._xmin = xmin self._ymin = ymin self._xmax = xmax self._ymax = ymax", "def oplot(self, x, y, **kw): \"\"\"generic plotting method, overplotting any existing plot \"\"\" self.panel.oplot(x, y, **kw)", "def point8_to_box(points): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points.reshape((-1, 4, 2)) minxy = p.min(axis=1) # nx2 maxxy = p.max(axis=1) # nx2 return np.concatenate((minxy, maxxy), axis=1)"], "rank": 1}
{"nl": "python glpk read from lp file", "code": ["def glpk_read_cplex(path): \"\"\"Reads cplex file and returns glpk problem. Returns ------- glp_prob A glpk problems (same type as returned by glp_create_prob) \"\"\" from swiglpk import glp_create_prob, glp_read_lp problem = glp_create_prob() glp_read_lp(problem, None, path) return problem", "def load(path): \"\"\"Loads a pushdb maintained in a properties file at the given path.\"\"\" with open(path, 'r') as props: properties = Properties.load(props) return PushDb(properties)", "def _windowsLdmodTargets(target, source, env, for_signature): \"\"\"Get targets for loadable modules.\"\"\" return _dllTargets(target, source, env, for_signature, 'LDMODULE')", "def parse(filename): \"\"\"Parse ASDL from the given file and return a Module node describing it.\"\"\" with open(filename) as f: parser = ASDLParser() return parser.parse(f.read())", "def _fast_read(self, infile): \"\"\"Function for fast reading from sensor files.\"\"\" infile.seek(0) return(int(infile.read().decode().strip()))"], "rank": 1}
{"nl": "python threadpool close join", "code": ["def join(self): \"\"\"Note that the Executor must be close()'d elsewhere, or join() will never return. \"\"\" self.inputfeeder_thread.join() self.pool.join() self.resulttracker_thread.join() self.failuretracker_thread.join()", "def terminate(self): \"\"\"Terminate the pool immediately.\"\"\" if self._pool is not None: self._pool.terminate() self._pool.join() self._pool = None", "def Join(self): \"\"\"Waits until all outstanding tasks are completed.\"\"\" for _ in range(self.JOIN_TIMEOUT_DECISECONDS): if self._queue.empty() and not self.busy_threads: return time.sleep(0.1) raise ValueError(\"Timeout during Join() for threadpool %s.\" % self.name)", "def join(self): \"\"\"Joins the coordinator thread and all worker threads.\"\"\" for thread in self.worker_threads: thread.join() WorkerThread.join(self)", "def wait_until_exit(self): \"\"\" Wait until all the threads are finished. \"\"\" [t.join() for t in self.threads] self.threads = list()"], "rank": 1}
{"nl": "python sklearn onehotencoder string values", "code": ["def one_hot_encoding(input_tensor, num_labels): \"\"\" One-hot encode labels from input \"\"\" xview = input_tensor.view(-1, 1).to(torch.long) onehot = torch.zeros(xview.size(0), num_labels, device=input_tensor.device, dtype=torch.float) onehot.scatter_(1, xview, 1) return onehot.view(list(input_tensor.shape) + [-1])", "def to_one_hot(dataY): \"\"\"Convert the vector of labels dataY into one-hot encoding. :param dataY: vector of labels :return: one-hot encoded labels \"\"\" nc = 1 + np.max(dataY) onehot = [np.zeros(nc, dtype=np.int8) for _ in dataY] for i, j in enumerate(dataY): onehot[i][j] = 1 return onehot", "def one_hot(x, size, dtype=np.float32): \"\"\"Make a n+1 dim one-hot array from n dim int-categorical array.\"\"\" return np.array(x[..., np.newaxis] == np.arange(size), dtype)", "def lmx_h1k_f64k(): \"\"\"HParams for training languagemodel_lm1b32k_packed. 880M Params.\"\"\" hparams = lmx_base() hparams.hidden_size = 1024 hparams.filter_size = 65536 hparams.batch_size = 2048 return hparams", "def one_hot2string(arr, vocab): \"\"\"Convert a one-hot encoded array back to string \"\"\" tokens = one_hot2token(arr) indexToLetter = _get_index_dict(vocab) return [''.join([indexToLetter[x] for x in row]) for row in tokens]"], "rank": 5}
{"nl": "python change to bytes", "code": ["def str2bytes(x): \"\"\"Convert input argument to bytes\"\"\" if type(x) is bytes: return x elif type(x) is str: return bytes([ ord(i) for i in x ]) else: return str2bytes(str(x))", "def to_bytes(value): \"\"\" str to bytes (py3k) \"\"\" vtype = type(value) if vtype == bytes or vtype == type(None): return value try: return vtype.encode(value) except UnicodeEncodeError: pass return value", "def u2b(string): \"\"\" unicode to bytes\"\"\" if ((PY2 and isinstance(string, unicode)) or ((not PY2) and isinstance(string, str))): return string.encode('utf-8') return string", "def tob(data, enc='utf8'): \"\"\" Convert anything to bytes \"\"\" return data.encode(enc) if isinstance(data, six.text_type) else bytes(data)", "def to_bytes(s, encoding=\"utf-8\"): \"\"\"Convert a string to bytes.\"\"\" if isinstance(s, six.binary_type): return s if six.PY3: return bytes(s, encoding) return s.encode(encoding)"], "rank": 6}
{"nl": "python json loads try", "code": ["def json_decode(data): \"\"\" Decodes the given JSON as primitives \"\"\" if isinstance(data, six.binary_type): data = data.decode('utf-8') return json.loads(data)", "def load(cls, fp, **kwargs): \"\"\"wrapper for :py:func:`json.load`\"\"\" json_obj = json.load(fp, **kwargs) return parse(cls, json_obj)", "def from_file(file_path) -> dict: \"\"\" Load JSON file \"\"\" with io.open(file_path, 'r', encoding='utf-8') as json_stream: return Json.parse(json_stream, True)", "def json(body, charset='utf-8', **kwargs): \"\"\"Takes JSON formatted data, converting it into native Python objects\"\"\" return json_converter.loads(text(body, charset=charset))", "def serialize_json_string(self, value): \"\"\" Tries to load an encoded json string back into an object :param json_string: :return: \"\"\" # Check if the value might be a json string if not isinstance(value, six.string_types): return value # Make sure it starts with a brace if not value.startswith('{') or value.startswith('['): return value # Try to load the string try: return json.loads(value) except: return value"], "rank": 4}
{"nl": "create an empty column in data frame python", "code": ["def add_blank_row(self, label): \"\"\" Add a blank row with only an index value to self.df. This is done inplace. \"\"\" col_labels = self.df.columns blank_item = pd.Series({}, index=col_labels, name=label) # use .loc to add in place (append won't do that) self.df.loc[blank_item.name] = blank_item return self.df", "def append_table(self, name, **kwargs): \"\"\"Create a new table.\"\"\" self.stack.append(Table(name, **kwargs))", "def update_table_row(self, table, row_idx): \"\"\"Add this instance as a row on a `astropy.table.Table` \"\"\" try: table[row_idx]['timestamp'] = self.timestamp table[row_idx]['status'] = self.status except IndexError: print(\"Index error\", len(table), row_idx)", "def _insert_row(self, i, index): \"\"\" Insert a new row in the Series. :param i: index location to insert :param index: index value to insert into the index list :return: nothing \"\"\" if i == len(self._index): self._add_row(index) else: self._index.insert(i, index) self._data.insert(i, None)", "def SetValue(self, row, col, value): \"\"\" Set value in the pandas DataFrame \"\"\" self.dataframe.iloc[row, col] = value"], "rank": 1}
{"nl": "remove whitespace at end of line in python", "code": ["def _delete_whitespace(self): \"\"\"Delete all whitespace from the end of the line.\"\"\" while isinstance(self._lines[-1], (self._Space, self._LineBreak, self._Indent)): del self._lines[-1]", "def fix_line_breaks(s): \"\"\" Convert \\r\\n and \\r to \\n chars. Strip any leading or trailing whitespace on each line. Remove blank lines. \"\"\" l = s.splitlines() x = [i.strip() for i in l] x = [i for i in x if i] # remove blank lines return \"\\n\".join(x)", "def clean(s): \"\"\"Removes trailing whitespace on each line.\"\"\" lines = [l.rstrip() for l in s.split('\\n')] return '\\n'.join(lines)", "def get_line_ending(line): \"\"\"Return line ending.\"\"\" non_whitespace_index = len(line.rstrip()) - len(line) if not non_whitespace_index: return '' else: return line[non_whitespace_index:]", "def text_remove_empty_lines(text): \"\"\" Whitespace normalization: - Strip empty lines - Strip trailing whitespace \"\"\" lines = [ line.rstrip() for line in text.splitlines() if line.strip() ] return \"\\n\".join(lines)"], "rank": 3}
{"nl": "python file opening modes", "code": ["def open_file(file, mode): \"\"\"Open a file. :arg file: file-like or path-like object. :arg str mode: ``mode`` argument for :func:`open`. \"\"\" if hasattr(file, \"read\"): return file if hasattr(file, \"open\"): return file.open(mode) return open(file, mode)", "def fopen(name, mode='r', buffering=-1): \"\"\"Similar to Python's built-in `open()` function.\"\"\" f = _fopen(name, mode, buffering) return _FileObjectThreadWithContext(f, mode, buffering)", "def fopenat(base_fd, path): \"\"\" Does openat read-only, then does fdopen to get a file object \"\"\" return os.fdopen(openat(base_fd, path, os.O_RDONLY), 'rb')", "def open_with_encoding(filename, encoding, mode='r'): \"\"\"Return opened file with a specific encoding.\"\"\" return io.open(filename, mode=mode, encoding=encoding, newline='')", "def copen(filepath, flag='r', encoding=None): \"\"\" FIXME: How to test this ? >>> c = copen(__file__) >>> c is not None True \"\"\" if encoding is None: encoding = locale.getdefaultlocale()[1] return codecs.open(filepath, flag, encoding)"], "rank": 1}
{"nl": "python code input prompt for questions", "code": ["def input(self, prompt, default=None, show_default=True): \"\"\"Provide a command prompt.\"\"\" return click.prompt(prompt, default=default, show_default=show_default)", "def prompt_yes_or_no(message): \"\"\" prompt_yes_or_no: Prompt user to reply with a y/n response Args: None Returns: None \"\"\" user_input = input(\"{} [y/n]:\".format(message)).lower() if user_input.startswith(\"y\"): return True elif user_input.startswith(\"n\"): return False else: return prompt_yes_or_no(message)", "def yn_prompt(msg, default=True): \"\"\" Prompts the user for yes or no. \"\"\" ret = custom_prompt(msg, [\"y\", \"n\"], \"y\" if default else \"n\") if ret == \"y\": return True return False", "def string_input(prompt=''): \"\"\"Python 3 input()/Python 2 raw_input()\"\"\" v = sys.version[0] if v == '3': return input(prompt) else: return raw_input(prompt)", "def getpass(self, prompt, default=None): \"\"\"Provide a password prompt.\"\"\" return click.prompt(prompt, hide_input=True, default=default)"], "rank": 4}
{"nl": "python parse a log file that is logging", "code": ["def parse(self): \"\"\" Parse file specified by constructor. \"\"\" f = open(self.parse_log_path, \"r\") self.parse2(f) f.close()", "def load_config(filename=\"logging.ini\", *args, **kwargs): \"\"\" Load logger config from file Keyword arguments: filename -- configuration filename (Default: \"logging.ini\") *args -- options passed to fileConfig **kwargs -- options passed to fileConfigg \"\"\" logging.config.fileConfig(filename, *args, **kwargs)", "def setupLogFile(self): \"\"\"Set up the logging file for a new session- include date and some whitespace\"\"\" self.logWrite(\"\\n###############################################\") self.logWrite(\"calcpkg.py log from \" + str(datetime.datetime.now())) self.changeLogging(True)", "def __init__(self, filename, mode, encoding=None): \"\"\"Use the specified filename for streamed logging.\"\"\" FileHandler.__init__(self, filename, mode, encoding) self.mode = mode self.encoding = encoding", "def getLinesFromLogFile(stream): \"\"\" Returns all lines written to the passed in stream \"\"\" stream.flush() stream.seek(0) lines = stream.readlines() return lines"], "rank": 1}
{"nl": "remove an entry from a dict python", "code": ["def __delitem__ (self, key): \"\"\"Remove key from dict.\"\"\" self._keys.remove(key) super(ListDict, self).__delitem__(key)", "def pop (self, key): \"\"\"Remove key from dict and return value.\"\"\" if key in self._keys: self._keys.remove(key) super(ListDict, self).pop(key)", "def dictlist_wipe_key(dict_list: Iterable[Dict], key: str) -> None: \"\"\" Process an iterable of dictionaries. For each dictionary ``d``, delete ``d[key]`` if it exists. \"\"\" for d in dict_list: d.pop(key, None)", "def remove(parent, idx): \"\"\"Remove a value from a dict.\"\"\" if isinstance(parent, dict): del parent[idx] elif isinstance(parent, list): del parent[int(idx)] else: raise JSONPathError(\"Invalid path for operation\")", "def __delitem__(self, key): \"\"\"Remove item with given key from the mapping. Runs in O(n), unless removing last item, then in O(1). \"\"\" index, value = self._dict.pop(key) key2, value2 = self._list.pop(index) assert key == key2 assert value is value2 self._fix_indices_after_delete(index)"], "rank": 3}
{"nl": "new line statemnt pythong write", "code": ["def write_line(self, line, count=1): \"\"\"writes the line and count newlines after the line\"\"\" self.write(line) self.write_newlines(count)", "def _write_separator(self): \"\"\" Inserts a horizontal (commented) line tot the generated code. \"\"\" tmp = self._page_width - ((4 * self.__indent_level) + 2) self._write_line('# ' + ('-' * tmp))", "def end_block(self): \"\"\"Ends an indentation block, leaving an empty line afterwards\"\"\" self.current_indent -= 1 # If we did not add a new line automatically yet, now it's the time! if not self.auto_added_line: self.writeln() self.auto_added_line = True", "def comment (self, s, **args): \"\"\"Write GML comment.\"\"\" self.writeln(s=u'comment \"%s\"' % s, **args)", "def comment (self, s, **args): \"\"\"Write DOT comment.\"\"\" self.write(u\"// \") self.writeln(s=s, **args)"], "rank": 1}
{"nl": "python view as series column format string", "code": ["def _series_col_letter(self, series): \"\"\" The letter of the Excel worksheet column in which the data for a series appears. \"\"\" column_number = 1 + series.categories.depth + series.index return self._column_reference(column_number)", "def format(x, format): \"\"\"Uses http://www.cplusplus.com/reference/string/to_string/ for formatting\"\"\" # don't change the dtype, otherwise for each block the dtype may be different (string length) sl = vaex.strings.format(x, format) return column.ColumnStringArrow(sl.bytes, sl.indices, sl.length, sl.offset, string_sequence=sl)", "def x_values_ref(self, series): \"\"\" The Excel worksheet reference to the X values for this chart (not including the column label). \"\"\" top_row = self.series_table_row_offset(series) + 2 bottom_row = top_row + len(series) - 1 return \"Sheet1!$A$%d:$A$%d\" % (top_row, bottom_row)", "def from_series(cls, series): \"\"\"Convert a pandas.Series into an xarray.DataArray. If the series's index is a MultiIndex, it will be expanded into a tensor product of one-dimensional coordinates (filling in missing values with NaN). Thus this operation should be the inverse of the `to_series` method. \"\"\" # TODO: add a 'name' parameter name = series.name df = pd.DataFrame({name: series}) ds = Dataset.from_dataframe(df) return ds[name]", "def to_monthly(series, method='ffill', how='end'): \"\"\" Convenience method that wraps asfreq_actual with 'M' param (method='ffill', how='end'). \"\"\" return series.asfreq_actual('M', method=method, how=how)"], "rank": 2}
{"nl": "python select not null column values", "code": ["def selectnotnone(table, field, complement=False): \"\"\"Select rows where the given field is not `None`.\"\"\" return select(table, field, lambda v: v is not None, complement=complement)", "def selectnone(table, field, complement=False): \"\"\"Select rows where the given field is `None`.\"\"\" return select(table, field, lambda v: v is None, complement=complement)", "def selectnotin(table, field, value, complement=False): \"\"\"Select rows where the given field is not a member of the given value.\"\"\" return select(table, field, lambda v: v not in value, complement=complement)", "def _notnull(expr): \"\"\" Return a sequence or scalar according to the input indicating if the values are not null. :param expr: sequence or scalar :return: sequence or scalar \"\"\" if isinstance(expr, SequenceExpr): return NotNull(_input=expr, _data_type=types.boolean) elif isinstance(expr, Scalar): return NotNull(_input=expr, _value_type=types.boolean)", "def selecttrue(table, field, complement=False): \"\"\"Select rows where the given field evaluates `True`.\"\"\" return select(table, field, lambda v: bool(v), complement=complement)"], "rank": 1}
{"nl": "how do functions in python know the parametr type", "code": ["def parse_parameter(value): \"\"\" @return: The best approximation of a type of the given value. \"\"\" if any((isinstance(value, float), isinstance(value, int), isinstance(value, bool))): return value try: return int(value) except ValueError: try: return float(value) except ValueError: if value in string_aliases.true_boolean_aliases: return True elif value in string_aliases.false_boolean_aliases: return False else: return str(value)", "def return_type(type_name, formatter=None): \"\"\"Specify that this function returns a typed value. Args: type_name (str): A type name known to the global typedargs type system formatter (str): An optional name of a formatting function specified for the type given in type_name. \"\"\" def _returns(func): annotated(func) func.metadata.typed_returnvalue(type_name, formatter) return func return _returns", "def get_function_class(function_name): \"\"\" Return the type for the requested function :param function_name: the function to return :return: the type for that function (i.e., this is a class, not an instance) \"\"\" if function_name in _known_functions: return _known_functions[function_name] else: raise UnknownFunction(\"Function %s is not known. Known functions are: %s\" % (function_name, \",\".join(_known_functions.keys())))", "def _guess_type(val): \"\"\"Guess the input type of the parameter based off the default value, if unknown use text\"\"\" if isinstance(val, bool): return \"choice\" elif isinstance(val, int): return \"number\" elif isinstance(val, float): return \"number\" elif isinstance(val, str): return \"text\" elif hasattr(val, 'read'): return \"file\" else: return \"text\"", "def get_Callable_args_res(clb): \"\"\"Python version independent function to obtain the parameters of a typing.Callable object. Returns as tuple: args, result. Tested with CPython 2.7, 3.5, 3.6 and Jython 2.7.1. \"\"\" try: return clb.__args__, clb.__result__ except AttributeError: # Python 3.6 return clb.__args__[:-1], clb.__args__[-1]"], "rank": 324}
{"nl": "cursor positioning python windows", "code": ["def get_cursor(self): \"\"\"Return the virtual cursor position. The cursor can be moved with the :any:`move` method. Returns: Tuple[int, int]: The (x, y) coordinate of where :any:`print_str` will continue from. .. seealso:: :any:move` \"\"\" x, y = self._cursor width, height = self.parent.get_size() while x >= width: x -= width y += 1 if y >= height and self.scrollMode == 'scroll': y = height - 1 return x, y", "def set_cursor_position(self, position): \"\"\"Set cursor position\"\"\" position = self.get_position(position) cursor = self.textCursor() cursor.setPosition(position) self.setTextCursor(cursor) self.ensureCursorVisible()", "def move_to(self, ypos, xpos): \"\"\" move the cursor to the given co-ordinates. Co-ordinates are 1 based, as listed in the status area of the terminal. \"\"\" # the screen's co-ordinates are 1 based, but the command is 0 based xpos -= 1 ypos -= 1 self.exec_command(\"MoveCursor({0}, {1})\".format(ypos, xpos).encode(\"ascii\"))", "def move(self, x, y): \"\"\"Move the virtual cursor. Args: x (int): x-coordinate to place the cursor. y (int): y-coordinate to place the cursor. .. seealso:: :any:`get_cursor`, :any:`print_str`, :any:`write` \"\"\" self._cursor = self._normalizePoint(x, y)", "def update_cursor_position(self, line, index): \"\"\"Update cursor position.\"\"\" value = 'Line {}, Col {}'.format(line + 1, index + 1) self.set_value(value)"], "rank": 9}
{"nl": "python how to equally space points in an ellipse", "code": ["def create_ellipse(width,height,angle): \"\"\"Create parametric ellipse from 200 points.\"\"\" angle = angle / 180.0 * np.pi thetas = np.linspace(0,2*np.pi,200) a = width / 2.0 b = height / 2.0 x = a*np.cos(thetas)*np.cos(angle) - b*np.sin(thetas)*np.sin(angle) y = a*np.cos(thetas)*np.sin(angle) + b*np.sin(thetas)*np.cos(angle) z = np.zeros(thetas.shape) return np.vstack((x,y,z)).T", "def getTopRight(self): \"\"\" Retrieves a tuple with the x,y coordinates of the upper right point of the ellipse. Requires the radius and the coordinates to be numbers \"\"\" return (float(self.get_cx()) + float(self.get_rx()), float(self.get_cy()) + float(self.get_ry()))", "def signed_area(coords): \"\"\"Return the signed area enclosed by a ring using the linear time algorithm. A value >= 0 indicates a counter-clockwise oriented ring. \"\"\" xs, ys = map(list, zip(*coords)) xs.append(xs[1]) ys.append(ys[1]) return sum(xs[i]*(ys[i+1]-ys[i-1]) for i in range(1, len(coords)))/2.0", "def line_segment(X0, X1): r\"\"\" Calculate the voxel coordinates of a straight line between the two given end points Parameters ---------- X0 and X1 : array_like The [x, y] or [x, y, z] coordinates of the start and end points of the line. Returns ------- coords : list of lists A list of lists containing the X, Y, and Z coordinates of all voxels that should be drawn between the start and end points to create a solid line. \"\"\" X0 = sp.around(X0).astype(int) X1 = sp.around(X1).astype(int) if len(X0) == 3: L = sp.amax(sp.absolute([[X1[0]-X0[0]], [X1[1]-X0[1]], [X1[2]-X0[2]]])) + 1 x = sp.rint(sp.linspace(X0[0], X1[0], L)).astype(int) y = sp.rint(sp.linspace(X0[1], X1[1], L)).astype(int) z = sp.rint(sp.linspace(X0[2], X1[2], L)).astype(int) return [x, y, z] else: L = sp.amax(sp.absolute([[X1[0]-X0[0]], [X1[1]-X0[1]]])) + 1 x = sp.rint(sp.linspace(X0[0], X1[0], L)).astype(int) y = sp.rint(sp.linspace(X0[1], X1[1], L)).astype(int) return [x, y]", "def _linepoint(self, t, x0, y0, x1, y1): \"\"\" Returns coordinates for point at t on the line. Calculates the coordinates of x and y for a point at t on a straight line. The t parameter is a number between 0.0 and 1.0, x0 and y0 define the starting point of the line, x1 and y1 the ending point of the line. \"\"\" # Originally from nodebox-gl out_x = x0 + t * (x1 - x0) out_y = y0 + t * (y1 - y0) return (out_x, out_y)"], "rank": 1}
{"nl": "how to see how similar two images are in python", "code": ["def _sim_fill(r1, r2, imsize): \"\"\" calculate the fill similarity over the image \"\"\" bbsize = ( (max(r1[\"max_x\"], r2[\"max_x\"]) - min(r1[\"min_x\"], r2[\"min_x\"])) * (max(r1[\"max_y\"], r2[\"max_y\"]) - min(r1[\"min_y\"], r2[\"min_y\"])) ) return 1.0 - (bbsize - r1[\"size\"] - r2[\"size\"]) / imsize", "def is_same_shape(self, other_im, check_channels=False): \"\"\" Checks if two images have the same height and width (and optionally channels). Parameters ---------- other_im : :obj:`Image` image to compare check_channels : bool whether or not to check equality of the channels Returns ------- bool True if the images are the same shape, False otherwise \"\"\" if self.height == other_im.height and self.width == other_im.width: if check_channels and self.channels != other_im.channels: return False return True return False", "def normalized_distance(self, image): \"\"\"Calculates the distance of a given image to the original image. Parameters ---------- image : `numpy.ndarray` The image that should be compared to the original image. Returns ------- :class:`Distance` The distance between the given image and the original image. \"\"\" return self.__distance( self.__original_image_for_distance, image, bounds=self.bounds())", "def compute_ssim(image1, image2, gaussian_kernel_sigma=1.5, gaussian_kernel_width=11): \"\"\"Computes SSIM. Args: im1: First PIL Image object to compare. im2: Second PIL Image object to compare. Returns: SSIM float value. \"\"\" gaussian_kernel_1d = get_gaussian_kernel( gaussian_kernel_width, gaussian_kernel_sigma) return SSIM(image1, gaussian_kernel_1d).ssim_value(image2)", "def tanimoto_set_similarity(x: Iterable[X], y: Iterable[X]) -> float: \"\"\"Calculate the tanimoto set similarity.\"\"\" a, b = set(x), set(y) union = a | b if not union: return 0.0 return len(a & b) / len(union)"], "rank": 1}
{"nl": "how to make a input to have no spaces in python\\", "code": ["def normalize_value(text): \"\"\" This removes newlines and multiple spaces from a string. \"\"\" result = text.replace('\\n', ' ') result = re.subn('[ ]{2,}', ' ', result)[0] return result", "def strip_spaces(s): \"\"\" Strip excess spaces from a string \"\"\" return u\" \".join([c for c in s.split(u' ') if c])", "def strip_spaces(x): \"\"\" Strips spaces :param x: :return: \"\"\" x = x.replace(b' ', b'') x = x.replace(b'\\t', b'') return x", "def remove_blank_lines(string): \"\"\" Removes all blank lines in @string -> #str without blank lines \"\"\" return \"\\n\".join(line for line in string.split(\"\\n\") if len(line.strip()))", "def indent(s, spaces=4): \"\"\" Inserts `spaces` after each string of new lines in `s` and before the start of the string. \"\"\" new = re.sub('(\\n+)', '\\\\1%s' % (' ' * spaces), s) return (' ' * spaces) + new.strip()"], "rank": 96}
{"nl": "next line to read in python", "code": ["def readline(self): \"\"\"Get the next line including the newline or '' on EOF.\"\"\" self.lineno += 1 if self._buffer: return self._buffer.pop() else: return self.input.readline()", "def next(self): \"\"\"Provides hook for Python2 iterator functionality.\"\"\" _LOGGER.debug(\"reading next\") if self.closed: _LOGGER.debug(\"stream is closed\") raise StopIteration() line = self.readline() if not line: _LOGGER.debug(\"nothing more to read\") raise StopIteration() return line", "def __next__(self): \"\"\" :return: a pair (1-based line number in the input, row) \"\"\" # Retrieve the row, thereby incrementing the line number: row = super(UnicodeReaderWithLineNumber, self).__next__() return self.lineno + 1, row", "def next (self): # File-like object. \"\"\"This is to support iterators over a file-like object. \"\"\" result = self.readline() if result == self._empty_buffer: raise StopIteration return result", "def rAsciiLine(ifile): \"\"\"Returns the next non-blank line in an ASCII file.\"\"\" _line = ifile.readline().strip() while len(_line) == 0: _line = ifile.readline().strip() return _line"], "rank": 3}
{"nl": "move an item in list to front python", "code": ["def list_move_to_front(l,value='other'): \"\"\"if the value is in the list, move it to the front and return it.\"\"\" l=list(l) if value in l: l.remove(value) l.insert(0,value) return l", "def insort_no_dup(lst, item): \"\"\" If item is not in lst, add item to list at its sorted position \"\"\" import bisect ix = bisect.bisect_left(lst, item) if lst[ix] != item: lst[ix:ix] = [item]", "def push(self, el): \"\"\" Put a new element in the queue. \"\"\" count = next(self.counter) heapq.heappush(self._queue, (el, count))", "def up(self): \"\"\"Moves the layer up in the stacking order. \"\"\" i = self.index() if i != None: del self.canvas.layers[i] i = min(len(self.canvas.layers), i+1) self.canvas.layers.insert(i, self)", "def insert_ordered(value, array): \"\"\" This will insert the value into the array, keeping it sorted, and returning the index where it was inserted \"\"\" index = 0 # search for the last array item that value is larger than for n in range(0,len(array)): if value >= array[n]: index = n+1 array.insert(index, value) return index"], "rank": 1}
{"nl": "python wrap (s,w) print", "code": ["def raw_print(*args, **kw): \"\"\"Raw print to sys.__stdout__, otherwise identical interface to print().\"\"\" print(*args, sep=kw.get('sep', ' '), end=kw.get('end', '\\n'), file=sys.__stdout__) sys.__stdout__.flush()", "def replace_print(fileobj=sys.stderr): \"\"\"Sys.out replacer, by default with stderr. Use it like this: with replace_print_with(fileobj): print \"hello\" # writes to the file print \"done\" # prints to stdout Args: fileobj: a file object to replace stdout. Yields: The printer. \"\"\" printer = _Printer(fileobj) previous_stdout = sys.stdout sys.stdout = printer try: yield printer finally: sys.stdout = previous_stdout", "def pstd(self, *args, **kwargs): \"\"\" Console to STDOUT \"\"\" kwargs['file'] = self.out self.print(*args, **kwargs) sys.stdout.flush()", "def print_out(self, *lst): \"\"\" Print list of strings to the predefined stdout. \"\"\" self.print2file(self.stdout, True, True, *lst)", "def _stdout_raw(self, s): \"\"\"Writes the string to stdout\"\"\" print(s, end='', file=sys.stdout) sys.stdout.flush()"], "rank": 138}
{"nl": "how to check if missing values are blanks or nan or none in python", "code": ["def _isnan(self): \"\"\" Return if each value is NaN. \"\"\" if self._can_hold_na: return isna(self) else: # shouldn't reach to this condition by checking hasnans beforehand values = np.empty(len(self), dtype=np.bool_) values.fill(False) return values", "def warn_if_nans_exist(X): \"\"\"Warn if nans exist in a numpy array.\"\"\" null_count = count_rows_with_nans(X) total = len(X) percent = 100 * null_count / total if null_count > 0: warning_message = \\ 'Warning! Found {} rows of {} ({:0.2f}%) with nan values. Only ' \\ 'complete rows will be plotted.'.format(null_count, total, percent) warnings.warn(warning_message, DataWarning)", "def is_not_null(df: DataFrame, col_name: str) -> bool: \"\"\" Return ``True`` if the given DataFrame has a column of the given name (string), and there exists at least one non-NaN value in that column; return ``False`` otherwise. \"\"\" if ( isinstance(df, pd.DataFrame) and col_name in df.columns and df[col_name].notnull().any() ): return True else: return False", "def remove_na_arraylike(arr): \"\"\" Return array-like containing only true/non-NaN values, possibly empty. \"\"\" if is_extension_array_dtype(arr): return arr[notna(arr)] else: return arr[notna(lib.values_from_object(arr))]", "def clean_dataframe(df): \"\"\"Fill NaNs with the previous value, the next value or if all are NaN then 1.0\"\"\" df = df.fillna(method='ffill') df = df.fillna(0.0) return df"], "rank": 2}
{"nl": "python to get the indices of bin edges", "code": ["def val_to_bin(edges, x): \"\"\"Convert axis coordinate to bin index.\"\"\" ibin = np.digitize(np.array(x, ndmin=1), edges) - 1 return ibin", "def getEdges(npArr): \"\"\"get np array of bin edges\"\"\" edges = np.concatenate(([0], npArr[:,0] + npArr[:,2])) return np.array([Decimal(str(i)) for i in edges])", "def get_bin_indices(self, values): \"\"\"Returns index tuple in histogram of bin which contains value\"\"\" return tuple([self.get_axis_bin_index(values[ax_i], ax_i) for ax_i in range(self.dimensions)])", "def get_bin_edges_from_axis(axis) -> np.ndarray: \"\"\" Get bin edges from a ROOT hist axis. Note: Doesn't include over- or underflow bins! Args: axis (ROOT.TAxis): Axis from which the bin edges should be extracted. Returns: Array containing the bin edges. \"\"\" # Don't include over- or underflow bins bins = range(1, axis.GetNbins() + 1) # Bin edges bin_edges = np.empty(len(bins) + 1) bin_edges[:-1] = [axis.GetBinLowEdge(i) for i in bins] bin_edges[-1] = axis.GetBinUpEdge(axis.GetNbins()) return bin_edges", "def lon_lat_bins(bb, coord_bin_width): \"\"\" Define bin edges for disaggregation histograms. Given bins data as provided by :func:`collect_bin_data`, this function finds edges of histograms, taking into account maximum and minimum values of magnitude, distance and coordinates as well as requested sizes/numbers of bins. \"\"\" west, south, east, north = bb west = numpy.floor(west / coord_bin_width) * coord_bin_width east = numpy.ceil(east / coord_bin_width) * coord_bin_width lon_extent = get_longitudinal_extent(west, east) lon_bins, _, _ = npoints_between( west, 0, 0, east, 0, 0, numpy.round(lon_extent / coord_bin_width + 1)) lat_bins = coord_bin_width * numpy.arange( int(numpy.floor(south / coord_bin_width)), int(numpy.ceil(north / coord_bin_width) + 1)) return lon_bins, lat_bins"], "rank": 1}
{"nl": "python comma separated value", "code": ["def vectorize(values): \"\"\" Takes a value or list of values and returns a single result, joined by \",\" if necessary. \"\"\" if isinstance(values, list): return ','.join(str(v) for v in values) return values", "def list_to_csv(value): \"\"\" Converts list to string with comma separated values. For string is no-op. \"\"\" if isinstance(value, (list, tuple, set)): value = \",\".join(value) return value", "def seq_to_str(obj, sep=\",\"): \"\"\" Given a sequence convert it to a comma separated string. If, however, the argument is a single object, return its string representation. \"\"\" if isinstance(obj, string_classes): return obj elif isinstance(obj, (list, tuple)): return sep.join([str(x) for x in obj]) else: return str(obj)", "def _return_comma_list(self, l): \"\"\" get a list and return a string with comma separated list values Examples ['to', 'ta'] will return 'to,ta'. \"\"\" if isinstance(l, (text_type, int)): return l if not isinstance(l, list): raise TypeError(l, ' should be a list of integers, \\ not {0}'.format(type(l))) str_ids = ','.join(str(i) for i in l) return str_ids", "def split_comma_argument(comma_sep_str): \"\"\"Split a comma separated option into a list.\"\"\" terms = [] for term in comma_sep_str.split(','): if term: terms.append(term) return terms"], "rank": 2}
{"nl": "python requests disable ssl certificate verification", "code": ["def disable_insecure_request_warning(): \"\"\"Suppress warning about untrusted SSL certificate.\"\"\" import requests from requests.packages.urllib3.exceptions import InsecureRequestWarning requests.packages.urllib3.disable_warnings(InsecureRequestWarning)", "def inject_into_urllib3(): \"\"\" Monkey-patch urllib3 with SecureTransport-backed SSL-support. \"\"\" util.ssl_.SSLContext = SecureTransportContext util.HAS_SNI = HAS_SNI util.ssl_.HAS_SNI = HAS_SNI util.IS_SECURETRANSPORT = True util.ssl_.IS_SECURETRANSPORT = True", "def disable_cert_validation(): \"\"\"Context manager to temporarily disable certificate validation in the standard SSL library. Note: This should not be used in production code but is sometimes useful for troubleshooting certificate validation issues. By design, the standard SSL library does not provide a way to disable verification of the server side certificate. However, a patch to disable validation is described by the library developers. This context manager allows applying the patch for specific sections of code. \"\"\" current_context = ssl._create_default_https_context ssl._create_default_https_context = ssl._create_unverified_context try: yield finally: ssl._create_default_https_context = current_context", "def enable_ssl(self, *args, **kwargs): \"\"\" Transforms the regular socket.socket to an ssl.SSLSocket for secure connections. Any arguments are passed to ssl.wrap_socket: http://docs.python.org/dev/library/ssl.html#ssl.wrap_socket \"\"\" if self.handshake_sent: raise SSLError('can only enable SSL before handshake') self.secure = True self.sock = ssl.wrap_socket(self.sock, *args, **kwargs)", "def _shutdown_transport(self): \"\"\"Unwrap a Python 2.6 SSL socket, so we can call shutdown()\"\"\" if self.sock is not None: try: unwrap = self.sock.unwrap except AttributeError: return try: self.sock = unwrap() except ValueError: # Failure within SSL might mean unwrap exists but socket is not # deemed wrapped pass"], "rank": 1}
{"nl": "python var and distribution of probability", "code": ["def pdf(x, mu, std): \"\"\"Probability density function (normal distribution)\"\"\" return (1.0 / (std * sqrt(2 * pi))) * np.exp(-(x - mu) ** 2 / (2 * std ** 2))", "def beta_pdf(x, a, b): \"\"\"Beta distirbution probability density function.\"\"\" bc = 1 / beta(a, b) fc = x ** (a - 1) sc = (1 - x) ** (b - 1) return bc * fc * sc", "def EvalPoissonPmf(k, lam): \"\"\"Computes the Poisson PMF. k: number of events lam: parameter lambda in events per unit time returns: float probability \"\"\" # don't use the scipy function (yet). for lam=0 it returns NaN; # should be 0.0 # return scipy.stats.poisson.pmf(k, lam) return lam ** k * math.exp(-lam) / math.factorial(k)", "def EvalGaussianPdf(x, mu, sigma): \"\"\"Computes the unnormalized PDF of the normal distribution. x: value mu: mean sigma: standard deviation returns: float probability density \"\"\" return scipy.stats.norm.pdf(x, mu, sigma)", "def Gaussian(x, mu, sig): \"\"\" Gaussian pdf. :param x: free variable. :param mu: mean of the distribution. :param sig: standard deviation of the distribution. :return: sympy.Expr for a Gaussian pdf. \"\"\" return sympy.exp(-(x - mu)**2/(2*sig**2))/sympy.sqrt(2*sympy.pi*sig**2)"], "rank": 1}
{"nl": "how to determine it's a orthogonal matrix using python", "code": ["def is_orthogonal( matrix: np.ndarray, *, rtol: float = 1e-5, atol: float = 1e-8) -> bool: \"\"\"Determines if a matrix is approximately orthogonal. A matrix is orthogonal if it's square and real and its transpose is its inverse. Args: matrix: The matrix to check. rtol: The per-matrix-entry relative tolerance on equality. atol: The per-matrix-entry absolute tolerance on equality. Returns: Whether the matrix is orthogonal within the given tolerance. \"\"\" return (matrix.shape[0] == matrix.shape[1] and np.all(np.imag(matrix) == 0) and np.allclose(matrix.dot(matrix.T), np.eye(matrix.shape[0]), rtol=rtol, atol=atol))", "def is_unitary(matrix: np.ndarray) -> bool: \"\"\" A helper function that checks if a matrix is unitary. :param matrix: a matrix to test unitarity of :return: true if and only if matrix is unitary \"\"\" rows, cols = matrix.shape if rows != cols: return False return np.allclose(np.eye(rows), matrix.dot(matrix.T.conj()))", "def is_square_matrix(mat): \"\"\"Test if an array is a square matrix.\"\"\" mat = np.array(mat) if mat.ndim != 2: return False shape = mat.shape return shape[0] == shape[1]", "def isSquare(matrix): \"\"\"Check that ``matrix`` is square. Returns ======= is_square : bool ``True`` if ``matrix`` is square, ``False`` otherwise. \"\"\" try: try: dim1, dim2 = matrix.shape except AttributeError: dim1, dim2 = _np.array(matrix).shape except ValueError: return False if dim1 == dim2: return True return False", "def MatrixInverse(a, adj): \"\"\" Matrix inversion op. \"\"\" return np.linalg.inv(a if not adj else _adjoint(a)),"], "rank": 1}
{"nl": "strip html tags in python", "code": ["def strip_html(string, keep_tag_content=False): \"\"\" Remove html code contained into the given string. :param string: String to manipulate. :type string: str :param keep_tag_content: True to preserve tag content, False to remove tag and its content too (default). :type keep_tag_content: bool :return: String with html removed. :rtype: str \"\"\" r = HTML_TAG_ONLY_RE if keep_tag_content else HTML_RE return r.sub('', string)", "def do_striptags(value): \"\"\"Strip SGML/XML tags and replace adjacent whitespace by one space. \"\"\" if hasattr(value, '__html__'): value = value.__html__() return Markup(unicode(value)).striptags()", "def _breakRemNewlines(tag): \"\"\"non-recursively break spaces and remove newlines in the tag\"\"\" for i,c in enumerate(tag.contents): if type(c) != bs4.element.NavigableString: continue c.replace_with(re.sub(r' {2,}', ' ', c).replace('\\n',''))", "def cleanup_nodes(doc): \"\"\" Remove text nodes containing only whitespace \"\"\" for node in doc.documentElement.childNodes: if node.nodeType == Node.TEXT_NODE and node.nodeValue.isspace(): doc.documentElement.removeChild(node) return doc", "def remove_namespaces(root): \"\"\"Call this on an lxml.etree document to remove all namespaces\"\"\" for elem in root.getiterator(): if not hasattr(elem.tag, 'find'): continue i = elem.tag.find('}') if i >= 0: elem.tag = elem.tag[i + 1:] objectify.deannotate(root, cleanup_namespaces=True)"], "rank": 2}
{"nl": "center align python text", "code": ["def center_text(text, width=80): \"\"\"Center all lines of the text. It is assumed that all lines width is smaller then B{width}, because the line width will not be checked. Args: text (str): Text to wrap. width (int): Maximum number of characters per line. Returns: str: Centered text. \"\"\" centered = [] for line in text.splitlines(): centered.append(line.center(width)) return \"\\n\".join(centered)", "def text_alignment(x, y): \"\"\" Align text labels based on the x- and y-axis coordinate values. This function is used for computing the appropriate alignment of the text label. For example, if the text is on the \"right\" side of the plot, we want it to be left-aligned. If the text is on the \"top\" side of the plot, we want it to be bottom-aligned. :param x, y: (`int` or `float`) x- and y-axis coordinate respectively. :returns: A 2-tuple of strings, the horizontal and vertical alignments respectively. \"\"\" if x == 0: ha = \"center\" elif x > 0: ha = \"left\" else: ha = \"right\" if y == 0: va = \"center\" elif y > 0: va = \"bottom\" else: va = \"top\" return ha, va", "def _pad(self, text): \"\"\"Pad the text.\"\"\" top_bottom = (\"\\n\" * self._padding) + \" \" right_left = \" \" * self._padding * self.PAD_WIDTH return top_bottom + right_left + text + right_left + top_bottom", "def indent(text, amount, ch=' '): \"\"\"Indents a string by the given amount of characters.\"\"\" padding = amount * ch return ''.join(padding+line for line in text.splitlines(True))", "def wrap(text, indent=' '): \"\"\"Wrap text to terminal width with default indentation\"\"\" wrapper = textwrap.TextWrapper( width=int(os.environ.get('COLUMNS', 80)), initial_indent=indent, subsequent_indent=indent ) return '\\n'.join(wrapper.wrap(text))"], "rank": 1}
{"nl": "python get list of keys on an object", "code": ["def get_keys_from_class(cc): \"\"\"Return list of the key property names for a class \"\"\" return [prop.name for prop in cc.properties.values() \\ if 'key' in prop.qualifiers]", "def fields(self): \"\"\"Returns the list of field names of the model.\"\"\" return (self.attributes.values() + self.lists.values() + self.references.values())", "def keys(self): \"\"\"Return a list of all keys in the dictionary. Returns: list of str: [key1,key2,...,keyN] \"\"\" all_keys = [k.decode('utf-8') for k,v in self.rdb.hgetall(self.session_hash).items()] return all_keys", "def get_object_attrs(obj): \"\"\" Get the attributes of an object using dir. This filters protected attributes \"\"\" attrs = [k for k in dir(obj) if not k.startswith('__')] if not attrs: attrs = dir(obj) return attrs", "def keys(self, index=None): \"\"\"Returns a list of keys in the database \"\"\" with self._lmdb.begin() as txn: return [key.decode() for key, _ in txn.cursor()]"], "rank": 1}
{"nl": "is there any python function to check for nan valu", "code": ["def warn_if_nans_exist(X): \"\"\"Warn if nans exist in a numpy array.\"\"\" null_count = count_rows_with_nans(X) total = len(X) percent = 100 * null_count / total if null_count > 0: warning_message = \\ 'Warning! Found {} rows of {} ({:0.2f}%) with nan values. Only ' \\ 'complete rows will be plotted.'.format(null_count, total, percent) warnings.warn(warning_message, DataWarning)", "def _isnan(self): \"\"\" Return if each value is NaN. \"\"\" if self._can_hold_na: return isna(self) else: # shouldn't reach to this condition by checking hasnans beforehand values = np.empty(len(self), dtype=np.bool_) values.fill(False) return values", "def reduce_fn(x): \"\"\" Aggregation function to get the first non-zero value. \"\"\" values = x.values if pd and isinstance(x, pd.Series) else x for v in values: if not is_nan(v): return v return np.NaN", "def is_finite(value: Any) -> bool: \"\"\"Return true if a value is a finite number.\"\"\" return isinstance(value, int) or (isinstance(value, float) and isfinite(value))", "def nan_pixels(self): \"\"\" Return an array of the NaN pixels. Returns ------- :obj:`numpy.ndarray` Nx2 array of the NaN pixels \"\"\" nan_px = np.where(np.isnan(np.sum(self.raw_data, axis=2))) nan_px = np.c_[nan_px[0], nan_px[1]] return nan_px"], "rank": 3}
{"nl": "python get cookie for request", "code": ["def _get_data(self): \"\"\" Extracts the session data from cookie. \"\"\" cookie = self.adapter.cookies.get(self.name) return self._deserialize(cookie) if cookie else {}", "def parse_cookies(self, req, name, field): \"\"\"Pull the value from the cookiejar.\"\"\" return core.get_value(req.COOKIES, name, field)", "def cookies(self) -> Dict[str, str]: \"\"\"The parsed cookies attached to this request.\"\"\" cookies = SimpleCookie() cookies.load(self.headers.get('Cookie', '')) return {key: cookie.value for key, cookie in cookies.items()}", "def get_csrf_token(response): \"\"\" Extract the CSRF token out of the \"Set-Cookie\" header of a response. \"\"\" cookie_headers = [ h.decode('ascii') for h in response.headers.getlist(\"Set-Cookie\") ] if not cookie_headers: return None csrf_headers = [ h for h in cookie_headers if h.startswith(\"csrftoken=\") ] if not csrf_headers: return None match = re.match(\"csrftoken=([^ ;]+);\", csrf_headers[-1]) return match.group(1)", "def parse_cookies_str(cookies): \"\"\" parse cookies str to dict :param cookies: cookies str :type cookies: str :return: cookie dict :rtype: dict \"\"\" cookie_dict = {} for record in cookies.split(\";\"): key, value = record.strip().split(\"=\", 1) cookie_dict[key] = value return cookie_dict"], "rank": 2}
{"nl": "python is list no na", "code": ["def _isnan(self): \"\"\" Return if each value is NaN. \"\"\" if self._can_hold_na: return isna(self) else: # shouldn't reach to this condition by checking hasnans beforehand values = np.empty(len(self), dtype=np.bool_) values.fill(False) return values", "def is_nullable_list(val, vtype): \"\"\"Return True if list contains either values of type `vtype` or None.\"\"\" return (isinstance(val, list) and any(isinstance(v, vtype) for v in val) and all((isinstance(v, vtype) or v is None) for v in val))", "def remove_na_arraylike(arr): \"\"\" Return array-like containing only true/non-NaN values, possibly empty. \"\"\" if is_extension_array_dtype(arr): return arr[notna(arr)] else: return arr[notna(lib.values_from_object(arr))]", "def is_a_sequence(var, allow_none=False): \"\"\" Returns True if var is a list or a tuple (but not a string!) \"\"\" return isinstance(var, (list, tuple)) or (var is None and allow_none)", "def _not_none(items): \"\"\"Whether the item is a placeholder or contains a placeholder.\"\"\" if not isinstance(items, (tuple, list)): items = (items,) return all(item is not _none for item in items)"], "rank": 15}
{"nl": "python determine if a file is image", "code": ["def is_image(filename): \"\"\"Determine if given filename is an image.\"\"\" # note: isfile() also accepts symlinks return os.path.isfile(filename) and filename.lower().endswith(ImageExts)", "def is_image_file_valid(file_path_name): \"\"\" Indicate whether the specified image file is valid or not. @param file_path_name: absolute path and file name of an image. @return: ``True`` if the image file is valid, ``False`` if the file is truncated or does not correspond to a supported image. \"\"\" # Image.verify is only implemented for PNG images, and it only verifies # the CRC checksum in the image. The only way to check from within # Pillow is to load the image in a try/except and check the error. If # as much info as possible is from the image is needed, # ``ImageFile.LOAD_TRUNCATED_IMAGES=True`` needs to bet set and it # will attempt to parse as much as possible. try: with Image.open(file_path_name) as image: image.load() except IOError: return False return True", "def is_valid_image_extension(file_path): \"\"\"is_valid_image_extension.\"\"\" valid_extensions = ['.jpeg', '.jpg', '.gif', '.png'] _, extension = os.path.splitext(file_path) return extension.lower() in valid_extensions", "def _is_image_sequenced(image): \"\"\"Determine if the image is a sequenced image.\"\"\" try: image.seek(1) image.seek(0) result = True except EOFError: result = False return result", "def _is_video(filepath) -> bool: \"\"\"Check filename extension to see if it's a video file.\"\"\" if os.path.exists(filepath): # Could be broken symlink extension = os.path.splitext(filepath)[1] return extension in ('.mkv', '.mp4', '.avi') else: return False"], "rank": 1}
{"nl": "python reorganise a data frame", "code": ["def _preprocess(df): \"\"\" given a DataFrame where records are stored row-wise, rearrange it such that records are stored column-wise. \"\"\" df = df.stack() df.index.rename([\"id\", \"time\"], inplace=True) # .reset_index() df.name = \"value\" df = df.reset_index() return df", "def reverse_transform(self, col): \"\"\"Converts data back into original format. Args: col(pandas.DataFrame): Data to transform. Returns: pandas.DataFrame \"\"\" output = pd.DataFrame() output[self.col_name] = self.get_category(col[self.col_name]) return output", "def transform(self, df): \"\"\" Transforms a DataFrame in place. Computes all outputs of the DataFrame. Args: df (pandas.DataFrame): DataFrame to transform. \"\"\" for name, function in self.outputs: df[name] = function(df)", "def sort_data(data, cols): \"\"\"Sort `data` rows and order columns\"\"\" return data.sort_values(cols)[cols + ['value']].reset_index(drop=True)", "def to_dataframe(products): \"\"\"Return the products from a query response as a Pandas DataFrame with the values in their appropriate Python types. \"\"\" try: import pandas as pd except ImportError: raise ImportError(\"to_dataframe requires the optional dependency Pandas.\") return pd.DataFrame.from_dict(products, orient='index')"], "rank": 1}
{"nl": "read first line in txt file in python", "code": ["def getfirstline(file, default): \"\"\" Returns the first line of a file. \"\"\" with open(file, 'rb') as fh: content = fh.readlines() if len(content) == 1: return content[0].decode('utf-8').strip('\\n') return default", "def rAsciiLine(ifile): \"\"\"Returns the next non-blank line in an ASCII file.\"\"\" _line = ifile.readline().strip() while len(_line) == 0: _line = ifile.readline().strip() return _line", "def readline( file, skip_blank=False ): \"\"\"Read a line from provided file, skipping any blank or comment lines\"\"\" while 1: line = file.readline() #print \"every line: %r\" % line if not line: return None if line[0] != '#' and not ( skip_blank and line.isspace() ): return line", "def read(self, count=0): \"\"\" Read \"\"\" return self.f.read(count) if count > 0 else self.f.read()", "def get_lines(handle, line): \"\"\" Get zero-indexed line from an open file-like. \"\"\" for i, l in enumerate(handle): if i == line: return l"], "rank": 1}
{"nl": "pass defined parser object to subparser python", "code": ["def set_subparsers_args(self, *args, **kwargs): \"\"\" Sets args and kwargs that are passed when creating a subparsers group in an argparse.ArgumentParser i.e. when calling argparser.ArgumentParser.add_subparsers \"\"\" self.subparsers_args = args self.subparsers_kwargs = kwargs", "def sub(name, func,**kwarg): \"\"\" Add subparser \"\"\" sp = subparsers.add_parser(name, **kwarg) sp.set_defaults(func=func) sp.arg = sp.add_argument return sp", "def __init__(self): \"\"\"__init__: Performs basic initialisations\"\"\" # Root parser self.parser = argparse.ArgumentParser() # Subparsers self.subparsers = self.parser.add_subparsers() # Parser dictionary, to avoir overwriting existing parsers self.parsers = {}", "def add_to_parser(self, parser): \"\"\" Adds the argument to an argparse.ArgumentParser instance @param parser An argparse.ArgumentParser instance \"\"\" kwargs = self._get_kwargs() args = self._get_args() parser.add_argument(*args, **kwargs)", "def cli_parse(parser): \"\"\"Add method specific options to CLI parser. Parameters ---------- parser : argparse object Returns ---------- Updated argparse object \"\"\" parser.add_argument('-n', '--samples', type=int, required=True, help='Number of Samples') return parser"], "rank": 2}
{"nl": "how to print generic error in python", "code": ["def error(*args): \"\"\"Display error message via stderr or GUI.\"\"\" if sys.stdin.isatty(): print('ERROR:', *args, file=sys.stderr) else: notify_error(*args)", "def print_error(msg): \"\"\" Print an error message \"\"\" if IS_POSIX: print(u\"%s[ERRO] %s%s\" % (ANSI_ERROR, msg, ANSI_END)) else: print(u\"[ERRO] %s\" % (msg))", "def on_IOError(self, e): \"\"\" Handle an IOError exception. \"\"\" sys.stderr.write(\"Error: %s: \\\"%s\\\"\\n\" % (e.strerror, e.filename))", "def fail_print(error): \"\"\"Print an error in red text. Parameters error (HTTPError) Error object to print. \"\"\" print(COLORS.fail, error.message, COLORS.end) print(COLORS.fail, error.errors, COLORS.end)", "def err(msg): \"\"\"Pretty-print an error.\"\"\" click.echo(click.style(msg, fg=\"red\", bold=True))"], "rank": 10}
{"nl": "python change the name of a key", "code": ["def unit_key_from_name(name): \"\"\"Return a legal python name for the given name for use as a unit key.\"\"\" result = name for old, new in six.iteritems(UNIT_KEY_REPLACEMENTS): result = result.replace(old, new) # Collapse redundant underscores and convert to uppercase. result = re.sub(r'_+', '_', result.upper()) return result", "def _get_name(self, key): \"\"\" get display name for a key, or mangle for display \"\"\" if key in self.display_names: return self.display_names[key] return key.capitalize()", "def normalise_key(self, key): \"\"\"Make sure key is a valid python attribute\"\"\" key = key.replace('-', '_') if key.startswith(\"noy_\"): key = key[4:] return key", "def make_env_key(app_name, key): \"\"\"Creates an environment key-equivalent for the given key\"\"\" key = key.replace('-', '_').replace(' ', '_') return str(\"_\".join((x.upper() for x in (app_name, key))))", "def make_key(self, key, version=None): \"\"\"RedisCache will set prefix+version as prefix for each key.\"\"\" return '{}:{}:{}'.format( self.prefix, version or self.version, key, )"], "rank": 1}
{"nl": "moving mouse python click", "code": ["def mouse_move_event(self, event): \"\"\" Forward mouse cursor position events to the example \"\"\" self.example.mouse_position_event(event.x(), event.y())", "def mouseMoveEvent(self, event): \"\"\" Handle the mouse move event for a drag operation. \"\"\" self.declaration.mouse_move_event(event) super(QtGraphicsView, self).mouseMoveEvent(event)", "def buttonUp(self, button=mouse.LEFT): \"\"\" Releases the specified mouse button. Use Mouse.LEFT, Mouse.MIDDLE, Mouse.RIGHT \"\"\" self._lock.acquire() mouse.release(button) self._lock.release()", "def mouse_out(self): \"\"\" Performs a mouse out the element. Currently works only on Chrome driver. \"\"\" self.scroll_to() ActionChains(self.parent.driver).move_by_offset(0, 0).click().perform()", "def __init__(self, pos, cell, motion, cellmotion): self.pos = pos \"\"\"(x, y) position of the mouse on the screen. type: (int, int)\"\"\" self.cell = cell \"\"\"(x, y) position of the mouse snapped to a cell on the root console. type: (int, int)\"\"\" self.motion = motion \"\"\"(x, y) motion of the mouse on the screen. type: (int, int)\"\"\" self.cellmotion = cellmotion \"\"\"(x, y) mostion of the mouse moving over cells on the root console. type: (int, int)\"\"\""], "rank": 1}
{"nl": "python to determine if services are running", "code": ["def service_available(service_name): \"\"\"Determine whether a system service is available\"\"\" try: subprocess.check_output( ['service', service_name, 'status'], stderr=subprocess.STDOUT).decode('UTF-8') except subprocess.CalledProcessError as e: return b'unrecognized service' not in e.output else: return True", "def is_running(self): \"\"\"Returns a bool determining if the process is in a running state or not :rtype: bool \"\"\" return self.state in [self.STATE_IDLE, self.STATE_ACTIVE, self.STATE_SLEEPING]", "def is_alive(self): \"\"\" Will test whether the ACS service is up and alive. \"\"\" response = self.get_monitoring_heartbeat() if response.status_code == 200 and response.content == 'alive': return True return False", "def is_alive(self): \"\"\" @rtype: bool @return: C{True} if the process is currently running. \"\"\" try: self.wait(0) except WindowsError: e = sys.exc_info()[1] return e.winerror == win32.WAIT_TIMEOUT return False", "def get_services(): \"\"\" Retrieve a list of all system services. @see: L{get_active_services}, L{start_service}, L{stop_service}, L{pause_service}, L{resume_service} @rtype: list( L{win32.ServiceStatusProcessEntry} ) @return: List of service status descriptors. \"\"\" with win32.OpenSCManager( dwDesiredAccess = win32.SC_MANAGER_ENUMERATE_SERVICE ) as hSCManager: try: return win32.EnumServicesStatusEx(hSCManager) except AttributeError: return win32.EnumServicesStatus(hSCManager)"], "rank": 1}
{"nl": "python set contains multiple items", "code": ["def issuperset(self, items): \"\"\"Return whether this collection contains all items. >>> Unique(['spam', 'eggs']).issuperset(['spam', 'spam', 'spam']) True \"\"\" return all(_compat.map(self._seen.__contains__, items))", "def update(self, iterable): \"\"\" Return a new PSet with elements in iterable added >>> s1 = s(1, 2) >>> s1.update([3, 4, 4]) pset([1, 2, 3, 4]) \"\"\" e = self.evolver() for element in iterable: e.add(element) return e.persistent()", "def add(self, value): \"\"\"Add the element *value* to the set.\"\"\" if value not in self._set: self._set.add(value) self._list.add(value)", "def convert_args_to_sets(f): \"\"\" Converts all args to 'set' type via self.setify function. \"\"\" @wraps(f) def wrapper(*args, **kwargs): args = (setify(x) for x in args) return f(*args, **kwargs) return wrapper", "def pset(iterable=(), pre_size=8): \"\"\" Creates a persistent set from iterable. Optionally takes a sizing parameter equivalent to that used for :py:func:`pmap`. >>> s1 = pset([1, 2, 3, 2]) >>> s1 pset([1, 2, 3]) \"\"\" if not iterable: return _EMPTY_PSET return PSet._from_iterable(iterable, pre_size=pre_size)"], "rank": 1}
{"nl": "python protobyf parse from byte", "code": ["def read_proto_object(fobj, klass): \"\"\"Read a block of data and parse using the given protobuf object.\"\"\" log.debug('%s chunk', klass.__name__) obj = klass() obj.ParseFromString(read_block(fobj)) log.debug('Header: %s', str(obj)) return obj", "def decode(self, bytes, raw=False): \"\"\"decode(bytearray, raw=False) -> value Decodes the given bytearray according to this PrimitiveType definition. NOTE: The parameter ``raw`` is present to adhere to the ``decode()`` inteface, but has no effect for PrimitiveType definitions. \"\"\" return struct.unpack(self.format, buffer(bytes))[0]", "def from_pb(cls, pb): \"\"\"Instantiate the object from a protocol buffer. Args: pb (protobuf) Save a reference to the protocol buffer on the object. \"\"\" obj = cls._from_pb(pb) obj._pb = pb return obj", "def unpack_from(self, data, offset=0): \"\"\"See :func:`~bitstruct.unpack_from()`. \"\"\" return tuple([v[1] for v in self.unpack_from_any(data, offset)])", "def AsPrimitiveProto(self): \"\"\"Return an old style protocol buffer object.\"\"\" if self.protobuf: result = self.protobuf() result.ParseFromString(self.SerializeToString()) return result"], "rank": 2}
{"nl": "python check if a directory is writable", "code": ["def _writable_dir(path): \"\"\"Whether `path` is a directory, to which the user has write access.\"\"\" return os.path.isdir(path) and os.access(path, os.W_OK)", "def is_writable_by_others(filename): \"\"\"Check if file or directory is world writable.\"\"\" mode = os.stat(filename)[stat.ST_MODE] return mode & stat.S_IWOTH", "def is_readable_dir(path): \"\"\"Returns whether a path names an existing directory we can list and read files from.\"\"\" return os.path.isdir(path) and os.access(path, os.R_OK) and os.access(path, os.X_OK)", "def readable(path): \"\"\"Test whether a path exists and is readable. Returns None for broken symbolic links or a failing stat() and False if the file exists but does not have read permission. True is returned if the file is readable.\"\"\" try: st = os.stat(path) return 0 != st.st_mode & READABLE_MASK except os.error: return None return True", "def isdir(s): \"\"\"Return true if the pathname refers to an existing directory.\"\"\" try: st = os.stat(s) except os.error: return False return stat.S_ISDIR(st.st_mode)"], "rank": 1}
{"nl": "object as list python", "code": ["def as_list(self): \"\"\"Return all child objects in nested lists of strings.\"\"\" return [self.name, self.value, [x.as_list for x in self.children]]", "def property_as_list(self, property_name): \"\"\" property() but encapsulates it in a list, if it's a single-element property. \"\"\" try: res = self._a_tags[property_name] except KeyError: return [] if type(res) == list: return res else: return [res]", "def __as_list(value: List[JsonObjTypes]) -> List[JsonTypes]: \"\"\" Return a json array as a list :param value: array :return: array with JsonObj instances removed \"\"\" return [e._as_dict if isinstance(e, JsonObj) else e for e in value]", "def _listify(collection): \"\"\"This is a workaround where Collections are no longer iterable when using JPype.\"\"\" new_list = [] for index in range(len(collection)): new_list.append(collection[index]) return new_list", "def serialize(self, value, **kwargs): \"\"\"Serialize every item of the list.\"\"\" return [self.item_type.serialize(val, **kwargs) for val in value]"], "rank": 1}
{"nl": "replace function nan python", "code": ["def _replace_nan(a, val): \"\"\" replace nan in a by val, and returns the replaced array and the nan position \"\"\" mask = isnull(a) return where_method(val, mask, a), mask", "def impute_data(self,x): \"\"\"Imputes data set containing Nan values\"\"\" imp = Imputer(missing_values='NaN', strategy='mean', axis=0) return imp.fit_transform(x)", "def reduce_fn(x): \"\"\" Aggregation function to get the first non-zero value. \"\"\" values = x.values if pd and isinstance(x, pd.Series) else x for v in values: if not is_nan(v): return v return np.NaN", "def na_if(series, *values): \"\"\" If values in a series match a specified value, change them to `np.nan`. Args: series: Series or vector, often symbolic. *values: Value(s) to convert to `np.nan` in the series. \"\"\" series = pd.Series(series) series[series.isin(values)] = np.nan return series", "def fillna(series_or_arr, missing_value=0.0): \"\"\"Fill missing values in pandas objects and numpy arrays. Arguments --------- series_or_arr : pandas.Series, numpy.ndarray The numpy array or pandas series for which the missing values need to be replaced. missing_value : float, int, str The value to replace the missing value with. Default 0.0. Returns ------- pandas.Series, numpy.ndarray The numpy array or pandas series with the missing values filled. \"\"\" if pandas.notnull(missing_value): if isinstance(series_or_arr, (numpy.ndarray)): series_or_arr[numpy.isnan(series_or_arr)] = missing_value else: series_or_arr.fillna(missing_value, inplace=True) return series_or_arr"], "rank": 1}
{"nl": "python hash table check if key exist", "code": ["def check_key(self, key: str) -> bool: \"\"\" Checks if key exists in datastore. True if yes, False if no. :param: SHA512 hash key :return: whether or key not exists in datastore \"\"\" keys = self.get_keys() return key in keys", "def has_key(cls, *args): \"\"\" Check whether flyweight object with specified key has already been created. Returns: bool: True if already created, False if not \"\"\" key = args if len(args) > 1 else args[0] return key in cls._instances", "def check_hash_key(query_on, key): \"\"\"Only allows == against query_on.hash_key\"\"\" return ( isinstance(key, BaseCondition) and (key.operation == \"==\") and (key.column is query_on.hash_key) )", "def __contains__(self, key): \"\"\"Return ``True`` if *key* is present, else ``False``.\"\"\" pickled_key = self._pickle_key(key) return bool(self.redis.hexists(self.key, pickled_key))", "def check_if_branch_exist(db, root_hash, key_prefix): \"\"\" Given a key prefix, return whether this prefix is the prefix of an existing key in the trie. \"\"\" validate_is_bytes(key_prefix) return _check_if_branch_exist(db, root_hash, encode_to_bin(key_prefix))"], "rank": 1}
{"nl": "take all points in box python", "code": ["def point8_to_box(points): \"\"\" Args: points: (nx4)x2 Returns: nx4 boxes (x1y1x2y2) \"\"\" p = points.reshape((-1, 4, 2)) minxy = p.min(axis=1) # nx2 maxxy = p.max(axis=1) # nx2 return np.concatenate((minxy, maxxy), axis=1)", "def bounding_box(img): r\"\"\" Return the bounding box incorporating all non-zero values in the image. Parameters ---------- img : array_like An array containing non-zero objects. Returns ------- bbox : a list of slicer objects defining the bounding box \"\"\" locations = numpy.argwhere(img) mins = locations.min(0) maxs = locations.max(0) + 1 return [slice(x, y) for x, y in zip(mins, maxs)]", "def bounding_box_from(points, i, i1, thr): \"\"\"Creates bounding box for a line segment Args: points (:obj:`list` of :obj:`Point`) i (int): Line segment start, index in points array i1 (int): Line segment end, index in points array Returns: (float, float, float, float): with bounding box min x, min y, max x and max y \"\"\" pi = points[i] pi1 = points[i1] min_lat = min(pi.lat, pi1.lat) min_lon = min(pi.lon, pi1.lon) max_lat = max(pi.lat, pi1.lat) max_lon = max(pi.lon, pi1.lon) return min_lat-thr, min_lon-thr, max_lat+thr, max_lon+thr", "def get_bound(pts): \"\"\"Compute a minimal rectangle that covers all the points.\"\"\" (x0, y0, x1, y1) = (INF, INF, -INF, -INF) for (x, y) in pts: x0 = min(x0, x) y0 = min(y0, y) x1 = max(x1, x) y1 = max(y1, y) return (x0, y0, x1, y1)", "def bbox(self): \"\"\" The bounding box ``(ymin, xmin, ymax, xmax)`` of the minimal rectangular region containing the source segment. \"\"\" # (stop - 1) to return the max pixel location, not the slice index return (self._slice[0].start, self._slice[1].start, self._slice[0].stop - 1, self._slice[1].stop - 1) * u.pix"], "rank": 1}
{"nl": "python create enum by name", "code": ["def get_enum_from_name(self, enum_name): \"\"\" Return an enum from a name Args: enum_name (str): name of the enum Returns: Enum \"\"\" return next((e for e in self.enums if e.name == enum_name), None)", "def Value(self, name): \"\"\"Returns the value coresponding to the given enum name.\"\"\" if name in self._enum_type.values_by_name: return self._enum_type.values_by_name[name].number raise ValueError('Enum %s has no value defined for name %s' % ( self._enum_type.name, name))", "def dict_to_enum_fn(d: Dict[str, Any], enum_class: Type[Enum]) -> Enum: \"\"\" Converts an ``dict`` to a ``Enum``. \"\"\" return enum_class[d['name']]", "def EnumValueName(self, enum, value): \"\"\"Returns the string name of an enum value. This is just a small helper method to simplify a common operation. Args: enum: string name of the Enum. value: int, value of the enum. Returns: string name of the enum value. Raises: KeyError if either the Enum doesn't exist or the value is not a valid value for the enum. \"\"\" return self.enum_types_by_name[enum].values_by_number[value].name", "def _Enum(docstring, *names): \"\"\"Utility to generate enum classes used by annotations. Args: docstring: Docstring for the generated enum class. *names: Enum names. Returns: A class that contains enum names as attributes. \"\"\" enums = dict(zip(names, range(len(names)))) reverse = dict((value, key) for key, value in enums.iteritems()) enums['reverse_mapping'] = reverse enums['__doc__'] = docstring return type('Enum', (object,), enums)"], "rank": 1}
{"nl": "python matplotlib use arrow markers", "code": ["def add_arrow(self, x1, y1, x2, y2, **kws): \"\"\"add arrow to plot\"\"\" self.panel.add_arrow(x1, y1, x2, y2, **kws)", "def set_axis_options(self, row, column, text): \"\"\"Set additionnal options as plain text.\"\"\" subplot = self.get_subplot_at(row, column) subplot.set_axis_options(text)", "def plot_target(target, ax): \"\"\"Ajoute la target au plot\"\"\" ax.scatter(target[0], target[1], target[2], c=\"red\", s=80)", "def scatter(self, *args, **kwargs): \"\"\"Add a scatter plot.\"\"\" cls = _make_class(ScatterVisual, _default_marker=kwargs.pop('marker', None), ) return self._add_item(cls, *args, **kwargs)", "def finish_plot(): \"\"\"Helper for plotting.\"\"\" plt.legend() plt.grid(color='0.7') plt.xlabel('x') plt.ylabel('y') plt.show()"], "rank": 1}
{"nl": "python function default args", "code": ["def get_default_args(func): \"\"\" returns a dictionary of arg_name:default_values for the input function \"\"\" args, varargs, keywords, defaults = getargspec_no_self(func) return dict(zip(args[-len(defaults):], defaults))", "def with_defaults(method, nparams, defaults=None): \"\"\"Call method with nparams positional parameters, all non-specified defaults are passed None. :method: the method to call :nparams: the number of parameters the function expects :defaults: the default values to pass in for the last len(defaults) params \"\"\" args = [None] * nparams if not defaults else defaults + max(nparams - len(defaults), 0) * [None] return method(*args)", "def arg_default(*args, **kwargs): \"\"\"Return default argument value as given by argparse's add_argument(). The argument is passed through a mocked-up argument parser. This way, we get default parameters even if the feature is called directly and not through the CLI. \"\"\" parser = argparse.ArgumentParser() parser.add_argument(*args, **kwargs) args = vars(parser.parse_args([])) _, default = args.popitem() return default", "def validate_args(**args): \"\"\" function to check if input query is not None and set missing arguments to default value \"\"\" if not args['query']: print(\"\\nMissing required query argument.\") sys.exit() for key in DEFAULTS: if key not in args: args[key] = DEFAULTS[key] return args", "def _correct_args(func, kwargs): \"\"\" Convert a dictionary of arguments including __argv into a list for passing to the function. \"\"\" args = inspect.getargspec(func)[0] return [kwargs[arg] for arg in args] + kwargs['__args']"], "rank": 1}
{"nl": "python how to select first 100 rows", "code": ["def genfirstvalues(cursor: Cursor, arraysize: int = 1000) \\ -> Generator[Any, None, None]: \"\"\" Generate the first value in each row. Args: cursor: the cursor arraysize: split fetches into chunks of this many records Yields: the first value of each row \"\"\" return (row[0] for row in genrows(cursor, arraysize))", "def fetchallfirstvalues(self, sql: str, *args) -> List[Any]: \"\"\"Executes SQL; returns list of first values of each row.\"\"\" rows = self.fetchall(sql, *args) return [row[0] for row in rows]", "def get_last_row(dbconn, tablename, n=1, uuid=None): \"\"\" Returns the last `n` rows in the table \"\"\" return fetch(dbconn, tablename, n, uuid, end=True)", "def fetch(table, cols=\"*\", where=(), group=\"\", order=(), limit=(), **kwargs): \"\"\"Convenience wrapper for database SELECT and fetch all.\"\"\" return select(table, cols, where, group, order, limit, **kwargs).fetchall()", "def query_proc_row(procname, args=(), factory=None): \"\"\" Execute a stored procedure. Returns the first row of the result set, or `None`. \"\"\" for row in query_proc(procname, args, factory): return row return None"], "rank": 1}
{"nl": "removing columnsns in data frame python", "code": ["def remove_columns(self, data, columns): \"\"\" This method removes columns in data :param data: original Pandas dataframe :param columns: list of columns to remove :type data: pandas.DataFrame :type columns: list of strings :returns: Pandas dataframe with removed columns :rtype: pandas.DataFrame \"\"\" for column in columns: if column in data.columns: data = data.drop(column, axis=1) return data", "def clean_column_names(df: DataFrame) -> DataFrame: \"\"\" Strip the whitespace from all column names in the given DataFrame and return the result. \"\"\" f = df.copy() f.columns = [col.strip() for col in f.columns] return f", "def del_Unnamed(df): \"\"\" Deletes all the unnamed columns :param df: pandas dataframe \"\"\" cols_del=[c for c in df.columns if 'Unnamed' in c] return df.drop(cols_del,axis=1)", "def clean_colnames(df): \"\"\" Cleans the column names on a DataFrame Parameters: df - DataFrame The DataFrame to clean \"\"\" col_list = [] for index in range(_dutils.cols(df)): col_list.append(df.columns[index].strip().lower().replace(' ','_')) df.columns = col_list", "def _drop_str_columns(df): \"\"\" Parameters ---------- df : DataFrame Returns ------- \"\"\" str_columns = filter(lambda pair: pair[1].char == 'S', df._gather_dtypes().items()) str_column_names = list(map(lambda pair: pair[0], str_columns)) return df.drop(str_column_names)"], "rank": 2}
{"nl": "python array to torch tensor", "code": ["def astensor(array: TensorLike) -> BKTensor: \"\"\"Covert numpy array to tensorflow tensor\"\"\" tensor = tf.convert_to_tensor(value=array, dtype=CTYPE) return tensor", "def _parse_array(self, tensor_proto): \"\"\"Grab data in TensorProto and convert to numpy array.\"\"\" try: from onnx.numpy_helper import to_array except ImportError as e: raise ImportError(\"Unable to import onnx which is required {}\".format(e)) np_array = to_array(tensor_proto).reshape(tuple(tensor_proto.dims)) return mx.nd.array(np_array)", "def _convert_to_array(array_like, dtype): \"\"\" Convert Matrix attributes which are array-like or buffer to array. \"\"\" if isinstance(array_like, bytes): return np.frombuffer(array_like, dtype=dtype) return np.asarray(array_like, dtype=dtype)", "def _to_array(value): \"\"\"As a convenience, turn Python lists and tuples into NumPy arrays.\"\"\" if isinstance(value, (tuple, list)): return array(value) elif isinstance(value, (float, int)): return np.float64(value) else: return value", "def encode_dataset(dataset, vocabulary): \"\"\"Encode from strings to token ids. Args: dataset: a tf.data.Dataset with string values. vocabulary: a mesh_tensorflow.transformer.Vocabulary Returns: a tf.data.Dataset with integer-vector values ending in EOS=1 \"\"\" def encode(features): return {k: vocabulary.encode_tf(v) for k, v in features.items()} return dataset.map(encode, num_parallel_calls=tf.data.experimental.AUTOTUNE)"], "rank": 1}
{"nl": "how to turn a list into a csv python", "code": ["def list_to_csv(my_list, csv_file): \"\"\" Save a matrix (list of lists) to a file as a CSV .. code:: python my_list = [[\"Name\", \"Location\"], [\"Chris\", \"South Pole\"], [\"Harry\", \"Depth of Winter\"], [\"Bob\", \"Skull\"]] reusables.list_to_csv(my_list, \"example.csv\") example.csv .. code:: csv \"Name\",\"Location\" \"Chris\",\"South Pole\" \"Harry\",\"Depth of Winter\" \"Bob\",\"Skull\" :param my_list: list of lists to save to CSV :param csv_file: File to save data to \"\"\" if PY3: csv_handler = open(csv_file, 'w', newline='') else: csv_handler = open(csv_file, 'wb') try: writer = csv.writer(csv_handler, delimiter=',', quoting=csv.QUOTE_ALL) writer.writerows(my_list) finally: csv_handler.close()", "def list_to_csv(value): \"\"\" Converts list to string with comma separated values. For string is no-op. \"\"\" if isinstance(value, (list, tuple, set)): value = \",\".join(value) return value", "def write_tsv_line_from_list(linelist, outfp): \"\"\"Utility method to convert list to tsv line with carriage return\"\"\" line = '\\t'.join(linelist) outfp.write(line) outfp.write('\\n')", "def csvtolist(inputstr): \"\"\" converts a csv string into a list \"\"\" reader = csv.reader([inputstr], skipinitialspace=True) output = [] for r in reader: output += r return output", "def csv_matrix_print(classes, table): \"\"\" Return matrix as csv data. :param classes: classes list :type classes:list :param table: table :type table:dict :return: \"\"\" result = \"\" classes.sort() for i in classes: for j in classes: result += str(table[i][j]) + \",\" result = result[:-1] + \"\\n\" return result[:-1]"], "rank": 2}
{"nl": "how do i unzip file in python", "code": ["def _unzip_handle(handle): \"\"\"Transparently unzip the file handle\"\"\" if isinstance(handle, basestring): handle = _gzip_open_filename(handle) else: handle = _gzip_open_handle(handle) return handle", "def install_from_zip(url): \"\"\"Download and unzip from url.\"\"\" fname = 'tmp.zip' downlad_file(url, fname) unzip_file(fname) print(\"Removing {}\".format(fname)) os.unlink(fname)", "def extract_alzip (archive, compression, cmd, verbosity, interactive, outdir): \"\"\"Extract a ALZIP archive.\"\"\" return [cmd, '-d', outdir, archive]", "def extract_all(zipfile, dest_folder): \"\"\" reads the zip file, determines compression and unzips recursively until source files are extracted \"\"\" z = ZipFile(zipfile) print(z) z.extract(dest_folder)", "def extract(self, destination): \"\"\"Extract the archive.\"\"\" with zipfile.ZipFile(self.archive, 'r') as zip_ref: zip_ref.extractall(destination)"], "rank": 1}
